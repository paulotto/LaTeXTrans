\section{Tasks in AI Software Engineering} \label{sec:tasks-milestones}
We first provide a taxonomy of tasks in AI software engineering. To provide a structured way to consider concrete realizations of each task, we define three measures that apply across them: scope, logical complexity, and level of human intervention. To achieve an AI software engineer, we strive for AI to be capable across the board for all three measures.

\textbf{Scope Measure}: We define three levels of scope, the extent of changes that the AI makes to the codebase. \textit{Function-level} scope refers to single, self-contained functions such as in HumanEval \citep{chen2021evaluating} and MBPP \citep{austin2021program}. \textit{Self-contained unit} scope goes beyond singular functions and to larger chunks of code such as entire files and classes, such as FullStackBench \citep{liu2024fullstackbenchevaluatingllms} and BigCodeBench \citep{zhuo2024bigcodebench}. Finally, \textit{project-level} scope refers to larger codebases such as entire repositories, such as in Commit0 \citep{zhao2024commit0} and SWE-Bench \citep{jimenez2024swebench}.

\textbf{Logical Complexity Measure}: Tasks require a wide range of reasoning abilities when it comes to devising algorithms to solve them. \textit{Low logical complexity} tasks require little to no reasoning, such as writing CRUD (create, read, update, delete) applications or using APIs. \textit{Medium logical complexity} tasks include most LeetCode problems, finding inputs to fuzz simple programs, and reasoning about execution behavior of multithreaded programs. \textit{High logical complexity} tasks require meticulous and challenging levels of algorithmic and logical reasoning, either because the algorithm is complex or because the problem requires clever thinking and insights. This includes difficult competition programming problems, writing large thread-safe concurrent programs, cracking cryptographic ciphers, and solving SMT-like problems. Many popular coding benchmarks are function-level, medium-high logical complexity, such as APPS \citep{hendrycksapps2021}, CodeContests \citep{li2022competition}, and LiveCodeBench \citep{jain2024livecodebenchholisticcontaminationfree}. 
% \ks{I am not sure if logical complexity is the right phrase in this context.  Are you talking about O(1) vs. O(n) vs. O($2^n$) algorithms? I would use the phrase hardness or difficulty or logical complexity of the problem.}  

\textbf{Level of Human Intervention Measure}: AI coding is a collaborative task. \citet{treude2025developers} categorize interactions between developers and AI. Each interaction progresses through four phases: the \textit{trigger} for the interaction, the \textit{AI response} describing the system's output, the \textit{developer response} capturing how developers react to the AI response, and the \textit{output} of the interaction, the exact result. They characterize these developer-AI interactions into eleven types, including autocomplete code suggestions, conversational assistance (e.g., asking a question about a codebase), selection-based enhancements (e.g., refactoring a selected chunk of code), comment-guided prompts (e.g., natural language to code), check correctness, and more. 

We map these interactions to the autonomy taxonomy outlined in \citet{morris2023levels}\footnote{We follow page 9, Table 2 from their paper} to define three levels of human intervention. We distill their six levels of autonomy into three levels: low (\textit{No AI} and \textit{AI as a Tool}), medium (\textit{AI as a Consultant} and \textit{AI as a Collaborator}), and high (\textit{AI as an Expert} and \textit{AI as an Agent}). \textit{Low autonomy} is when the human has full control over the task and uses AI to automate simple sub-tasks. This might look like writing a codebase with tests while leaving small function-level snippets for the AI to fill in. \textit{Medium autonomy} is when there is a similar amount of human-AI collaboration, with interactive coordination of goals and tasks. Here, the human and AI might both suggest refactorings, optimizations during the development cycle. \textit{High autonomy} is when AI drives the interaction and tasks, identifying required changes and the changing demands of the user. The AI would defer to the human only when needed or for a final check, write the code and tests autonomously.

% \todo{Yijia: see if this looks reasonable?}

% \yijia{Different levels make a lot of sense to me as I can imagine different SE tasks may inherently lie in different levels. The only thing is that I cannot find ``low/medium/high autonomy'' in \citet{morris2023levels}. I don't know if this would be a concern. If you need other citations, you may consider drawing a connection with different levels of autonomous driving defined by Society of Automotive Engineers (SAE).}
% \todo{follow \url{https://arxiv.org/abs/2501.08774
% } to flesh this out?}

Next, with our taxonomy of measures in place, we turn to the set of tasks that are reflective of the tasks and capabilities of a human software engineer. We give a brief description of each task in this section, deferring a more extensive survey to Appendix \ref{appendix:tasks}.

% \ms{maybe add 1-2 examples for each? code completion is "low" ig}

% \todo{Talk about a few benchmarks or tools, and fit them on these three measures}.


% \todo{logical complexity vs Repository Scope (ex: CRUD app, SMT, Cipher). OOD stuff in high logical complexity.}

% \todo{Call these Measures (scale, complexity, lvl of human interv)}

% \textbf{Scope}: Tasks in the field of AI for code vary by scope. Here, we categorize scope into several groups:
% \begin{itemize}

% The AI for code community generally began with function-level benchmarks, where the goal was to write or complete a single self-contained function. In early function-level benchmarks, the specification generally contained a natural language description of the function and test cases. Examples of this are HumanEval \citep{chen2021evaluating} and MBPP \citep{austin2021program}. Later, to mimic more real-world scenarios, function-level benchmarks began to require usage of more specialized APIs and packages such as \texttt{numpy, pandas}, and \texttt{scipy}. Some of these benchmarks include ODEX \citep{wang2022execution}, Jigsaw \citep{jain2022jigsaw}, DS-1000 \citep{lai2023ds}, and JuICe \citep{agashe2019juice}, and BigCodeBench \citep{zhuo2024bigcodebench}.
% \item \textbf{Self-contained unit}: Beyond singular functions, people have studied coding capabilities in self-contained units like files, classes 
% \item \textbf{Repository-level}: With increasing capabilities of AI systems, repository-level 
% Commit0 \citep{zhao2024commit0}, SWE-Bench \citep{jimenez2024swebench}
% \end{itemize}

%     \item \textbf{Algorithmic and competition programming}: With the increased attention to the reasoning abilities of large language models, competition programming has emerged as an attractive domain to evaluate coding capabilities. Examples of competitive programming benchmarks include APPS \citep{hendrycksapps2021}, CodeContests \citep{li2022competition}, USACOBench \citep{shi2024can}, Code4Bench \citep{majd2019code4bench}, \todo{more}, and LiveCodeBench \citep{jain2024livecodebenchholisticcontaminationfree}. This domain has been attractive because problems are often crisp, concise, and have no ambiguity. Solving these problems requires sharp algorithmic reasoning capabilities, often necessitating insights that may not be obvious upon first reading the problem. Solutions are generally written in a single file comprising of at most a few functions and rarely requiring external packages beyond the standard library. 
%     \item \textbf{Self-contained unit}: Beyond singular functions, people have studied coding capabilities in self-contained units like files, classes 
%     \item \textbf{Repository-level}: With increasing capabilities of AI systems, repository-level 
%     Commit0 \citep{zhao2024commit0}, SWE-Bench \citep{jimenez2024swebench}
%     \item OOD: This could include niche repositories, domains, low-resource languages. CRUXEval, Test Generation, Vulnerability Detection, Code Optimization
% \end{itemize}


% \todo{diff domains have diff requirements for the 3 measures}

% \textbf{Domains}: Web, Games, Security ??
% Web: low reasoning complexity
% SMT: high reasoning complexity

% \todo{Strengths and weaknesses of a few interesting benchmarks}

\subsection{Code Generation}

Code generation is the task of generating code from a specification. In \textit{code completion}, the specification takes the form of a preexisting code snippet, and the goal is to complete the snippet. The most popular form of code completion is tab completion, where the user can press the tab key to complete a block of code (e.g. GitHub Copilot). Tab completion is often done at line-level or function-level scopes but needs to be fast to provide users with a seamless experience. Another paradigm is \textit{natural language to code}, where the specification is a natural language description with requirements such as the task description, input-output examples, or libraries to use. 
% NL to code has seen success in specialized domains such as text to SQL.
% \ks{again we should using "logical complexity"}

Recently, AI-driven IDEs, such as \href{https://docs.cursor.com/composer}{Cursor Composer} and \href{https://codeium.com/windsurf}{Codeium's Windsurf Editor}, have blurred the lines between the two paradigms. With the ultimate goal of decreasing the burden of human programmers, they aim to automatically infer the user's intent from the code context and user behavior (e.g. keystrokes, user edits, file navigation patterns). However, when intent is vague, they allow users to specify desired functionality via chat interfaces. Depending on scope and logical complexity, code generation can vary highly in difficulty. Reliable code generation in large codebases is still a challenge for state-of-the-art AI systems today.



\subsection{Code Transformation}

\subsubsection{Code Refactoring} \label{sec:t-code-refactoring}

In code refactoring, the goal is to take a working implementation of a piece of software and rewrite parts of it while maintaining correctness. One challenge with this task is that success extends beyond functional correctness or metrics. The goal is often to improve code maintainability, readability, or extensibility—qualities that can be inherently difficult to quantify and highly context-dependent. 

For instance, extracting shared functionality into helper methods presents trade-offs between modularity and cognitive complexity \citep{parnas1972criteria}. While there are no hard rules for when to extract functionality, one heuristic adopted by software engineers is the rule of three (``three strikes and you refactor'')--abstractions should only be used when a piece of code has been duplicated thrice. Because it can often be unclear what level of abstraction refactorings should be done at, completing a refactoring at a high autonomy level is also difficult. These challenges are further compounded by the need to understand implicit trade-offs customized to specific codebases, respect conventions, and reason about the long-term maintenance implications of structural changes. While code refactoring often has a low logical complexity, it can be laborious in practice due to scope, as seemingly small refactors can propagate across the entire codebase. 

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: React Fiber architecture refactor}: React's major refactoring was motivated by performance limitations in the original engine, particularly for complex UIs with animations and dynamic updates. Beyond challenges related to optimized implementation, a major challenge was providing backward compatibility while completely rewriting React's core algorithm. Being an open source tool, this refactor also required educating developers about new concepts without disrupting existing mental models highlighting nuances in real-world software system design. %\todo{claude generated -- confirm every point}
\end{tcolorbox}


\subsubsection{Code Migration and Translation} 

An incredibly resource-intensive (time and manual effort) task frequently affecting companies is migrating large amounts of code while preserving all the original functionality and semantics. Such high-value migrations present opportunities for AI-assisted automation to reduce cost and manual effort. Code migration often has a very high scope (many files and systems affected alongside their interdependencies) and high logical complexity (semantic depth of required transformations, constructs in different languages may be different). Current solutions may excel at migrations with high scope but modest logical demands (API migrations, type conversions) but struggle with changes across component boundaries~\citep{nikolov2025google}. 

A special case of code migration is code translation (transpilation): rewriting code from a source language to a target language. In industry, this task can be motivated by several reasons, such as security and scalability concerns in legacy languages, avoiding the technical debt a project has accumulated over the years, and improving the performance of a codebase. Due to the safety-critical and cross-system nature of many migrations, this task often requires substantial human oversight in practice and cannot be done fully autonomously. 

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: Scala version migration}: A recent Scala 2.13 to 3 migration~\citep{scalamigrate} illustrates these challenges, documenting a year-long effort. Critical issues included the loss of macro annotations, broken type projections, incompatible libraries, and compiler performance degradation—all requiring innovative workarounds and architectural changes. There have been many similar language migrations with analogous problems, famously Python 2 to 3 and Swift 4 to 5. 
\end{tcolorbox}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: COBOL}: COBOL powers 80\% of in-person financial services transactions and 95\% of ATM swipes while processing \$3 trillion in commerce a day, with over 220 billion lines of COBOL code in production \citep{Taulli_2020}. However, there are less and less COBOL programmers, leading to the desire to migrate out of COBOL and into a modern language like Java \citep{sneed2001extracting, sellink2002restructuring, sneed2010migrating}. However, because of the large scope and high logical complexity of existing COBOL code, migrating from COBOL to Java would be a monumental undertaking and many companies opt to continue using COBOL. These companies are still forced to migrate to newer versions like COBOL V6, because eearly versions of COBOL were gradually withdrawn from service. This task still requires skilled COBOL engineers and high precision, as it can often be difficult to understand the business logic of legacy code and introducing bugs can have dangerous implications. 
\end{tcolorbox}


% Works at Google~\citep{nikolov2025google} and Amazon~\citep{omidvar2024evaluating} have explored LLM-driven solutions for simple but vast migrations.
% % 
% Google's system identifies locations for changes, generates edits with LLMs fine-tuned on internal code, and automatically validates changes through compilation and test execution.
% % 
% For a 500+M LoC project migrating IDs from 32-bit to 64-bit integers, their system produced 80\% AI-authored modifications and reduced engineering time by approximately 50\%. 

% However, these approaches depend critically on the scope (number of files and systems affected) and logical complexity (semantic depth of required transformations). Current solutions may excel at migrations with high scope but modest logical demands (API migrations, type conversions) but struggle with changes across component boundaries~\citep{nikolov2025google}. Additionally, AI code migrations may require substantial human oversight due to their safety-critical and cross-system nature, motivating more work on Human-AI collaboration.

% \todo{At least 3 PL people I talked to mentioned the COBOL case study, probably worth to include it ;) \citep{sneed2001extracting, sellink2002restructuring, sneed2010migrating}}
% \todo{add code migration, deprecation, version upgrades}
% \ks{May be added another item on Code upgrade from one version to another.}

% Notes:
% talk about expensive and difficult, hence high-value for automation that AI brings
% 1. Some Py2 -> 3 migration examples
% 2. Scala 2.13 -> 3 migration pain points: https://blog.pierre-ricadat.com/scala-3-migration-report-from-the-field -- nice one!
% 3. Browser JS migrations?
% LLM for migration at google: https://research.google/blog/accelerating-code-migrations-with-ai/

% \subsubsection{Code Translation (Transpilation)} 


\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: Twitter migration to improve latency}: Twitter\footnote{\url{https://www.infoq.com/news/2012/11/twitter-ruby-to-java/}} built its initial platform using Ruby on Rails, facilitating rapid development. However, as the user base expanded, performance and scalability issues arose. They migrated key components to Java and Scala, achieving a 3X latency drop. This transition required re-architecting the system to adapt Ruby's dynamic features to the statically typed environments of Java and Scala, exemplifying the complexities of large-scale code translation.
\end{tcolorbox}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: C to Rust}: There has been a push to use translation as a proactive approach to eliminate memory safety vulnerabilities in C-based systems. This has even garnered attention from the US Department of Defense\footnote{\url{https://www.darpa.mil/news/2024/memory-safety-vulnerabilities}}, which has long-lived systems that disproportionately depend on C, supporting programs to translate C codebases to Rust (\href{https://www.darpa.mil/research/programs/translating-all-c-to-rust}{TRACTOR}). 
% 
Recent efforts like Syzygy~\citep{shetty2024syzygy}, C2SaferRust~\citep{nitin2025c2saferrust}, and AlphaTrans~\citep{alphatrans} have shown the potential for hybrid approaches combining LLMs with traditional program analysis techniques. 
However, some significant challenges remain, including ensuring correctness in large codebases while maintaining desirable attributes such as speed, reduced vulnerabilities, and idiomaticity.
\end{tcolorbox}


% 
% \todo{expand a little bit, motivate real-world importance (DARPA, cobol to *), checked-c maybe}
% 
% \todo{
% \\	- Don't banks still use COBOL code for this reason? Too expensive to migrate (maybe because of legislative/bureaucratic overhead)?
% }


\subsubsection{Code Optimization}

Transforming programs to improve performance characteristics while maintaining functional correctness is a critical software task. Optimizing real-world systems poses significant challenges due to the large scope and high logical complexity of the task, as performance bottlenecks must be identified and new algorithms to mitigate them must be proposed. Code optimization often has a large search and solution space with competing objectives like speed, memory efficiency, and readability, for example when optimizing kernel code at the PTX level for GPU-based AI model optimization~\citep{deepep2025, ouyang2024kernelbench}. In many scenarios, high levels of autonomy may not be desirable, as tradeoffs can depend heavily on external factors such as hardware, and the best optimizations may ultimately affect readability.

% 
% PIE
% LLM for compiler optimization \citep{llmcompiler}
% AlphaDev \citep{alphadev}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: Google Chrome performance improvements}: For over two decades, changes to the Chrome web browser have been an exemplar of optimization affecting real-world code~\citep{chromium10years}. Their V8 engine achieved a 20x performance improvement through coordinated optimizations across multiple layers - from implementing concurrent garbage collection that reduced bloat by 100x to developing specialized compilers like TurboFan that improved performance by 5-10\%, to enabling background parsing and compilation that reduced compile time by 20\%. The demand for cross-layer and low-level code changes (e.g., writing a new JavaScript interpreter) and building tools to measure and test representative performance metrics are key challenges for achieving this sort of real-world impact with LLMs.
\end{tcolorbox}


% https://blog.chromium.org/2018/09/10-years-of-speed-in-chrome_11.html
% https://v8.dev/blog/ignition-interpreter

\subsection{Software Testing and Program Analysis}

In the process of software development, there will inevitably be bugs. The difficulty of detecting these bugs can vary depending on their scope and logical complexity. For LLMs, minor typos or correctness bugs (small scope, low logical complexity) are easier to spot ~\citep{mosolygo2021rise} while complex concurrency bugs and security vulnerabilities (large scope, high logical complexity) can be tricky because they can be hidden deep in the call stack, contain subtle logic errors, or be hard to isolate due to the large scope~\citep{concurrencylucene}.


\subsubsection{Software Testing}
Software testing is a practical approach to prevent bugs, both during development and production.
% 
% 
There are several popular approaches to software testing, some short-term and others longer-term. \textit{Unit testing} refers to using input-output style tests that exercise the functionality of a piece of code. \textit{Property-based testing} is based on formal specifications and relies on specifying test cases that ensure that known properties of the code hold. \textit{Mutation testing} modifies a program subtly and ensures that the test suite can detect errors in these mutations. \textit{Fuzzing} refers to executing programs with unexpected inputs and monitoring for exceptions, usually over a more extended time period. 


\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: OSS-Fuzz on FreeType}: OSS-Fuzz~\citep{ossfuzzllm-results}, Google's automated fuzzing infrastructure, has proven its value by swiftly uncovering security flaws in critical software. For instance, when a recent source change was made to FreeType—a font rendering library deployed on over a billion devices—OSS-Fuzz detected a heap-buffer-overflow within hours:
\begin{verbatim}
ERROR: AddressSanitizer: heap-buffer-overflow on address 0x615000000ffa 
READ of size 2 at 0x615000000ffa thread T0
SCARINESS: 24 (2-byte-read-heap-buffer-overflow-far-from-bounds)
   #0 0x885e06 in tt_face_vary_cvtsrc/truetype/ttgxvar.c:1556:31
\end{verbatim}
\end{tcolorbox}

The goal of software testing is to design tests that can surface bugs reliably. This is evaluated through metrics such as code coverage--how much of the source code is executed when the test suite is run. An alternate to code coverage is mutation score, where mutants are generated, and the score is defined as the percentage of mutants causing the suite to fail. While practical, software testing faces challenges such as the scalability limits of traditional tools and the difficulty of manually designing tests with good coverage. As LLMs continue to improve at coding, they present a promising avenue for automatically generating high-quality tests.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Fault-based test generation at Meta}: Meta's Automated Compliance Hardening (ACH) system~\citep{foster2025mutation} is a system that generates tests aiming to catch real-world bugs. ACH works in three steps: first, the engineer describes the bugs they are worried about. Second, ACH combines LLMs with mutation testing to generate code with those bugs. Finally, these mutants were used to develop unit tests capturing them. ACH was used to generate tests for Messenger and WhatsApp, where engineers accepted 73\% of its tests.
\end{tcolorbox}

\subsubsection{Program Analysis}

While testing catches bugs, the most challenging software issues are security vulnerabilities and zero-day exploits, from memory corruption to privilege escalation. This requires a deep program understanding, that testing/fuzzing often misses. For instance, a \textit{zero-day} is a vulnerability unknown to the software developers that is found by an attacker, and there is no patch available from the vendor. In such cases, the only practical approach is offensive security research, manual source code audits, and root cause analysis of prior vulnerabilities to harden codebases.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: Variant Analysis:}
Project Zero’s~\citep{project-zero} investigations at Google reveal that many in-the-wild 0-day exploits aren’t entirely new—they’re often variants of vulnerabilities that had been patched before. In their analysis of recent 0-day reports, nearly half of the issues were closely related to earlier bugs (such as those affecting Windows win32k and iOS IOMobileFrameBuffer). This finding underscores the importance of performing rigorous root cause and variant analyses. Instead of just fixing a single exploit path, security teams must comprehensively address the underlying bug class, ensuring that alternate exploitation routes are closed off for good--making this task more challenging.
\end{tcolorbox}

Another example of a valuable but challenging analysis is invariant detection. A \textit{program invariant} is a property of a piece of code that is guaranteed to be true at a specified program point, no matter what the input is. A simple example is that after the line \texttt{int x = c * c;} is executed, \texttt{x} must be nonnegative. Identifying invariants in a program can be useful when testing, debugging, and modifying code. This task can be challenging because it requires reasoning abstractly about code execution across many different potential inputs and execution paths to determine what properties must hold for all possible inputs.


% \todo{Commment from Jiawei:
% I’d discuss a bit more on code auditing (https://arxiv.org/pdf/2501.18160v1) — which is a bit special for LLMs as static tools can already (mostly) solve static analysis.
% }

\subsubsection{Program Repair}

\textit{Bug localization} is a significant challenge in program repair, as pinpointing the exact site of a bug can be challenging, especially in large codebases. Issues like out-of-memory accesses often manifest themselves further downstream, making it difficult to identify the root cause. Once the bug is localized, the next step is to repair the bug. LLMs can be an ideal tool for this because they have seen a wide variety of bugs during training. Function-level, low-logical complexity bugs can often be easily fixed by feeding back error information to the model. It can be tricker to perform repair in larger scopes (e.g. repositories) where the code has higher logical complexity. This can often require several steps, including designing and implementing new algorithms or making complex refactorings across multiple files.

\subsection{Software Maintenance}

% Understanding the moving parts of a codebase is an important skill for being a stellar engineer. Code can often be difficult to understand due to many wrapper functions, error-handling boilerplate, deep call stacks, and sometimes even poor code cleanliness. We identify four practical code maintenance tasks that we find practical and important:

\subsubsection{Code Documentation and Summarization}

To ensure maintainability, readability, and ease of collaboration, code must be well documented. Good documentation needs to be written cleanly and crisply, describing what the function does and how the function works. It must also anticipate and address any misunderstandings that a programmer might have, such as potential side effects or special cases. Humans often see documentation as a chore and neglect it, leading to code and documentation frequently being out of sync. This has led to the concept of ``self-documenting code'', code that clearly conveys its purpose. As documentation is generally a task that has a low logical complexity and does not require too much human intervention, LLMs can help ensure that documentation is a continuously updated artifact in sync with the code. 

\subsubsection{Pull Request (PR) Review}
Reviewing pull requests is an integral aspect of the software development cycle. While the most essential requirement for PRs is that a new feature is implemented correctly, other important considerations include checking whether the repository's style conventions are satisfied, ensuring that the PR does not introduce any new bugs, verifying that program invariants and guarantees still hold, and inspecting whether tests are robust. Generally, reviewing PRs is a task requiring low logical complexity and can be automated relatively easily.

\subsubsection{Code Understanding, Navigation, and Question Answering}

When encountering a codebase for the first time, developers often find it challenging to understand and develop a good mental model of the code. This can be due to many reasons: too many wrapper functions, excessive error-handling boilerplate, deep call stacks, or poor code cleanliness. One important challenge in code understanding is \textit{code navigation}: finding where relevant functionality is implemented. Doing this well requires understanding the high-level layout of where every functionality lies in the codebase and the low-level understanding of which helper functions are used to implement each functionality. 

Another challenge is \textit{code question answering}: answering complex questions about a codebase, which requires sophisticated code understanding and reasoning abilities. Models should not hallucinate or give incorrect information that skews a developer's mental model of the code. Beyond other tasks mentioned in this section, developers might commonly ask questions related to data flow (when and where data structures get mutated), code functionality (whether there are any side effects), performance characteristics (determining the runtime and memory complexity of a function), or error handling (whether certain corner cases are handled). 

% \subsection{Program Repair}

% \citep{wang2023boosting}
% InferROI: LM to augment static analysis to detect resource leaks in Java programs

% IRIS \citep{li2024llm}: augment CodeQL with LM to improve taint analysis

% \textbf{Bug Detection}: Software inevitably will have bugs of a wide variety of flavors that vary in scope and logical complexity. Minor bugs might miss a few corner cases and require only a few lines of modification, e.g. \textit{simple, stupid bugs} \citep{mosolygo2021rise}. Complex bugs (e.g. concurrency bugs) might have very high logical complexity. Because bugs can often pass tests and syntax checks, even detecting them can be tricky and requires thorough testing both during development and production work. \todo{distribute across PL things / testing sub-sections and focus on only fixing parts in the sub-section}

% Program repair is the task of repairing software bugs. The difficulty of fixing a bug depends on the scope and logical complexity\ks{What do you mean scope?  What is logical complexity?  Why not consider the logical complexity of the code?}. Function-level, low logical complexity bugs can easily fixed by feeding in the error information. Repository-level, high logical complexity bugs could require locating the bug, re-thinking the algorithm, and restructuring or refactoring the codebase. In addition, these changes must be general and should not break the functionality of other parts of the codebase.


% multilingual \citep{liu2024mdeval}


% \citet{mosolygo2021rise} defines \textit{simple, stupid bugs} as ``minor inaccuracies that do not prevent the code from running or compiling, but change its behavior in a subtle way that may lead to unforeseen consequences''. 





\subsection{Scaffolding and Meta-Code} \label{subsec:scaffolding-metacode}

For a software system to work, the core logic must be written well, but that is not enough. Many infrastructural aspects must be in place to support the software. We group these into two main categories: \textit{scaffolding} and \textit{meta-code}. We define \textit{scaffolding} as a task outside of the code that must be done to get the software running properly. Examples of scaffolding include setting up Google authentication, subscribing to APIs, managing file storage, and generating API tokens. In contrast, we define \textit{meta-code} to be code that is important to make the system work but does not actually participate in the execution of its main logic. Examples of meta-code include test harnesses, configuration files, CI/CD code, Makefiles, Dockerfiles, sandbox databases, and preprocessors. Scaffolding and meta-code often are small in scope and have low logical complexity but can require a lot of domain-specific knowledge about the application, requiring human intervention.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Configuration validation:} Ciri \citep{lian2024large} is a tool that uses LLMs for configuration validation on open-source projects including Django, PostgreSQL, and Redis. They find that while Ciri excels at detecting misconfigurations of syntax and range violations, it struggles to detect dependency and version violations and is limited to a narrow range of misconfigurations. They also find that LLMs are biased towards more popular configuration parameters, which may lead to hallucinations in out-of-domain scenarios. 
\end{tcolorbox}

\textit{Infrastructure-as-code and Security.} A particularly challenging case is generating \textit{Infrastructure-as-code} such as \texttt{Terraform}, where you specify the type of infrastructure specifications (such as AWS EC2 instances, Kubernetes clusters, S3 buckets, VPC buckets) as code and execute it to create the infrastructure. When generating such code, LLMs struggle with security configurations due to the complex interplay between service-level permissions (e.g., AWS resource access), resource-level permissions (e.g., specific allowed actions), and provider-specific security primitives like IAM roles, security groups, and network access controls.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Distinguishing permission levels in cluster setup}: \cite{terrateam2024} show that on a task of bringing up a cluster, models fail to distinguish between ECS (Amazon Elastic Container Service) Task Execution Roles (for container operations) and Task Roles (for application-level permissions). This resulted in overly permissive policies where a single role was granted both image pull permissions and DynamoDB table access, violating principles of least privilege.
\end{tcolorbox}

% \todo{What other challenges here? Anything on test harnesses? examples from Devin blog?}

% \textit{Example. Playskript}: \href{https://code.playskript.com/}{Playskript} is a programming system that allows you to create interactive visualizations, presentations and games. AWS Lambda config, AWS virtual network + permissions, email service, google auth, databases, file storage. \todo{Ask Armando to flesh this out more}

\subsection{Formal Verification} \label{subsec:formal-verification}

The task of formal verification involves generating checkable, mechanized proofs that can guarantee that a piece of code works as intended. There are two major classes of formal verification: full functional verification (FFV) and property verification (PV). In FFV, the goal is to design a complete and precise formal specification that captures the desired behavior of the implementation, such as fully verified data structures (mutable lists, trees, graphs, hash tables) \citep{zee2008full}. The main challenge in full functional verification is in correctly writing the specification so that all desired properties are specified. FFV generally has a high scope and medium logical complexity, as the properties to verify are often straightforward to write once the correct abstractions are found.

While FFV provides a complete set of guarantees, it is usually sufficient to opt for PV, where a few key properties of a system are proven correct. Examples include: ensuring that two threads do not simultaneously enter a critical section of a program, verifying that a complex program will always terminate, proving the absence of security vulnerabilities like buffer overflows, and guaranteeing memory safety. One challenge that makes PV difficult to use in practice is the issue of false positives, where functionally correct code often does not pass property checks. A prime example is Rust: while the powerful type system enforces many desired guarantees, code with correct semantics often does not pass type checks. Another challenge is that many standalone tools for PV are often semantics-dependent, which can make them hard to maintain as language semantics change.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Costly disasters}: Formal verification of software is important in mission-critical applications such as aircraft software, as software bugs may lead to costly disasters. In the maiden flight of the Ariane 5 rocket, a floating-point conversion error caused it to explode forty seconds after liftoff. Another case is with the computer-controlled radiation therapy machine Therac-25, where concurrency bugs led to six people being massively overdosed, leading to serious injury and deaths.
\end{tcolorbox}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Verified Compiler}:
CompCert \citep{leroy2016compcert} is a formally verified optimizing C compiler that supports a restricted subset of C including most of the ISO C 99 language. CompCert has been formally verified using the Coq proof assistant \citep{Coq-refman}, eliminating the potential for compiler bugs. 
\end{tcolorbox}


While formal verification tools have begun to see adoption in industry, they has not yet become mainstream because of these challenges. Code LLMs could greatly ease this burden and make it more feasible to verify code at larger scales, especially verifying properties requiring lower logical complexity.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. Property Verification: Coverity}:
\href{https://scan.coverity.com/}{Coverity} is a static analysis tool meant to find generic errors (memory corruption, data races) and system-specific violations (e.g. function-ordering constraints). In their report \citep{bessey2010few}, they highlight two issues mentioned earlier: churn and false positives. The first issue, churn, deals with ensuring that the tool produces the same result both when the code base is modified and across different versions of the tool, making upgrades ``a constant headache''. The second issue is that when the false positive rate is more than 30\%, users ignore the tool and real bugs get lost among these false positives. 
\end{tcolorbox}