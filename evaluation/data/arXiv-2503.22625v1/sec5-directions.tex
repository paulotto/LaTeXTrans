\section{Paths Forward} \label{sec:paths}

% \todo{reorder}

\subsection{Data Collection} \label{sec:d-data}

% \subsubsection{Diverse Data Distribution}
% \todo{maybe some parts are too generic so can remove this}
% \naman{I think given agents, we might need to atleast change the pre-training data mixture during pre-training. similarly for supporting RL maybe some data mixtures would be better}

% \naman{similarly FIM style or different training objectives}

\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]

One bottleneck in the development of AI for SWE in the open-source community is the lack of access to fine-grained and high-quality code data. In Sec. \ref{sec:d-automatic-data}, we discuss how automated techniques can mitigate this by augmenting existing programs with symbolic information and generating synthetic data with symbolic verifiers. However, there are other crucial signals in programming that can be hard to automate. We envision that a large community-based coding data curation effort will be very impactful. In Sec. \ref{d:subsec-human-data}, we discuss examples of datasets that such a community could create that would unlock new capabilities in code LLMs.
\newline \\
\textit{Challenges addressed: \ref{sec:c-evaluation}, \ref{sec:c-hai-colab}, \ref{sec:c-global-understanding}}
\end{tcolorbox}

\subsubsection{Automatic Data Curation} \label{sec:d-automatic-data}

\textbf{Augmenting Data with Program Information}: One challenge in enabling LLMs to develop a world model of code is that programs are often treated like text: as tokens with no semantic information. However, modern programming tools allow us to extract rich semantic and structural information about code. By leveraging these tools, we can augment training datasets with detailed annotations describing various properties of programs. We hypothesize that this augmentation will significantly improve a model’s understanding of code, leading to better generalization and stronger coding capabilities. Information can include:
\begin{itemize}
    \item Static analysis: the syntactic structure of a program (abstract syntax trees, control flow graphs), information about the type of each variable, data flow analysis (reachability, liveness analysis)
    \item Program instrumentation: memory consumption, runtime analysis, aliasing, and code coverage (like statement or branch coverage)  
    \item Dynamic analysis: program states at various points in the program, call stacks, dynamically curated properties (often relies on instrumentation)
    \item Formal verification: concurrency analysis, program invariants, loop invariants, memory safety
\end{itemize}

There have been a few examples of this in the literature: \citet{ouyang2024kernelbench} leverage profiler feedback to improve GPU kernel generation, \citet{ding2024traced, ding2024semcoder, ni2024next} incorporate execution trace information, \citet{pei2023can} train with program invariants, GraphCodeBERT \citep{guo2020graphcodebert} incorporate data flow information, and \citet{pie} train on a dataset of performance-improving edits. 

% This training could also be done in a curriculum-like manner to gradually improve the world model capabilities. 
% As initial evidence of the promise of this approach, \cite{li2025codei} find that training models on input-output prediction data leads to consistent improvements on reasoning tasks. This training could also be done in a curriculum-like manner to gradually improve the world model capabilities.
% Similarly, we envision training models on diverse end to end tasks such as bug-finding, bug-fixing, code optimization, translation, which also incentivizes models to learn \textit{general} capabilities beyond just code generation. 

\textbf{High-quality, Verifiable Synthetic Data}: The advantage of code is it is possible to achieve strong, verifiable feedback with test cases, program execution engines, and other symbolic tools. This makes high-quality synthetic data generation viable, as it is possible to generate a large batch of data and filter out low-quality samples. For example, to generate code with interesting program invariants, we can sample a large batch of programs, run an invariant detection engine, and retain only programs with interesting invariants. While synthetic data in code has mostly been at the function-level scope, there are no fundamental bottlenecks to expanding to larger scopes. As code is quite compositional, individual building blocks can be combined to generate complex synthetic data at the repository-level scope, which can be very helpful in both training and evaluation. 

While the importance of having high-quality data vs. high quantities of data is debated, using verified data has proven to be useful. For example, \citet{code-r1} shows that simply removing bugs in existing datasets such as TACO \citep{li2023taco} can lead to significant boosts. KodCode \citep{xu2024benchmark} also showed that fine-tuning on verified synthetic data also leads to significant improvements. However, these works work with programs at the function-level scope with low to medium logical complexity, and we imagine that general SWE abilities can improve with synthetic data across scopes and logical complexities.

In DSLs, where programs can be cleanly described with semantics and rewrite rules, one can symbolically generate programs with desired properties via sampling, drawing on enumeration techniques from program synthesis \citep{gulwani2017program}. This technique has been successfully applied to make considerable progress in difficult reasoning tasks such as ARC-AGI \citep{li2024combining} and math olympiad problems \citep{trinh2024solving, deepmindalphaproof, chervonyi2025gold}.

\subsubsection{Human-Centric Data Curation} \label{d:subsec-human-data}
\label{d:sec-human-supervision}

Below, we list three classes of human-annotated data that would be invaluable for the next generation of coding LLMs.

\textbf{Fine-Grained Data of the Developmental Process}: Many code LMs are trained on datasets such as \textit{the Stack} \citep{kocetkov2022stack, lozhkov2024starcoder}, consisting of trillions of tokens sourced from GitHub. However, training on raw GitHub tokens omits many crucial human signals in the process of software development. For example, companies such as Google rely on internally captured logs of high-quality SWE data. This includes ''fine-grained code edits, build outcomes, edits to resolve build issues, code copy-paste actions, fixes of pasted code, code reviews, edits to fix reviewer issues, and change submissions to a repository`` \citep{chandra2024ai}. Similarly, Meta and GitHub Copilot use telemetry with their AI coding assistants to track and leverage signals from AI-generated code \citep{murali2024ai, ziegler2024measuring}. These tools, along with coding IDEs like Cursor, could provide a treasure trove of reward data for RL-based methods. With direct access to the full history and evolution of a codebase, they can track which suggestions are adopted over time. However, collecting data from human usage also raises critical privacy and intellectual property concerns.

\textbf{Data for Diverse SWE Tasks}: Most of today's code LLM training recipes still focus primarily on code generation because large-scale datasets are mostly in a continuous, tokenized format. However, as described in Sec. \ref{sec:tasks-milestones}), there are many tasks involved in software engineering which models lack exposure to. Training on a broader set of tasks would also incentivize models to learn \textit{general} capabilities of programs beyond just generation (e.g. a better understanding of program semantics). As initial evidence, \cite{li2025codei} find that training models on input-output prediction data leads to consistent improvements on reasoning tasks. 

The lack of high-quality data on these tasks makes it hard to train on them. It can also be hard to automatically curate them on GitHub. For example, for code refactoring (Sec. \ref{sec:t-code-refactoring}), we need paired repositories before and after refactoring, ideally with the refactoring changes described. While some signal such as commit messages and version releases can be used, many repositories lack clean commit histories and releases conflate many features at once. Therefore, to mitigate this, we envision large community-based efforts curating task-specific data on these diverse challenges. 

\textbf{Human-Centric Data}:
Code LLMs are typically trained and evaluated on carefully curated datasets with clear instructions and verifiable test cases. However, as discussed in Sec. \ref{sec:c-hai-colab}, these models are often deployed in real-world scenarios where users provide vague specifications or incomplete requirements in their queries. Collecting human-centric data that reflects real-world model usage is a promising approach to bridging the gap between model development and deployment. Recent efforts, such as Copilot Arena~\citep{chi2025copilotarena} and \href{https://web.lmarena.ai/}{WebDev Arena}, have explored gamified arenas to gather data on human preferences, offering an alternative to purposefully curated datasets. However, such data collection methods may introduce noise, and arena-style approaches are not well-suited for long-horizon, interactive tasks. One potential approach is to leverage existing coding tools and environments, such as developing plugins for GitHub Copilot~\citep{bajpai2024let} or open-source IDEs, to capture real-world interactions. Unlike static datasets, human-centric data can also be collected encompassing diverse interaction modalities, such as users providing sketches to AI coding systems for web development~\citep{li2024sketch2code}. As AI coding systems continue to emerge and evolve, launching data initiatives focused on human-centric SWE data is also a crucial direction for advancing human-AI collaboration in software development.
% \yijia{copilot arena, webdev arena}


\subsection{Training}

\subsubsection{Environment Design for Code RL} \label{sec:d-rl}
\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
Reinforcement Learning from Verifiable Rewards (RLVR)~\citep{lambert2025tulu3pushingfrontiers} has emerged as a powerful paradigm in math and coding domains where model outputs can be evaluated against a ground truth outcome such as exact match and passing a set of unit-tests. Towards this direction, promising avenues include collecting executable codebase environments, sourcing task prompts/rewards from GitHub, and designing non-execution based rewards based on program syntax and semantics.
\newline \\
\textit{Challenges addressed: \ref{sec:c-long-horizon}, \ref{sec:c-ood-domains}}
\end{tcolorbox}

\textbf{Collecting executable codebases:} In recent months, RLVR has seen success in solving algorithmic programming problems through DeepSeek-R1~\citep{deepseekai2025deepseekr1} and OpenAI o1. Recently, on SWE-Bench, SWE-RL \citep{wei2025swerladvancingllmreasoning} use RL on a rule-based reward to improve performance on SWE-Bench. We find it promising to continue scaling the RL approach to problems collected from real-world software engineering repositories. 
Towards this, we believe that collecting execution-assisted gym-like reinforcement-learning environments will lead to further performance improvements. 
These environments can be used further to improve reasoning skills, environment-interaction capabilities and tool usage.

Several prior works ~\citep{jain2024r2e, pan2024trainingsoftwareengineeringagents,guo2025syncmind, xie2025repost} curate executable environments for programming agents by supporting CI/heuristic-based repository installations.
However, these works are at a relatively small scale and limited in scope, offering only a few thousand tasks from a maximum of a thousand repositories and more importantly, limited to the Python language.
Scaling this up significantly requires solving several research and engineering problems. 
First, installing arbitrary repositories from Github, even using CI is challenging and we require smarter solutions potentially involving LLM-based installation agents.
Next, setting up execution infrastructure would require storing installed repository images in something akin to \verb|docker| for efficient storage and fast container startup times~\citep{team2025kimi}.
Notably, combined docker images can grow massively large and often grow at hundreds of gigabytes even at a modest scale of a few hundred repositories. They require engineering support for efficient storage and serving of such images.

% Currently, using repository programming data as text with next-word prediction systems. 
% A promising direction is to scale such reinforcement learning approaches to more mature real-world tasks by collecting execution-based gym-like reinforcement-learning environments.
% These environments will support training next-generation language models and agents. \naman{single-turn model is essentially agent with no tool}
% However a number of challenges follo:


% \textbf{Collecting Executable Codebases.}
% ~\citet{jain2024r2e, pan2024trainingsoftwareengineeringagents} have attempted to scale executable repositories for programming agents. 
% However, these works are typically small, offering only a few-thousand problems collected from a few-hundred codebases.
% Installation is challenging.
% The organization is even more challenging from a systems perspective.

\textbf{Sourcing task prompts and rewards:}
Beyond environments, performing large-scale reinforcement learning would require collecting diverse challenging problems with an appropriate way to compute the rewards. 
These task prompts can be collected from Github~\citep{pan2024trainingsoftwareengineeringagents} or generated synthetically from problems on Github.
Moreover, assuming access to many executable repositories, we can source various end-to-end problems for tasks beyond bug-fixing such as optimization, fuzzing, etc.
Access to pre-existing or generated test cases allows for measuring correctness and providing rewards. 


However, we envision many practical challenges to remain. 
% \textbf{Task ambiguity.}
For example, longer-horizon tasks are usually more ambiguous and approaches may require multi-turn interactions beyond autonomous coding agents.
% most long-horizon tasks will be ambiguous and will require multi-turn interactions.
This would pose a considerable challenge during reinforcement learning where ambiguity resolution might need to be modeled in the reinforcement learning process itself. 
We elaborate on human collaboration further in Section~\ref{sec:d-hai-training}.
% \textbf{Reward Computation.}
% Testing is key for reward computation. We can generate tests (similar to r2e). 
% Reward Hacking is still a concern. 
% Cite negative examples, some from r2e, some from sakana
Reward hacking~\cite{skalse2022defining} poses another challenge as we build more real-world coding challenges. 
Test cases often suffer from coverage issues and can grade correct solutions as incorrect.
For example, ~\cite{baker2025monitoring,denison2024sycophancy} identified that models attempt to bypass or cheat against the testing harness when optimized using reinforcement learning. 


% \textbf{Reward Hacking.}


% \textbf{Task ambiguity.}
% Most long-horizon tasks will be ambiguous and will require multi-turn interactions for specific
% Such problems will also pose a considerable challenge during reinforcement learning where potentially correct solutions would be penalized.
% Ambiguity resolution should be modeled in the reinforcement learning process itself and we elaborate this further in Section~\ref{subsec:humanai}.

% Real-world user prompts will be ambiguous and require multi-turn interactions for ambiguity resolution. 
% It might be necessary to model humans in the loop to allow such interactions but that seems very hard. 
% connect to human ai colab \todo{talk to yijia who might have  thought about it more}


\textbf{Rewards without execution}: As setting up execution environments can lead to considerable overhead, another potential strategy is to use proxy metrics and trained language models to judge correctness. This was common in the pre-LLM era, researchers often used BLEU/CodeBLEU \citep{papineni2002bleu, ren2020codebleu} and BERTScore/CodeBERTScore \citep{zhang2019bertscore, zhou2023codebertscore} to assess correctness of text and code. In code, semantic and structural properties can be used to improve similarity metrics. Two examples of this are Dolos \citep{maertens2022dolos}, an AST-aware plagiarism detector, and \texttt{difflib.SequenceMatcher}, which can be used to compute the similarity between two patches \citep{wei2025swerladvancingllmreasoning, ma2025sorft}. 
Beyond rule-based rewards, LLMs-as-a-judge approaches can also be used as reward functions, possibly in conjunction with other execution-based or execution-free approaches.

% Another idea is to rely on perplexity, which has been a successful proxy metric for correctness, as many language models trained with a perplexity loss still perform well on downstream tasks and have high accuracy. However, as code from LLM data sources such as GitHub is often incorrect, perplexity for incorrect code may still be high. To mitigate this, one strategy could be train  models to minimize the perplexity of correct code and maximize the perplexity of incorrect code, for example by relying on pull request data (merged PRs are more likely to be correct).  

% \subsubsection{Post-Training: Distillation}
% distillation can go to pre-training actually

% move synthetic data here. \naman{Personally -- should distinguish synthetic "prompts" from synthetic solutions as different approaches reqd for both}
% \subsection{World Models}

% \textit{Steering away from anti-patterns}: In software engineering, certain anti-patterns frequently lead to bugs. For example, common weakness enumeration (CWE) is a categorization of software and hardware weaknesses often leading to vulnerabilities. Because publicly available GitHub code often contains code with anti-patterns, bugs, and CWE vulnerabilities, LLMs often write code susceptible to these issues \citep{asare2023github, fu2023security}. We hypothesize that explicitly steering models against these vulnerabilities will lead to more secure and correct code. One way to do this is to collect a large number of program samples violating each CWE (either synthetically or on GitHub) and then use these samples as negative signal during further supervised fine-tuning or RL stages.

% \naman{this is generally fine -- but lately i have been thinking if supervising these tasks manually is necessary or can the model learn them as necessary in end-to-end RL like I think training for algorithmic-debugging with RL might be instilling some level of execution world-model}


\subsubsection{Adapting to Specialized and Quickly Changing Codebases} \label{sec:d-adaptation}

\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
Low-resource languages (Sec. \ref{sec:c-low-resource}), custom APIs, library version updates (Sec. \ref{sec:c-library-updates}), large codebases (Sec. \ref{sec:c-large-scope}), and custom coding styles all surface the fact that code LMs struggle to adapt to unseen specialized contexts. Customization can be achieved through test-time training, keeping specialized information in an information bank. A cheaper and alternative approach to test-time training is to apply prompt and prefix tuning, where codebase-specific embeddings are learned and applied depending on the context. 
\newline \\
\textit{Challenges addressed: \ref{sec:c-low-resource}, \ref{sec:c-library-updates}}
\end{tcolorbox}


% \textit{Adapting continual learning to the code domain}: Throughout the maturation cycle of a software project, more and more features are going to be added to a codebase that become available as new building blocks and APIs. In Lean, as eager contributors formalize more and more math textbooks, new definitions, theorems, and premises are being added to \texttt{Mathlib} every day and can often be leveraged directly to prove more complex theorems. Because code is always changing, solving this continual learning problem is important for productively working in new codebases.

% There is a large body of literature in continual learning, many of which may be applicable to this issue. However, continual learning in the code domain has one key difference: one of the biggest issues in traditional continual learning is catastrophic forgetting, that networks will forget old information when learning new information \citep{kirkpatrick2017overcoming}. To some extent however, especially in version and API updates, forgetting is actually a desirable behavior in order to avoid the use of deprecated API's or past version of libraries. While some deprecated usage patterns are completely useless, the tricky aspect here is that you don't want to forget everything from the old APIs, because there may still be useful implementation details. Therefore, we may need to design continual learning algorithms with controllable forgetting \citep{wu2024continual}.

\textbf{Test-time training (TTT) to custom codebases}: TTT is the recent paradigm of adapting to a specific problem instance by training on a narrow set of in-distribution examples~\citep{akyurek2024surprisingeffectivenesstesttimetraining,sun19ttt}. This can be used when working in a low-resource context, for example training on a specific codebase, new domain, or unseen API. One challenge in this setting is customizing the model to the particular codebase while retaining general coding knowledge, potentially by using algorithms that can induce controllable forgetting \citep{wu2024continual}. To get data in specialized contexts, we envision two mitigation strategies: generating synthetic data and collecting trajectories. In-distribution synthetic data can be generated in large quantities and then filtered and annotated with symbolic (e.g. compiler) information to gain a more global understanding of the current environment and setting. To gather agentic trajectories, we can keep track of previous model attempts and failures to learn from past successes and avoid making repeated mistakes. This will steer the model closer to the desired distribution--for example, to generate code in the specified version of libraries being used in the current context.

\textbf{Keeping an information bank of code information}: For library and versioning issues, retrieval (Sec. \ref{sec:directions-retrieval}) can be very effective for preventing hallucinations of wrong versions of libraries, which can inherently lead to better synthetic data and agentic trajectories. During the TTT process, we can also keep a large growing memory bank of code, documentation, synthetic code, and agentic trajectories in the specialized context. Retrieving from the memory bank would improve the success of generating code, which can then be augmented to the memory bank, and so on, continuously increasing the amount of data and knowledge available.

\textbf{Prompt and prefix tuning for specialized code contexts}: One issue that makes it difficult to continuously keep up with library updates is that doing full finetuning every time something changes is very expensive. Because only a small amount of knowledge needs to be learned compared to that of the pre-trained model, we believe less expensive approaches such as prompt tuning \citep{lester2021power} and prefix-tuning \citep{li2021prefix} could suffice. Both these methods append a set of learned task-specific vectors to the input sequence in order to model a specified context, though prompt tuning only modifies the input and prefix-tuning modifies the input at each layer. These methods have also been shown to have good OOD performance, and we believe they present a promising approach to dealing with multiple library versions. A separate prompt/prefix can be trained for each version and then applied according to the context. When an API has new updates, the prompt/prefix can then be cheaply re-tuned to reflect the new updates without undergoing full fine-tuning. This approach also applies to adhering to specific coding styles, where codebase-specific prompts/prefixes can also be learned.   

\textbf{Learning on the fly}: When humans are faced with a task they have never seen before, they are often able to draw from past experiences and quickly adapt and generalize to the new domain. This is one of the big unsolved challenges of today's LLMs: given an OOD coding task, how can models get up to speed and productively work on the task with few samples? On toy domains, an example of this is DreamCoder \citep{ellis2021dreamcoder}, a system that learns to solve problems by writing programs and automatically discovering domain concepts and composing concepts together. Designing such approaches for more practical applications is an exciting research direction that will have drastic implications for coding and reasoning.

%\subsubsection{Adapting to Interactive Setup}
\subsubsection{Training Code LLMs to Collaborate with Humans}
\label{sec:d-hai-training}
\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]

Training the next generation of code LLMs needs to account for human-AI collaboration, as these models will likely be deployed in ambiguous and interactive scenarios. We highlight two key directions for improving collaboration: First, learning to leverage specifications beyond natural language through formal methods and user-specified tests can mitigate vague specifications. Second, improving uncertainty quantification and proactive communication through post-training has the potential to prevent hallucination and misalignment.
\newline \\
\textit{Challenges addressed: \ref{sec:c-hai-colab}}
\end{tcolorbox}

\textbf{Learning to Leverage Specifications Beyond Natural Language}: As discussed in Section~\ref{sec:c-hai-colab}, while natural language prompts offer intuitive and flexible ways to express requirements, they often suffer from ambiguity and incompleteness. One direction to address this limitation is to train models to leverage enhanced specifications with more precise and verifiable representations, such as formal specifications and test-based specifications.

%\textit{Formal specifications}: Autoformalization can mitigate underspecification by translating user intent into formal specifications~\citep{szegedy2020promising}. Ideally, a formal language could completely and unambiguously capture the desired software behavior. However, autoformalization may also suffer from the misalignment problem by misinterpreting user intent, resulting in a formal specification that does not accurately reflect the user's actual requirements. Therefore, users must understand and carefully verify the formalized specification generated by autoformalization tools.

\textit{Formal specifications}: To mitigate underspecification issues, one solution is to develop systems that can translate user intent into formal specifications~\citep{szegedy2020promising, endres2024can}. While current autoformalization approaches face challenges in accurately capturing user intent (see example below), we envision next-generation systems that will iteratively refine formal specifications through interactive verification with human feedback. These systems would present intermediate formalizations in accessible notation, enabling non-expert users to verify correctness before code generation. % Research directions include developing bidirectional translation mechanisms between natural language and formal specifications, and creating user-friendly interfaces for formal verification.

%\yijia{I am not familar with formal specifications. Please help add citations or more content.}


\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Incomplete specification in Verus}: Here, we show a failure mode of LLMs when writing specifications and proofs in Verus. The LLM is asked to write the \texttt{ensures} postcondition clause for a a ring buffer enqueue function\footnote{Full example \href{https://github.com/microsoft/verus-proof-synthesis/blob/main/benchmarks/interprocedural/tock/verified/rb1.rs\#L270-L297}{here}}. Here, the postcondition is incomplete: it does not check, for example, that the original elements were maintained in the ring buffer.

\begin{lstlisting}[label={lst:verus-spec}, captionpos=t, breaklines=true, language=Rust, style=mystyle]
fn enqueue(&mut self, val: T) -> (ret: bool)
    !*
    ensures
        ret == !old(self).is_full(),
        self.inv(),
        if ret { self.view() === old(self).view().push(val) } else { self.view() === old(self).view() }
    *!
{
    if self.is_full() {
        false
    } else {
        self.ring.set(self.tail, val);
        self.tail = (self.tail + 1) % self.ring.len();
        true
    }
}
\end{lstlisting}
\end{tcolorbox}

\textit{Tests as specifications}: Another approach to specify software behavior is through tests. These range from input-output examples and assertions to property-based tests. %While tests provide more concrete specifications than natural language, they can still be underspecified. 
However, in practice, hand-crafted test suites are often incomplete, failing to capture the full intended behavior, particularly edge cases. This can lead to misalignment, where AI-generated code passes tests but does not genuinely meet functional requirements, potentially misleading users. Moving forward, a direction is training models to generate high-quality test cases based on the user's initial query, ensuring more comprehensive specification coverage.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example}: For instance, in a release of \href{https://sakana.ai/ai-cuda-engineer/}{AI CUDA Engineer} by Sakana AI, an AI-generated CUDA kernel for lower triangular matrix multiplication—purportedly achieving significant speedups—was later found to exploit out-of-bounds memory access to bypass correctness checks\footnote{The full LLM-generated kernel code can be found in Listing 3, pg. 46-47 of \cite{lange2025ai}}. Advancing research on frameworks that facilitate test generation and automated adversarial testing represents an important direction.
\end{tcolorbox}

%\textbf{Uncertainty Quantification and Proactive Communication}:
\textbf{Learning to Quantify Uncertainty and Communicate Proactively}: As AI coding systems are increasingly deployed to complex software engineering tasks, they encounter more ambiguous and uncertain scenarios compared to traditional benchmarks for coding models. Ideally, in such situations, these systems should proactively communicate with users to clarify tasks and acknowledge its own limitations rather than becoming stuck in endless failure loops or generating buggy code. A key challenge is enabling models to distinguish between well-specified and ambiguous instructions while quantifying uncertainty in a robust manner. While early studies, such as \citet{vijayvargiya2025interactiveagentsovercomeambiguity} and the example below, demonstrate that interactive LLMs can improve performance through clarification-seeking behavior, current models still struggle with uncertainty estimation. Equipping models with the ability to quantify uncertainty will likely require incorporating corresponding reasoning data into the post-training stage.

Besides uncertainty quantification, \citet{shao2024collaborativegym} identify communication as a primary challenge in human-agent collaboration, highlighting the need for improving models' proactive communication capability. Current models often fail to ask meaningful questions when user input is ambiguous or insufficient, and they struggle to provide progress updates or verify plans in interactive settings. Enhancing models’ proactive communication abilities requires innovative approaches to reward behaviors that yield benefits over multiple steps. Since communication with users does not immediately resolve the task at hand but may improve long-term outcomes, effective strategies must account for delayed rewards in training.


% Moving forward, a crucial direction is to develop AI coding systems that explicitly reason about uncertainty and engage in effective, context-aware communication—paving the way for more reliable and collaborative AI-assisted software development.


%While research on interactive AI coding systems remains limited, \citet{vijayvargiya2025interactiveagentsovercomeambiguity} demonstrated promising results, showing that LLMs can leverage interaction to improve performance on underspecified tasks when given the ability to ask clarification questions. However, their study also revealed that current models struggle to differentiate between well-specified and ambiguous instructions, lacking robust uncertainty quantification—even with the latest reasoning models that perform exceptionally well on complex reasoning benchmarks. \citet{shao2024collaborativegym} also finds communication issue is the top-one challenge in human-agent collaboration. Training models to explicitly reason about uncertainty and communicate effectively presents an exciting direction for advancing the deployment of AI coding systems.

%\todo{Add example of pydata\_\_xarray-4750. The initial issue does not specify the max row needs to be configurable; but it was brought up in issue discussion.}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example: Discussion Helps Coding Agents Resolve Github Issues:} In SWE-bench~\citep{jimenez2024swebench} \texttt{pydata\_\_xarray-4750}, the original \href{https://github.com/pydata/xarray/issues/4736}{issue description} requests limiting the number of data rows displayed in \texttt{repr}. While it suggests a maximum of 25 rows, it does not specify whether this number should be configurable—a key requirement that emerged during the issue discussion. When SWE-Agent~\citep{yang2024sweagent}, powered by GPT-4o, uses only the issue description as the problem statement, it generates a function that hardcodes the maximum at 25, causing the solution to fail the test. However, incorporating the issue discussion allows the agent to produce a correct, test-passing implementation (see Listing \ref{lst:swe-agent-with-discussion}). This suggests that enabling coding agents to engage in discussions with users could potentially improve the issue solving rate.
\end{tcolorbox}

\begin{figure}[ht]
\begin{minipage}{\textwidth}
\begin{lstlisting}[caption={SWE-Agent improves when incorporating issue discussions},label={lst:swe-agent-with-discussion}, captionpos=t, breaklines=true, language=Python]
# Use only the issue description
def _mapping_repr(mapping, title, summarizer, col_width=None):
    if col_width is None:
        col_width = _calculate_col_width(mapping)
        summary = [f"{title}:"]
    if not mapping:
        summary += [EMPTY_REPR]
    else:
        items = list(mapping.items())
        if len(items) > 25:  # Show first 12 and last 12 items
            first = items[:12]
            last = items[-12:]
            summary += [summarizer(k, v, col_width) for k, v in first]
            summary += ["    ..."]
            summary += [summarizer(k, v, col_width) for k, v in last]
        else:
            summary += [summarizer(k, v, col_width) for k, v in items]
    return "\n".join(summary)

# Use the issue description and discussion
def _mapping_repr(mapping, title, summarizer, col_width=None):
    col_width = _calculate_col_width(mapping)
    summary = [f"{title}:"]
    if mapping:
        items = list(mapping.items())
        if title == "Data variables" and len(items) > OPTIONS["display_max_rows"]:
            # Show first and last variables if there are too many
            first_n = OPTIONS["display_max_rows"] // 2
            last_n = OPTIONS["display_max_rows"] - first_n
            selected_items = items[:first_n] + [("...", "...")] + items[-last_n:]
        else:
            selected_items = items
        summary += [summarizer(k, v, col_width) if k != "..." else "    ..."
            for k, v in selected_items]
    else:
        summary += [EMPTY_REPR]
    return "\n".join(summary)
\end{lstlisting}
\end{minipage}
\end{figure}

%Beyond code LLMs alone, state-of-the-art AI coding systems are typically equipped with various tools, such as terminals, profiling tools, interactive code executors, \textit{etc}. In the Collaborative Gym tabular analysis task~\citep{shao2024collaborativegym}, for example, models have access to a Jupyter code executor, allowing them to run python code interactively during human-AI collaboration. While agents generally achieve better performance in such interactive setups, error analysis shows that in 13\% of cases, they fail to assess the feasibility of requests within the constraints of available tools and resources. For instance, the agent attempts to execute CUDA code to satisfy user requests despite the absence of a GPU. Thus, beyond simply training models to use tools, future work should focus on improving their environmental awareness and incentivizing proactive communication when user instructions exceed the capabilities of the system’s available resources.


% - knowing own limits (e.g. should not attempt to write concurrent code), reject incorrect instructions
% - asking clarification questions when things are vague

% - being able to know when code it wrote might be wrong

% \textbf{Reward Design Beyond Correctness}: 

% - Effectiveness + Correctness: \citep{ouyang2024kernelbench}, \citep{yang2024acecode}


% \textit{Controllable forgetting}: If the correct repository version can be identified, the model can also be trained on the specific version of each API, reducing version-related hallucinations. This can also be combined with algorithms that can induce controllable forgetting \citep{wu2024continual}. 

% \textit{Retrieval-augmentation}: In lower-resource domains, the challenge is purely syntactic rather than algorithmic, making RAG-based methods ideal. In these situations, relevant retrievals can be very instructive. For example, when using APIs with multiple versions, providing retrievals in the current version can steer the model. These retrievals can be in the form of documentation, API function definitions, or example use cases of a desired function.


\subsection{Inference Time Approaches}

\subsubsection{Semantic-Aware Embeddings and Retrieval} \label{sec:directions-retrieval}

\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
In contrast to text, embeddings for code should incorporate execution and semantic information, improving retrieval. RAG benefits from both context-aware retrievals and explicit training on how to use them, enhancing code reuse across languages and APIs. Beyond static retrieval, AI agents could also dynamically navigate codebases using command-line tools and IDE functions.
\newline \\
\textit{Challenges addressed: \ref{sec:c-large-scope}}
\end{tcolorbox}


\textbf{Semantic and execution aware code embeddings}: When training LLMs, code is often treated as pure tokens (just like text) rather than explicitly incorporating code-specific information such as program execution and semantics. As a result, code that is close in embedding space is more often syntactically similar than semantically similar \citep{utpala2023language, zhao2023get}, and there are few reliable methods today to retrieve semantically similar code. However, before the LLM era, there were a variety of efforts to incorporate code properties when training embeddings. For example, \citet{nye2020representing} train neural modules to represent program operations, leading to compositional program representations that encode the semantics of the underlying programming language. Many other works \citep{zohar2018automatic, ellis2019write, chen2021latent} attempt to learn execution-aware latent representations for partial and full programs, taking semantics into account. 

We speculate that incorporating these techniques to train models to have better and more semantically aware representations may lead to models with a more general understanding of code (Sec. \ref{sec:c-global-understanding}). For example, if correct and buggy programs could hypothetically be separated in embedding space, then models could be steered away from the incorrect program space. While such a clean separation might not be possible, we believe that training embeddings to have interesting semantic properties is worth exploring.

\textbf{Better retrieval-augmented code generation}: When retrieval-augmented language models were first introduced, they often relied on training the retriever and language model jointly, as in FiD \citep{izacard2020leveraging}, RETRO \citep{borgeaud2022improving}, and Atlas \citep{izacard2023atlas}. As language models increased in size, the field shifted to a black-box setting \citep{shi2023replug}, where the retrieval module is tuned independently to adapt to the pretrained black-box LLM. This setting is much more cost-effective, but the language model is not explicitly trained on how to use its retrievals.

The black-box setting is ideal for challenges such as low-resource languages or specialized contexts. In these situations, the model has not seen enough training data to fully grasp the context, and the challenge is often syntactic rather than algorithmic. For example, when adapting to a domain or a codebase where the relevant API functionality or code style, retrievals can be very instructive. When using APIs with multiple versions, providing retrievals in the correct version can inform the model of how to use the API. When writing code in a completely new language, showing examples of \texttt{for} loops and \texttt{while} statements will teach the model the syntax of these constructs. Retrievals should be diverse and given in multiple forms, including documentation, function definitions of APIs that are used, and example use cases of target functions.

In many other cases, however, we believe that a black-box setting is insufficient. As described in Sec. \ref{sec:c-large-scope}, there are two challenges: 1) knowing what to retrieve and 2) using the retrieval. The first challenge relies on retrieving relevant examples, both syntactically and semantically. We believe that having more semantically aware embeddings, as mentioned above, will drastically improve this. For example, embeddings can be trained contrastively to minimize the distance between semantically similar programs. Another potential direction is to consider a diverse set of potential retrievals and then train the retriever to prefer samples that help during generation, as in Atlas \citep{izacard2023atlas}.

The second challenge, using the retrieval, is a code reuse task, which requires complex reasoning and code understanding. Algorithms provided in retrievals may often need to be modified and adapted significantly to adapt to the current setting. An example of this might be writing a C++ version of a shortest path algorithm
when the retrieval is a Java version, a translation task that models may not have been trained for explicitly. Long chunks of retrieved documentation may need to be understood precisely so that correct hyperparameters and flags can be used. Yet, in a black-box setting, models have not been explicitly trained to leverage this information. Therefore, just as training on incorrect-correct code pairs can improve program repair, we believe that direct training can be very beneficial for code reuse and retrieval-augmented generation. Execution information could also be useful, as code reuse often requires understanding the situation well enough to identify subtle differences between the context of the retrieved code and the current context. 

\textbf{Retrieving via code navigation on the fly}: Standard retrieval-augmented methods keep a large retrieval index containing millions of embeddings, which can require a high one-time cost to create. As the codebase evolves, these embeddings may also need to be continuously updated. Instead of keeping track of embeddings, another approach is to find retrievals on the fly by navigating the codebase. We can imagine an agent that learns to use command line functions such as \texttt{cd}, \texttt{ls}, and \texttt{grep}, as well as IDE functions such as jumping to function definitions or finding all references of a function. Static analysis tools can also be paired with the agent to improve code navigation, such as providing the abstract syntax tree (AST) or file structure of a codebase.





% \subsection{Training: New Objectives and Synthetic Data}



% \textit{Reinforcement Learning (RL)}: 
% RL has delivered considerable improvements in isolated but challenging algorithmic  problems~\cite{deepseekai2025deepseekr1}.
% A promising direction is to scale such reinforcement learning approaches to more mature real-world tasks by collecting execution-assisted gym-like programming environments~\cite{jain2024r2e, pan2024trainingsoftwareengineeringagents}.
% Particularly, we believe it will be important to incorporate tasks with high construct validity and consider ways of mitigating and model task ambiguity~\cite{shao2024collaborativegym}. \todo{}

% \textit{Steering away from anti-patterns}: In software engineering, there are certain anti-patterns that frequently lead to bugs. For example, common weakness enumeration (CWE) is a categorization of software and hardware weaknesses often leading to vulnerabilities. Because publicly available GitHub code often contains code with anti-patterns, bugs, and CWE vulnerabilities, LLMs often write code susceptible to these issues \citep{asare2023github, fu2023security}. We hypothesize that explicitly steering models against these vulnerabilities will lead to more secure and correct code. One way to do this is to collect a large number of program samples violating each CWE (either synthetically or on GitHub) and then use these samples as negative signal during further supervised fine-tuning or RL stages.

% \naman{this is generally fine -- but lately i have been thinking if supervising these tasks manually is necessary or can the model learn them as necessary in end-to-end RL like I think training for algorithmic-debugging with RL might be instilling some level of execution world-model}


% \textit{Synthetic Data}: In contrast to text, code contains strong, verifiable feedback such as test cases, program execution engines, and other symbolic tools. This structure and feedback makes synthetic data generation a viable way of gathering more samples, as it is possible to first generate a large batch of data and then filter out the low-quality ones with the feedback. Therefore, it is possible to have a set of baseline quality guarantees on synthetic data. For example, to generate code with interesting program invariants, we can sample a large batch of programs, run an invariant detection engine, and retain only programs with interesting invariants. While synthetic data in code has mostly been at the function-level scope, there are no fundamental bottlenecks to expanding to larger scopes. As code is quite compositional, individual building blocks can be combined to generate complex synthetic data at the repository-level scope, which can be very helpful in both training and evaluation. 

% In DSLs, where programs can be cleanly described with semantics and rewrite rules, one can generate programs with desired properties via sampling. This technique has been successfully applied to make considerable progress in difficult reasoning tasks such as ARC-AGI \citep{li2024combining} and math olympiad problems \citep{trinh2024solving, deepmindalphaproof, chervonyi2025gold}.

\subsubsection{Integration with SWE Development Frameworks} \label{sec:d-swe-integration}

\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
Integrating AI with SWE development frameworks is critical for practical applications and impact on developer workflows. While software development is inherently integrated with tools, workflows, scaffolding, and meta-code, these are often absent from source code and scarce in AI training data. Ensuring that AI deeply understands software deployment beyond code editing is crucial, as writing code is only a small part of the development cycle. These can include automated reviews, deployment risk assessments, and documentation generation. We can also fine-tune LLMs to recognize and avoid known software anti-patterns such as CWEs. 
\newline \\
\textit{Challenges addressed: \ref{sec:c-long-horizon}, \ref{sec:c-large-scope}}
\end{tcolorbox}

\textbf{Incorporating AI into the CI/CD process}: In continuous integration and continuous deployment (CI/CD), automated pipelines are the backbone for building, testing, and deploying code changes. CI/CD accelerates feedback cycles and minimizes integration issues. AI offers several integration points within CI/CD. AI-powered code review tools can be incorporated into CI pipelines to automatically identify and flag style violations, potential security vulnerabilities, and code smells before human reviewers are involved. Furthermore, AI can provide intelligent deployment risk assessments. By analyzing code changes, test outcomes, and historical deployment data, AI can predict the likelihood of deployment issues, informing decisions about whether to proceed with automated deployment or mandate manual verification steps. Finally, AI can automate the generation of release notes by summarizing commit messages, issue tracker data, and relevant code modifications within the CI/CD process.

\textbf{Steering away from software anti-patterns}: In software engineering, certain anti-patterns frequently lead to bugs. For example, common weakness enumeration (CWE) is a categorization of software and hardware weaknesses often leading to vulnerabilities. Because publicly available GitHub code often contains code with anti-patterns, bugs, and CWE vulnerabilities, LLMs often write code susceptible to these issues \citep{asare2023github, fu2023security}. We hypothesize that explicitly steering models against these vulnerabilities will lead to more secure and correct code. One way to do this is to collect a large number of program samples violating each CWE (either synthetically or on GitHub) and then use these samples as negative signal during further supervised fine-tuning or RL stages.


\subsubsection{Incorporating SWE Tools} \label{d:sec-agents-tools}
% static symbolic analysis?
% \textit{Learned Tool Usage}: To improve the tool use capabilities of AI agents (Sec. \ref{sec:tool-use}), we should teach agents to understand the intricacies of a variety of tools so that they can autonomously invoke them as needed when writing code. Drawing inspiration from learning how to play games, we imagine a potential RL approach where the model can learn the strengths and weaknesses of each tool by trying it out at various points in code writing.

\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
Software engineers integrate a variety of domain-specific tools when writing code. By repeatedly interacting with tools in an RL-style manner, AI can develop the ability to do the same. Beyond tool use, using neurosymbolic approaches such as incorporating program analysis and type-checking can also help enhance LLM capabilities.
\newline \\
\textit{Challenges addressed: \ref{sec:c-tool-use}}
\end{tcolorbox}

\textbf{Learning to use SWE Tools}: As mentioned in Sec. \ref{sec:c-tool-use}, we believe SWE agents should understand the intricacies of programming tools and be able to autonomously invoke them as needed. There are three skills to learn: which tool to use, how to use the tool, and how to incorporate the results of the tool. Similar to how models learn to play complicated games, we believe that intelligent tool integration can be learned through repeated interactions with the tool in a RL-style manner. One way we envision this is as follows: first, the interface of the tool must be precisely specified. Next, data containing repeated interactions from the tool (with varying degrees of success) should be collected. Finally, multiple rounds of RL and expert iteration can be done to improve understanding of the tool and learn from misuses. 

% Agents and tool use have recently become a popular paradigm for tackling complex tasks such as programming. However, the majority of agentic coding systems use tools in a relatively primitive way (Sec. \ref{sec:tool-use}), and the scope of tools currently used is quite narrow. In addition, the use of tools is often superficial, such as feeding compiler feedback into the model. While tools are an integrated part of the human programming workflow, today's agents do not yet autonomously decide how and when to use each tool. To be more successful, we believe that these agents should understand the intricacies of each tool and be able to autonomously invoke them as needed while performing software engineering. 


Evidence that learning higher-level strategies might be possible is that through test-time techniques, OpenAI's \texttt{o3} model learned to write brute-force solutions to verify the correctness of more complicated solutions \citep{el2025competitive}. We envision that after learning to use tools, AI coding agents can autonomously invoke tools as needed to improve its overall world model of the code and hence its software engineering capabilities.

\textbf{Neurosymbolic Approaches}: Code is a unique domain because there is a vast body of techniques from programming languages (PL) research to build off of, but the majority of AI for code research today does not leverage the symbolic properties of code. Some of these PL techniques are as follows: abstract interpretation \citep{cousot1977abstract} is a technique to compute over-approximations of program state in order to prove the soundness of program properties at points in the code. Concolic testing \citep{godefroid2005dart, sen2005cute} finds bugs in software by combining concrete and symbolic execution. Model checking \citep{clarke1997model} is a way to prove properties of execution traces via temporal logic. Linting and type-checking \citep{cardelli1996type} provide a static check to ensure that variables, expressions, and functions adhere to a programming language's rules. Finally, many other program analysis algorithms leveraging these tools have been designed to prevent bugs and ensure code correctness properties.

Traditional PL approaches have a few common shortcomings, which overlap with some of the issues mentioned in Sec. \ref{subsec:formal-verification}. First, they often require very complete and precise specifications. Many tools need to have specifications for all library functions, need to specialize to a precise version of the language, and need to specialize to the build system. Second, there is often a high computational cost due to the large search space. Third, there can be many false positives due to the limitations of the tool. We believe that deeply integrating these symbolic tools with LLMs can partially mitigate these challenges. 

We provide a few examples of this potential integration. When generating code, program analysis techniques could be applied on shorter snippets of AI-generated code to surface potential bugs or prove properties of the generated code. To improve general code understanding, LLMs can be trained with information about program structure such as abstract syntax trees \citep{gong2024ast}. When debugging a large codebase, when the scale is too large to directly apply PL techniques, AI could be first used to narrow down potentially problematic sections of the code which are then handed off to PL tools for debugging. During code generation in DSLs, LLMs can leverage the grammar of the programming language to do constrained decoding~\citep{poesia2022synchromesh, geng2023grammar, wei2023copiloting} to mitigate syntactic errors. During code refactoring, abstract interpretation and static analysis can be used to identify whether new errors have been introduced and preemptively cut off unpromising search paths.

\textbf{Deductive Synthesis and Intermediate Languages}: Early program synthesis relied on \textit{deductive synthesis} approaches \citep{burstall1977transformation}, where programmers would write a clean simple implementation and then apply transformation rules to convert it into a more efficient one. The appeal of deductive approaches is that because these rewrite rules are semantics preserving, there is a correct-by-construction guarantee. One success story of deductive synthesis is Spiral \citep{puschel2005spiral}, a DSL for signal processing kernels that takes advantage of domain-specific transformation rules to produce implementations beating expert hand-crafted code. Another example is Halide \citep{ragan2013halide}, a DSL for high-performance image and array processing code. Due to the difficulty of writing optimized code, humans generally opt for writing code in these intermediate DSLs, and we find it promising for LLMs to do the same.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt]
\textit{Example. LLM-aided Compilation for Tensor Accelerators}: As an example, \citet{hong2024llm} consider the task of generating highly optimized, hardware-efficient code for a tensor accelerator from a high-level specification of a computation (e.g. C++ code). Their pipeline works in two steps: first, the high-level specification is translated to a DSL. Then, the DSL code is symbolically compiled to hardware-specific instructions. The LLM is also used to optimize the DSL code via a cost model driven search, where it suggests rewrites and scheduling operations (e.g. loop reordering) that guarantee semantic equivalence.
\end{tcolorbox}


\subsubsection{Scaffolding Human Supervision}
\label{d:sec-human-supervision}
\begin{tcolorbox}[colback=lightorange, boxrule=0pt, arc=5pt, outer arc=5pt]
At inference time, most machine-generated code will be presented to humans in a format shaped by the human-AI interface design. Since AI may be responsible for generating the majority of the code within a human-AI team, it is important to ensure human control and oversight. By scaffolding human supervision with techniques like summarization and interactive verification, we could potentially improve trust in AI-generated code.
\newline \\
\textit{Challenges addressed: \ref{sec:c-hai-colab}}
\end{tcolorbox}
Once code LLMs are deployed for inference, it is crucial to scaffold human supervision of AI-generated code. This goes beyond merely enhancing the accuracy of AI-generated code, as humans often still need to make the final decision on whether to accept the code or understand it for future integration and maintenance. A study on Github Copilot usage~\citep{10.1145/3551349.3560438} revealed that programmers tend to allocate less visual attention to AI-generated code. %, which further highlights the importance of designing AI systems that scaffold human oversight while reducing cognitive load during code review.
While one solution is to train humans to better identify issues in AI-generated code~\citep{10.1145/3627217.3627238}, a more desirable approach is to design AI systems that scaffold human supervision, reducing their cognitive load when reviewing generated code.

% This scaffolding principle has proven successful across domains.
One way to achieve this is by enriching AI-generated content with additional contextual information. Modern LLM chatbots now routinely generate text with citations for knowledge-intensive queries. In Collaborative STORM~\citep{jiang2024into}, researchers demonstrated that dynamically presenting hierarchical ``mind maps'' alongside the actual collected information significantly enhanced human-AI collaboration, particularly in long sessions. In software engineering specifically, \citet{sun2024source} highlighted the benefits of high-quality source code summarization in aiding software developers in understanding and maintaining machine-generated code. Second, interactive approaches can also enhance supervision. One example is \textit{Live Programming} \citep{ferdowsi2024validating}, a continuous display of the runtime values of a program, as a means of lowering the cost of validating AI-generated code. However, these existing studies are largely limited to specific programming languages and small codebases. Finally, improving the readability and interpretability of AI-generated code itself presents a promising direction. For example, \citet{pu2020program} showed that modeling program synthesis as rational communication improved end-user interpretation and subsequent communication of code. Expanding on these ideas, future research should prioritize human interpretability in the design and optimization of AI coding systems, fostering greater trust and control in AI-assisted software development.

%We envision expanding these approaches while prioritizing human interpretability in the design and optimization of AI coding systems, offers a promising path to enhancing trust and control in AI-assisted software development.



% Similarly,  Scaffolding human supervision can also be interactive. 

%\subsection{New Human-AI Paradigms}
%\label{sec:d-human-ai}










% \subsection{Developing a World Model for Code}

% \todo{data to data vs. modelling. like javascript DOM. multimodal aspects can help. but scalable way to generalize. multimodal world model. }

% \todo{put continual learning here}

% In Sec. \ref{sec:c-global-understanding}, we discussed how LLMs do not have a global understanding of codebases. We believe it is important for a coding language model to have a good ``world model'' for code in the same way that students completing a programming course will gain a holistic understanding of coding. 
% Especially in OOD domain tasks (Sec. \ref{sec:c-ood-domains}), code understanding will be crucial in order to make novel advances.


% \textit{Broadening the Training Task Distribution}: Another way to steer code models to have a more general understanding of code is to widen the distribution of tasks that models are exposed to during training, such as including the tasks in Sec. \ref{sec:tasks-milestones}. In particular, training could include more tasks that improve a model's semantic understanding, such as describing errors in subtle bugs or predicting the execution states of a program. As an example, \cite{li2025codei} find that training models on input-output prediction data leads to consistent improvements on reasoning tasks. This training could also be done in a curriculum-like manner to gradually improve the world model capabilities.

% \naman{this is generally fine -- but lately i have been thinking if supervising these tasks manually is necessary or can the model learn them as necessary in end-to-end RL like I think training for algorithmic-debugging with RL might be instilling some level of execution world-model}

% That being said, we do not believe that it is necessary for LLMs to have a perfect world model of code. In mathematics, we do not expect LLMs to perform hundred digit multiplication problems. However, we might expect them to be able to estimate the order of magnitude of the result. Similarly, perfectly predicting the program state of a piece of code after every line is not necessary, but having a high-level understanding of a program's execution flow is expected. A code LLM should have this general world model of code, enabling it to then intelligently use tools like debuggers and code executors for instances that require more precision. 
