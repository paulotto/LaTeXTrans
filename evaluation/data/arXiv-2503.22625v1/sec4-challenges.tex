\section{Challenges} \label{sec:challenges}

% \todo{say challenges span many of these tasks; (Make the transition from sec 2 to sec 3 smooth)}

While the field of AI for code has made fruitful progress, cutting-edge AI still struggles with SWE tasks, especially at larger scopes and higher levels of logical complexity. Next, we discuss ten key challenges in AI for code. Each challenge spans multiple tasks, and progress on any can lead to improvements on many tasks at once.
% We order these challenges roughly by how easy it is to resolve them.}

\subsection{Evaluation and Benchmarks} \label{sec:c-evaluation}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Today's code LLM evaluations focus on a narrow set of tasks, suffer from potential contamination, and do not reliably measure real-world software engineering abilities.
\\
\newline
\textit{Potential solutions}: \ref{sec:d-data}
\end{tcolorbox}

Our taxonomy of tasks and measures highlights some of the shortcomings of today's evaluations and benchmarks. For example, the majority of today's coding evaluations have no level of human intervention, with a few, such as Copilot-Arena \citep{chi2025copilotarena}, having low to medium autonomy. 
% \ms{nit: copilot-arena style evals}
HumanEval, MBPP, APPS, CodeContests, and LiveCodeBench are all at function-level scope, with low to medium-high logical complexity. 
Commit0 \citep{zhao2024commit0}, SWE-Bench \citep{jimenez2024swebench}, TestGenEval~\citep{jain2024testgeneval}, RefactorBench~\citep{gautam2024refactorbench}, SWE-Lancer \citep{miserendino2025swe} are at project-level scope with low to medium logical complexity. %Today, we lack benchmarks 1) at project-level scope with high logical complexity, 2) for many tasks other than code generation, and 3) requiring human intervention.

\textbf{Task Diversity and Capability Isolation}: 
Current coding evaluations primarily focus on the code generation task, while most of the tasks discussed in Section~\ref{sec:tasks-milestones} are either not studied such as Code QA or only studied in limited scopes like EvalPerf~\citep{evalperf}, vulnerability detection~\citep{mei2024arvo}, formal verification~\citep{sun2024clover}. 
As more agent-based approaches are introduced for software engineering (e.g. pairing a code generation model with a debugging model), these engineering-related capabilities beyond just code generation will be important towards designing a maximally performant system. 
Solely relying on end-to-end coding evaluations that focus on the overall correctness of a codebase makes it difficult to precisely measure progress and learn from the failure modes on individual tasks.


\textbf{Contamination}: 
Data contamination is a serious issue that, if not taken into account, can affect the soundness of various conclusions drawn from a set of benchmark results. In coding, the performance of LLMs on competitive programming \citep{xu2024benchmark, jain2024livecodebenchholisticcontaminationfree} and SWE-Bench \citep{aleithan2024swe} tasks has been shown to degrade over time, indicating the possibility of older problems being contaminated due to public exposure on the internet. For simpler function-level HumanEval style problems, \citet{matton2024leakage} suggest three potential causes of contamination: direct data leakage (benchmarks are on GitHub), synthetic data leakage (there are only a limited number of interview problems), and overfitting to test sets (benchmark hacking). In addition, for code, contamination can be hard to detect, as semantically equivalent code that is syntactically distinct could be thought of as contamination \citep{riddell2024quantifying}. A recent benchmark, the Konwinski Prize\footnote{\url{https://www.kprize.ai/}}, is a promising way to fairly evaluate SoTA LLM models by only using new GitHub issues.

\textbf{Construct Validity}:
Construct validity refers to how closely a measurement reflects the underlying concept. 
Given the implications of rapid performance improvement in the AI for the code domain, it is essential to have high-construct validity benchmarks evaluating how well programming agents can perform.
While benchmarks like SWE-Bench come close, user experiences do not currently match rapid performance gains obtained from them. 
This is partially because many desiderata in software engineering cannot be described cleanly via automated unit testing. Things like multi-turn code generation, designing an UI, and writing clean and idiomatic code are all difficult to quantitatively measure with precision. Designing reliable proxy metrics for these desired goals remains a challenge. 

% \todo{most of SWE cannot be described via automated unit testing WD}

% Existing works that focus on 

% \todo{talk about challenge in programing systems community to measure the success of new PLs. notorously dificult to eavluate. User studies fail to capture maintainability, learning effects.}



%\todo{I wonder if it makes sense to make a list of benchmarks and which category they belong to}

% Given the rapid pace of progress of LLMs, 

% Machine learning progress is in a large part guided by making steady progress (``hill climbing'') on some set of benchmarks.
% For example, \naman{ImageNet served ...}.
% Similarly, AI for code has witnessed a rising interest in the development of better coding evaluations

% Thus the choice of benchmarks developed and used in this process 

% Machine learning approaches are typically about optimization and progress is made through improving (``hill climbing'') on benchmarks. 
% Therefore, developing appropriate benchmarks that allow generalization to 
% the choice of benchmarks developed or used during the machine learning life-cycle is very important.
% Here, we draw upon learnings from prior works and present directions for 

% The rate of progress in artificial intelligence is considerably aff


\subsection{Effective Tool Usage} \label{sec:c-tool-use}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
While software engineers use a wide suite of programming tools when programming, most of today's AI coding systems do not invoke tools. AI needs to be able to select which tool to use, decide how to use it, and interpret the outputs in order to continue making progress on the task.
\newline \\
\textit{Potential solutions}: \ref{d:sec-agents-tools}
\end{tcolorbox}

Software engineering has witnessed the development of various open and proprietary tooling support for programming, debugging, analysis, and code management over time.
% Software engineering is a long-standing field and has support for mature tooling support f
% which has led to development of various mature tooling to support developers in programming and debugging tasks. 
For example, program analysis tools provide static and dynamic assurances on code correctness. 
Print statements and debuggers are used for dynamically analyzing and debugging programs at a fine-grained level.
Beyond programming, such tools are richly integrated into the entire software development lifecycle, e.g., code navigation or search, reviewing code, CI testing.

There have been efforts combining LLMs with tools such as calculators and search engines \citep{schick2023toolformer, patil2023gorilla}. 
However, effective integration of LLMs with software engineering tools is a more challenging problem. 
Several early works have incorporated such tool feedback in code generation in an automated fashion, for example, linter or execution feedback in \citep{olausson2023self, zhong2024ldb, gehring2024rlef}.
However, these works do not actively \textit{interact} with tools. 
More recently, programming agents have started incorporating tool use within their workflows termed as Agent-Computer-Interface~\citep{yang2024sweagent}. 
These tools range from aiding in general search (\texttt{grep}), providing code editor for making changes \citep{openhands,claude35swebench}, language server for static analysis \citep{liu2024marscode},
dependency analyzer \citep{bairi2024codeplan}, terminal access for bash commands including code execution ~\citep{yang2024sweagent}, debugger \citep{bigsleep}. 
% For example, ~\citet{sonnet35swe, openhands} uses string replace code editor, some of these tools at increasingly rapid pace, but their use is relatively limited \citep{openhands, yang2024sweagent}.

% \todo{Cite \citep{deshpande2024class}}

% \todo{More SWE-Agent stuff? [Naman]}

\textbf{Dynamic and Effective Tool Usage}: While many efforts combine LLMs and agents with tools, they do not achieve fully dynamic and effective software engineering tool usage. This involves an AI system seamlessly and proactively integrating appropriate tools depending on the task at hand. There are a few challenges towards achieving this goal. First, the AI system must identify which tools could potentially be useful for the task at hand. Second, the system then needs to decide when to invoke the tool. A complex debugging task might require the use of \texttt{pdb} or \texttt{gdb} to track intermediate program states, while looking at input-output pairs may be sufficient for simple debugging tasks. Third, the agent then must figure out how to invoke the tool. If the agent knows that a certain function in a program has an error, it may wish to walk through only that function instead of the entire code from start to finish. Finally, the agent needs to incorporate the output provided by the tool in order to inform its next steps, e.g. edit the code if a bug was uncovered or run the tool again otherwise.

% \textit{Example: Performance Instrumentation}: CSI \citep{schardl2017csi} is a way to instrument software by programatically inserting hooks to track objects such as memory loads/stores and function entry/exits to measure code coverage and performance. For an LLM agent to use CSI effectively, it must 1) learn the CSI API, 2) decide when and how to use CSI (such as when it writes a suspected performance bottleneck), and 3) use the CSI output to decide how to proceed.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Performance Instrumentation}: A common way to instrument software systems is known as \textit{compiler-inserted program instrumentation}. CSI \citep{schardl2017csi} is a tool that inserts instrumentation hooks to track objects such as memory loads/stores, function entry/exits, and basic blocks. CSI contains tools like code coverage tools, a memory-operations counter, a performance profiler, and a call-graph generator. To use the tool, the user must follow the API in order to write hooks so the correct aspects can be profiled. Tools like CSI are very valuable when trying to improve the performance of a piece of code, but are not trivial to use. In order for an LLM agent to use CSI effectively, it must first familiarize itself with the CSI API. Then, it needs to know exactly which aspects of the code to instrument, such as placing hooks before and after a function suspected to be a bottleneck. Finally, the agent needs to learn how to use the output of the tool to inform its approach to the task, such as deciding whether a block of code can be further optimized after seeing its performance profile.
\end{tcolorbox}

% \textbf{Leveraging program analysis}: Apart from LLMs, there are a wide variety of program analysis tools designed to prevent bugs and ensure code correctness properties. Abstract interpretation \citep{cousot1977abstract} is a technique to compute over-approximations of program state in order to prove the soundness of program properties at points in the code. Concolic testing \citep{godefroid2005dart, sen2005cute} finds bugs in software by combining concrete and symbolic execution. Model checking \citep{clarke1997model} is a way to prove properties of execution traces via temporal logic. Linting and type-checking \citep{cardelli1996type} provide a static check to ensure that variables, expressions, and functions adhere to a programming language's rules. 


% There are a lot of programming and debugging tools that people use that most AI programming systems probably don't leverage or know how to use effectively:

% \begin{itemize}
%     \item Print statement debugging.
%     \item Debuggers (pdb/gdb).
%     \item Linters.
%     \item For optimization, things like Valgrind.
% \end{itemize}

\subsection{Human-AI Collaboration} \label{sec:c-hai-colab}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Human-AI collaboration is still far from seamless. First, specifications written by humans are often vague and leave out many details, leading LLMs to produce code misaligned with humans. There is also very little controllability with coding LLMs, and today's human-AI collaboration interfaces are still limited.
\newline \\
\textit{Potential solutions}: \ref{d:subsec-human-data}, \ref{sec:d-hai-training}, \ref{d:sec-human-supervision}%\ref{sec:d-human-ai}
\end{tcolorbox}


While AI coding systems are increasingly more powerful, the majority of them are still at a low to medium autonomy level, serving as engineer assistance rather than achieving high or full automation. We identify a few key challenges of today's AI coding systems that prevent these systems from working with humans effectively at higher levels of autonomy.

\textbf{Vague Specifications and User Misalignment}: When using code LLMs or coding agents, we typically prompt them with a natural language specification. This can include a natural language description of the desired code, input-output examples, relevant code snippets, and other functional requirements. However, there is a gap in the level of abstraction between English and code, leading to incomplete or ambiguous specifications. This issue becomes more pronounced in longer programs, where the number of ambiguous decision points increases, and choices traditionally made by humans are instead implicitly embedded in the LLM’s generated code. Consequently, users often experience misalignment due to vague specifications. While many code LLMs support multi-turn interactions, it remains inherently challenging for users to articulate their thought processes into follow-up natural language instructions.

\textit{Specifications beyond text}: While today's specifications predominantly rely on text, there are many domains for which pure text is insufficient as a specification. In domains like robotics, virtual reality, embedded devices, and user interfaces, specifications often need to be multi-modal (e.g. showing the model a picture of an UI to create) and world-interfacing (e.g. providing simulation code describing a robot will interact with its environment).

\textit{Inherent trade-offs in software development}: Designing large software systems always surfaces trade-offs between various desiderata such as readability, scalability, performance, maintainability, reliability, security, etc. These trade-offs are often context-dependent. A long-term and rapidly moving project may be willing to trade off some performance to have simplicity and readability. Performance-critical applications may completely sacrifice readability to eke out every millisecond of speed 
(such as using \href{https://graphics.stanford.edu/~seander/bithacks.html}{bit-twiddling hacks}).
Finding the sweet spot among these trade-offs can often involve extensive prototyping and benchmarking to understand the performance characteristics of different approaches. However, user specifications in the initial prompt rarely include details about these trade-offs, nor do models often take them into account.

\textit{Implicit constraints}: Aside from functional/semantic correctness, there are also often implicit constraints in writing code. For example, many companies such as \href{https://opensource.janestreet.com/standards/}{Jane Street} and \href{https://google.github.io/styleguide/}{Google} have style guides, and many GitHub repositories explicitly outline style elements that new code ought to follow. \cite{zou2019does} find that GitHub pull requests that are more consistent with the style of the existing code get merged faster. Additionally, corporations may enforces codes of conduct or \href{https://developers.google.com/checks/guide/code-compliance/overview}{compliance requirements} at the code level. Furthermore, codebases follow common programming patterns or system design patterns that are implicitly specified by the way the current code is written. However, when using code LLMs, these constraints are often inferred incorrectly \citep{wang2024beyond}.
% \naman{good point but would nitpick that models probably learn "style" quite well.. abstractions i think less so?}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Serializer-Deserializer pattern for objects:} Consider the issue \href{https://github.com/astropy/astropy/issues/14181}{astropy-\#14181} from the \texttt{astropy} Python library. The issue requests support for a new input file format (reStructuredText) to load astronomical data into the codebase more flexibly. While the issue does not mention it explicitly, as per common practices, developers implement read (deserializer) and write (serializer) operations when implementing support for a new file format. This ensures data can flow bidirectionally between the file format and the application's internal data structures. However, models evaluated on this issue, as part of the SWEBench benchmark, only implemented the \texttt{read} method.
\end{tcolorbox}

% As discussed in Section \ref{sec:c-hai-colab}, today, unless the specification is overly vague, code LLMs prioritize a functionally correct code snippet without clarifying ambiguities with the user.
% We envision a future where the LLM can proactively surface these decision points and defer back to the user for their resolution.

% If the autoformalization is inaccurate, it introduces another source of potential misalignment, even with the use of formal specifications.

\textbf{Lack of Controllability}: When using AI coding systems, programmers often seek specific functionality, yet they lack reliable ways to steer LLMs toward generating precisely the desired code. Instead, they typically rely on a trial-and-error approach, repeatedly sampling outputs or providing feedback until the AI produces an acceptable solution. Consequently, significant human effort is still required to review and modify the code to ensure it meets the intended functionality \citep{weisz2024examining}.

A way to improve controllability is for AI coding systems to recognize when human input is needed and communicate effectively—yet this remains the top-reported challenge in human-agent collaboration~\citep{shao2024collaborative}. LLMs rarely defer to humans for clarification, while developers often ask questions to clarify the description of a task provided by their peers. For example, when a product manager refines a requirements document, developers who are unclear about the scope or specifications ask questions and leave comments, which the manager resolves iteratively to disambiguate requirements \citep{nahar2022collaboration}. Based on its knowledge of existing software, AI should be able to incorporate implicit priors from a specification while keeping the user in the loop. For instance, when designing an academic website, certain expectations—such as including a list of publications and contact information—are implicit. However, whether to include a person’s GPA would require explicit clarification.

\textbf{Restricted Human-AI Interface}: Existing interfaces for code LLMs primarily manifest as intelligence features embedded within integrated development environments (IDEs). \citet{treude2025developers} establishes a taxonomy of developer-AI tool interactions, emphasizing low-level support mechanisms such as auto-complete suggestions, selection-based enhancements, and conversational assistance within the codebase context. While this taxonomy comprehensively covers existing AI coding systems that function primarily as engineering assistants, its applicability becomes questionable as these systems advance toward higher levels of automation. For instance, the ubiquitous ``Tab'' interaction paradigm prevalent in intelligent IDEs may prove inadequate when AI systems transition from completing developer-scaffolded functions to authoring the majority of the codebase autonomously.

Current interfaces for coding agents, such as \href{https://devin.ai/}{Devin}, typically stream raw actions without adequate context or explanation. Given that these agents can execute numerous actions rapidly, developers face significant challenges in effectively monitoring the process, implementing timely interventions, or reasserting control when necessary.  This lack of transparency can also undermine trust in AI-generated code~\citep{10.1145/3630106.3658984}. While human-AI interface design has received extensive attention in autonomous vehicle research \citep{benderius2017best,tinga2022human}, similar consideration for AI coding systems remains notably absent.

% \textbf{Maximizing information from human interactions}: 
% - Identifying when human input is necessary

% - Maximizing information from human interactions (pragmatic stuff)

% - Support for multimodal (Zoom call with AI agent + screen share)

% {\color{blue}Fully autonomous software development remains an open challenge. Therefore, a better interface / workflow for human oversight is essential to integrate AI into development workflows effectively.}



% \begin{itemize}
%     % \item Lack of controllability, developers don't have reliable ways of model steering. Things are very guess and check right now, sample and test.
%     % \item From HCI perspective, we haven't found the right interface between humans and programmers.
%     % \item Generally, humans still have to check if AI copilot-written code is correct, people often say they spend more time debugging AI code than it would have taken to just write it themselves.
%     \item Still a lot of differences in human workflow vs. AI agent workflow. e.g. when debugging a pipeline with a large transformer, a human might use a smaller transformer to make loading time faster.
%     \item How can AI be used to augment humans rather than replace humans?
% \end{itemize}


\subsection{Long-Horizon Code Planning} \label{sec:c-long-horizon}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Large software engineering projects often require long-term planning about the design and structure of the code. LLMs struggle at two key aspects of this: designing good, lasting abstractions and respecting modularity and code quality principles.
\newline \\
\textit{Potential solutions}: \ref{sec:d-rl}, \ref{sec:d-swe-integration}
\end{tcolorbox}

When working on large projects, engineers and tech leads often make complex decisions about how to design and structure the code to best support the various functionalities that will eventually be needed. To build a long-lasting software system, an engineer must know the potential paths that the system's evolution might take. This requires domain expertise and experience in how different code structures require different forms of extension. We believe that today's language models are unable to perform this level of sophisticated planning.

\textbf{Designing Good Abstractions}:
One instance of long-horizon code planning is choosing the right abstractions from the outset. An API designed with good abstractions will allow new features to be implemented seamlessly with minimal user overhead, while an API designed with poor abstractions may lead to excessive code duplication, refactoring, or even debugging. We discuss two examples of this, library learning and data representation.

\textit{Library learning}: Designing APIs and libraries are designed with useful abstractions often leads to more code reuse and more intuitive interfaces. The challenge of library learning is to derive a library of useful abstractions from a corpus of programs by abstracting out common reusable features \citep{ellis2021dreamcoder,10.5555/3692070.3693967,10.1145/3571234}. While the traditional library learning literature has focused primarily on code reuse, a truly effective library must also prioritize ease of use and maintainability, as well as be robust and adaptable to future extensions. %within the software system.

\textit{Data representation}: The choice between data structures leads to a variety of trade-offs when it comes to performance aspects such as memory usage and processing speed. For example, database engineers need to decide between various data models, storage formats, and indexing methods to balance performance. 

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Database Design for Web Applications}: Database engineers strive to design their databases in a way that minimizes memory usage and maximizes query performance (speed). To achieve this goal, the databases community has spent considerable efforts optimizing both the high-level data representation and the underlying data structures \citep{kraska2018case, hawkins2011data}. Consider the task of designing a database schema for a restaurant owner to manage their business: keeping track of customers, managing a rewards program, maintaining the restaurant's inventory of ingredients, etc. One important design decision to make is deciding on a schema: while having a \texttt{reservation} and \texttt{customer} table is fairly straightforward, should we include a table for customer reviews or simply add rating and review fields in the \texttt{customer} table? Another important design decision is choosing which database indexes to include. While choosing the correct indexes can speed up queries significantly, indexes cost additional memory and must be kept updated. Making decisions like these requires knowledge of the application, context, and the effects of each option.
%\naman{the spirit of example is nice but just maybe a bit toyish?}
\end{tcolorbox}

\textbf{Modularity and Code Quality}: %\todo{written code may not be of high quality, lack modularity}
LLMs are trained and optimized primarily for code correctness with insufficient focus on other aspects of code like quality and maintainability. This is further exacerbated with large scale reinforcement learning being performed using test cases which can lead to unintended consequences regarding code quality, as correct but poor quality code is still often given a high reward.
Empirically, it has been observed that LLM written solutions are often more complex than human-written counterparts. 
% \todo{cite? personally observed this a bit in competition coding where even o1 solutions are wayyy more complex. WD: can we cite the paper "Library Learning Doesn’t: The Curious Case of the Single-Use Library" saying library reuse is not trivial for LLM? \url{https://arxiv.org/pdf/2410.20274}}
For example, \citet{jain2024r2e} identified that LLMs prefer to repeat existing code instead of making use of abstractions in the existing code. 
One aspect of code quality is modularity, ensuring that code does not get duplicated too often. Here, \citet{berlot2024library} identified that library or tool reuse is non-trivial for LLMs in coding and formal math domains. 

% \todo{Comments from Jiawei
% \\
% - “Modularity and Code Quality” …. code quality is pretty important (to me at least) and could have a lot of connection with many sections in the manuscript:
% \\
% - Reward: R1-Zero like approaches tend to produce ugly code as code quality is not modelled as reward — it’s also hard to be modelled — except for using static analyzers to screen CWE issues such as CodeGuru, CodeQL, Bandit — these signals diversifies the reward (we have wip projects on this).
% --> {naman - empirical}
% \\
% - Also code quality might not be planned but post-processed — so I feel this is a bit more related to code review/editing/transformation.
% }
% \citep{kang2024revisiting}


% \textbf{Code Debugging}:
% \naman{
% Bug-fixing can be particularly challenging since the bugs may present themselves in considerably higher-order effects of the underlying issue.
% For example, \todo{find a swebench or github issue amplifying this aspect}.
% Solving bugs from such bug reports requires reasoning and codebase understanding to localize the cause of such underlying failure. \todo{this should be well-studied in SE research, cite from there?}
% Next, fixing the issue requires rethinking the implementation to fix the issue. 
% These implementation changes should be carefully planned, attempting to come up with \textit{most general} version of the changes while also keeping in mind other parts of the codebase that are affected.
% }


% \textbf{Abstractions in Code Refactoring}: Being able to perfectly predict the evolution of a software project is nearly impossible, and even highly experienced senior engineers with domain expertise fail to design all the abstractions correctly on the first try. Therefore, software often has to undergo major global refactorings. Fundamentally, refactoring is a process of mapping components in the old abstraction to those in the new one in a way that maintains semantic equivalence. This can be difficult for language models because these mappings can be relatively complex: large sections of code can be consolidated or split, the relationships between components may be redefined, and successfully maintaining the semantics properly requires precision. These challenges are reminiscent of those faced by the programming languages community when designing good refactoring tools.
% \naman{hmm i think refactoring is "too concrete" to talk about planning/abstractions -- the abstract bit in refactoring is making design decisions keeping long-term in mind..
% the content here on maintaining mapping seems lower level changes
% }
% First, there is a specification issue, as writing a full specification for a refactoring can be extremely complex, if not impossible. Second, there is a long-context issue, as refactorings can involve 

% In a sense, refactoring is about swapping out one set of abstractions for a new set of abstractions that is easier to use or can better support new use cases. Refactoring can be difficult for LLMs for several reasons. 

%  Finally, refactoring requires a deep understanding of both the original set of abstractions and the new ones. 
 
% \todo{refactoring. highly experienced senior engineers with domain expertise fail to get the abstractions right on the first try. a lot of things have to go through major global refactorings. discuss some of the challenges that are hard to achieve through pure LLM. 1) lots of LOC, 10k+, context window; 2) describing new structure and how it relates to the old structure. this is one of the challenges of PL tools too. 3) hard to express refactoring with a simple specification. MSR agentic refactoring work.  }

% \todo{Other points about long-horizon code planning?}

% \naman{\textbf{Tradeoffs}: I think talking about tradeoffs should be useful here.. rich space with long-term feedback..
% also acknowledging humans don't get it right in first attempt and major software do refactor
% /VLLM just did a major refactoring for v1 release actually!!}

\subsection{Large Scope and Long Contexts} \label{sec:c-large-scope}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]

Coding is a domain where required context lengths can be very long, as codebases can consist of millions of lines of code, posing challenges to today's models. In addition, today's retrieval-based methods are still limited: models often retrieve incorrect information and can still struggle with leveraging retrievals effectively due to the difficulty of code reuse.
\newline \\
\textit{Potential solutions}: \ref{sec:directions-retrieval}, \ref{sec:d-swe-integration}
\end{tcolorbox}

\textbf{Large Scopes}: At the repository level, the tasks in Sec. \ref{sec:tasks-milestones} become significantly more difficult and require many steps. In code generation, user alignment can be an issue because there are many decision points and tradeoffs that can compound. In code refactoring, modifications will touch multiple parts of the codebase, and it can be tricky to keep the repository consistent. In code debugging, functions can be large and bugs can be nested deeply within stacks of function calls. In code navigation, because there are so many functions interacting in various ways, it can be difficult to know where each piece of functionality is implemented and how the code is pieced together.

Another issue with large scopes is large context lengths. Software engineering often requires dealing with very large codebases--for example, Google has repositories with over a billion lines of code \citep{potvin2016google}. As this is far too large for modern-day LLMs, choosing the correct context to include when using LLMs is important.

% Dealing with large context lengths has long been an outstanding challenge for LLMs, and progress has been rapid with Gemini providing over 1M tokens of context. 
% In coding, Magic has even trained models with 100M token context.
% While these models show good performance in toy benchmarks such as needle retrieval, we have not seen evidence of how this performance translates to large real-world code-bases.

% \textbf{The limits of long-context models}: Software engineering often requires dealing with very large codebases--for example, Google has repositories with over a billion lines of code \citep{potvin2016google}. 
% Dealing with large context lengths has long been an outstanding challenge for LLMs, and progress has been rapid with Gemini providing over 1M tokens of context. 
% In coding, Magic has even trained models with 100M token context.
% While these models show good performance in toy benchmarks such as needle retrieval, we have not seen evidence of how this performance translates to large real-world code-bases.


\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Debugging Cloud Applications}: Organizations often rely on monitoring and observability tools to track the performance of their applications. One such tool is Datadog, an observability service for cloud applications that can monitor infrastructure, detect security anomalies, and track database performance. For larger applications with more moving parts, these logs can consist of thousands of lines of JSON payloads. For humans, sifting through these logs is usually a matter of searching for certain keywords that they know will appear in the logs. However, LLMs often have a hard time interpreting large amounts of logs like these. 
% \todo{Check with Silas for concrete example?}
\end{tcolorbox}

% Examples of benchmarks measuring long-context coding capabilities are SWE-Bench \citep{jimenez2024swebench} and Long Code Arena \citep{bogomolov2024long}. These long-context problems are often solved by ...

% \todo{Summarize how long-context coding is dealt with in the literature.}

\textbf{Limits of Retrieval-Augmented Generation (RAG)}: Retrieval-based algorithms have been the predominant way to deal with long-context coding issues. First, the retriever retrieves relevant functions. Then, the generator leverages the retrieval to improve generation. While RAG has proven effective in many NLP tasks such as question answering \citep{gao2023retrieval, lewis2020retrieval}, the code domain provides new challenges for these methods.

% \textit{Retrieval}: In the code domain, it is often unclear what the correct item to retrieve is. In addition, code embeddings generally group code together via syntactic similarity \citep{ma2024unveiling, utpala2023language}, which can make it hard to retrieve crucial snippets that are semantically relevant but syntactically unrelated.

\textit{Retrieval}: In most NLP tasks, the retrieval step can be done relatively well because keywords that are in the query are often keywords that need to be retrieved. Unlike answering NL questions, writing code often requires drawing inspiration from code snippets that may be completely different syntactically. This can include programs with similar semantics, algorithms, or API calls, all of which potentially have very little in common when it comes to syntax. For example, the implementation of Dijkstra's algorithm in a GPS navigation application can guide the implementation of a shortest-path algorithm in a social media application. Because retrievers often rely on syntactic matching, these relevant programs can be hard to retrieve \citep{ma2024unveiling, utpala2023language}.

When deciding what to retrieve, it is also necessary to have a sufficient awareness of other parts of the codebase so that you know which building blocks are necessary to construct the new function. 

This can make the retrieval task relatively tricky, as shown by failure modes on two benchmarks, CodeRAGBench \citep{wang2024coderag} and BRIGHT \citep{su2024bright}.

%\todo{Example of bad retrieval}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Failure Case of Finding Relevant Files When Resolving Issues:} BM25, despite its widespread use in code retrieval, demonstrates limitations in scenarios that involve large and complex codebases. For instance, in \href{https://github.com/chartjs/Chart.js/pull/7951}{chartjs\_\_Chart.js-7951} from SWE-bench Multimodal~\citep{yang2024swe}, BM25 retrieval using the issue description returns suboptimal results. The top-3 retrieved files from \texttt{src/} are \texttt{src/scales/scale.radialLinear.js}, \texttt{src/scales/scale.linearbase.js}, and \texttt{src/helpers/helpers.canvas.js}. However, the critical modifications required to resolve the issue should occur in \texttt{src/elements/element.bar.js} and \texttt{src/controllers/controller.bar.js}. This retrieval failure impedes the effectiveness of coding agents, many of which are augmented with code retrieval systems. When agents focus their attention on irrelevant files, their ability to resolve the issue successfully becomes substantially compromised.
\end{tcolorbox}

\textit{Generation}: 
% In many NLP tasks, having the retrieval makes generation straightforward-- in question answering, the retrieval directly states the necessary piece of knowledge to answer the question. However, for code tasks, this is rarely the case.
In NLP tasks, the generation step often is a straightforward application of the retrieved information. However, in code, writing a new function requires more than copy and paste. This is closely tied to the problem of code reuse: piecing together relevant snippets of code in a precise and productive way to fit the current context. Depending on what is retrieved, each piece of retrieved content provides different information. This can include information about the language's syntax, documentation about the API, clues about the algorithm to be written, or examples of similar functionality being written. \citet{ding2023crosscodeeval} find that even when the oracle context is retrieved, LLMs tend to misuse it, highlighting a lack of semantic understanding, which we discuss in the next section.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Bad Generation Despite Identifying the Correct Context:} \citet{ding2023crosscodeeval} highlights a failure case where a code LLM fails to complete a Python test case correctly, even though it has the correct context. The function name from the context, \texttt{test\_case\_convert\_camel\_to\_snake}, suggests that the function being completed is a test case for \texttt{convert\_camel\_to\_snake}. With the given context, the model generates the function as \texttt{convert\_camel\_to\_snake}, which however does not match the larger codebase as other pieces of code expect this function name to be \texttt{camel\_to\_snake}. While this issue can partly be attributed to incomplete retrieval of relevant information, it also presents a challenge for code LLMs, as they must recognize such inconsistencies—especially when the immediate context is correctly provided—thereby avoiding high-confidence errors.
\end{tcolorbox}


% \todo{Example of bad generation with good retrieval}
% \begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
% \textit{Example: Bad Generation Despite Good Retrieval:}  \citet{zhou2022docprompting} introduces DocPrompting, a method that generates code by retrieving relevant documents from source-code libraries. While DocPrompting demonstrates benefits from retrieved documentation in most test cases, it can occasionally be misled by similar content within these documents. For example, when prompted with ``Exclude columns names when writing dataframe `df' to a csv file `filename.csv''', DocPrompting incorrectly generates  \texttt{df.to\_csv('filename.csv', skiprows=1)} instead of the correct solution \texttt{df.to\_csv('filename.csv', header=False)}. This error occurs because the retrieved documentation for \texttt{df.to\_csv} also contains information about \texttt{df.read\_csv}, which includes a \texttt{skiprows} parameter. The model fails to distinguish between the similar but functionally distinct methods even though the retrieved content includes sufficient information.
% \todo{example is old, replace or remove?}
% \end{tcolorbox}

% \naman{\textbf{The limits of long-context models}: 
% Context length for LLMs have been increasing rapidly with Gemini providing over 1 million token context. 
% In the space of coding models, we have witnessed developments of massive context lengths, example over 100 million tokens ~\citep{magic-dev}.
% Long context provides an alternative but complementary approach for dealing with large repositories. 
% These models have been achieving good performance in toy benchmarks such as needle retrieval, however, it is unclear how their performance would scale with real-world large context use cases.
% }


\subsection{Semantic Understanding of Codebases} \label{sec:c-global-understanding}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Being able to effectively write code relies on having a strong semantic understanding (somewhat like a world model) of the entire codebase: structurally seeing how various parts of the code go together, knowing what is implemented where, understanding how the algorithms work, and keeping track of program invariants at certain program points. LLMs struggle with this global semantic understanding
\newline \\
\textit{Potential solutions}: \ref{sec:d-data}
\end{tcolorbox}

% \naman{why is this not a part of abstractions? -- comment based on subsec title}

% After working with a codebase for a prolonged time, programmers have a global understanding and mental model of the codebase. This includes overall codebase structure, different algorithms, stylistic aspects, data representation, program invariants, package versions, test coverage, and the interplay between different functions. This holistic understanding is necessary for performing many tasks.

A global and holistic semantic understanding of a codebase is important for performing almost all code-related tasks. For example, let's say an engineer is asked to improve the runtime performance of a query. To do so, they must first understand the codebase's structure well enough to know where all the pieces of the algorithm are implemented. Then, they need to understand the algorithm and implementation in detail. This includes both the high-level algorithm (including its time complexity) and the low-level implementation details to identify both algorithmic and implementation bottlenecks. Finally, after coming up with a solution, they must then return to their understanding of the global code structure so that they can integrate their new algorithm without introducing new bugs. 

% \textbf{Understanding invariants for code refactoring}: In order to refactor a large codebase, it is essential to have a complete understanding of how different parts of the codebase interact. These interactions are governed by a set of rules known as program invariants, properties about the code that are assumed to hold. While some of these invariants can be maintained and guaranteed explicitly with assertions, many of these invariants are implicit and may only be obvious to developers working on the codebase itself. When performing code refactoring, developers often have a mental model of these invariants and know what needs to be changed so that these invariants continue to hold. LLMs, on the other hand, lack such a nuanced understanding of programs and may not be able to infer or maintain complex invariants.

% \todo{Example of LLM not being able to understand an invariant.}

% \textbf{Understanding program execution for code debugging}: Debugging code is another task that relies on having a complete global understanding of a codebase. In order to debug programs productively, we need a good internal model of the intermediate execution states of a program. Step-by-step debuggers are generally helpful because the programmer has a mental model of properties that should be true at certain program states, and the debugger can surface whether those properties are indeed true. 

LLMs struggle at semantic understanding of codebases for several reasons. First, the way that code is pieced together can be relatively intricate, and understanding all these complex relationships can be difficult. Second, code can often have units with high logical complexity that contain custom algorithms that may never have appeared anywhere in the training data. Finally, because a disproportionately large number of LLM training tokens are spent on code generation rather than other coding tasks, models may lack a holistic awareness and world model of code. 

One desiderata is that models can generalize knowledge across various coding tasks \citep{roychoudhury2025will}. However, this may not be straightforward as just training on more tasks: \citet{gu2024cruxeval} found that coding models fine-tuned on additional natural language/code pairs saw significant improvements on code generation but did not transfer to improving code understanding and execution reasoning. While there have been successful efforts to augment code LLM training with execution information to improve general coding capabilities \citep{ni2024next, ding2024traced}, imbuing code LLMs with a general and holistic understanding of code remains an important challenge today.

% \todo{Example of LLM not being to reason about the execution of a piece of code}

% \todo{\url{https://www.expresscomputer.in/news/quantifying-github-copilots-impact-on-code-quality-ai/104480/} cite and also r2e discussion section}


\subsection{Low-Resource Languages and Specialized Libraries} \label{sec:c-low-resource}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Models struggle with low-resource languages and codebases with specialized libraries. Because of the limited exposure they have in these contexts, models fail in a variety of ways including generating syntactically incorrect code, misunderstanding the semantics of certain constructs, and using libraries improperly.
\newline \\
\textit{Potential solutions}: \ref{sec:d-adaptation}
\end{tcolorbox}

% \todo{Personalization and proprietary code}

As we adapt code LLMs to individual codebases, generating correct code in out of distribution (OOD) scenarios becomes crucial. Much of software development in business contexts revolves around proprietary codebases, which is a distribution shift from the open-source code that dominates LLM training data  \citep{ahmed2024studying}. These OOD scenarios include domain-specific languages (DSLs), custom internal libraries, low-resource APIs, and company-specific coding styles and conventions.
% For example, \citet{ahmed2024studying} find a distribution shift between open-source and proprietary C\#/C++ code at Microsoft.

% \textbf{Syntactic failures}: Models often hallucinate constructs from higher resource languages when working in low-resource languages. \citet{blinn2024statically} found that when writing Hazel, LLMs often borrowed syntax and library functions from OCaml and Elm, higher-resource languages. When writing Triton, models often use syntactically incorrect constructs such as array indexing.

\textbf{Syntactic Failures}: Models have been shown to hallucinate constructs from higher resource languages when working in low-resource languages. \citet{blinn2024statically} remark that \textit{''contemporary LLMs fail to follow Hazel’s syntax and semantics, often borrowing syntactic forms and library functions from [higher-resource languages like] OCaml and Elm``}. 
% In a low-resource GPU programming language called Triton, models often generate syntactically incorrect code such as array indexing when this is not a supported construct.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]

\textit{Example. Syntax error in Triton}: In Listing \ref{lst:triton-indexing}, we show an attempt from Gemma-3 27B to write a dot-product kernel in a low-resource GPU programming language called Triton (part of docstring omitted for brevity). Gemma uses indexing notation such as \texttt{a[index]}, which is not a valid Triton construct. Models like o1 and o3, however, do not make this mistake.

\begin{lstlisting}[label={lst:triton-indexing}, captionpos=t, breaklines=true, language=triton]
@triton.jit
def dot_product_indexed_kernel(a, b, indexes, out, N):
    """
    Computes the dot product of two vectors a and b using an index vector.
    ...
    """
    block_size = 64  # Adjust block size for optimal performance
    grid_size = (N + block_size - 1) // block_size

    block_id = tl.program_id(0)
    start = block_id * block_size
    end = min(start + block_size, N)

    accumulator = tl.zeros(1, dtype=tl.float32)
    !*
    for i in range(start, end):
        index = indexes[i]
        accumulator += a[index] * b[index]
    *!
    tl.store(out, 0, accumulator)
\end{lstlisting}
% \todo{are we sure that o1 or even o3 does not just single-shot this? WD: o1 or o3-mini-high can do this fine, but small model even recent gemma3 27b still struggle with this. }
% \naman{better example}
\end{tcolorbox}

\textbf{Poor Semantic Understanding}: In low-resource languages, models have less exposure to the various language constructs. Therefore, they have a weaker semantic understanding of the language. Many studies reveal that code LLMs perform poorly when asked to write code in low-resource languages. Due to the lack of training data in these OOD domains, models may struggle to write common primitives or piece together functionality coherently. On HumanEval, Qwen 2.5 Coder Instruct (32B) \citep{hui2024qwen2} has an accuracy of 83\% in Python but only 27\% in D.\footnote{As reported by the \href{https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard}{BigCode Models Leaderboard} on the MultiPL-E benchmark \citep{cassano2023multipl}}

% \todo{Example of LLM not understanding low-resource language semantics}

% \textbf{Domain-Specific Languages (DSLs)}: Similar to low-resource programming languages, generating programs in a DSL is also challenging for LLMs due to the out-of-distribution nature of this task. In Section 6.5 of \citet{qiu2024tenspiler}, the authors find that Claude Opus struggles to generate code in three DSLs, making mistakes such as hallucinating libraries that do not exist, including the wrong imports, and using APIs improperly. For a more comprehensive survey of low-resource languages and DSLs, including current progress in these directions, we refer the reader to \citet{joel2024survey}. 

\textbf{Library Usage Failures}: In OOD scenarios, LLMs lack awareness of the libraries and functions available for use. In new codebases using custom libraries, many functions appear only a few times, providing limited training data for AI models to learn their usage. This scarcity can lead to overfitting, where models fail to recognize an effective use-case of these functions. Models also frequently hallucinate non-existent functions based on patterns that it infers.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example. Hallucinating theorems in Lean}: In Listing \ref{lst:lean-hallucination}, the LLM makes up a theorem named \texttt{infinitelyManyPrimes\_arithmeticSequence}. While the model understands that it should use Dirichlet's theorem, the actual theorem is named \texttt{forall\_exists\_prime\_gt\_and\_eq\_mod}. This theorem occurs very few times in all of the currently available Lean code on GitHub, so it is likely that LLMs did not train on it many times.

\begin{lstlisting}[caption={LLM-generated Lean code snippet that incorrectly uses indexing notation},label={lst:lean-hallucination}, captionpos=t, language=lean, breaklines=true, style=mystyle]
theorem infinitely_many_primes_of_form_6k_plus_1 :
  ∀ n : ℕ, ∃ p : ℕ, p > n ∧ Prime p ∧ ∃ k : ℕ, p = 6 * k + 1 := by
  intro n
  -- Use Dirichlet's theorem for arithmetic progressions
  !*have h := Nat.infinitelyManyPrimes_arithmeticSequence 6 1
\end{lstlisting}
\end{tcolorbox}


% OOD \citep{zhou2023devil}

% high resource intermediate language \citep{mora2024synthetic}


% \begin{itemize}
%     \item Low-resource languages.
%     \item Different programming paradigms, like imperative vs declarative.
%     \item Even for high-resource languages, can models handle OOD scenarios well?
%     \item Low-resource library usage.
%     \item Codebase-specific style guidelines.
%     \item There are no idiomatic evaluations.
% \end{itemize}

% \todo{relationship between OOD/custom libraries and code reuse case. code reuse is where you have a set of building blocks and need to combine them, the difficulty is that the amount of blocks is very large.}

\subsection{Library and API Version Updates} \label{sec:c-library-updates}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Software engineering has a unique property that repositories and libraries are constantly changing. Code LLMs have trouble adapting to these rapid changes, often struggling to use the correct version of libraries and ignoring new paradigms and features. 
\newline \\
\textit{Potential solutions}: \ref{sec:d-adaptation}
\end{tcolorbox}

Continual learning, the idea of training an AI system to take in new information continually, has been a long-standing challenge in AI and NLP  \citep{wu2024continual, wang2024comprehensive}. In software engineering, codebases are continuously changing as new features are supported and awkward design patterns are reworked. While backwards compatibility is often prioritized in software design, it inevitably becomes broken as codebases evolve further. Therefore, programming libraries have version releases, each release supporting and deprecating features in the last version. 

There have been a few works exposing this issue. For example, CodeUpdateArena \citep{liu2024codeupdatearena} and GitChameleon \citep{islah2024gitchameleon} are two benchmarks exploring the ability of LLMs to write version-specific code, examining this issue at the function and file level. They find that language models struggle to adapt to these changes even with this limited scope. In theorem proving (Lean), \citet{kumarappan2024leanagent} try to mitigate this by developing a lifelong learning framework that continuously learns and uses new theorems. In real-world engineering, the challenge of library and API versioning generally spans across an entire repository, as everything must be kept consistent. To our knowledge, there are no techniques that successfully deal with this challenge at such a large scale. This problem is difficult for a few reasons, which we discuss below. 
% Currently, there are no benchmarks or techniques that successfully deal with this challenge at such a large scale. 

\textbf{Version Identification}: In order to to successfully deal with version changes, a LLM must first identify which version of each library is being used in a codebase. This may often be quite difficult, because versioning information can be hidden deeply within a codebase. Sometimes, it can be found in comments or configuration files, but in the worst case, it must be inferred from the library calls being used. To make things worse, some code may be compatible across multiple versions, while other code will cause errors only in specific versions. Therefore, the model will often require a deep understanding of both the codebase and the nuances between different versions in order to infer the version at hand.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Debugging Frontend Code:} Frontend framework usually has more frequent versions update, making it hard for code LLMs to work with. For example, when helping a user debug the ``NextRouter was not mounted'' issue, Claude 3.7 tries various solutions without recognizing that the core problem requires importing \texttt{useRouter} from \texttt{'next/navigation'} instead of \texttt{'next/router'}, a crucial distinction since the user's codebase leverages App Router in Next.js 13.
\end{tcolorbox}
%\todo{Example of version identification failure?}


\textbf{Version Adaptation}: Many fast-changing libraries are not backward compatible as older features become deprecated. It can be difficult for LLMs to implicitly keep track of which constructs and patterns are associated with each version. Therefore, consistently using constructs from the right version can be difficult. As we will see in the examples below, LLMs often write code that mixes and matches API constructs from different versions of the same library. 


% \todo{Perhaps add probing tasks to see if LLMs explicitly know which constructs are in which library}

% \todo{Example: Jane Street OCaml vs. general OCaml}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example. Typing Hints}: While Python 3.5 required importing types from the typing module, Python 3.9's PEP 585 enabled direct use of built-in types for generics (e.g., \texttt{\small list[int]} vs \texttt{\small typing.List[int]}). However, language models tend to default to the older \texttt{typing} module syntax.
\end{tcolorbox}


\textbf{Continuous Adaptation to Paradigms, Features, and APIs}: New styles, patterns, and paradigms are often introduced to replace older, more cumbersome ways to write code. For example, React came out with its \textit{Hooks} paradigm in version 16.8 (2019). Over the next few years, developers transitioned from the old class components paradigm to using hooks, as hooks made code cleaner and more maintainable. Only in early 2023, with the launch of \texttt{react.dev}, were Hooks the default paradigm in the documentation. For language models, incorporating these features can take a long time, because code in these new paradigms are initially completely absent in the training data and inherently in the low-resource regime. In \citet{kharma2025security}, the authors find that LLMs fail to utilize security features in compiler and toolkit updates (such as in Java 17), still relying on legacy methods such as insecure random number generation. While it is possible to use retrieved examples and documentation in order to coerce language models to write code using new and updated features, we should strive to create AI coding assistants that can quickly internalize new changes and be able to naturally incorporate new features and paradigms, even without an abundance of training data. For each task, the language model should be able to reason about the best way to write the code, independently of the number of occurrences seen in the training data.

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example. Lean 3 vs. Lean 4}: Lean \citep{de2015lean} is a programming language that allows users to write formal proofs of mathematical theorems. In 2017, using Lean 3, enthusiasts implemented a library for mathematics called \texttt{mathlib}, with over half a million lines of code. Because Lean 3 had many shortcomings, Lean 4 \citep{moura2021lean} was initiated at the beginning of 2021 to address many of these issues. There was a massive undertaking to port all of the \texttt{mathlib} code over to Lean 4, and only in September 2023 was there a stable release of Lean 4, the version of Lean that is predominantly used today. The two versions are generally incompatible. We hypothesize that, due to the recency of Lean 4, most language models have been trained on much more Lean 3 code compared to Lean 4 code. When asking models to generate code in Lean 4, it sometimes generates code with Lean 3 coding conventions. Other times, it uses theorems and lemmas from Lean 3 that are deprecated in Lean 4. In Listing \ref{lst:lean-version}, we show an example of prompting \texttt{o3-mini} with a Lean 4 problem, where it generates Lean 3 syntax (e.g. \texttt{begin}).

\begin{lstlisting}[breaklines=true, style=mystyle]
Prompt: Write and prove the following in Lean 4: the sum of the first n positive integers is n*(n+1)/2. Begin with "import Mathlib". Surround your Lean 4 code with ```lean4 and ```
\end{lstlisting}

\begin{lstlisting}[label={lst:lean-version}, captionpos=t, language=lean, breaklines=true, style=mystyle]
/-- The sum of the first n positive integers (i.e. 0 + 1 + … + n) is n * (n + 1) / 2. -/
theorem sum_first_n (n : ℕ) : (range (n + 1)).sum id = n * (n + 1) / 2 :=
begin
  induction n with n ih,
  { -- Base case: when n = 0, we have range (0 + 1) = {0} and so the sum is 0, which matches 0 * (0 + 1) / 2.
    simp [range],
  },
  { -- Inductive step: assume the formula holds for n, and prove it for n + 1.
    -- First, note that range (n + 2) = insert (n + 1) (range (n + 1)).
    rw [range_succ],
    -- Since (n + 1) ∉ range (n + 1), we can express the sum as:
    rw [sum_insert (not_mem_range_self (n + 1))],
    -- Substitute the induction hypothesis:
    rw [ih],
    -- We now have to verify:
    -- n * (n + 1) / 2 + (n + 1) = (n + 1) * (n + 2) / 2.
    -- This equality follows by algebra (ring calculation).
    ring,
  }
end
\end{lstlisting}
\end{tcolorbox}

% \begin{figure}[ht]
% \begin{minipage}{\linewidth}
% \begin{lstlisting}[caption={Incorrect Lean code generated by o3-mini in Lean 3 syntax},label={lst:lean-version}, captionpos=t, language=lean, breaklines=true, style=mystyle]
% import Mathlib

% open Finset

% /-- The sum of the first n positive integers (i.e. 0 + 1 + … + n) is n * (n + 1) / 2. -/
% theorem sum_first_n (n : ℕ) : (range (n + 1)).sum id = n * (n + 1) / 2 :=
% begin
%   induction n with n ih,
%   { -- Base case: when n = 0, we have range (0 + 1) = {0} and so the sum is 0, which matches 0 * (0 + 1) / 2.
%     simp [range],
%   },
%   { -- Inductive step: assume the formula holds for n, and prove it for n + 1.
%     -- First, note that range (n + 2) = insert (n + 1) (range (n + 1)).
%     rw [range_succ],
%     -- Since (n + 1) ∉ range (n + 1), we can express the sum as:
%     rw [sum_insert (not_mem_range_self (n + 1))],
%     -- Substitute the induction hypothesis:
%     rw [ih],
%     -- We now have to verify:
%     -- n * (n + 1) / 2 + (n + 1) = (n + 1) * (n + 2) / 2.
%     -- This equality follows by algebra (ring calculation).
%     ring,
%   }
% end
% \end{lstlisting}
% \end{minipage}
% \end{figure}

% \textit{Example. Dynamo}:  \todo{Armando's example: Copilot is bad at dynamo tasks because there are a bunch of versions of the API so mixes things up.}


% \todo{Example of LLM doing something in a complicated way when there is a new feature to do it in an easier fashion}


\subsection{High Logical Complexity and OOD Domains} \label{sec:c-ood-domains}

\begin{tcolorbox}[colback=lightgreen, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
Tasks such as writing highly concurrent code or discovering  performance optimizations have a high logical complexity, often proving difficult for even the best human coders. Similar to solving research-level math problems, these out-of-distribution domains are very hard for LLMs.
\newline \\
\textit{Potential solutions}: \ref{sec:d-rl}
\end{tcolorbox}

Some programming tasks are challenging for even the best human programmers, requiring approaches with a very high logical complexity. Examples of tasks that fall into this category include superoptimizing programs, discovering attacks for purportedly secure code, writing performant compilers, optimizing GPU kernels \citep{ouyang2024kernelbench}, and writing very error-prone and very technical code. 

% \todo{Flesh out the difficulties more}

% \todo{Add more examples of superhuman / OOD domains}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example. Synthesis of Sorting Kernels}: An example of an out-of-distribution domain is synthesizing fast assembly code for sorting kernels. In 2023, AlphaDev \citep{mankowitz2023faster} used reinforcement learning to find a SoTA kernel for sorting length 3-5 arrays. While this appeared to be a superhuman performance, shortly after, \cite{neri2023shorter} hand-wrote a kernel shorter and faster than the one found by AlphaDev. Later, \citep{ullrich2024synthesis} developed an algorithm based on enumeration and intelligent heuristic-based sampling that beat both of these. In addition, the algorithm ran faster than AlphaDev by two orders of magnitude. In this case, while AI was able to achieve an impressive performance, humans were able to discover better algorithms.
\end{tcolorbox}

\begin{tcolorbox}[colback=lightblue, boxrule=0pt, arc=5pt, outer arc=5pt, after skip=10pt plus 2pt]
\textit{Example: Verifying File System Properties}: In formal verification, when working with new domains, it is necessary to devise new theories to faithfully represent desired properties. For example, FSCQ is a formally certified crash-proof file system with the provable guarantee that under any sequence of crashes followed by reboots, FSCQ will recover the file system correctly without losing data \citep{chen2015using}. In this domain, one challenge is that proving safety cannot be done at the source code level--because instructions are not atomic, data may be lost if the crash occurs within a non-atomic instruction. Instead, a new logic known as the Crash Hoare logic (CHL) needed to be developed, and constructs representing a crash condition and recovery procedure needed to be described. Constructing a logic like this would be very difficult for AI systems.
\end{tcolorbox}

\textbf{Limits of Symbolic Techniques}: When it comes to applying symbolic techniques to these tasks, there are a few limiting factors that make them difficult to tackle. First, for synthesis-style tasks, the search space can be very large. Deductive and rewrite-based synthesis techniques are unable to explore a majority of the search space. Second, verifiers can be limited in power, such as when dealing with properties in concurrency or weak memory models. Third, many domains lack clean models to reason about properties, such as dealing with memory bandwidth in GPU kernels. 

Because they are hard for humans, these tasks are very rarely in the training data of today's language models. They have unique, domain-specific, challenges that making generalizing from existing data difficult. For these problems, language models rely heavily on feedback-driven search algorithms \citep{mankowitz2023faster}, and it can be difficult to navigate the search space effectively. In addition, many of these tasks lack feedback mechanisms, which is crucial for AI to pick up learning signals. When designing a complex algorithm or data structure, it is often hard to know if you are on the right track until you get to the correct result. When writing code for a large multithreaded operation, it may be hard to know if the algorithm has concurrency issues until all the parts are fully fleshed out. Without feedback, incremental improvement is nearly impossible.


% \begin{itemize}
%     \item New systems/languages (particularly excited for supporting verification where from very limited understanding bottleneck is good general systems since domain experts write one of things (or proprietary)).
% \end{itemize}