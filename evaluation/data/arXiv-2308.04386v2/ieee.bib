@article{austin2021program,
 author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
 journal = {ArXiv preprint},
 title = {Program synthesis with large language models},
 year = {2021}
}

@inproceedings{brown2020language,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 booktitle = {Proc. of NeurIPS},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@inproceedings{burns2023weak,
 author = {Collin Burns and
Pavel Izmailov and
Jan Hendrik Kirchner and
Bowen Baker and
Leo Gao and
Leopold Aschenbrenner and
Yining Chen and
Adrien Ecoffet and
Manas Joglekar and
Jan Leike and
Ilya Sutskever and
Jeffrey Wu},
 booktitle = {Proc. of ICML},
 title = {Weak-to-Strong Generalization: Eliciting Strong Capabilities With
Weak Supervision},
 year = {2024}
}

@inproceedings{chan2023chateval,
 author = {Chi{-}Min Chan and
Weize Chen and
Yusheng Su and
Jianxuan Yu and
Wei Xue and
Shanghang Zhang and
Jie Fu and
Zhiyuan Liu},
 booktitle = {Proc. of ICLR},
 title = {ChatEval: Towards Better LLM-based Evaluators through Multi-Agent
Debate},
 year = {2024}
}

@inproceedings{chiang2023large,
 author = {Chiang, Cheng-Han  and
Lee, Hung-yi},
 booktitle = {Proc. of ACL},
 pages = {15607--15631},
 title = {Can Large Language Models Be an Alternative to Human Evaluations?},
 year = {2023}
}

@article{chowdhery2022palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {ArXiv preprint},
 title = {Palm: Scaling language modeling with pathways},
 year = {2022}
}

@inproceedings{christiano2017deep,
 author = {Paul F. Christiano and
Jan Leike and
Tom B. Brown and
Miljan Martic and
Shane Legg and
Dario Amodei},
 booktitle = {Proc. of NeurIPS},
 pages = {4299--4307},
 title = {Deep Reinforcement Learning from Human Preferences},
 year = {2017}
}

@inproceedings{conneau2019unsupervised,
 author = {Conneau, Alexis  and
Khandelwal, Kartikay  and
Goyal, Naman  and
Chaudhary, Vishrav  and
Wenzek, Guillaume  and
Guzm{\'a}n, Francisco  and
Grave, Edouard  and
Ott, Myle  and
Zettlemoyer, Luke  and
Stoyanov, Veselin},
 booktitle = {Proc. of ACL},
 pages = {8440--8451},
 title = {Unsupervised Cross-lingual Representation Learning at Scale},
 year = {2020}
}

@article{cui2023ultrafeedback,
 author = {Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong},
 journal = {ArXiv preprint},
 title = {Ultrafeedback: Boosting language models with high-quality feedback},
 year = {2023}
}

@inproceedings{ding2022gpt,
 author = {Ding, Bosheng  and
Qin, Chengwei  and
Liu, Linlin  and
Chia, Yew Ken  and
Li, Boyang  and
Joty, Shafiq  and
Bing, Lidong},
 booktitle = {Proc. of ACL},
 pages = {11173--11195},
 title = {Is {GPT}-3 a Good Data Annotator?},
 year = {2023}
}

@article{ding2023image,
 author = {Ding, Ning and Deng, Chaorui and Tan, Mingkui and Du, Qing and Ge, Zhiwei and Wu, Qi},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 title = {Image Captioning With Controllable and Adaptive Length Levels},
 year = {2023}
}

@article{donato2022mad,
 author = {Donato, Domenic and Yu, Lei and Ling, Wang and Dyer, Chris},
 journal = {ArXiv preprint},
 title = {MAD for Robust Reinforcement Learning in Machine Translation},
 year = {2022}
}

@article{dong2023raft,
 author = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
 journal = {ArXiv preprint},
 title = {Raft: Reward ranked finetuning for generative foundation model alignment},
 year = {2023}
}

@inproceedings{dubois2024alpacafarm,
 author = {Yann Dubois and
Chen Xuechen Li and
Rohan Taori and
Tianyi Zhang and
Ishaan Gulrajani and
Jimmy Ba and
Carlos Guestrin and
Percy Liang and
Tatsunori B. Hashimoto},
 booktitle = {Proc. of NeurIPS},
 title = {AlpacaFarm: {A} Simulation Framework for Methods that Learn from Human
Feedback},
 year = {2023}
}

@inproceedings{duh2008beyond,
 author = {Duh, Kevin  and
Kirchhoff, Katrin},
 booktitle = {Proceedings of ACL-08: HLT, Short Papers},
 pages = {37--40},
 title = {Beyond Log-Linear Models: Boosted Minimum Error Rate Training for N-best Re-ranking},
 year = {2008}
}

@article{fabbri2021summeval,
 author = {Fabbri, Alexander R.  and
Kry{\'s}ci{\'n}ski, Wojciech  and
McCann, Bryan  and
Xiong, Caiming  and
Socher, Richard  and
Radev, Dragomir},
 journal = {TACL},
 pages = {391--409},
 title = {{S}umm{E}val: Re-evaluating Summarization Evaluation},
 year = {2021}
}

@inproceedings{fernandes2022quality,
 author = {Fernandes, Patrick  and
Farinhas, Ant{\'o}nio  and
Rei, Ricardo  and
C. de Souza, Jos{\'e} G.  and
Ogayo, Perez  and
Neubig, Graham  and
Martins, Andre},
 booktitle = {Proc. of NAACL},
 pages = {1396--1412},
 title = {Quality-Aware Decoding for Neural Machine Translation},
 year = {2022}
}

@article{fomicheva2020unsupervised,
 author = {Fomicheva, Marina  and
Sun, Shuo  and
Yankovskaya, Lisa  and
Blain, Fr{\'e}d{\'e}ric  and
Guzm{\'a}n, Francisco  and
Fishel, Mark  and
Aletras, Nikolaos  and
Chaudhary, Vishrav  and
Specia, Lucia},
 journal = {TACL},
 pages = {539--555},
 title = {Unsupervised Quality Estimation for Neural Machine Translation},
 year = {2020}
}

@inproceedings{freitag2017beam,
 author = {Freitag, Markus  and
Al-Onaizan, Yaser},
 booktitle = {Proceedings of the First Workshop on Neural Machine Translation},
 pages = {56--60},
 title = {Beam Search Strategies for Neural Machine Translation},
 year = {2017}
}

@article{freitag2021experts,
 author = {Freitag, Markus  and
Foster, George  and
Grangier, David  and
Ratnakar, Viresh  and
Tan, Qijun  and
Macherey, Wolfgang},
 journal = {TACL},
 pages = {1460--1474},
 title = {Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation},
 year = {2021}
}

@inproceedings{freitag2022results,
 author = {Freitag, Markus  and
Rei, Ricardo  and
Mathur, Nitika  and
Lo, Chi-kiu  and
Stewart, Craig  and
Avramidis, Eleftherios  and
Kocmi, Tom  and
Foster, George  and
Lavie, Alon  and
Martins, Andr{\'e} F. T.},
 booktitle = {Proceedings of the Seventh Conference on Machine Translation (WMT)},
 pages = {46--68},
 title = {Results of {WMT}22 Metrics Shared Task: Stop Using {BLEU} {--} Neural Metrics Are Better and More Robust},
 year = {2022}
}

@inproceedings{fu2023gptscore,
 author = {Fu, Jinlan  and
Ng, See-Kiong  and
Jiang, Zhengbao  and
Liu, Pengfei},
 booktitle = {Proc. of NAACL},
 pages = {6556--6576},
 title = {{GPTS}core: Evaluate as You Desire},
 year = {2024}
}

@article{grattafiori2024llama,
 author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 year = {2024}
}

@inproceedings{he2024improving,
 author = {He, Zhiwei  and
Wang, Xing  and
Jiao, Wenxiang  and
Zhang, Zhuosheng  and
Wang, Rui  and
Shi, Shuming  and
Tu, Zhaopeng},
 booktitle = {Proc. of NAACL},
 pages = {8164--8180},
 title = {Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model},
 year = {2024}
}

@inproceedings{hermann2015teaching,
 author = {Karl Moritz Hermann and
Tom{\'{a}}s Kocisk{\'{y}} and
Edward Grefenstette and
Lasse Espeholt and
Will Kay and
Mustafa Suleyman and
Phil Blunsom},
 booktitle = {Proc. of NeurIPS},
 pages = {1693--1701},
 title = {Teaching Machines to Read and Comprehend},
 year = {2015}
}

@inproceedings{ho2022large,
 author = {Ho, Namgyu  and
Schmid, Laura  and
Yun, Se-Young},
 booktitle = {Proc. of ACL},
 pages = {14852--14882},
 title = {Large Language Models Are Reasoning Teachers},
 year = {2023}
}

@inproceedings{holtzman2019curious,
 author = {Ari Holtzman and
Jan Buys and
Li Du and
Maxwell Forbes and
Yejin Choi},
 booktitle = {Proc. of ICLR},
 title = {The Curious Case of Neural Text Degeneration},
 year = {2020}
}

@inproceedings{hsieh2023distilling,
 author = {Hsieh, Cheng-Yu  and
Li, Chun-Liang  and
Yeh, Chih-kuan  and
Nakhost, Hootan  and
Fujii, Yasuhisa  and
Ratner, Alex  and
Krishna, Ranjay  and
Lee, Chen-Yu  and
Pfister, Tomas},
 booktitle = {Proc. of ACL Findings},
 pages = {8003--8017},
 title = {Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
 year = {2023}
}

@inproceedings{hu2021ranknas,
 author = {Hu, Chi  and
Wang, Chenglong  and
Ma, Xiangnan  and
Meng, Xia  and
Li, Yinqiao  and
Xiao, Tong  and
Zhu, Jingbo  and
Li, Changliang},
 booktitle = {Proc. of EMNLP},
 pages = {2469--2480},
 title = {{R}ank{NAS}: Efficient Neural Architecture Search by Pairwise Ranking},
 year = {2021}
}

@article{jiang2023one,
 author = {Jiang, Congcong and Qian, Tieyun and Liu, Bing},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 title = {One General Teacher for Multi-Data Multi-Task: A New Knowledge Distillation Framework for Discourse Relation Analysis},
 year = {2023}
}

@inproceedings{kang2023distill,
 author = {Kang, Junmo  and
Xu, Wei  and
Ritter, Alan},
 booktitle = {Proc. of ACL},
 pages = {11100--11119},
 title = {Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models},
 year = {2023}
}

@inproceedings{kiegeland2021revisiting,
 author = {Kiegeland, Samuel  and
Kreutzer, Julia},
 booktitle = {Proc. of NAACL},
 pages = {1673--1681},
 title = {Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation},
 year = {2021}
}

@inproceedings{kim2016sequence,
 author = {Kim, Yoon  and
Rush, Alexander M.},
 booktitle = {Proc. of EMNLP},
 pages = {1317--1327},
 title = {Sequence-Level Knowledge Distillation},
 year = {2016}
}

@inproceedings{kocmi2023large,
 author = {Kocmi, Tom  and
Federmann, Christian},
 booktitle = {Proceedings of the 24th Annual Conference of the European Association for Machine Translation},
 pages = {193--203},
 title = {Large Language Models Are State-of-the-Art Evaluators of Translation Quality},
 year = {2023}
}

@inproceedings{lai2021thank,
 author = {Lai, Huiyuan  and
Toral, Antonio  and
Nissim, Malvina},
 booktitle = {Proc. of ACL},
 pages = {484--494},
 title = {Thank you {BART}! Rewarding Pre-Trained Models Improves Formality Style Transfer},
 year = {2021}
}

@article{lai2023multidimensional,
 author = {Lai, Huiyuan and Toral, Antonio and Nissim, Malvina},
 journal = {ArXiv preprint},
 title = {Multidimensional Evaluation for Text Style Transfer Using ChatGPT},
 year = {2023}
}

@inproceedings{lee2021discriminative,
 author = {Lee, Ann  and
Auli, Michael  and
Ranzato, Marc{'}Aurelio},
 booktitle = {Proc. of ACL},
 pages = {7250--7264},
 title = {Discriminative Reranking for Neural Machine Translation},
 year = {2021}
}

@article{lee2023rlaif,
 author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
 journal = {ArXiv preprint},
 title = {Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
 year = {2023}
}

@inproceedings{lewis2019bart,
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proc. of ACL},
 pages = {7871--7880},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 year = {2020}
}

@article{li2018romir,
 author = {Li, Jun and Xu, Chang and Yang, Wankou and Sun, Changyin and Kotagiri, Ramamohanarao and Tao, Dacheng},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 pages = {2393--2406},
 title = {ROMIR: Robust multi-view image re-ranking},
 year = {2018}
}

@inproceedings{li2019niutrans,
 author = {Li, Bei  and
Li, Yinqiao  and
Xu, Chen  and
Lin, Ye  and
Liu, Jiqiang  and
Liu, Hui  and
Wang, Ziyang  and
Zhang, Yuhao  and
Xu, Nuo  and
Wang, Zeyang  and
Feng, Kai  and
Chen, Hexuan  and
Liu, Tengbo  and
Li, Yanyang  and
Wang, Qiang  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
 pages = {257--266},
 title = {The {N}iu{T}rans Machine Translation Systems for {WMT}19},
 year = {2019}
}

@inproceedings{li2022learning,
 author = {Bei Li and
Tong Zheng and
Yi Jing and
Chengbo Jiao and
Tong Xiao and
Jingbo Zhu},
 booktitle = {Proc. of ICML},
 pages = {13225--13241},
 title = {Learning Multiscale Transformer Models for Sequence Generation},
 year = {2022}
}

@inproceedings{li2022ode,
 author = {Li, Bei  and
Du, Quan  and
Zhou, Tao  and
Jing, Yi  and
Zhou, Shuhan  and
Zeng, Xin  and
Xiao, Tong  and
Zhu, JingBo  and
Liu, Xuebo  and
Zhang, Min},
 booktitle = {Proc. of ACL},
 pages = {8335--8351},
 title = {{ODE} Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation},
 year = {2022}
}

@inproceedings{li2023synthetic,
 author = {Li, Zhuoyan  and
Zhu, Hangxiao  and
Lu, Zhuoran  and
Yin, Ming},
 booktitle = {Proc. of EMNLP},
 pages = {10443--10461},
 title = {Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations},
 year = {2023}
}

@inproceedings{lin2004rouge,
 author = {Lin, Chin-Yew},
 booktitle = {Text Summarization Branches Out},
 pages = {74--81},
 title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
 year = {2004}
}

@inproceedings{lin2020weight,
 author = {Lin, Ye  and
Li, Yanyang  and
Wang, Ziyang  and
Li, Bei  and
Du, Quan  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proc. of ACL},
 pages = {2076--2088},
 title = {Weight Distillation: Transferring the Knowledge in Neural Network Parameters},
 year = {2021}
}

@article{liu2019roberta,
 author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
 journal = {ArXiv preprint},
 title = {Roberta: A robustly optimized bert pretraining approach},
 year = {2019}
}

@article{liu2021addressing,
 author = {Liu, Rui and Lin, Zheng and Wang, Weiping},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {3180--3191},
 title = {Addressing extraction and generation separately: Keyphrase prediction with pre-trained language models},
 year = {2021}
}

@article{liu2022decoding,
 author = {Liu, Rui and Sisman, Berrak and Gao, Guanglai and Li, Haizhou},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {1789--1802},
 title = {Decoding knowledge transfer for neural text-to-speech training},
 year = {2022}
}

@article{liu2023gpteval,
 author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
 journal = {ArXiv preprint},
 title = {Gpteval: Nlg evaluation using gpt-4 with better human alignment},
 year = {2023}
}

@article{luo2023chatgpt,
 author = {Luo, Zheheng and Xie, Qianqian and Ananiadou, Sophia},
 journal = {ArXiv preprint},
 title = {Chatgpt as a factual inconsistency evaluator for abstractive text summarization},
 year = {2023}
}

@inproceedings{magister2022teaching,
 author = {Magister, Lucie Charlotte  and
Mallinson, Jonathan  and
Adamek, Jakub  and
Malmi, Eric  and
Severyn, Aliaksei},
 booktitle = {Proc. of ACL},
 pages = {1773--1781},
 title = {Teaching Small Language Models to Reason},
 year = {2023}
}

@article{mohtashami2023learning,
 author = {Mohtashami, Amirkeivan and Verzetti, Mauro and Rubenstein, Paul K},
 journal = {ArXiv preprint},
 title = {Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models},
 year = {2023}
}

@article{myung2003tutorial,
 author = {Myung, In Jae},
 journal = {Journal of mathematical Psychology},
 title = {Tutorial on maximum likelihood estimation},
 year = {2003}
}

@inproceedings{neubig2013travatar,
 author = {Neubig, Graham},
 booktitle = {Proc. of ACL},
 pages = {91--96},
 title = {{T}ravatar: A Forest-to-String Machine Translation Engine based on Tree Transducers},
 year = {2013}
}

@inproceedings{nijkamp2022codegen,
 author = {Erik Nijkamp and
Bo Pang and
Hiroaki Hayashi and
Lifu Tu and
Huan Wang and
Yingbo Zhou and
Silvio Savarese and
Caiming Xiong},
 booktitle = {Proc. of ICLR},
 title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program
Synthesis},
 year = {2023}
}

@inproceedings{ott2019fairseq,
 author = {Ott, Myle  and
Edunov, Sergey  and
Baevski, Alexei  and
Fan, Angela  and
Gross, Sam  and
Ng, Nathan  and
Grangier, David  and
Auli, Michael},
 booktitle = {Proc. of NAACL},
 pages = {48--53},
 title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
 year = {2019}
}

@inproceedings{ouyang2022training,
 author = {Long Ouyang and
Jeffrey Wu and
Xu Jiang and
Diogo Almeida and
Carroll L. Wainwright and
Pamela Mishkin and
Chong Zhang and
Sandhini Agarwal and
Katarina Slama and
Alex Ray and
John Schulman and
Jacob Hilton and
Fraser Kelton and
Luke Miller and
Maddie Simens and
Amanda Askell and
Peter Welinder and
Paul F. Christiano and
Jan Leike and
Ryan Lowe},
 booktitle = {Proc. of NeurIPS},
 title = {Training language models to follow instructions with human feedback},
 year = {2022}
}

@inproceedings{papineni2002bleu,
 author = {Papineni, Kishore  and
Roukos, Salim  and
Ward, Todd  and
Zhu, Wei-Jing},
 booktitle = {Proc. of ACL},
 pages = {311--318},
 title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
 year = {2002}
}

@inproceedings{popovic2015chrf,
 author = {Popovi{\'c}, Maja},
 booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
 pages = {392--395},
 title = {chr{F}: character n-gram {F}-score for automatic {MT} evaluation},
 year = {2015}
}

@article{raffel2020exploring,
 author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
 journal = {J. Mach. Learn. Res.},
 pages = {140:1--140:67},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
 year = {2020}
}

@inproceedings{rao2018dear,
 author = {Rao, Sudha  and
Tetreault, Joel},
 booktitle = {Proc. of NAACL},
 pages = {129--140},
 title = {Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},
 year = {2018}
}

@inproceedings{rei2020comet,
 author = {Rei, Ricardo  and
Stewart, Craig  and
Farinha, Ana C  and
Lavie, Alon},
 booktitle = {Proc. of EMNLP},
 pages = {2685--2702},
 title = {{COMET}: A Neural Framework for {MT} Evaluation},
 year = {2020}
}

@inproceedings{rei2020unbabel,
 author = {Rei, Ricardo  and
Stewart, Craig  and
Farinha, Ana C  and
Lavie, Alon},
 booktitle = {Proceedings of the Fifth Conference on Machine Translation},
 pages = {911--920},
 title = {Unbabel{'}s Participation in the {WMT}20 Metrics Shared Task},
 year = {2020}
}

@inproceedings{rei2022comet,
 author = {Rei, Ricardo  and
C. de Souza, Jos{\'e} G.  and
Alves, Duarte  and
Zerva, Chrysoula  and
Farinha, Ana C  and
Glushkova, Taisiya  and
Lavie, Alon  and
Coheur, Luisa  and
Martins, Andr{\'e} F. T.},
 booktitle = {Proceedings of the Seventh Conference on Machine Translation (WMT)},
 pages = {578--585},
 title = {{COMET}-22: Unbabel-{IST} 2022 Submission for the Metrics Shared Task},
 year = {2022}
}

@inproceedings{rei2023scaling,
 author = {Rei, Ricardo  and
Guerreiro, Nuno M.  and
Pombal, Jos{\~A}{\copyright}  and
van Stigt, Daan  and
Treviso, Marcos  and
Coheur, Luisa  and
C. de Souza, Jos{\'e} G.  and
Martins, Andr{\'e}},
 booktitle = {Proceedings of the Eighth Conference on Machine Translation},
 pages = {841--848},
 title = {Scaling up {C}omet{K}iwi: Unbabel-{IST} 2023 Submission for the Quality Estimation Shared Task},
 year = {2023}
}

@article{roberts2020decoding,
 author = {Roberts, Nicholas and Liang, Davis and Neubig, Graham and Lipton, Zachary C},
 journal = {ArXiv preprint},
 title = {Decoding and diversity in machine translation},
 year = {2020}
}

@inproceedings{schick2021generating,
 author = {Schick, Timo  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proc. of EMNLP},
 pages = {6943--6951},
 title = {Generating Datasets with Pretrained Language Models},
 year = {2021}
}

@inproceedings{sellam2020bleurt,
 author = {Sellam, Thibault  and
Das, Dipanjan  and
Parikh, Ankur},
 booktitle = {Proc. of ACL},
 pages = {7881--7892},
 title = {{BLEURT}: Learning Robust Metrics for Text Generation},
 year = {2020}
}

@article{shen2014dependency,
 author = {Shen, Mo and Kawahara, Daisuke and Kurohashi, Sadao},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 title = {Dependency parse reranking with rich subtree features},
 year = {2014}
}

@inproceedings{shen2015minimum,
 author = {Shen, Shiqi  and
Cheng, Yong  and
He, Zhongjun  and
He, Wei  and
Wu, Hua  and
Sun, Maosong  and
Liu, Yang},
 booktitle = {Proc. of ACL},
 pages = {1683--1692},
 title = {Minimum Risk Training for Neural Machine Translation},
 year = {2016}
}

@article{shen2020simple,
 author = {Shen, Dinghan and Zheng, Mingzhi and Shen, Yelong and Qu, Yanru and Chen, Weizhu},
 journal = {ArXiv preprint},
 title = {A simple but tough-to-beat data augmentation approach for natural language understanding and generation},
 year = {2020}
}

@inproceedings{shimanaka2018ruse,
 author = {Shimanaka, Hiroki  and
Kajiwara, Tomoyuki  and
Komachi, Mamoru},
 booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
 pages = {751--758},
 title = {{RUSE}: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation},
 year = {2018}
}

@article{shu2021reward,
 author = {Shu, Raphael and Yoo, Kang Min and Ha, Jung-Woo},
 journal = {ArXiv preprint},
 title = {Reward optimization for neural machine translation with learned metrics},
 year = {2021}
}

@article{tan2024large,
 author = {Tan, Zhen and Li, Dawei and Wang, Song and Beigi, Alimohammad and Jiang, Bohan and Bhattacharjee, Amrita and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
 journal = {ArXiv preprint},
 title = {Large language models for data annotation and synthesis: A survey},
 year = {2024}
}

@inproceedings{tang2019natural,
 author = {Tang, Raphael  and
Lu, Yao  and
Lin, Jimmy},
 booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
 pages = {202--208},
 title = {Natural Language Generation for Effective Knowledge Distillation},
 year = {2019}
}

@inproceedings{thompson2020automatic,
 author = {Thompson, Brian  and
Post, Matt},
 booktitle = {Proc. of EMNLP},
 pages = {90--121},
 title = {Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing},
 year = {2020}
}

@article{tripathi2023divide,
 author = {Tripathi, Achyut Mani and Pandey, Om Jee},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {1100--1113},
 title = {Divide and distill: new outlooks on knowledge distillation for environmental sound classification},
 year = {2023}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 booktitle = {Proc. of NeurIPS},
 pages = {5998--6008},
 title = {Attention is All you Need},
 year = {2017}
}

@inproceedings{wang2021niutrans,
 author = {Wang, Chenglong  and
Hu, Chi  and
Mu, Yongyu  and
Yan, Zhongxiang  and
Wu, Siming  and
Hu, Yimin  and
Cao, Hang  and
Li, Bei  and
Lin, Ye  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proceedings of the Sixth Conference on Machine Translation},
 pages = {787--794},
 title = {The {N}iu{T}rans System for the {WMT} 2021 Efficiency Task},
 year = {2021}
}

@inproceedings{wang2022self,
 author = {Wang, Yizhong  and
Kordi, Yeganeh  and
Mishra, Swaroop  and
Liu, Alisa  and
Smith, Noah A.  and
Khashabi, Daniel  and
Hajishirzi, Hannaneh},
 booktitle = {Proc. of ACL},
 pages = {13484--13508},
 title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
 year = {2023}
}

@inproceedings{wang2023chatgpt,
 author = {Wang, Jiaan  and
Liang, Yunlong  and
Meng, Fandong  and
Sun, Zengkui  and
Shi, Haoxiang  and
Li, Zhixu  and
Xu, Jinan  and
Qu, Jianfeng  and
Zhou, Jie},
 booktitle = {Proceedings of the 4th New Frontiers in Summarization Workshop},
 pages = {1--11},
 title = {Is {C}hat{GPT} a Good {NLG} Evaluator? A Preliminary Study},
 year = {2023}
}

@inproceedings{wang2023esrl,
 author = {Chenglong Wang and
Hang Zhou and
Yimin Hu and
Yifu Huo and
Bei Li and
Tongran Liu and
Tong Xiao and
Jingbo Zhu},
 booktitle = {Proc. of AAAI},
 pages = {19107--19115},
 title = {{ESRL:} Efficient Sampling-Based Reinforcement Learning for Sequence
Generation},
 year = {2024}
}

@inproceedings{wang2023improved,
 author = {Wang, Chenglong  and
Lu, Yi  and
Mu, Yongyu  and
Hu, Yimin  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proc. of EMNLP Findings},
 pages = {6232--6244},
 title = {Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection},
 year = {2022}
}

@inproceedings{wang2024helpsteer2,
 author = {Zhilin Wang and
Yi Dong and
Olivier Delalleau and
Jiaqi Zeng and
Gerald Shen and
Daniel Egert and
Jimmy Zhang and
Makesh Narsimhan Sreedhar and
Oleksii Kuchaiev},
 booktitle = {Proc. of NeurIPS},
 title = {HelpSteer 2: Open-source dataset for training top-performing reward
models},
 year = {2024}
}

@article{wang2024survey,
 author = {Wang, Ke and Zhu, Jiahui and Ren, Minjie and Liu, Zeming and Li, Shiwei and Zhang, Zongye and Zhang, Chenkai and Wu, Xiaoyu and Zhan, Qiqi and Liu, Qingjie and others},
 journal = {ArXiv preprint},
 title = {A survey on data synthesis and augmentation for large language models},
 year = {2024}
}

@inproceedings{wieting2019beyond,
 author = {Wieting, John  and
Berg-Kirkpatrick, Taylor  and
Gimpel, Kevin  and
Neubig, Graham},
 booktitle = {Proc. of ACL},
 pages = {4344--4355},
 title = {Beyond {BLEU}: Training Neural Machine Translation with Semantic Similarity},
 year = {2019}
}

@article{williams1992simple,
 author = {Williams, Ronald J},
 journal = {Reinforcement learning},
 title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
 year = {1992}
}

@article{wu2023large,
 author = {Wu, Ning and Gong, Ming and Shou, Linjun and Liang, Shining and Jiang, Daxin},
 journal = {ArXiv preprint},
 title = {Large language models are diverse role-players for summarization evaluation},
 year = {2023}
}

@inproceedings{xia2022structured,
 author = {Xia, Mengzhou  and
Zhong, Zexuan  and
Chen, Danqi},
 booktitle = {Proc. of ACL},
 pages = {1513--1528},
 title = {Structured Pruning Learns Compact and Accurate Models},
 year = {2022}
}

@article{xiao2023introduction,
 author = {Xiao, Tong and Zhu, Jingbo},
 journal = {ArXiv preprint},
 title = {Introduction to transformers: an nlp perspective},
 year = {2023}
}

@misc{xiao2025foundationslargelanguagemodels,
 author = {Tong Xiao and Jingbo Zhu},
 journal = {ArXiv preprint},
 title = {Foundations of Large Language Models},
 year = {2025}
}

@article{yang2024qwen2,
 author = {Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
 journal = {ArXiv preprint},
 title = {Qwen2. 5 technical report},
 year = {2024}
}

@article{ye2022generalized,
 author = {Ye, Han-Jia and Lu, Su and Zhan, De-Chuan},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 pages = {1817--1834},
 title = {Generalized knowledge distillation via relationship matching},
 year = {2022}
}

@inproceedings{yehudai2022reinforcement,
 author = {Yehudai, Asaf  and
Choshen, Leshem  and
Fox, Lior  and
Abend, Omri},
 booktitle = {Proc. of COLING},
 pages = {4544--4556},
 title = {Reinforcement Learning with Large Action Spaces for Neural Machine Translation},
 year = {2022}
}

@article{yoon2021tutornet,
 author = {Yoon, Ji Won and Lee, Hyeonseung and Kim, Hyung Yong and Cho, Won Ik and Kim, Nam Soo},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {1626--1638},
 title = {TutorNet: Towards flexible knowledge distillation for end-to-end speech recognition},
 year = {2021}
}

@inproceedings{yuan2021bartscore,
 author = {Weizhe Yuan and
Graham Neubig and
Pengfei Liu},
 booktitle = {Proc. of NeurIPS},
 pages = {27263--27277},
 title = {BARTScore: Evaluating Generated Text as Text Generation},
 year = {2021}
}

@article{zhang2016learning,
 author = {Zhang, Jing and Wu, Xindong and Sheng, Victor S},
 journal = {Artificial Intelligence Review},
 title = {Learning from crowdsourced labeled data: a survey},
 year = {2016}
}

@inproceedings{zhang2019bertscore,
 author = {Tianyi Zhang and
Varsha Kishore and
Felix Wu and
Kilian Q. Weinberger and
Yoav Artzi},
 booktitle = {Proc. of ICLR},
 title = {BERTScore: Evaluating Text Generation with {BERT}},
 year = {2020}
}

@inproceedings{zhao2019moverscore,
 author = {Zhao, Wei  and
Peyrard, Maxime  and
Liu, Fei  and
Gao, Yang  and
Meyer, Christian M.  and
Eger, Steffen},
 booktitle = {Proc. of EMNLP},
 pages = {563--578},
 title = {{M}over{S}core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance},
 year = {2019}
}

@inproceedings{zhou2021niutrans,
 author = {Li, Bei  and
Li, Yinqiao  and
Xu, Chen  and
Lin, Ye  and
Liu, Jiqiang  and
Liu, Hui  and
Wang, Ziyang  and
Zhang, Yuhao  and
Xu, Nuo  and
Wang, Zeyang  and
Feng, Kai  and
Chen, Hexuan  and
Liu, Tengbo  and
Li, Yanyang  and
Wang, Qiang  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
 pages = {257--266},
 title = {The {N}iu{T}rans Machine Translation Systems for {WMT}19},
 year = {2019}
}

@article{zhuo2023large,
 author = {Zhuo, Terry Yue},
 journal = {ArXiv preprint},
 title = {Large Language Models Are State-of-the-Art Evaluators of Code Generation},
 year = {2023}
}

@misc{ziya-reward-7B,
 author = {IDEA-CCNL},
 title = {Ziya-LLaMA-7B-Reward},
 year = {2023}
}
