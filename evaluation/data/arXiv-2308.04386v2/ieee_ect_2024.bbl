% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{papineni2002bleu}
K.~Papineni, S.~Roukos, T.~Ward, and W.-J. Zhu, ``{B}leu: a method for
  automatic evaluation of machine translation,'' in \emph{Proc. of ACL}, 2002,
  pp. 311--318.

\bibitem{lin2004rouge}
C.-Y. Lin, ``{ROUGE}: A package for automatic evaluation of summaries,'' in
  \emph{Text Summarization Branches Out}, 2004, pp. 74--81.

\bibitem{zhang2019bertscore}
T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi, ``Bertscore:
  Evaluating text generation with {BERT},'' in \emph{Proc. of ICLR}, 2020.

\bibitem{wieting2019beyond}
J.~Wieting, T.~Berg-Kirkpatrick, K.~Gimpel, and G.~Neubig, ``Beyond {BLEU}:
  Training neural machine translation with semantic similarity,'' in
  \emph{Proc. of ACL}, 2019, pp. 4344--4355.

\bibitem{sellam2020bleurt}
T.~Sellam, D.~Das, and A.~Parikh, ``{BLEURT}: Learning robust metrics for text
  generation,'' in \emph{Proc. of ACL}, 2020, pp. 7881--7892.

\bibitem{yuan2021bartscore}
W.~Yuan, G.~Neubig, and P.~Liu, ``Bartscore: Evaluating generated text as text
  generation,'' in \emph{Proc. of NeurIPS}, 2021, pp. 27\,263--27\,277.

\bibitem{rei2022comet}
R.~Rei, J.~G. C.~de Souza, D.~Alves, C.~Zerva, A.~C. Farinha, T.~Glushkova,
  A.~Lavie, L.~Coheur, and A.~F.~T. Martins, ``{COMET}-22: Unbabel-{IST} 2022
  submission for the metrics shared task,'' in \emph{Proceedings of the Seventh
  Conference on Machine Translation (WMT)}, 2022, pp. 578--585.

\bibitem{wang2022self}
Y.~Wang, Y.~Kordi, S.~Mishra, A.~Liu, N.~A. Smith, D.~Khashabi, and
  H.~Hajishirzi, ``Self-instruct: Aligning language models with self-generated
  instructions,'' in \emph{Proc. of ACL}, 2023, pp. 13\,484--13\,508.

\bibitem{nijkamp2022codegen}
E.~Nijkamp, B.~Pang, H.~Hayashi, L.~Tu, H.~Wang, Y.~Zhou, S.~Savarese, and
  C.~Xiong, ``Codegen: An open large language model for code with multi-turn
  program synthesis,'' in \emph{Proc. of ICLR}, 2023.

\bibitem{xiao2025foundationslargelanguagemodels}
T.~Xiao and J.~Zhu, ``Foundations of large language models,'' 2025.

\bibitem{schick2021generating}
T.~Schick and H.~Sch{\"u}tze, ``Generating datasets with pretrained language
  models,'' in \emph{Proc. of EMNLP}, 2021, pp. 6943--6951.

\bibitem{wang2024survey}
K.~Wang, J.~Zhu, M.~Ren, Z.~Liu, S.~Li, Z.~Zhang, C.~Zhang, X.~Wu, Q.~Zhan,
  Q.~Liu \emph{et~al.}, ``A survey on data synthesis and augmentation for large
  language models,'' \emph{ArXiv preprint}, 2024.

\bibitem{austin2021program}
J.~Austin, A.~Odena, M.~Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang,
  C.~Cai, M.~Terry, Q.~Le \emph{et~al.}, ``Program synthesis with large
  language models,'' \emph{ArXiv preprint}, 2021.

\bibitem{fu2023gptscore}
J.~Fu, S.-K. Ng, Z.~Jiang, and P.~Liu, ``{GPTS}core: Evaluate as you desire,''
  in \emph{Proc. of NAACL}, 2024, pp. 6556--6576.

\bibitem{kocmi2023large}
T.~Kocmi and C.~Federmann, ``Large language models are state-of-the-art
  evaluators of translation quality,'' in \emph{Proceedings of the 24th Annual
  Conference of the European Association for Machine Translation}, 2023, pp.
  193--203.

\bibitem{wang2023chatgpt}
J.~Wang, Y.~Liang, F.~Meng, Z.~Sun, H.~Shi, Z.~Li, J.~Xu, J.~Qu, and J.~Zhou,
  ``Is {C}hat{GPT} a good {NLG} evaluator? a preliminary study,'' in
  \emph{Proceedings of the 4th New Frontiers in Summarization Workshop}, 2023,
  pp. 1--11.

\bibitem{lai2023multidimensional}
H.~Lai, A.~Toral, and M.~Nissim, ``Multidimensional evaluation for text style
  transfer using chatgpt,'' \emph{ArXiv preprint}, 2023.

\bibitem{liu2023gpteval}
Y.~Liu, D.~Iter, Y.~Xu, S.~Wang, R.~Xu, and C.~Zhu, ``Gpteval: Nlg evaluation
  using gpt-4 with better human alignment,'' \emph{ArXiv preprint}, 2023.

\bibitem{luo2023chatgpt}
Z.~Luo, Q.~Xie, and S.~Ananiadou, ``Chatgpt as a factual inconsistency
  evaluator for abstractive text summarization,'' \emph{ArXiv preprint}, 2023.

\bibitem{wu2023large}
N.~Wu, M.~Gong, L.~Shou, S.~Liang, and D.~Jiang, ``Large language models are
  diverse role-players for summarization evaluation,'' \emph{ArXiv preprint},
  2023.

\bibitem{chan2023chateval}
C.~Chan, W.~Chen, Y.~Su, J.~Yu, W.~Xue, S.~Zhang, J.~Fu, and Z.~Liu,
  ``Chateval: Towards better llm-based evaluators through multi-agent debate,''
  in \emph{Proc. of ICLR}, 2024.

\bibitem{lin2020weight}
Y.~Lin, Y.~Li, Z.~Wang, B.~Li, Q.~Du, T.~Xiao, and J.~Zhu, ``Weight
  distillation: Transferring the knowledge in neural network parameters,'' in
  \emph{Proc. of ACL}, 2021, pp. 2076--2088.

\bibitem{tripathi2023divide}
A.~M. Tripathi and O.~J. Pandey, ``Divide and distill: new outlooks on
  knowledge distillation for environmental sound classification,''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, pp.
  1100--1113, 2023.

\bibitem{wang2023improved}
C.~Wang, Y.~Lu, Y.~Mu, Y.~Hu, T.~Xiao, and J.~Zhu, ``Improved knowledge
  distillation for pre-trained language models via knowledge selection,'' in
  \emph{Proc. of EMNLP Findings}, 2022, pp. 6232--6244.

\bibitem{ho2022large}
N.~Ho, L.~Schmid, and S.-Y. Yun, ``Large language models are reasoning
  teachers,'' in \emph{Proc. of ACL}, 2023, pp. 14\,852--14\,882.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann \emph{et~al.}, ``Palm: Scaling language
  modeling with pathways,'' \emph{ArXiv preprint}, 2022.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{J. Mach. Learn. Res.}, pp.
  140:1--140:67, 2020.

\bibitem{kim2016sequence}
Y.~Kim and A.~M. Rush, ``Sequence-level knowledge distillation,'' in
  \emph{Proc. of EMNLP}, 2016, pp. 1317--1327.

\bibitem{hsieh2023distilling}
C.-Y. Hsieh, C.-L. Li, C.-k. Yeh, H.~Nakhost, Y.~Fujii, A.~Ratner, R.~Krishna,
  C.-Y. Lee, and T.~Pfister, ``Distilling step-by-step! outperforming larger
  language models with less training data and smaller model sizes,'' in
  \emph{Proc. of ACL Findings}, 2023, pp. 8003--8017.

\bibitem{lee2023rlaif}
H.~Lee, S.~Phatale, H.~Mansoor, K.~Lu, T.~Mesnard, C.~Bishop, V.~Carbune, and
  A.~Rastogi, ``Rlaif: Scaling reinforcement learning from human feedback with
  ai feedback,'' \emph{ArXiv preprint}, 2023.

\bibitem{cui2023ultrafeedback}
G.~Cui, L.~Yuan, N.~Ding, G.~Yao, W.~Zhu, Y.~Ni, G.~Xie, Z.~Liu, and M.~Sun,
  ``Ultrafeedback: Boosting language models with high-quality feedback,''
  \emph{ArXiv preprint}, 2023.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, J.~Schulman, J.~Hilton, F.~Kelton, L.~Miller,
  M.~Simens, A.~Askell, P.~Welinder, P.~F. Christiano, J.~Leike, and R.~Lowe,
  ``Training language models to follow instructions with human feedback,'' in
  \emph{Proc. of NeurIPS}, 2022.

\bibitem{ziya-reward-7B}
IDEA-CCNL, ``Ziya-llama-7b-reward,'' 2023.

\bibitem{mohtashami2023learning}
A.~Mohtashami, M.~Verzetti, and P.~K. Rubenstein, ``Learning translation
  quality evaluation on low resource languages from large language models,''
  \emph{ArXiv preprint}, 2023.

\bibitem{thompson2020automatic}
B.~Thompson and M.~Post, ``Automatic machine translation evaluation in many
  languages via zero-shot paraphrasing,'' in \emph{Proc. of EMNLP}, 2020, pp.
  90--121.

\bibitem{fomicheva2020unsupervised}
M.~Fomicheva, S.~Sun, L.~Yankovskaya, F.~Blain, F.~Guzm{\'a}n, M.~Fishel,
  N.~Aletras, V.~Chaudhary, and L.~Specia, ``Unsupervised quality estimation
  for neural machine translation,'' \emph{TACL}, pp. 539--555, 2020.

\bibitem{myung2003tutorial}
I.~J. Myung, ``Tutorial on maximum likelihood estimation,'' \emph{Journal of
  mathematical Psychology}, 2003.

\bibitem{williams1992simple}
R.~J. Williams, ``Simple statistical gradient-following algorithms for
  connectionist reinforcement learning,'' \emph{Reinforcement learning}, 1992.

\bibitem{shen2015minimum}
S.~Shen, Y.~Cheng, Z.~He, W.~He, H.~Wu, M.~Sun, and Y.~Liu, ``Minimum risk
  training for neural machine translation,'' in \emph{Proc. of ACL}, 2016, pp.
  1683--1692.

\bibitem{kiegeland2021revisiting}
S.~Kiegeland and J.~Kreutzer, ``Revisiting the weaknesses of reinforcement
  learning for neural machine translation,'' in \emph{Proc. of NAACL}, 2021,
  pp. 1673--1681.

\bibitem{donato2022mad}
D.~Donato, L.~Yu, W.~Ling, and C.~Dyer, ``Mad for robust reinforcement learning
  in machine translation,'' \emph{ArXiv preprint}, 2022.

\bibitem{shen2014dependency}
M.~Shen, D.~Kawahara, and S.~Kurohashi, ``Dependency parse reranking with rich
  subtree features,'' \emph{IEEE/ACM Transactions on Audio, Speech, and
  Language Processing}, 2014.

\bibitem{lee2021discriminative}
A.~Lee, M.~Auli, and M.~Ranzato, ``Discriminative reranking for neural machine
  translation,'' in \emph{Proc. of ACL}, 2021, pp. 7250--7264.

\bibitem{liu2021addressing}
R.~Liu, Z.~Lin, and W.~Wang, ``Addressing extraction and generation separately:
  Keyphrase prediction with pre-trained language models,'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, pp. 3180--3191,
  2021.

\bibitem{shu2021reward}
R.~Shu, K.~M. Yoo, and J.-W. Ha, ``Reward optimization for neural machine
  translation with learned metrics,'' \emph{ArXiv preprint}, 2021.

\bibitem{fernandes2022quality}
P.~Fernandes, A.~Farinhas, R.~Rei, J.~G. C.~de Souza, P.~Ogayo, G.~Neubig, and
  A.~Martins, ``Quality-aware decoding for neural machine translation,'' in
  \emph{Proc. of NAACL}, 2022, pp. 1396--1412.

\bibitem{zhang2016learning}
J.~Zhang, X.~Wu, and V.~S. Sheng, ``Learning from crowdsourced labeled data: a
  survey,'' \emph{Artificial Intelligence Review}, 2016.

\bibitem{rei2020comet}
R.~Rei, C.~Stewart, A.~C. Farinha, and A.~Lavie, ``{COMET}: A neural framework
  for {MT} evaluation,'' in \emph{Proc. of EMNLP}, 2020, pp. 2685--2702.

\bibitem{hermann2015teaching}
K.~M. Hermann, T.~Kocisk{\'{y}}, E.~Grefenstette, L.~Espeholt, W.~Kay,
  M.~Suleyman, and P.~Blunsom, ``Teaching machines to read and comprehend,'' in
  \emph{Proc. of NeurIPS}, 2015, pp. 1693--1701.

\bibitem{ding2022gpt}
B.~Ding, C.~Qin, L.~Liu, Y.~K. Chia, B.~Li, S.~Joty, and L.~Bing, ``Is {GPT}-3
  a good data annotator?'' in \emph{Proc. of ACL}, 2023, pp. 11\,173--11\,195.

\bibitem{kang2023distill}
J.~Kang, W.~Xu, and A.~Ritter, ``Distill or annotate? cost-efficient
  fine-tuning of compact models,'' in \emph{Proc. of ACL}, 2023, pp.
  11\,100--11\,119.

\bibitem{chiang2023large}
C.-H. Chiang and H.-y. Lee, ``Can large language models be an alternative to
  human evaluations?'' in \emph{Proc. of ACL}, 2023, pp. 15\,607--15\,631.

\bibitem{shimanaka2018ruse}
H.~Shimanaka, T.~Kajiwara, and M.~Komachi, ``{RUSE}: Regressor using sentence
  embeddings for automatic machine translation evaluation,'' in
  \emph{Proceedings of the Third Conference on Machine Translation: Shared Task
  Papers}, 2018, pp. 751--758.

\bibitem{he2024improving}
Z.~He, X.~Wang, W.~Jiao, Z.~Zhang, R.~Wang, S.~Shi, and Z.~Tu, ``Improving
  machine translation with human feedback: An exploration of quality estimation
  as a reward model,'' in \emph{Proc. of NAACL}, 2024, pp. 8164--8180.

\bibitem{yehudai2022reinforcement}
A.~Yehudai, L.~Choshen, L.~Fox, and O.~Abend, ``Reinforcement learning with
  large action spaces for neural machine translation,'' in \emph{Proc. of
  COLING}, 2022, pp. 4544--4556.

\bibitem{neubig2013travatar}
G.~Neubig, ``{T}ravatar: A forest-to-string machine translation engine based on
  tree transducers,'' in \emph{Proc. of ACL}, 2013, pp. 91--96.

\bibitem{hu2021ranknas}
C.~Hu, C.~Wang, X.~Ma, X.~Meng, Y.~Li, T.~Xiao, J.~Zhu, and C.~Li,
  ``{R}ank{NAS}: Efficient neural architecture search by pairwise ranking,'' in
  \emph{Proc. of EMNLP}, 2021, pp. 2469--2480.

\bibitem{rao2018dear}
S.~Rao and J.~Tetreault, ``Dear sir or madam, may {I} introduce the {GYAFC}
  dataset: Corpus, benchmarks and metrics for formality style transfer,'' in
  \emph{Proc. of NAACL}, 2018, pp. 129--140.

\bibitem{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~B. Brown, M.~Martic, S.~Legg, and D.~Amodei,
  ``Deep reinforcement learning from human preferences,'' in \emph{Proc. of
  NeurIPS}, 2017, pp. 4299--4307.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' \emph{ArXiv preprint}, 2019.

\bibitem{conneau2019unsupervised}
A.~Conneau, K.~Khandelwal, N.~Goyal, V.~Chaudhary, G.~Wenzek, F.~Guzm{\'a}n,
  E.~Grave, M.~Ott, L.~Zettlemoyer, and V.~Stoyanov, ``Unsupervised
  cross-lingual representation learning at scale,'' in \emph{Proc. of ACL},
  2020, pp. 8440--8451.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in \emph{Proc.
  of NeurIPS}, 2017, pp. 5998--6008.

\bibitem{lai2021thank}
H.~Lai, A.~Toral, and M.~Nissim, ``Thank you {BART}! rewarding pre-trained
  models improves formality style transfer,'' in \emph{Proc. of ACL}, 2021, pp.
  484--494.

\bibitem{lewis2019bart}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART}: Denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in \emph{Proc.
  of ACL}, 2020, pp. 7871--7880.

\bibitem{wang2023esrl}
C.~Wang, H.~Zhou, Y.~Hu, Y.~Huo, B.~Li, T.~Liu, T.~Xiao, and J.~Zhu, ``{ESRL:}
  efficient sampling-based reinforcement learning for sequence generation,'' in
  \emph{Proc. of AAAI}, 2024, pp. 19\,107--19\,115.

\bibitem{xiao2023introduction}
T.~Xiao and J.~Zhu, ``Introduction to transformers: an nlp perspective,''
  \emph{ArXiv preprint}, 2023.

\bibitem{li2022ode}
B.~Li, Q.~Du, T.~Zhou, Y.~Jing, S.~Zhou, X.~Zeng, T.~Xiao, J.~Zhu, X.~Liu, and
  M.~Zhang, ``{ODE} transformer: An ordinary differential equation-inspired
  model for sequence generation,'' in \emph{Proc. of ACL}, 2022, pp.
  8335--8351.

\bibitem{rei2020unbabel}
R.~Rei, C.~Stewart, A.~C. Farinha, and A.~Lavie, ``Unbabel{'}s participation in
  the {WMT}20 metrics shared task,'' in \emph{Proceedings of the Fifth
  Conference on Machine Translation}, 2020, pp. 911--920.

\bibitem{fabbri2021summeval}
A.~R. Fabbri, W.~Kry{\'s}ci{\'n}ski, B.~McCann, C.~Xiong, R.~Socher, and
  D.~Radev, ``{S}umm{E}val: Re-evaluating summarization evaluation,''
  \emph{TACL}, pp. 391--409, 2021.

\bibitem{zhao2019moverscore}
W.~Zhao, M.~Peyrard, F.~Liu, Y.~Gao, C.~M. Meyer, and S.~Eger,
  ``{M}over{S}core: Text generation evaluating with contextualized embeddings
  and earth mover distance,'' in \emph{Proc. of EMNLP}, 2019, pp. 563--578.

\bibitem{wang2021niutrans}
C.~Wang, C.~Hu, Y.~Mu, Z.~Yan, S.~Wu, Y.~Hu, H.~Cao, B.~Li, Y.~Lin, T.~Xiao,
  and J.~Zhu, ``The {N}iu{T}rans system for the {WMT} 2021 efficiency task,''
  in \emph{Proceedings of the Sixth Conference on Machine Translation}, 2021,
  pp. 787--794.

\bibitem{burns2023weak}
C.~Burns, P.~Izmailov, J.~H. Kirchner, B.~Baker, L.~Gao, L.~Aschenbrenner,
  Y.~Chen, A.~Ecoffet, M.~Joglekar, J.~Leike, I.~Sutskever, and J.~Wu,
  ``Weak-to-strong generalization: Eliciting strong capabilities with weak
  supervision,'' in \emph{Proc. of ICML}, 2024.

\bibitem{rei2023scaling}
R.~Rei, N.~M. Guerreiro, J.~Pombal, D.~van Stigt, M.~Treviso, L.~Coheur, J.~G.
  C.~de Souza, and A.~Martins, ``Scaling up {C}omet{K}iwi: Unbabel-{IST} 2023
  submission for the quality estimation shared task,'' in \emph{Proceedings of
  the Eighth Conference on Machine Translation}, 2023, pp. 841--848.

\bibitem{freitag2017beam}
M.~Freitag and Y.~Al-Onaizan, ``Beam search strategies for neural machine
  translation,'' in \emph{Proceedings of the First Workshop on Neural Machine
  Translation}, 2017, pp. 56--60.

\bibitem{roberts2020decoding}
N.~Roberts, D.~Liang, G.~Neubig, and Z.~C. Lipton, ``Decoding and diversity in
  machine translation,'' \emph{ArXiv preprint}, 2020.

\bibitem{holtzman2019curious}
A.~Holtzman, J.~Buys, L.~Du, M.~Forbes, and Y.~Choi, ``The curious case of
  neural text degeneration,'' in \emph{Proc. of ICLR}, 2020.

\bibitem{freitag2021experts}
M.~Freitag, G.~Foster, D.~Grangier, V.~Ratnakar, Q.~Tan, and W.~Macherey,
  ``Experts, errors, and context: A large-scale study of human evaluation for
  machine translation,'' \emph{TACL}, pp. 1460--1474, 2021.

\bibitem{grattafiori2024llama}
A.~Grattafiori, A.~Dubey, A.~Jauhri, A.~Pandey, A.~Kadian, A.~Al-Dahle,
  A.~Letman, A.~Mathur, A.~Schelten, A.~Vaughan \emph{et~al.}, ``The llama 3
  herd of models,'' \emph{ArXiv preprint}, 2024.

\bibitem{yang2024qwen2}
A.~Yang, B.~Yang, B.~Zhang, B.~Hui, B.~Zheng, B.~Yu, C.~Li, D.~Liu, F.~Huang,
  H.~Wei \emph{et~al.}, ``Qwen2. 5 technical report,'' \emph{ArXiv preprint},
  2024.

\bibitem{wang2024helpsteer2}
Z.~Wang, Y.~Dong, O.~Delalleau, J.~Zeng, G.~Shen, D.~Egert, J.~Zhang, M.~N.
  Sreedhar, and O.~Kuchaiev, ``Helpsteer 2: Open-source dataset for training
  top-performing reward models,'' in \emph{Proc. of NeurIPS}, 2024.

\bibitem{dong2023raft}
H.~Dong, W.~Xiong, D.~Goyal, R.~Pan, S.~Diao, J.~Zhang, K.~Shum, and T.~Zhang,
  ``Raft: Reward ranked finetuning for generative foundation model alignment,''
  \emph{ArXiv preprint}, 2023.

\end{thebibliography}
