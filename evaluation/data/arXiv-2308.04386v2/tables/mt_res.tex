\begin{table*}[]
    \centering
    \caption{Results on the machine translation task.}
    \scalebox{0.88}{
        \begin{tabular}{l>{\centering\arraybackslash}p{1.3cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.3cm}>{\centering\arraybackslash}p{1.2cm}>{\centering\arraybackslash}p{1.2cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.2cm}>{\centering\arraybackslash}p{1.2cm}}
        \toprule[1.1pt]
        \multirow{2}{*}{Method} & \multicolumn{5}{c}{IWSLT’14 De-En}    & \multicolumn{5}{c}{WMT’14 En-De}\\ \cmidrule(l){2-6}  \cmidrule(l){7-11}                 
                                & BLEU  & COMET-20 & COMET-22 & Chat. & ChatRef. &BLEU  & COMET-20 & COMET-22 & Chat. & ChatRef.\\ \midrule
        MLE               & 34.57 & 37.07  &79.32& 75.03                & 65.61  &27.26  &49.79 &83.36 &91.32 & 68.13                 \\ \midrule
        \multicolumn{11}{l}{\textit{\textbf{with one multi-aspect evaluation model}}}                                 \\ \midrule
        RL w/ CSEM-base-ref&    \bf{34.34}&     38.24&  79.61&  75.58&  66.20&  \bf{27.03}&     51.10&  83.64&  91.85&  69.14   \\
        RL w/ CSEM-base&        33.35&  37.82&  79.46&  75.46&  66.10&  26.31&  50.75&  83.54&  92.07&  69.12   \\
        RL+RR w/ CSEM-base&     28.12&  \bf{40.20}&     \bf{80.16}&     \bf{75.97}&     \bf{66.69}&     20.63&  \bf{52.35}&     \bf{84.56}&     \bf{93.13}&     \bf{70.12}      \\ \midrule
        RL w/ CSEM-large-ref&   \bf{34.73}&     38.06&  79.55&  75.76&  66.16&  \bf{27.34}&     51.35&  83.96&  91.83&  69.38   \\
        RL w/ CSEM-large&       33.46&  38.02&  79.56&  75.64&  66.23&  26.00&  51.01&  83.67&  92.12&  69.26   \\
        RL+RR w/ CSEM-large&    28.97&  \bf{44.53}&     \bf{81.71}&     \bf{78.53}&     \bf{68.19}&     21.65&  \bf{52.97}&     \bf{85.25}&     \bf{93.55}&     \bf{70.83}      \\ \midrule
        \multicolumn{11}{l}{\textit{\textbf{with multiple single-aspect evaluation models}}}                                 \\ \midrule
        RR w/ CSEM-base&        28.03&  40.37&  80.17&  75.34&  66.58&  18.96&  51.12&  84.12&  92.59&  69.83   \\
        RR w/ CSEM-large&       \bf{28.90}&     \bf{44.37}&     \bf{81.61}&     \bf{78.32}&     \bf{67.70}&     \bf{20.72}&     \bf{51.74}&     \bf{84.67}&     \bf{93.21}&     \bf{70.57}      \\
        \toprule[1.1pt]
        \multicolumn{11}{l}{\parbox{20cm}{RR denotes the reranking approach.
        The performance of the pre-trained pre-trained generative model is shown in the “MLE” row.
        We report some commonly used evaluation metrics on test sets, including BLEU \cite{papineni2002bleu}, COMET-20 \cite{rei2020unbabel}, and COMET-22 \cite{rei2022comet}.
        We also report the multi-aspect evaluation scores obtained from ChatGPT using reference-based (ChatRef.) and reference-free (Chat.) evaluations. ChatGPT scores are scaled by min-max normalization and multiplied by 100, resulting in a range from 0 to 100. We propose integrating RL with reranking (RL+RR) to enhance the optimization against our evaluation models. Specifically, we use RL to optimize a multi-aspect and reference-free evaluation model and then rerank using this evaluation model. 
        }}
        \end{tabular} 
    }
    \vspace{-4mm}
    \label{tab:mt-res}
\end{table*}