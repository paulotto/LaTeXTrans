\begin{table*}[]
    \centering
    \caption{
    Performance of the learned reference-based and reference-free evaluation models on the SummEval benchmark.
    }
    \label{tab:summary-multi-aspect}
    \scalebox{0.88}{
    \begin{tabular}{lcccccccccccccccc}
    \toprule[1.1pt]
    \multirow{2}{*}{Method}& \multirow{2}{*}{\#Params} & \multicolumn{3}{c}{Coherence} & \multicolumn{3}{c}{Relevance} & \multicolumn{3}{c}{Consistency} & \multicolumn{3}{c}{Fluency}   & \multicolumn{3}{c}{Avg.}  \\ \cmidrule(l){3-5} \cmidrule(l){6-8} \cmidrule(l){9-11} \cmidrule(l){12-14} \cmidrule(l){15-17}
    & & Kend. & Spear. & Pear. & Kend. & Spear. & Pear. & Kend. & Spear. & Pear. & Kend. & Spear. & Pear. & Kend. & Spear. & Pear. \\ \midrule
    \multicolumn{17}{l}{\textit{\textbf{Reference-based Evaluation}}} \\ \midrule
    ChatGPT                  & -&0.407 & 0.474  & 0.491 & 0.378 & 0.430  & 0.457 & 0.375 & 0.403  & 0.489 & 0.319 & 0.339  & 0.409 & 0.370 & 0.411 & 0.461      \\ \hdashline
    ROUGE-1                  & -& 0.126 & 0.167  & 0.160 & 0.252 & 0.326  & 0.359 & 0.130 & 0.160  & 0.224 & 0.094 & 0.115 & 0.158 & 0.150 & 0.192 & 0.225        \\
    ROUGE-2                  & -& 0.139 & 0.184  & 0.174 & 0.219 & 0.290  & 0.327 & 0.155 & 0.187  & 0.246 & 0.128 & 0.159 & 0.185 & 0.160 & 0.205 & 0.233        \\
    ROUGE-L                  & -& 0.099 & 0.128  & 0.102 & 0.237 & 0.311  & 0.342 & 0.092 & 0.115  & 0.189 & 0.084 & 0.105 & 0.141 & 0.128 & 0.165 & 0.194          \\
    BERTScore                & 110M& 0.211 & 0.283  & 0.310 & 0.243 & 0.311  & 0.346 & 0.090 & 0.110  & 0.152 & 0.158 & 0.192 & 0.209 & 0.175 & 0.224 & 0.254          \\
    MoverScore               & 110M& 0.118 & 0.159  & 0.167 & 0.244 & 0.318  & 0.371 & 0.127 & 0.157  & 0.224 & 0.105 & 0.129 & 0.176 & 0.148 & 0.191 & 0.234          \\
    BARTScore                & 406M&0.250 & 0.322  & 0.345 & 0.197 & 0.264  & 0.290 & 0.256 & 0.311  & 0.321 & 0.203 & 0.248 & 0.260 & 0.227 & 0.286 & 0.304          \\
    BARTScore-FT & 406M& 0.342 & 0.448  & 0.458 & 0.273 & 0.356  & 0.369 & 0.315 & 0.382  & 0.422 & \bf0.292 & \bf0.356 & \bf0.407 & 0.305 & 0.385 & 0.414  \\
    CSEM-base (Ours)    & 125M &0.372 & 0.512  & 0.535 & 0.356 & 0.487  & 0.517 & 0.364 & 0.423  & 0.502 & 0.252 & 0.291  & 0.372 & 0.336 & 0.428 & 0.482      \\
    CSEM-large (Ours)   & 355M&\bf0.386 & \bf0.529  & \bf0.566 & \bf0.365 & \bf0.497  & \bf0.531 & \bf0.382 & \bf0.431  &\bf0.513 & 0.275 & 0.312  & 0.393 & \bf0.352 & \bf0.442 & \bf0.501     \\ \midrule
    \multicolumn{17}{l}{\textit{\textbf{Reference-free Evaluation}}} \\ \midrule
    ChatGPT                  & -   &0.403 & 0.470 & 0.484 & 0.374 & 0.428  & 0.454 & 0.389 & 0.419  & 0.517 & 0.329 & 0.353 & 0.415 & 0.374 & 0.417 & 0.468    \\  \hdashline
    CSEM-base (Ours)    & 125M&\bf{0.336} & \bf{0.467} & \bf{0.481} & \bf{0.324} & \bf{0.446}  & 0.461 & 0.370 & 0.429  & 0.508 & 0.196 & 0.252 & 0.326 & \bf{0.306} & \bf{0.399} & 0.444   \\
    CSEM-large (Ours)         & 355M &0.307 & 0.428 & 0.461 & 0.322 & 0.442  & \bf0.474 & \bf{0.372} & \bf0.433  & \bf0.524 & \bf{0.198} & \bf{0.254} & \bf{0.342} & 0.300 & 0.389 & \bf{0.450}  \\ 
    \toprule[1.1pt]
    \multicolumn{17}{l}{\parbox{20cm}{All evaluation models are single-aspect evaluation models, focusing on coherence, relevance, consistency, and fluency aspects, respectively. BARTScore-FT is an improved version of BARTScore based on the BART-large fine-tuned with the CNN/DM dataset \cite{yuan2021bartscore}. Given that we conducted experiments with the same prompts, hyper-parameters, and version of the ChatGPT model as \cite{wang2023chatgpt}, the results of ChatGPT are taken from this work. 
    The best result for each group is \textbf{bolded}. Note that we do not include comparisons with ChatGPT, as it is utilized to annotate the data.
    “Avg.” denotes the average performance.
    “Kend.”, “Spear.”, and “Pear.” denote Sample-level Kendall-Tau, Spearman, and Pearson correlation scores, respectively.
    The suffix “-base” and “-large” denote that we employ RoBERTa-base and RoBERTa-large in our evaluation model architecture.}} \\
    \end{tabular}
    }
\end{table*}