\section{Introduction}
The advancement of reasoning models has significantly improved the mathematical capabilities of large language models (LLMs). Evaluation efforts like MathArena \citep{matharena} demonstrate that these models achieve impressive performance on mathematical competitions such as AIME and HMMT. However, these competitions only evaluate final numerical answers and do not require rigorous proof-based reasoning essential for most mathematical tasks. 

Current benchmarks that mitigate this issue either rely on formal verification tools like Lean \citep{minif2f,fimo,putnambench} or focus on the evaluation of constructive proofs \citep{mathconstruct}. While these approaches are useful, the former does not take advantage of LLMs' strong natural language generation capabilities, and the latter covers only a limited subset of proofs. Therefore, it remains uncertain whether LLMs can reliably address complex mathematical questions requiring rigorous reasoning, which are crucial in real-world mathematical contexts.

To overcome these limitations, we conduct the first evaluation of natural language proofs by LLMs on challenging problems from the 2025 USA Mathematical Olympiad (USAMO). The USAMO represents one of the highest tiers of high school mathematics competitions in the United States, demanding detailed proofs and explanations analogous to the International Mathematical Olympiad (IMO). Participants qualify through prior competitions, including the AIME, but USAMO problems require significantly more rigorous and well-explained solutions.

Overall, we find that current LLMs struggle significantly on USAMO problems, with the best-performing model achieving an average score of less than $25\%$. Our evaluation reveals several critical failure modes, including flawed logic, unjustified assumptions, and a lack of creativity in reasoning. These findings underscore the substantial limitations of current LLMs in generating rigorous mathematical proofs. In this report, we first outline our methodology in \cref{sec:meth}, present detailed results and identify critical weaknesses in \cref{sec:results}, and discuss several qualitative observations in \cref{sec:discussion}.

