\section{Conclusion}\label{sec:conclusion}

In this study, we comprehensively analyzed the performance of six state-of-the-art LLMs on problems from the USAMO 2025 competition. Using a rigorous human evaluation setup, we found that all evaluated models performed very poorly, with even the best-performing model achieving an average accuracy of less than $25\%$. Through detailed examination of the models' reasoning traces, we identified several critical failure modes, including significant artifacts arising from the optimization strategies employed during model training. These findings underscore the substantial limitations of current LLMs in the rigorous mathematical reasoning required for high-level olympiad competitions, highlighting the need for substantial improvements in  proof generation capabilities.