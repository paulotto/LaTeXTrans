@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  year={2015},
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NeurIPS},
  year={2012}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={ICML},
  year={2019},
}

@inproceedings{tan2021efficientnetv2,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={ICML},
  year={2021},
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={CVPR},
  year={2022}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{woo2023convnext,
  title={Convnext v2: Co-designing and scaling convnets with masked autoencoders},
  author={Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  booktitle={CVPR},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@inproceedings{wu2018group,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={ECCV},
  year={2018}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  year={1997},
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={NeurIPS},
  year={2019}
}

@article{carandini2012normalization,
  title={Normalization as a canonical neural computation},
  author={Carandini, Matteo and Heeger, David J},
  journal={Nature Reviews Neuroscience},
  year={2012},
}

@inproceedings{lecun1998efficient,
  title={Efficient BackProp},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural Networks: Tricks of the Trade},
  year={1998}
}

@inproceedings{jarrett2009best,
  title={What is the best multi-stage architecture for object recognition?},
  author={Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
  booktitle={ICCV},
  year={2009},
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={ICML},
  year={2020},
}

@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={ICML},
  year={2020},
}

@article{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={NeurIPS},
  year={2018}
}

@article{bjorck2018understanding,
  title={Understanding batch normalization},
  author={Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  journal={NeurIPS},
  year={2018}
}

@article{brock2021characterizing,
  title={Characterizing signal propagation to close the performance gap in unnormalized resnets},
  author={Brock, Andrew and De, Soham and Smith, Samuel L},
  journal={arXiv preprint arXiv:2101.08692},
  year={2021}
}

@article{daneshmand2020batch,
  title={Batch normalization provably avoids ranks collapse for randomly initialised deep networks},
  author={Daneshmand, Hadi and Kohler, Jonas and Bach, Francis and Hofmann, Thomas and Lucchi, Aurelien},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{balduzzi2017shattered,
  title={The shattered gradients problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle={ICML},
  year={2017},
}

@article{karakida2019normalization,
  title={The normalization method for alleviating pathological sharpness in wide neural networks},
  author={Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{yong2020gradient,
  title={Gradient centralization: A new optimization technique for deep neural networks},
  author={Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{brock2021high,
  title={High-performance large-scale image recognition without normalization},
  author={Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle={ICML},
  year={2021},
}

@article{he2023simplifying,
  title={Simplifying transformer blocks},
  author={He, Bobby and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2311.01906},
  year={2023}
}

@article{zhang2019fixup,
  title={Fixup initialization: Residual learning without normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}

@article{de2020batch,
  title={Batch normalization biases residual blocks towards the identity function in deep networks},
  author={De, Soham and Smith, Sam},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={UAI},
  year={2021},
}

@article{ulyanov2016instance,
  title={Instance normalization: The missing ingredient for fast stylization},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1607.08022},
  year={2016}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{lyu2008nonlinear,
  title={Nonlinear image representation using divisive normalization},
  author={Lyu, Siwei and Simoncelli, Eero P},
  booktitle={CVPR},
  year={2008},
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={ECCV},
  year={2014},
}

@article{sermanet2013overfeat,
  title={Overfeat: Integrated recognition, localization and detection using convolutional networks},
  author={Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6229},
  year={2013}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={JMLR},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{shao2020normalization,
  title={Is normalization indispensable for training deep neural network?},
  author={Shao, Jie and Hu, Kai and Wang, Changhu and Xue, Xiangyang and Raj, Bhiksha},
  journal={NeurIPS},
  year={2020}
}

@article{arora2018theoretical,
  title={Theoretical analysis of auto rate-tuning by batch normalization},
  author={Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
  journal={arXiv preprint arXiv:1812.03981},
  year={2018}
}

@article{tanaka2021noether,
  title={Noether’s learning dynamics: Role of symmetry breaking in neural networks},
  author={Tanaka, Hidenori and Kunin, Daniel},
  journal={NeurIPS},
  year={2021}
}

@article{nguyen2019transformers,
  title={Transformers without tears: Improving the normalization of self-attention},
  author={Nguyen, Toan Q and Salazar, Julian},
  journal={arXiv preprint arXiv:1910.05895},
  year={2019}
}

@article{xu2019understanding,
  title={Understanding and improving layer normalization},
  author={Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{huang2017centered,
  title={Centered weight normalization in accelerating training of deep neural networks},
  author={Huang, Lei and Liu, Xianglong and Liu, Yang and Lang, Bo and Tao, Dacheng},
  booktitle={ICCV},
  year={2017}
}

@article{qiao2019micro,
  title={Micro-batch training with batch-channel normalization and weight standardization},
  author={Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  journal={arXiv preprint arXiv:1903.10520},
  year={2019}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={JMLR},
  year={2014},
}

@inproceedings{huang2016deep,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={CVPR Workshops},
  year={2020}
}

@article{he2023deep,
  title={Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation},
  author={He, Bobby and Martens, James and Zhang, Guodong and Botev, Aleksandar and Brock, Andrew and Smith, Samuel L and Teh, Yee Whye},
  journal={arXiv preprint arXiv:2302.10322},
  year={2023}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@article{smith2023convnets,
  title={Convnets match vision transformers at scale},
  author={Smith, Samuel L and Brock, Andrew and Berrada, Leonard and De, Soham},
  journal={arXiv preprint arXiv:2310.16764},
  year={2023}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  year={2017},
}

@article{luo2018towards,
  title={Towards understanding regularization in batch normalization},
  author={Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
  journal={arXiv preprint arXiv:1809.00846},
  year={2018}
}

@article{liu2021rethinking,
  title={Rethinking skip connection with layer normalization in transformers and resnets},
  author={Liu, Fenglin and Ren, Xuancheng and Zhang, Zhiyuan and Sun, Xu and Zou, Yuexian},
  journal={arXiv preprint arXiv:2105.07205},
  year={2021}
}

@article{takase2022b2t,
  title={B2t connection: Serving stability and performance in deep transformers},
  author={Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun},
  journal={arXiv preprint arXiv:2206.00330},
  year={2022}
}

@article{brody2023expressivity,
  title={On the Expressivity Role of LayerNorm in Transformers' Attention},
  author={Brody, Shaked and Alon, Uri and Yahav, Eran},
  journal={arXiv preprint arXiv:2305.02582},
  year={2023}
}

@article{klambauer2017self,
  title={Self-normalizing neural networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  journal={NeurIPS},
  year={2017}
}

@article{wang2019learning,
  title={Learning deep transformer models for machine translation},
  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  journal={arXiv preprint arXiv:1906.01787},
  year={2019}
}

@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={ICML},
  year={2010}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009},
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={ICCV},
  year={2021}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={ICASSP},
  year={2015},
}

@article{nguyen2024hyenadna,
  title={Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution},
  author={Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Wornow, Michael and Birch-Sykes, Callum and Massaroli, Stefano and Patel, Aman and Rabideau, Clayton and Bengio, Yoshua and others},
  journal={NeurIPS},
  year={2024}
}

@misc{grch382013p13,
  title={p13 (Genome Reference Consortium Human Build 38), INSDC Assembly},
  author={GRCh38, Ensembl},
  year={2013}
}

@article{grevsova2023genomic,
  title={Genomic benchmarks: a collection of datasets for genomic sequence classification},
  author={Gre{\v{s}}ov{\'a}, Katar{\'\i}na and Martinek, Vlastimil and {\v{C}}ech{\'a}k, David and {\v{S}}ime{\v{c}}ek, Petr and Alexiou, Panagiotis},
  journal={BMC Genomic Data},
  year={2023},
}

@article{nicolae2018plu,
  title={PLU: The piecewise linear unit activation function},
  author={Nicolae, Andrei},
  journal={arXiv preprint arXiv:1809.09534},
  year={2018}
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={ICML},
  year={2021},
}



@article{adrian1926impulses,
  title={The impulses produced by sensory nerve endings: Part 1},
  author={Adrian, Edgar D},
  journal={The Journal of Physiology},
  year={1926},
}

@article{adrian1926impulses2,
  title={The impulses produced by sensory nerve-endings: Part 2. The response of a Single End-Organ},
  author={Adrian, Edgar D and Zotterman, Yngve},
  journal={The Journal of Physiology},
  year={1926},
}

@article{adrian1926impulses3,
  title={The impulses produced by sensory nerve endings: Part 3. Impulses set up by Touch and Pressure},
  author={Adrian, Edgar D and Zotterman, Yngve},
  journal={The Journal of Physiology},
  year={1926},
}

@article{sun2024massive,
  title={Massive Activations in Large Language Models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={AISTATS},
  year={2010},
}

@article{carandini2012normalization,
  title={Normalization as a canonical neural computation},
  author={Carandini, Matteo and Heeger, David J},
  journal={Nature Reviews Neuroscience},
  year={2012},
}

@article{heeger1996computational,
  title={Computational models of cortical visual processing.},
  author={Heeger, David J and Simoncelli, Eero P and Movshon, J Anthony},
  journal={PNAS},
  year={1996},
}

@article{heeger1992normalization,
  title={Normalization of cell responses in cat striate cortex},
  author={Heeger, David J},
  journal={Visual Neuroscience},
  year={1992},
}

@article{ni2024nonlinearity,
  title={On the Nonlinearity of Layer Normalization},
  author={Ni, Yunhao and Guo, Yuxin and Jia, Junlong and Huang, Lei},
  journal={arXiv preprint arXiv:2406.01255},
  year={2024}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9.2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  year={2022}
}

@misc{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{zhang2024internlm,
  title={Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output},
  author={Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Cao, Yuhang and Qian, Rui and Chen, Lin and Guo, Qipeng and Duan, Haodong and Wang, Bin and Ouyang, Linke and others},
  journal={arXiv preprint arXiv:2407.03320},
  year={2024}
}

@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}



@article{wang2022understanding,
  title={Understanding the failure of batch normalization for transformers in nlp},
  author={Wang, Jiaxi and Wu, Ji and Huang, Lei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37617--37630},
  year={2022}
}

@article{sun2024learning,
  title={Learning to (learn at test time): Rnns with expressive hidden states},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}

@article{feng2024were,
  title={Were RNNs All We Needed?},
  author={Feng, Leo and Tung, Frederick and Ahmed, Mohamed Osama and Bengio, Yoshua and Hajimirsadegh, Hossein},
  journal={arXiv preprint arXiv:2410.01201},
  year={2024}
}


@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, T},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@inproceedings{yao2021leveraging,
  title={Leveraging batch normalization for vision transformers},
  author={Yao, Zhuliang and Cao, Yue and Lin, Yutong and Liu, Ze and Zhang, Zheng and Hu, Han},
  booktitle={ICCV},
  year={2021}
}

@article{loshchilov2024ngpt,
  title={ngpt: Normalized transformer with representation learning on the hypersphere},
  author={Loshchilov, Ilya and Hsieh, Cheng-Ping and Sun, Simeng and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2410.01131},
  year={2024}
}




%%% Low citation

@article{lubana2021beyond,
  title={Beyond batchnorm: Towards a unified understanding of normalization in deep learning},
  author={Lubana, Ekdeep S and Dick, Robert and Tanaka, Hidenori},
  journal={NeurIPS},
  year={2021}
}

@article{lyu2022understanding,
  title={Understanding the generalization benefit of normalization layers: Sharpness reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={NeurIPS},
  year={2022}
}


@software{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    author       = {TorchVision maintainers and contributors},
    year         = 2016,
    journal      = {GitHub repository},
    publisher    = {GitHub},
    howpublished = {\url{https://github.com/pytorch/vision}}
}

@misc{FMS-FSDP,
 author={{Foundation Model Stack}},
  title = {GitHub: {FMS} {FSDP}},
  howpublished = {\url{{https://github.com/foundation-model-stack/fms-fsdp}}},
  note = {Accessed: 2025-01-23}
}


@misc{convnext,
 author = {{Meta Research}},
  title ={GitHub: {ConvNeXt}},
  howpublished = {\url{{https://github.com/facebookresearch/ConvNeXt}}},
  note = {Accessed: 2025-01-23}
}

@misc{MAE,
 author = {{Meta Research}},
  title ={GitHub: {MAE}},
  howpublished = {\url{{https://github.com/facebookresearch/mae}}},
  note = {Accessed: 2025-01-23}
}

@misc{DINO,
    author = {{Meta Research}},
  title =        {GitHub: {DINO}},
  howpublished = {\url{{https://github.com/facebookresearch/dino}}},
  note = {Accessed: 2025-01-23}
}

@misc{DiT,
 author = {{Meta Research}},
  title =        {GitHub: {DiT}},
  howpublished = {\url{{https://github.com/facebookresearch/DiT}}},
  note = {Accessed: 2025-01-23}
}

@misc{wav2vec2,
    author = {{Meta Research}},
  title =        {GitHub: wav2vec 2.0},
  howpublished = {\url{{https://github.com/facebookresearch/fairseq}}},
  note = {Accessed: 2025-01-23}
}






@misc{caduceus,
    author={{Kuleshov Group}},
  title =        {Github: Caduceus},
  howpublished = {\url{{https://github.com/kuleshov-group/caduceus.git}}},
  note = {Accessed: 2025-01-23}
}

@misc{hyena,
    author={HazyResearch},
  title =        {Github: HyenaDNA},
  howpublished = {\url{{https://github.com/HazyResearch/hyena-dna.git}}},
  note = {Accessed: 2025-01-23}
}


@article{huang2023normalization,
  title={Normalization techniques in training dnns: Methodology, analysis and application},
  author={Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  journal={TPAMI},
  year={2023},
  publisher={IEEE}
}

@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
    journal={arXiv preprint arXiv:2412.19437},
    year={2024}      
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  year={2021},
}


@article{heimersheim2024you,
  title={You can remove GPT2's LayerNorm by fine-tuning},
  author={Stefan Heimersheim},
  journal={arXiv preprint arXiv:2409.13710},
  year={2024}
}

@article{li2024mix,
  title={Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN},
  author={Li, Pengxiang and Yin, Lu and Liu, Shiwei},
  journal={arXiv preprint arXiv:2412.13795},
  year={2024}
}


@article{schiff2024caduceus,
  title={Caduceus: Bi-directional equivariant long-range dna sequence modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2403.03234},
  year={2024}
}

@inproceedings{zhai2023stabilizing,
  title={Stabilizing transformer training by preventing attention entropy collapse},
  author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
  booktitle={ICML},
  year={2023},
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={NeurIPS},
  year={2016}
}



@article{mueller2024normalization,
  title={Normalization layers are all that sharpness-aware minimization needs},
  author={Mueller, Maximilian and Vlaar, Tiffany and Rolnick, David and Hein, Matthias},
  journal={NeurIPS},
  year={2024}
}

@article{dai2024crucial,
  title={The crucial role of normalization in sharpness-aware minimization},
  author={Dai, Yan and Ahn, Kwangjun and Sra, Suvrit},
  journal={NeurIPS},
  year={2024}
}


@article{sun2025cursedepthlargelanguage,
    title={The Curse of Depth in Large Language Models}, 
    author={Wenfang Sun and Xinyuan Song and Pengxiang Li and Lu Yin and Yefeng Zheng and Shiwei Liu},
    journal={arXiv preprint arXiv:2502.05795},
    year={2025}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}



@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  year={2016}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}




@article{chandra2021novel,
  title={A novel method for scalable VLSI implementation of hyperbolic tangent function},
  author={Chandra, Mahesh},
  journal={IEEE Design \& Test},
  volume={39},
  number={1},
  pages={85--91},
  year={2021},
  publisher={IEEE}
}