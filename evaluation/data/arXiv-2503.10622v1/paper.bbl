\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adrian(1926)]{adrian1926impulses}
Edgar~D Adrian.
\newblock The impulses produced by sensory nerve endings: Part 1.
\newblock \emph{The Journal of Physiology}, 1926.

\bibitem[Adrian and Zotterman(1926{\natexlab{a}})]{adrian1926impulses2}
Edgar~D Adrian and Yngve Zotterman.
\newblock The impulses produced by sensory nerve-endings: Part 2. the response of a single end-organ.
\newblock \emph{The Journal of Physiology}, 1926{\natexlab{a}}.

\bibitem[Adrian and Zotterman(1926{\natexlab{b}})]{adrian1926impulses3}
Edgar~D Adrian and Yngve Zotterman.
\newblock The impulses produced by sensory nerve endings: Part 3. impulses set up by touch and pressure.
\newblock \emph{The Journal of Physiology}, 1926{\natexlab{b}}.

\bibitem[Arora et~al.(2018)Arora, Li, and Lyu]{arora2018theoretical}
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock \emph{arXiv preprint arXiv:1812.03981}, 2018.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bachlechner et~al.(2021)Bachlechner, Majumder, Mao, Cottrell, and McAuley]{bachlechner2021rezero}
Thomas Bachlechner, Bodhisattwa~Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock In \emph{UAI}, 2021.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and Auli]{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and McWilliams]{balduzzi2017shattered}
David Balduzzi, Marcus Frean, Lennox Leary, JP~Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
\newblock The shattered gradients problem: If resnets are the answer, then what is the question?
\newblock In \emph{ICML}, 2017.

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and Weinberger]{bjorck2018understanding}
Nils Bjorck, Carla~P Gomes, Bart Selman, and Kilian~Q Weinberger.
\newblock Understanding batch normalization.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Brock et~al.(2021{\natexlab{a}})Brock, De, and Smith]{brock2021characterizing}
Andrew Brock, Soham De, and Samuel~L Smith.
\newblock Characterizing signal propagation to close the performance gap in unnormalized resnets.
\newblock \emph{arXiv preprint arXiv:2101.08692}, 2021{\natexlab{a}}.

\bibitem[Brock et~al.(2021{\natexlab{b}})Brock, De, Smith, and Simonyan]{brock2021high}
Andrew Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Cai et~al.(2024)Cai, Cao, Chen, Chen, Chen, Chen, Chen, Chen, Chen, Chu, et~al.]{cai2024internlm2}
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et~al.
\newblock Internlm2 technical report.
\newblock \emph{arXiv preprint arXiv:2403.17297}, 2024.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced search space.
\newblock In \emph{CVPR Workshops}, 2020.

\bibitem[Dai et~al.(2024)Dai, Ahn, and Sra]{dai2024crucial}
Yan Dai, Kwangjun Ahn, and Suvrit Sra.
\newblock The crucial role of normalization in sharpness-aware minimization.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Daneshmand et~al.(2020)Daneshmand, Kohler, Bach, Hofmann, and Lucchi]{daneshmand2020batch}
Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi.
\newblock Batch normalization provably avoids ranks collapse for randomly initialised deep networks.
\newblock \emph{NeurIPS}, 2020.

\bibitem[De and Smith(2020)]{de2020batch}
Soham De and Sam Smith.
\newblock Batch normalization biases residual blocks towards the identity function in deep networks.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Feng et~al.(2024)Feng, Tung, Ahmed, Bengio, and Hajimirsadegh]{feng2024were}
Leo Feng, Frederick Tung, Mohamed~Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh.
\newblock Were rnns all we needed?
\newblock \emph{arXiv preprint arXiv:2410.01201}, 2024.

\bibitem[{Foundation Model Stack}()]{fms-fsdp}
{Foundation Model Stack}.
\newblock Github: {FMS} {FSDP}.
\newblock \url{{https://github.com/foundation-model-stack/fms-fsdp}}.
\newblock Accessed: 2025-01-23.

\bibitem[Gao et~al.()Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation.
\newblock \url{https://zenodo.org/records/10256836}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Geng and Liu(2023)]{openlm2023openllama}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama, 2023.
\newblock \url{https://github.com/openlm-research/open_llama}.

\bibitem[GRCh38(2013)]{grch382013p13}
Ensembl GRCh38.
\newblock p13 (genome reference consortium human build 38), insdc assembly, 2013.

\bibitem[Gre{\v{s}}ov{\'a} et~al.(2023)Gre{\v{s}}ov{\'a}, Martinek, {\v{C}}ech{\'a}k, {\v{S}}ime{\v{c}}ek, and Alexiou]{grevsova2023genomic}
Katar{\'\i}na Gre{\v{s}}ov{\'a}, Vlastimil Martinek, David {\v{C}}ech{\'a}k, Petr {\v{S}}ime{\v{c}}ek, and Panagiotis Alexiou.
\newblock Genomic benchmarks: a collection of datasets for genomic sequence classification.
\newblock \emph{BMC Genomic Data}, 2023.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Zhang, Xu, Zhu, Ma, Wang, Bi, et~al.]{guo2025deepseek}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}, 2025.

\bibitem[HazyResearch()]{hyena}
HazyResearch.
\newblock Github: Hyenadna.
\newblock \url{{https://github.com/HazyResearch/hyena-dna.git}}.
\newblock Accessed: 2025-01-23.

\bibitem[He and Hofmann(2023)]{he2023simplifying}
Bobby He and Thomas Hofmann.
\newblock Simplifying transformer blocks.
\newblock \emph{arXiv preprint arXiv:2311.01906}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Heimersheim(2024)]{heimersheim2024you}
Stefan Heimersheim.
\newblock You can remove gpt2's layernorm by fine-tuning.
\newblock \emph{arXiv preprint arXiv:2409.13710}, 2024.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and Weinberger]{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{ECCV}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Liu, Lang, and Tao]{huang2017centered}
Lei Huang, Xianglong Liu, Yang Liu, Bo~Lang, and Dacheng Tao.
\newblock Centered weight normalization in accelerating training of deep neural networks.
\newblock In \emph{ICCV}, 2017.

\bibitem[Huang et~al.(2023)Huang, Qin, Zhou, Zhu, Liu, and Shao]{huang2023normalization}
Lei Huang, Jie Qin, Yi~Zhou, Fan Zhu, Li~Liu, and Ling Shao.
\newblock Normalization techniques in training dnns: Methodology, analysis and application.
\newblock \emph{TPAMI}, 2023.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
Xiao~Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{ICML}, 2020.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Karakida et~al.(2019)Karakida, Akaho, and Amari]{karakida2019normalization}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock The normalization method for alleviating pathological sharpness in wide neural networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[{Kuleshov Group}()]{caduceus}
{Kuleshov Group}.
\newblock Github: Caduceus.
\newblock \url{{https://github.com/kuleshov-group/caduceus.git}}.
\newblock Accessed: 2025-01-23.

\bibitem[LeCun(2022)]{lecun2022path}
Yann LeCun.
\newblock A path towards autonomous machine intelligence version 0.9.2, 2022-06-27.
\newblock \emph{Open Review}, 2022.

\bibitem[Li et~al.(2024)Li, Yin, and Liu]{li2024mix}
Pengxiang Li, Lu~Yin, and Shiwei Liu.
\newblock Mix-ln: Unleashing the power of deeper layers by combining pre-ln and post-ln.
\newblock \emph{arXiv preprint arXiv:2412.13795}, 2024.

\bibitem[Liu et~al.(2024)Liu, Feng, Wang, Wang, Liu, Zhao, Dengr, Ruan, Dai, Guo, et~al.]{liu2024deepseek}
Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo~Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et~al.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.
\newblock \emph{arXiv preprint arXiv:2405.04434}, 2024.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In \emph{CVPR}, 2022.

\bibitem[Loshchilov et~al.(2024)Loshchilov, Hsieh, Sun, and Ginsburg]{loshchilov2024ngpt}
Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg.
\newblock ngpt: Normalized transformer with representation learning on the hypersphere.
\newblock \emph{arXiv preprint arXiv:2410.01131}, 2024.

\bibitem[Lubana et~al.(2021)Lubana, Dick, and Tanaka]{lubana2021beyond}
Ekdeep~S Lubana, Robert Dick, and Hidenori Tanaka.
\newblock Beyond batchnorm: Towards a unified understanding of normalization in deep learning.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora.
\newblock Understanding the generalization benefit of normalization layers: Sharpness reduction.
\newblock \emph{NeurIPS}, 2022.

\bibitem[{Meta Research}({\natexlab{a}})]{convnext}
{Meta Research}.
\newblock Github: {ConvNeXt}.
\newblock \url{{https://github.com/facebookresearch/ConvNeXt}}, {\natexlab{a}}.
\newblock Accessed: 2025-01-23.

\bibitem[{Meta Research}({\natexlab{b}})]{dino}
{Meta Research}.
\newblock Github: {DINO}.
\newblock \url{{https://github.com/facebookresearch/dino}}, {\natexlab{b}}.
\newblock Accessed: 2025-01-23.

\bibitem[{Meta Research}({\natexlab{c}})]{dit}
{Meta Research}.
\newblock Github: {DiT}.
\newblock \url{{https://github.com/facebookresearch/DiT}}, {\natexlab{c}}.
\newblock Accessed: 2025-01-23.

\bibitem[{Meta Research}({\natexlab{d}})]{mae}
{Meta Research}.
\newblock Github: {MAE}.
\newblock \url{{https://github.com/facebookresearch/mae}}, {\natexlab{d}}.
\newblock Accessed: 2025-01-23.

\bibitem[{Meta Research}({\natexlab{e}})]{wav2vec2}
{Meta Research}.
\newblock Github: wav2vec 2.0.
\newblock \url{{https://github.com/facebookresearch/fairseq}}, {\natexlab{e}}.
\newblock Accessed: 2025-01-23.

\bibitem[Mueller et~al.(2024)Mueller, Vlaar, Rolnick, and Hein]{mueller2024normalization}
Maximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein.
\newblock Normalization layers are all that sharpness-aware minimization needs.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Nguyen et~al.(2024)Nguyen, Poli, Faizi, Thomas, Wornow, Birch-Sykes, Massaroli, Patel, Rabideau, Bengio, et~al.]{nguyen2024hyenadna}
Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et~al.
\newblock Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Nguyen and Salazar(2019)]{nguyen2019transformers}
Toan~Q Nguyen and Julian Salazar.
\newblock Transformers without tears: Improving the normalization of self-attention.
\newblock \emph{arXiv preprint arXiv:1910.05895}, 2019.

\bibitem[Ni et~al.(2024)Ni, Guo, Jia, and Huang]{ni2024nonlinearity}
Yunhao Ni, Yuxin Guo, Junlong Jia, and Lei Huang.
\newblock On the nonlinearity of layer normalization.
\newblock \emph{arXiv preprint arXiv:2406.01255}, 2024.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and Khudanpur]{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{ICASSP}, 2015.

\bibitem[Peebles and Xie(2023)]{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{ICCV}, 2023.

\bibitem[Qiao et~al.(2019)Qiao, Wang, Liu, Shen, and Yuille]{qiao2019micro}
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille.
\newblock Micro-batch training with batch-channel normalization and weight standardization.
\newblock \emph{arXiv preprint arXiv:1903.10520}, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{JMLR}, 2020.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and Madry]{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock \emph{NeurIPS}, 2018.

\bibitem[Schiff et~al.(2024)Schiff, Kao, Gokaslan, Dao, Gu, and Kuleshov]{schiff2024caduceus}
Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov.
\newblock Caduceus: Bi-directional equivariant long-range dna sequence modeling.
\newblock \emph{arXiv preprint arXiv:2403.03234}, 2024.

\bibitem[Shao et~al.(2020)Shao, Hu, Wang, Xue, and Raj]{shao2020normalization}
Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj.
\newblock Is normalization indispensable for training deep neural network?
\newblock \emph{NeurIPS}, 2020.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith et~al.(2023)Smith, Brock, Berrada, and De]{smith2023convnets}
Samuel~L Smith, Andrew Brock, Leonard Berrada, and Soham De.
\newblock Convnets match vision transformers at scale.
\newblock \emph{arXiv preprint arXiv:2310.16764}, 2023.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Sun et~al.(2025)Sun, Song, Li, Yin, Zheng, and Liu]{sun2025cursedepthlargelanguage}
Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu~Yin, Yefeng Zheng, and Shiwei Liu.
\newblock The curse of depth in large language models.
\newblock \emph{arXiv preprint arXiv:2502.05795}, 2025.

\bibitem[Sun et~al.(2024)Sun, Li, Dalal, Xu, Vikram, Zhang, Dubois, Chen, Wang, Koyejo, et~al.]{sun2024learning}
Yu~Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et~al.
\newblock Learning to (learn at test time): Rnns with expressive hidden states.
\newblock \emph{arXiv preprint arXiv:2407.04620}, 2024.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Tanaka and Kunin(2021)]{tanaka2021noether}
Hidenori Tanaka and Daniel Kunin.
\newblock Noether’s learning dynamics: Role of symmetry breaking in neural networks.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2016instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{ECCV}, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{ICML}, 2020.

\bibitem[Xu et~al.(2019)Xu, Sun, Zhang, Zhao, and Lin]{xu2019understanding}
Jingjing Xu, Xu~Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin.
\newblock Understanding and improving layer normalization.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, et~al.]{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al.
\newblock Qwen2 technical report.
\newblock \emph{arXiv preprint arXiv:2407.10671}, 2024.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram, Zhang, Gu, and Susskind]{zhai2023stabilizing}
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua~M Susskind.
\newblock Stabilizing transformer training by preventing attention entropy collapse.
\newblock In \emph{ICML}, 2023.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhang et~al.(2024)Zhang, Dong, Zang, Cao, Qian, Chen, Guo, Duan, Wang, Ouyang, et~al.]{zhang2024internlm}
Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et~al.
\newblock Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output.
\newblock \emph{arXiv preprint arXiv:2407.03320}, 2024.

\end{thebibliography}
