\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\usepackage[numbers,sort&compress]{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table]{xcolor}  % for colored tables
\usepackage{amsmath}        % for aligned equations
\usepackage{enumitem}       % for customizing itemize
\usepackage{graphicx}       % for images
\usepackage{multirow}       % for multi-row cells in tables
\usepackage{listings}       % for code listings
\usepackage{tcolorbox}      % for colored boxes

\newcommand{\thinkrow}[1]{\rowcolor{blue!10} \texttt{<think>} #1 \texttt{</think>} \\}
\newcommand{\searchrow}[1]{\rowcolor{green!10} \texttt{<search>} #1 \texttt{</search>} \\}
\newcommand{\resultrow}[1]{\rowcolor{gray!10} \texttt{<result>} #1 \texttt{</result>} \\}
\newcommand{\answerrow}[1]{\rowcolor{yellow!10} \texttt{<answer>} #1 \texttt{</answer>} \\}

% for prompt box
\definecolor{mygray}{rgb}{0.94,0.95,0.95}
\lstdefinelanguage{prompt}{
    basicstyle=\normalfont\fontfamily{pcr}\selectfont,
    showstringspaces=false,
    breaklines=True,
    backgroundcolor=\color{mygray},
}
\tcbuselibrary{listings,skins,breakable}
\tcbset{
    enhanced,
    breakable,
    colback=mygray, %
    colframe=black, %
    boxrule=0.2mm, %
    arc=1mm, %
    top=1pt,
    bottom=1pt,
    left=2pt,
    right=2pt,
    listing only,
    listing options={
        basicstyle=\ttfamily\small, %
        language=Java, %
    },
}

\hypersetup{
    colorlinks=true, % Enable colored links
    linkcolor=blue,  % Color of internal links (sections, equations, etc.)
    citecolor=blue,  % Color of citation links (bibliography)
    filecolor=blue,  % Color of file links
    urlcolor=blue    % Color of external links (URLs)
}

\title{\textit{ReSearch}: Learning to \textit{Re}ason with \textit{Search} for LLMs via Reinforcement Learning}

\vspace{-1.0em}
\author{
  Mingyang Chen$^{1}$, Tianpeng Li$^{1}$, Haoze Sun$^{1}$, Yijie Zhou$^{1}$, Chenzheng Zhu$^{1}$, \\
  \textbf{Haofen Wang$^{2}$,} \textbf{Jeff Z. Pan$^{3}$,} \textbf{Wen Zhang$^{4}$,} \textbf{Huajun Chen$^{4}$,} \\ 
  \textbf{Fan Yang$^{1}$\thanks{Corresponding author}~,} \textbf{Zenan Zhou$^{1}$,} \textbf{Weipeng Chen$^{1}$} \\[2pt]
  $^{1}$Baichuan Inc. 
  $^{2}$Tongji University
  $^{3}$The University of Edinburgh
  $^{4}$Zhejiang University \\[2pt]
  \texttt{\{chenmingyang, yangfan\}@baichuan-inc.com} \\[2pt]
  \texttt{\url{https://github.com/Agent-RL/ReSearch}}
}

\begin{document}

\maketitle
\vspace{-1.0em}
\begin{abstract}
  Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. 
  However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. 
  We propose \textbf{\textit{ReSearch}}, a novel framework that trains LLMs to \textbf{\textit{Re}}ason with \textbf{\textit{Search}} via reinforcement learning without using any supervised data on reasoning steps. 
  Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning.
  We train \textit{ReSearch} on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments.
  Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks.
  Analysis reveals that \textit{ReSearch} naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.
\end{abstract}

\vspace{-1.0em}
\begin{figure}[htbp]
  \centering
  \hspace{-0.07\textwidth}
  \includegraphics[width=\textwidth]{fig/intro_bar.pdf}
  \caption{Performance of \textit{ReSearch} and baselines on benchmarks.}
  \label{fig:intro}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% llm overview
In recent years, Large Language Models (LLMs) have demonstrated remarkable performance across a wide array of tasks \cite{qwen25-tech-report,baichuan-tech-report,deepseek-r1,claude-37-sonnet}. Beyond leveraging internal knowledge acquired during pre-training, LLMs exhibit the capability to utilize external tools, particularly search engines, to retrieve factual and time-sensitive information, thereby mitigating instances of hallucination \cite{toolformer,hugginggpt,agentboard,button}. This capability, often referred to as Retrieval-Augmented Generation (RAG), has been the subject of extensive investigation in recent literature \cite{rag-survey,crag,selfrag,retrieval-survey}.
Despite the effectiveness of RAG, designing robust multi-step RAG strategies applicable to complex real-world problems remains a significant challenge. This is particularly crucial, as many real-world issues are inherently complex and necessitate several steps of reasoning \cite{lpkg,iterretgen,ircot}.

% reasoning and rag need reasoning
The past year has witnessed considerable advancements in LLMs' reasoning abilities, particularly through chain-like reasoning before producing final outputs \cite{cot,star}.
This progress is exemplified by the success of OpenAI-o1 \cite{openai-o1}, and DeepSeek-R1 \cite{deepseek-r1}. 
These developments emphasize the importance of test-time scaling in reasoning, enabling LLMs to decompose intricate problems into manageable intermediate steps \cite{test-time-scaling,s1}. This reasoning capacity is also vital for the efficacy of RAG, especially when addressing complex questions that require multiple retrieval steps. Nonetheless, training LLMs to conduct interactive reasoning alongside information retrieval continues to present an open challenge for the research community.
Most existing approaches to multi-step RAG rely on manually designed prompts or heuristics, which are not only labor-intensive but also lack scalability for more intricate problems \cite{iterretgen,ircot,selfask}. Additionally, labeling reasoning steps in a multi-step RAG framework is often impractical due to the associated costs and time constraints.

Reinforcement learning (RL) has emerged as a promising avenue for enhancing reasoning capabilities without the need for supervised data regarding reasoning steps \cite{deepseek-r1,deepseekmath}. This approach holds potential for training LLMs to exhibit reasoning skills solely based on simple reward signals derived from final outcomes. 
Recent advancements in RL-based training for LLMs have demonstrated significant improvements in complex reasoning tasks, where models learn to decompose problems into manageable steps through trial and error rather than explicit instruction. Models such as DeepSeek-R1 have shown that rule-based reward functions can effectively guide LLMs to develop sophisticated reasoning patterns autonomously. Despite these successes, current approaches primarily focus on enhancing internal reasoning capabilities, with limited exploration of how to effectively combine this reasoning process with external knowledge retrieval. 

% our work
In this paper, we propose a novel framework for training LLMs to \textit{Re}ason with \textit{Search} via reinforcement learning, which we term \textbf{\textit{ReSearch}}.
The reasoning chain in this framework is not only composed of text-based thinking (i.e., enclosed by \texttt{<think>} \texttt{</think>}) as DeepSeek-R1, but also search query (i.e., enclosed by \texttt{<search>} \texttt{</search>}) and retrieval results (i.e., enclosed by \texttt{<result>} \texttt{</result>}).
We treat the search operation as part of the chain-like reasoning process, and the search operation will interact with text-based thinking. Specifically, when and how to perform search will be steered by previous text-based thinking and the search results will infuence subsequent text-based thinking.
In the framework, we don't provide any supervised data on reasoning steps for LLMs to imitate, instead, we leverage reinforcement learning (i.e., GRPO) to incentivize LLMs to perform reasoning with search.

We train \textit{ReSearch} from scratch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct), and conduct extensive experiments on multi-hop question answering benchmarks that need multi-step reasoning and multiple information retrieval. Our trained models show significant absolute improvements range from 8.9\% to 22.4\% over the baselines.
Furthermore, our training is only conducted on one specific training set, and trained models are evaluated on multiple benchmarks, showing the generalizability of our framework.
Our contributions are as follows:
\begin{itemize}[leftmargin=*]
  \item By emphasizing the interaction between reasoning and search, we propose a novel framework \textit{ReSearch} that using reinforcement learning to train LLMs to reason with search from scratch, without any supervised data on reasoning steps.
  \item We train \textit{ReSearch} on different scales of models, and conduct extensive experiments on multi-hop question answering benchmarks, showing the effectiveness of this framework. The trained models show significant generalizability and potential for more realistic scenarios.
  \item By analyzing the training process, we demonstrate that \textit{ReSearch} can effectively elicit reasoning capabilities with search progressively itself, and that reasoning abilities such as reflection and self-correction can be incentivized without relying on any pre-defined heuristics.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Drawing inspiration from the success of OpenAI-o1 and DeepSeek-R1 in learning to reason, we incorporate search operation into the reasoning process and train LLMs from scratch using reinforcement learning (i.e., GRPO) without any lableled data on reasoning chains, making LLMs learn to \textit{Re}ason with \textit{Search} (\textit{ReSearch}).
In this section, we first show the details of training \textit{ReSearch}, dive into the details of the GRPO and how to conduct rollout with search during reinforcement learning (\S \ref{sec:method-rl}). 
Then, we demonstrate the prompt template design directing the LLMs to generate the defined format of rollout (\S \ref{sec:method-template}), and finally, we introduce the reward modeling for guiding the optimization of reinforcement learning (\S \ref{sec:method-reward}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/method.pdf}
  \caption{The training overview of \textit{ReSearch}. (a) The GRPO pipeline. (b) The details of the rollout generation process.}
  \label{fig:method}
\end{figure}

\subsection{Reinforcement Learning}
\label{sec:method-rl}

When handling complex multi-step tasks needing retrieval (i.e., multi-step RAG), reasoning is crucial for steering multiple retrieval (i.e., search) operations, mainly on when and how to perform search. It's challenging to collect labeled reasoning data with search for supervised fine-tuning LLMs to imitate how to reason with search.
Fortunately, reinforcement learning has shown impressive performance in training LLMs to conduct reasoning, which can elicit reasoning capabilities from LLMs without any supervised data. 
In general, the main idea behind reinforcement learning here is to sample multiple reasoning-with-search chains (i.e., rollouts) and optimize the policy (i.e., LLMs) to maximize the probability of generating rollouts with higher rewards, as described in Figure \ref{fig:method}.

\paragraph{Group Relative Policy Optimization}
Specifically, in this work, we use Group Relative Policy Optimization (GRPO) as the learning algorithm, which estimate the baseline from a group of rollouts instead of training a separate critic model in Proximal Policy Optimization (PPO).
Given an existing policy $\pi_{\theta_{\text{old}}}$ and an reference policy $\pi_{\theta_{\text{ref}}}$, base on $G$ rollouts $\tau = \{y_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\cdot|x)$ for each input $x \sim \mathcal{D}$, the objective of GRPO is to optimize the policy $\pi_{\theta}$ by maximizing the following objective:
\begin{equation}
\begin{aligned}
\label{eq:grpo}
  \mathcal{J}(\theta) & = \mathbb{E}_{x \sim \mathcal{D}, \{y_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\cdot|x)} \\
  & \frac{1}{G} \sum_{i=1}^{G} \left[\min \left( \frac{\pi_{\theta}(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)} A_{i}, \text{clip} \left( \frac{\pi_{\theta}(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)}, 1-\epsilon, 1+\epsilon \right) A_{i} \right) - \beta \mathbb{D}_{\text{KL}} \left( \pi_{\theta} || \pi_{\theta_{\text{ref}}} \right)\right],
\end{aligned}
\end{equation}
where $A_{i} = \left(r_i - \text{mean}(\{r_j\}_{j=1}^{G})\right) / \text{std}(\{r_j\}_{j=1}^{G})$ is the normalized advantage of the $i$-th rollout in current group, $\epsilon$ is the clipping ratio, and $\beta$ is the KL loss coefficient. 
Moreover, a KL divergence penalty is added to the objective to prevent the policy from deviating too much from the original reference policy LLMs. The illustration of GRPO is shown in Figure \ref{fig:method}(a).

\paragraph{Rollout with Search}
Compared with conventional rollout that only contains text-based thinking as reasoning, the rollout in \textit{ReSearch} also contains search queries and retrieval results.
We use \texttt{<search>} and \texttt{</search>} to enclose the search queries and \texttt{<result>} and \texttt{</result>} to enclose the retrieval results, and such instruction is described in the prompt templates, which will be introduced later in \S \ref{sec:method-template}.
The rollout process is an iterative process between text-based thinking, search queries, and retrieval results as described in Figure \ref{fig:method}(b). Specifically, when the generation process encounters \texttt{</search>} tag, the query between the last \texttt{<search>} and current \texttt{</search>} tags will be used as the search query to retrieve relevant factual information, and the retrieval results will be enclosed by \texttt{<result>} and \texttt{</result>} tags. Then, existing rollout concated with the retrieval results will be used as the next input to generate following response iteratively, until the generation encounters end-of-sentence (eos) tag (i.e., \texttt{<endoftext>} or \texttt{<im\_end>} in Qwen-2.5 Models).

\paragraph{Retrieval Result Masking}
In original GRPO, the loss is calculated by all the generated tokens in the whole rollout.
However, in \textit{ReSearch}, the rollout contains retrieval results, which are not generated by the training policy, but retrieved by the search environment.
Therefore, we mask the retrieval results in the loss calculation to avoid the training policy from being biased towards the retrieval results.
That is, during the computation of Equation \ref{eq:grpo}, we only consider the tokens in the text-based thinking and the search queries, and ignore the tokens in the retrieval results.

\subsection{Training Template}
\label{sec:method-template}

Since we orchestrate the rollout process by identifying our defined special tags (e.g., stopping at \texttt{</search>} and transferring control to the search environment), it is crucial for policy LLMs to generate output in the defined format. 
To guide the LLMs in understanding this rollout format—specifically, the tags indicating when the search operation is invoked—we created two prompt templates: one for the base (i.e., pre-trained) model and another for the instruction-tuned model. As shown in Table \ref{tab:prompt-template}, inspired by DeepSeek-R1, these templates are designed to be simple and concise, ensuring that the model can act as a natural progression during the reinforcement learning process. 
Specifically, for the \textit{base model}, this template, filled with a specific user question, will be used as direct input to the LLMs. For the \textit{instruction-tuned model}, its prompt template serves as the system prompt, utilized in conjunction with the corresponding chat template of the instruction-tuned LLM.

\begin{table}[t]
\caption{Prompt templates for training from base model and instruction-tuned model. For the base model, \textcolor{red}{prompt} will be replaced with the actual question. For the instruction-tuned model, this template is used as the system prompt.}
\label{tab:prompt-template}
\begin{center}
\begin{tabular}{p{0.95\textwidth}}
\toprule
\textbf{Prompt Template For Base Model} \\
\midrule 
A conversation between User and Assistant. \
The user asks a question, and the assistant solves it. \
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \
During thinking, the assistant can invoke the wikipedia search tool to search for fact information about specific topics if needed. \
The reasoning process and answer are enclosed within \texttt{<think>} \texttt{</think>} and \texttt{<answer>} \texttt{</answer>} tags respectively, \
and the search query and result are enclosed within \texttt{<search>} \texttt{</search>} and \texttt{<result>} \texttt{</result>} tags respectively. \
For example, \texttt{<think>} This is the reasoning process. \texttt{</think>} \texttt{<search>} search query here \texttt{</search>} \texttt{<result>} search result here \texttt{</result>} \
\texttt{<think>} This is the reasoning process. \texttt{</think>} \texttt{<answer>} The final answer is \verb|\boxed{answer here}| \texttt{</answer>}. \
In the last part of the answer, the final exact answer is enclosed within \verb|\boxed{}| with latex format. \
User: \textcolor{red}{\texttt{prompt}}. Assistant: \\
\midrule
\textbf{System Prompt Template For Instruction-Tuned Model} \\
\midrule
You are a helpful assistant that can solve the given question step by step with the help of the wikipedia search tool. \
Given a question, you need to first think about the reasoning process in the mind and then provide the answer. \
During thinking, you can invoke the wikipedia search tool to search for fact information about specific topics if needed. \
The reasoning process and answer are enclosed within \texttt{<think>} \texttt{</think>} and \texttt{<answer>} \texttt{</answer>} tags respectively, \
and the search query and result are enclosed within \texttt{<search>} \texttt{</search>} and \texttt{<result>} \texttt{</result>} tags respectively. \
For example, \texttt{<think>} This is the reasoning process. \texttt{</think>} \texttt{<search>} search query here \texttt{</search>} \texttt{<result>} search result here \texttt{</result>} \
\texttt{<think>} This is the reasoning process. \texttt{</think>} \texttt{<answer>} The final answer is \verb|\boxed{answer here}| \texttt{</answer>}. \
In the last part of the answer, the final exact answer is enclosed within \verb|\boxed{}| with latex format. \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Reward Modeling}
\label{sec:method-reward}

During reinforcement learning of \textit{ReSearch}, there is no supervised reasoning data, and we only use a simple reward on rollouts to guide the optimization of LLMs. Experimentally, only rule-based reward function is enough to successfully elicit capabilities of reasoning with search for LLMs. Our reward function considers following two parts: answer reward and format reward. 
\begin{itemize}[leftmargin=*]
  \item \textbf{Answer Reward}: We calculate the correctness of the final answer in $\verb|\boxed{}|$ and the ground truth answer via F1 score.
  \item \textbf{Format Reward}: We check whether the rollout correctly follows our defined format as described in the prompt templates, mainly checking the correctness of tags and existence of $\verb|\boxed{}|$ in the answer.
\end{itemize}
Specifically, for the final reward of a rollout:
\begin{equation}
  r = 
  \begin{cases}
    \text{f1}(a_{\text{pred}}, a_{\text{gt}}), & \text{if f1 score is not 0} \\
    0.1, & \text{if f1 score is 0 and format is correct} \\
    0, & \text{if f1 score is 0 and format is incorrect} \\
  \end{cases}
\end{equation}
where $a_{\text{pred}}$ is the final answer in $\verb|\boxed{}|$ and $a_{\text{gt}}$ is the ground truth answer, and $\text{f1}(a_{\text{pred}}, a_{\text{gt}})$ is the F1 score between $a_{\text{pred}}$ and $a_{\text{gt}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experiment Setup}

To evaluate the effectiveness of \textit{ReSearch}, we conduct extensive experiments mainly on multi-hop question answering benchmarks that need multi-step reasoning and multiple information retrieval. 
Our \textit{ReSearch} is trained from Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-32B and Qwen2.5-32B-Instruct \cite{qwen25-tech-report}. During training, we only use the data from training set of MuSiQue \cite{musique}, since it has various types of multi-hop questions and constructed via fine-grained quality control.

\paragraph{Benchmarks} 
We use four standard benchmarks on multi-hop question answering tasks, including HotpotQA \cite{hotpotqa}, 2WikiMultiHopQA \cite{2wiki}, MuSiQue \cite{musique}, and Bamboogle \cite{selfask}.
Specifically, HotpotQA, 2WikiMultiHopQA, and MuSiQue are constructed among wikipedia or wikidata \cite{wikidata}, via different multi-hop mining strategies with crowd-sourcing, while Bamboogle is manually constructed dataset with 2-hop questions, where all questions are sufficiently difficult to be unanswerable by a popular internet search engine. 
Our evaluation is conducted on the full dev set of HotpotQA, 2WikiMultiHopQA, and MuSiQue, and the test set of Bamboogle, including 7405, 12576, 2417, 125 samples respectively.
Note that we discard the context documents from the original datasets for HotpotQA, 2WikiMultiHopQA, and MuSiQue, and only use the question and answer pairs for evaluation. 
We use an open-ended retrieval environment based on wikipedia to retrieve the background knowledge for all the datasets, which we introduce later.

\paragraph{Baselines}
We first compare \textit{ReSearch} with two naive baselines: (1) \textit{No RAG}: Use corresponding instruction-tuned model to generate answer directly without any RAG, and (2) \textit{Naive RAG}: A naive retrieval-based setting that concatenate the retrieval results with question and then generate answer directly.
Furthermore, we also consider two approaches focusing on improving multi-step RAG: (3) \textit{Iter-RetGen} \cite{iterretgen}: A method synergizes retrieval and generation in an iterative manner, and (4) \textit{IRCoT} \cite{ircot}: An iterleaving method, which use retrieval and the chain-of-thought (CoT) guide each other. Since these methods are prompt-based, we use instruction-tuned models in same size as our \textit{ReSearch} to implement them for fair comparison.

\paragraph{Evaluation Metrics}
For evaluate the correctness of the final answer, we first use Exact Match (\textit{EM}) where the prediction is correct if it matches the ground truth answer exactly.
However, such exact match is too strict for our setting, since the retrieval environment is open-ended and the result is described by natural language.
Therefore, we also consider LLM-as-a-judge (\textit{LJ}) for automatic evaluation, where we use gpt-4o-mini with our defined judge prompt to score the correctness of the final answer. Such judge prompt is shown in Appendix \ref{app:prompt-judge}.

\paragraph{Implementation Details}
We conduct our training and evaluation on Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-32B and Qwen2.5-32B-Instruct. The reinforcement learning framework is built on verl \cite{hybridflow}. We only use the training set (19938 samples) of MuSiQue for training, and the number of training epochs is 2.
The retrieval environment is based on FlashRAG \cite{flashrag}, a standard toolkit for RAG research. We use E5-base-v2 \cite{e5} as the retriever and Wikipedia data from Dec. 2018 as the knowledge base \cite{dpr}. All the corpus indexing and embedding has been preprocessed by FlashRAG. 
During the rollout in training and evaluation, we retrieve top-5 results for each query. For baseline methods, we use the implementation from FlashRAG.
For details about model training, please refer to Appendix \ref{app:implementation-details}.

\subsection{Main Results}

The main results of baselines and \textit{ReSearch} are demonstrated in Table \ref{tab:main-result}, and we show the methods based on LLMs with different sizes respectively. From the main results, we can draw the following observations:

\paragraph{Effectiveness of \textit{ReSearch}} 
Compared with all the baselines, \textit{ReSearch} achieves significant improvements on all the benchmarks, which demonstrates the effectiveness of our proposed framework. Specifically, among all the benchmarks, the average improvement of \textit{ReSearch} over the best baseline is \textbf{15.81\%} in exact match and \textbf{17.56\%} in LLM-as-a-judge, for Qwen2.5 model with 7B parameters. For Qwen2.5 model with 32B parameters, the average improvement is \textbf{14.82\%} in exact match and \textbf{15.46\%} in LLM-as-a-judge. 

\paragraph{Comparison between base and instruction-tuned models}
We train \textit{ReSearch} from both base and instruction-tuned models with 7B and 32B parameters respectively, and note that they are all trained using reinforcement learning from scratch without any supervised fine-tuning. 
From the results, we can observe training from the instruction-tuned model can further improve the performance of \textit{ReSearch}. Such observation is consistent among all the benchmarks and model sizes.

\paragraph{Generalization Ability}
During reinforcement learning, \textit{ReSearch} learns the ability of reasoning with search, which is independent of specific knowledge or multi-hop patterns, and such ability is generalizable.
Our model \textit{ReSearch} is only trained on the training set of MuSiQue dataset, but from the results, we can observe that it can generalize to other benchmarks with different question types and structures, which demonstrates the generalization ability of \textit{ReSearch}.

\begin{table}[htbp]
\caption{Exact Match (EM, \%) and LLM-as-a-Judge (LJ, \%) results on multi-hop question answering benchmarks. The best results are highlighted in bold, and the best results across baselines are underlined.} 
\label{tab:main-result}
\begin{center}
\begin{tabular}{lcc|cc|cc|cc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{HotpotQA}} & \multicolumn{2}{c}{\textbf{2Wiki}} & \multicolumn{2}{c}{\textbf{MuSiQue}} & \multicolumn{2}{c}{\textbf{Bamboogle}} \\
% \multicolumn{1}{c}{{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{HotpotQA}} & \multicolumn{2}{c}{\textbf{2Wiki}} & \multicolumn{2}{c}{\textbf{MuSiQue}} & \multicolumn{2}{c}{\textbf{Bamboogle}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& \multicolumn{1}{c}{\textit{EM}} & \multicolumn{1}{c}{\textit{LJ}} & \multicolumn{1}{c}{\textit{EM}} & \multicolumn{1}{c}{\textit{LJ}} & \multicolumn{1}{c}{\textit{EM}} & \multicolumn{1}{c}{\textit{LJ}} & \multicolumn{1}{c}{\textit{EM}} & \multicolumn{1}{c}{\textit{LJ}} \\
\midrule
\multicolumn{9}{l}{\textbf{Qwen2.5-7B(-Instruct)}} \\
Naive Generation & 19.18 & 30.64 & 25.76 & 27.87 & 3.76 & 10.38 & 10.40 & 22.40  \\
Naive RAG & 31.90 & 49.59 & 25.78 & 29.52 & 6.21 & 12.78 & 20.80 & 32.00  \\
Iter-RetGen & \underline{34.36} & \underline{52.22} & \underline{27.92} & \underline{31.86} & \underline{8.69} & \underline{16.14} & 21.60 & 35.20  \\
IRCoT & 30.33 & 52.06 & 21.57 & 30.65 & 6.99 & 14.19 & \underline{24.80} & \underline{36.80}  \\
\midrule
\textit{ReSearch}-{\small Qwen-7B} & 40.57 & 60.26 & 44.67 & 50.06 & 21.68 & 32.19 & \textbf{43.20} & \textbf{54.40}  \\
\textit{ReSearch}-{\small Qwen-7B-Instruct} & \textbf{43.52} & \textbf{63.62} & \textbf{47.59} & \textbf{54.22} & \textbf{22.30} & \textbf{33.43} & 42.40 & \textbf{54.40}  \\
\toprule
\multicolumn{9}{l}{\textbf{Qwen2.5-32B(-Instruct)}} \\
Naive Generation & 24.63 & 38.26 & 27.23 & 29.68 & 6.12 & 14.23 & 18.40 & 29.60  \\
Naive RAG & 36.46 & 55.73 & 30.38 & 34.87 & 9.27 & 15.97 & 23.20 & 40.80  \\
Iter-RetGen & \underline{39.81} & \underline{58.80} & \underline{33.64} & \underline{38.22} & \underline{12.49} & \underline{20.11} & 29.60 & 44.80  \\
IRCoT & 28.44 & 55.44 & 13.53 & 29.50 & 7.82 & 18.20 & \underline{31.20} & \underline{47.20}  \\
\midrule
\textit{ReSearch}-{\small Qwen-32B} & 42.77 & 64.27 & 38.52 & 45.59 & \textbf{26.40} & 37.57 & 54.40 & 66.40  \\
\textit{ReSearch}-{\small Qwen-32B-Instruct} & \textbf{46.73} & \textbf{67.70} & \textbf{44.90} & \textbf{50.30} & \textbf{26.40} & \textbf{38.56} & \textbf{56.80} & \textbf{67.20}  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Further Analysis}

We investigate the important metrics during training \textit{ReSearch} in this section.
Specifically, the response length and number of search operations during training are shown in Figure \ref{fig:len} respectively. The curve of training reward and validation reward are shown in Figure \ref{fig:reward}. The validation is conducted on a part of development set of MuSiQue dataset with 100 random samples, and conducted every 10 steps during training.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/len_train_val.pdf}
  \caption{Response length and number of search operations during training.}
  \label{fig:len}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/reward_train_val.pdf}
  \caption{Training and validation reward during training.}
  \label{fig:reward}
\end{figure}

\paragraph{Response Length}

We define the response length as the total number of tokens in a model's output, excluding retrieval results, which can be interpreted as the test-time cost of reasoning. 
From the first two figures in Figure \ref{fig:len}, it is evident that the response length generally increases throughout the training process. Instruction-tuned models exhibit longer response lengths compared to base models for both the 7B and 32B parameters. 
Additionally, for the 32B models, the response length initially decreases during the first 20 training steps before increasing again after approximately the 60th step. This observed behavior may be attributed to the 32B models having more inherent knowledge, leading them to generate longer responses initially, and do not utilize retrieval.
However, after receiving some guidance from reward signals early in the training, they begin to leverage searches more frequently, thereby reducing their reliance on generated knowledge.

\paragraph{Number of Search Operations}

We also count the average number of search operations in one rollout during training, which is shown in the last two figures in Figure \ref{fig:len}.
As depicted in the figures, the number of search operations shows consistent growth throughout the training process. Such trend indicates that for complex multi-hop questions, the model gradually learns to use search to retrieve relevant information iteratively multiple times.

\paragraph{Training and Validation Reward}

We show the training and validation reward during reinforcement learning of \textit{ReSearch} in Figure \ref{fig:reward}. The reward increases sharply during the first 20 training steps, and then gradually increases. The cold-start reward is higher for instruction-tuned models for both 7B and 32B models. For 7B models, the rewards converge to a similar level for both base and instruction-tuned models, while for 32B models, the training reward of instruction-tuned models is higher than that of base models.

\subsection{Case Study}

\begin{table}[htbp]
  \caption{A case study of \textit{ReSearch} during training.}
  \label{tab:case-study}
  \begin{center}
  \begin{tabular}{p{0.95\textwidth}}
  \toprule
  \textbf{Question:} Who won the 2018 presidential election in the country where the political party of Martín Ramírez Pech operates? \\
  \textbf{Answer:} Andrés Manuel López Obrador \\
  \midrule 
  \thinkrow{To find the answer, I first need to determine the country in which Martín Ramírez Pech's political party operates. I will search for "Martín Ramírez Pech" to find out this information.}
  \searchrow{Martín Ramírez Pech}
  \resultrow{"Martín Ramírez", Martín Ramírez Martín Ramírez (January 30, 1895 – February 17, 1963) was ... ... \texttt{$\backslash$n} "Martín Ramírez", and \$134,500 in New York in 2011. Martín Ramírez Martín Ramírez (January 30, 1895 – ... ... \texttt{$\backslash$n} "Martín Ramírez", including brown paper bags, scraps of examining-table paper, and book pages ... ...}
  \thinkrow{It appears that \textcolor{red}{I made a mistake} in my search query. Instead of searching for "Martín Ramírez Pech", the results are showing information about a different "Martín Ramírez". To find out the right information, I need to look for Martín Ramírez Pech specifically and the political party associated with him. I will modify the search query to "Martín Ramírez Pech political party" for the correct information.}
  \searchrow{Martín Ramírez Pech political party}
  \resultrow{"Cuauhtémoc Blanco", of Morelos. In the 2015 legislative elections, he won in a closely contested election ... ... \texttt{$\backslash$n} "Labor Party (Mexico)", for PT and 75 deputies and 16 senators for the PES. The alliance has received ... ... \texttt{$\backslash$n} "Graco Ramírez", Graco Ramírez Graco Luis Ramírez Garrido Abreu (born 26 June 1949, in Villahermosa, Tabasco) is a Mexican ... ...}
  \thinkrow{Now, we see that Martín Ramírez Pech is associated with the political party known as the Labor Party (PT) in Mexico. I now need to continue this line of reasoning to find out who won the presidential election in 2018 in Mexico.}
  \searchrow{2018 Mexican presidential election}
  \resultrow{"2018 Mexican general election", (INE) officially declared the new process underway on 8 September 2017. The presidential election was won by Andrés Manuel López Obrador of ... ... \texttt{$\backslash$n} "2018 Mexican general election", irregularities including reports of vote-buying, spending beyond legal campaign ... ... \texttt{$\backslash$n} "2018 Mexican general election", 2018 Mexican general election General elections were held in Mexico on 1 July 2018 ... ...}
  \thinkrow{According to the search results, the 2018 Mexican presidential election was won by Andrés Manuel López Obrador as the candidate for the National Regeneration Movement (MORENA).}
  \rowcolor{yellow!10} <answer> The final answer is \verb|\boxed{Andrés Manuel López Obrador}| </answer> \\
  \bottomrule
  \end{tabular}
  \end{center}
  \end{table}

To give a more intuitive understanding of the utility of \textit{ReSearch}, we show a case study in Table \ref{tab:case-study}. This case is from the reinforcement learning process of Qwen2.5-32B-Instruct model.
Where text enclosed by \texttt{<think>} and \texttt{</think>}, \texttt{<search>} and \texttt{</search>}, and \texttt{<answer>} and \texttt{</answer>} are generated by the model, and the text enclosed by \texttt{<result>} and \texttt{</result>} are retrieved from the retrieval environment. 
For clarity, we use "... ..." to represent the truncation of the retrieval results.
From this case, we can see that the model can effectively break down the complex question and conduct reasoning within \texttt{<think>} and \texttt{</think>}. 
Such reasoning process is crucial for guiding when and what to search, and leading to the final answer in a multi-step manner.

\paragraph{Self-elicited Reflection}
In addition, we also observe reflection phenomenon in the model's response. 
As depicted in the second thinking step in Table \ref{tab:case-study}, the model states, "\textcolor{red}{I made a mistake}," recognizing that the previous search query failed to retrieve useful information. It then corrects itself in the third thinking step by generating a more effective search query to obtain the relevant information.
Note that such reflection ability is not explicitly trained or designed in the prompt templates, but is naturally elicited from the model itself during reinforcement learning.

\section{Conclusion}
In this paper, we introduced \textit{ReSearch}, a novel framework that trains LLMs to reason with search via reinforcement learning without requiring any supervised data on reasoning steps. Our approach integrates search operations as integral components of the reasoning chain, where text-based thinking guides when and how to perform searches, and search results subsequently influence further reasoning. 
Through extensive experiments on multiple multi-hop question answering benchmarks, we demonstrated that \textit{ReSearch} achieves significant improvements over baseline methods. The results also indicate the framework's potential for more realistic scenarios. Analysis of the training process revealed that \textit{ReSearch} naturally elicits advanced reasoning capabilities such as reflection and self-correction, without relying on pre-defined heuristics. This work highlights the effectiveness of integrating reasoning and search operations through reinforcement learning, offering a promising direction for developing more capable and reliable LLM-based systems for complex multi-hop tasks. 
Future work could explore extending this approach to more diverse domains and incorporating additional types of tools beyond search to further enhance LLMs' reasoning capabilities.

% \newpage
\bibliography{bibliography}
\bibliographystyle{plainnat}

% \newpage
\appendix

\section{Prompt for LLM-as-a-Judge}
\label{app:prompt-judge}

\begin{tcolorbox}[title=Prompt for Extracting Scenarios]
\begin{lstlisting}[language=prompt]
You will be given a question and its ground truth answer list where each item can be a ground truth answer. Provided a pred_answer, you need to judge if the pred_answer correctly answers the question based on the ground truth answer list.You should first give your rationale for the judgement, and then give your judgement result (i.e., correct or incorrect).

Here is the criteria for the judgement:
1. The pred_answer doesn't need to be exactly the same as any of the ground truth answers, but should be semantically same for the question.
2. Each item in the ground truth answer list can be viewed as a ground truth answer for the question, and the pred_answer should be semantically same to at least one of them.

question: {question}
ground truth answers: {gt_answer}
pred_answer: {pred_answer}

The output should in the following json format:
```json 
{
    "rationale": "your rationale for the judgement, as a text",
    "judgement": "your judgement result, can only be 'correct' or 'incorrect'"
}
```

Your output:
\end{lstlisting}
\end{tcolorbox}

\section{Implementation Details}
\label{app:implementation-details}

Our training is conduct on $8\times8$ Nvidia H800 GPUs, with full parameter optimization and gradient checkpointing. We show some important parameter settings in Table \ref{tab:implementation-details}.

\begin{table}[htbp]
\caption{Implementation details of \textit{ReSearch}.}
\label{tab:implementation-details}
\begin{center}
\begin{tabular}{l|l}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Learning Rate & 1e-6 \\
Train Batch Size & 256 \\
Number of Training Epochs & 2 \\
Number of Rollout & 5 \\
Rollout Temperature & 1.0 \\
KL Loss Coefficient & 0.001 \\
Clip Ratio & 0.2 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\end{document}