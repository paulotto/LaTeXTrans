% \begin{table*}[!ht]
% \setlength{\tabcolsep}{6pt}
% \centering
% % \vspace{-10pt}
% \caption{Domain adaptation results.}
%   % \vspace{-8pt}
%   \begin{adjustbox}{width=0.979\textwidth}
%   \input{tables/table1}
%   \end{adjustbox}
%   % \vskip -0.14in
%   % % \vspace{-6pt}
% \label{tab:heart}
% % \tabcolsep2pt
% \end{table*}

\section{Experiments}
% We conducted extensive evaluations of our multi-matching approach on two 2D segmentation benchmark tasks: optic disc and cup segmentation in retinal fundus images, and polyp segmentation datasets.

\subsection{Datasets}

\textbf{The retinal fundus segmentation datasets} comprise five public datasets from different medical centers, denoted as Site A (RIM-ONE~\cite{fumero2011rim}), B (REFUGE~\cite{orlando2020refuge}), C (ORIGA~\cite{zhang2010origa}), D (REFUGE-Test~\cite{orlando2020refuge}), and E (Drishti-GS~\cite{sivaswamy2014drishti}), with 159, 400, 650, 800, and 101 images respectively, all consistently annotated for optic disc (OD) and optic cup (OC) segmentation. We adopted the preprocessing method outlined in~\cite{liu2022single,chen2024each}, where each image's region of interest is cropped to $800\times800$ pixels and normalized using min-max normalization.

\noindent \textbf{The polyp segmentation datasets} consist of four public datasets collected from different medical centers, denoted as Site A (BKAI-IGH-NEOPolyp~\cite{ngoc2021neounet}), B (CVC-ClinicDB/CVC-612~\cite{bernal2015wm}), C (ETIS~\cite{silva2014toward}), and D (Kvasir~\cite{jha2020kvasir}), containing 1000, 612, 196, and 1000 images, respectively. We followed the preprocessing steps in~\cite{chen2024each}, resizing the images to $800 \times 800$ pixels and normalizing them using ImageNet-derived statistics.

\subsection{Experimental Setup}
\label{exp_setup}
\textbf{Implementation Details.} To ensure a fair comparison across all methods, we followed the protocol in~\cite{chen2024each} and split each dataset into an $8:2$ ratio for training and testing. We employed the ResNet-50~\cite{he2016deep} pre-trained on ImageNet as the feature extractor. The training was optimized using the SGD optimizer with a momentum of 0.9 and a learning rate of 0.001. For source model training, we set the mini-batch size to 8, allowing simultaneous matching of 8 graphs per batch. The universe embedding $\mathcal{U}$ was treated as an additional trainable tensor, integrated into the network weights without influencing subsequent experiments with other methods.
During the TTA phase, all methods were trained on the target data without access to their labels. The mini-batch size was set to 4, and the previously learned $\mathcal{U}$ from source model training was frozen. All experiments were implemented using PyTorch and conducted on 4 NVIDIA 3090 GPUs.

\noindent \textbf{Evaluation Metrics.} The Dice score (DSC, \%) was used to quantify the accuracy of the predicted masks, serving as the primary metric for evaluating segmentation performance. Additionally, the enhanced alignment metric $E_\phi^{max}$~\cite{fan2018enhanced} was adopted to measure both pixel-level and global-level similarity. To further assess the consistency between predictions and ground truths, we also employed the structural similarity metric $S_{\alpha}$~\cite{fan2017structure}.

\subsection{Experimental Results}
We selected U-Net~\cite{ronneberger2015u} as the benchmark model for the segmentation task, training it on the source domains and testing on the target domain without adaptation (\textit{No Adapt}). In addition, we compared eight state-of-the-art (SOTA) methods, including two entropy-based approaches (TENT~\cite{wangtent} and SAR~\cite{niu2023towards}), a shape template-based method (TASD~\cite{liu2022single}), a dynamically adjusted learning rate method (DLTTA~\cite{yang2022dltta}), batch normalization-based methods (DomainAdaptor~\cite{zhang2023domainadaptor}, VPTTA~\cite{chen2024each}), and noise estimation-based approaches (DeY-Net~\cite{wen2024denoising} and NC-TTT~\cite{osowiechi2024nc}).


\begin{table*}[h!]
% \vspace{-9pt}
% \setlength{\tabcolsep}{1pt}
\centering
  \caption{Multi sources domain generaliation in retinal fundus segmentation. The average performance (mean $\pm$ standard deviation) of three trials for our method and nine SOTA methods. ``Site A'' means training on Sites B-E and testing on Site A, and similarly for the others. The best results are highlighted in \textcolor{red}{red}.}
  \vspace{-5pt}
  \begin{adjustbox}{width=0.99\linewidth}
    \input{tables/table_fundus}
  \end{adjustbox}
  \label{tab:tabel_fundus}
\end{table*}

\begin{table*}[h!]
% \vspace{-9pt}
% \setlength{\tabcolsep}{1pt}
\centering
  \caption{Multi sources domain generaliation in the polyp segmentation. The average performance (mean $\pm$ standard deviation) of three trials for our method and nine SOTA methods. ``Site A'' means training on Sites B-D and testing on Site A, and similarly for the others. The best results are highlighted in \textcolor{red}{red}.}
  \vspace{-9pt}
  \begin{adjustbox}{width=0.99\linewidth}
    \input{tables/table_polyp}
  \end{adjustbox}
  \label{tab:tabel_polyp}
\end{table*}
% \vspace{-0.1cm}

\noindent \textbf{Multi-Source Generalization.} In these experiments, we adopted a leave-one-out training strategy~\cite{chen2023improved} (i.e., with $S = |\mathcal{D}_s \cup \mathcal{D}_t|-1$ and $T = 1$). Although some methods, such as TASD~\cite{liu2022single}, DeY-Net~\cite{wen2024denoising} and VPTTA~\cite{chen2024each}, were originally designed for single-source domain training, we simulated single-domain conditions by mixing multi-source domain data, making the experimental setup still feasible for these approaches. Furthermore, we have also included experiments specifically focused on single-source domains later in the study. The segmentation results for the fundus and polyp datasets are shown in Tables~\ref{tab:tabel_fundus} and \ref{tab:tabel_polyp}, respectively. For OD/OC segmentation, all TTA methods outperformed the \textit{No Adapt} baseline, highlighting their effectiveness in addressing domain shifts. Comparatively, our method consistently achieved better average performance than all other approaches. Specifically, for the key segmentation metric DSC, our method exceeded the second-best (DeY-Net~\cite{wen2024denoising}) by 2.88\% and outperformed the \textit{No Adapt} by 19.09\%. A similar trend was observed in the polyp segmentation results, where we outperformed the second-best method (NC-TTT~\cite{osowiechi2024nc}) and \textit{No Adapt} by 2.52\% and 7.89\%, respectively. Additionally, our approach exhibited greater stability across multiple trials compared to the other methods.

\begin{table*}[h!]
% \vspace{-9pt}
% \setlength{\tabcolsep}{1pt}
\centering
  \caption{Single source domain generalization in the polyp segmentation. The average performance of three trials for our method and nine SOTA methods. A $\rightarrow$ B represents models trained on Site A and tested on Site B, and similar for others. Best results are colored as \textcolor{red}{red}.}
  \vspace{-10pt}
  \begin{adjustbox}{width=0.99\linewidth}
    \input{tables/table_single1}
  \end{adjustbox}
  \label{tab:tabel_single}
\end{table*}
    
\noindent \textbf{Single-Source Generalization.} In these experiments, the models are trained on one domain and tested on the remaining datasets (i.e., with $S=1$ and $T=|\mathcal{D}_s \cup \mathcal{D}_t|-1$). Compared to multi-source generalization, single-source training is considered more challenging due to the limited domain information available during the training phase. By comparing Tables~\ref{tab:tabel_polyp} and \ref{tab:tabel_single}, we can observe that all models show lower segmentation performance (in terms of DSC) in the single-source setting compared to the multi-source results. Some methods even perform worse than the \textit{No Adapt} baseline in single-source scenarios.
However, our method still achieves the best average performance across 12 domain transfer experiments, outperforming the second-best approach (NC-TTT~\cite{osowiechi2024nc}) by 1.83\% and exceeding the \textit{No Adapt} by 9.77\% in terms of DSC. 
% This demonstrates the effectiveness and robustness of our multi-graph matching approach, which incorporates geometric priors from medical images. Notably, in the B $\rightarrow$ D and D $\rightarrow$ A experiments, the single-source results are very close to the multi-source ones, indicating that TTA strategies may offer a promising solution for mitigating domain shifts. Please refer to the appendix for visualization results and additional experimental outcomes.

Fig.~\ref{fig:visual} presents the visualization results for ``Site A'' in Table~\ref{tab:tabel_fundus}. The \textit{No Adapt} baseline exhibits significant misalignment, with notably distorted shapes and inaccurate boundaries. VPTTA offers some improvement but still suffers from distortions and incomplete segmentation, particularly in the optic cup area. NC-TTT shows instances of overlapping segmentation. In contrast, our method delivers the most precise segmentation, closely aligning with the ground truth. The Grad-CAM~\cite{selvaraju2017grad} visualizations reinforce this, as the attention of network is focused specifically on the relevant regions. These results demonstrate our method's enhanced generalization to unseen domains by effectively incorporating morphological priors, resulting in higher segmentation accuracy. For further visualization and additional experimental results, please refer to the appendix.

\begin{figure}[!t]
    \centering
\includegraphics[width=0.999\linewidth]{Figures/visual_compare.pdf}
    % \vspace{-10pt}
    \caption{Visualization comparison of segmentation results and Grad-CAM outputs from the final layer of the backbone network for the \textit{No Adapt} baseline, VPTTA~\cite{chen2024each}, NC-TTT~\cite{osowiechi2024nc}, and our proposed method on retinal fundus images. Additional visual comparisons are provided in the supplementary material.}
     \vspace{-0.5cm}
    \label{fig:visual}
\end{figure}