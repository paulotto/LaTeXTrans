\section{Related works}
\paragraph{Person Re-Identification}
Person re-identification (ReID) is a critical task in computer vision that focuses on identifying individuals across different camera views. It plays a significant role in surveillance and security applications. Recent advancements in ReID have leveraged deep learning techniques to enhance performance, particularly using convolutional neural networks (CNNs\cite{krizhevsky2012imagenet}) and vision transformers (ViTs \cite{alexey2020image}). The deep learning based methods\cite{bak2010person, layne2012person, sun2017svdnet, hermans2017defense, liao2020interpretable, fu2021unsupervised, zhang2023protohpe, zhang2023pha} that focus on feature extraction and metric learning\cite{koestinger2012large, hirzer2012relaxed, liao2015efficient, yu2018unsupervised} have improved feature extraction by learning robust and discriminative embeddings that capture identity-specific information.

Standard datasets like Market-1501 \cite{zheng2015scalable} have been widely used to benchmark ReID algorithms under normal conditions. Moreover, there are some challenging scenarios such as occlusions and cross-modality matching. Occluded-REID \cite{zhuo2018occluded} addresses the difficulties of identifying partially obscured individuals, while SYSU-MM01 \cite{wu2017rgb} focuses on matching identities between visible and infrared images, crucial for nighttime surveillance.

\paragraph{Feature Enhancement in Re-Identification}
Extracting robust feature representations is one of the key challenges in re-identification. Feature enhancement could help the ReID model easily differentiate between two people. Data augmentation techniques\cite{mclaughlin2015data, zhong2020random, ma2019true} were enhanced for feature enhancement. By increasing the diversity of training data, ReID model could extract robust and discriminative features.

Apart from improving the quality of the generated images, some Gan-based methods couple feature extraction and data generation end-to-end to distill identity related feature. FD-GAN\cite{ge2018fd} separates identity from the pose by generating images across multiple poses, enhancing the ReID system's robustness to pose changes. Zheng et al.\cite{zheng2019joint} separately encodes each person into an appearance code and a structure code. Eom el al.\cite{eom2019learning} propose to disentangle identity-related and identity-unrelated features from person images.  However, GAN-based methods face challenges such as training instability and mode collapse, which may not keep identity consistency.

In addition, the re-ranking technique refines feature-based distances to improve ReID accuracy. Such as k-Reciprocal Encoding Re-ranking\cite{zhong2017re}, which improves retrieval accuracy by leveraging the mutual neighbors between images to refine the distance metric. 

\paragraph{Person Generation Models}
Recent approaches have incorporated generative models, particularly generative adversarial networks (GANs) \cite{goodfellow2014generative}, to augment ReID data or enhance feature quality. Style transfer GAN-based methods\cite{zhong2018camstyle, dai2018cross, huang2019sbsgan, pang2022cross} transfer labeled training images to artificially generated images in diffrent camera domains, background domains or RGB-infrared domains. Pose-transfer GAN-based methods\cite{siarohin2018deformable,qian2018pose, liu2018pose, borgia2019gan, zhang2020pac} enable the synthesis of person images with variations in pose and appearance, enriching the dataset and making feature representations more robust to changes in poses. Random generation GAN-based mothods \cite{zheng2017unlabeled, ainam2019sparse, hussin2021stylegan} generate random images of persons and use Label Smooth Regularization (LSR \cite{szegedy2016rethinking}) or other methods to automatically label them. However, these methods often struggle to maintain identity consistency in pose variation, as generated images are susceptible to identity drift. 
The emergence of diffusion models has advanced the field of generative modeling, showing remarkable results in image generation tasks \cite{ho2020denoising}. Leveraging the capabilities of pre-trained models like Stable Diffusion \cite{rombach2022high}, researchers have developed techniques\cite{bhunia2023person, zhang2023adding} to generate high-quality human images conditioned on 2D human poses. Such as ControlNet\cite{zhang2023adding}, which integrates conditional control into diffusion models, allowing precise manipulation of generated images based on pose inputs.


% TO BE DONE


% In our work, we propose a framework that utilizes the inherent generalizability of existing diffusion models to generate different poses of the same individual based on reference images and target pose information. By extracting features from these generated images, we aim to enhance the original features without any extra parameters. This approach focuses on improving feature quality during inference and can be seamlessly integrated into various ReID tasks, including standard, occluded, and cross-modality scenarios. It can be used with various SOTA ReID model.