% \begin{table*}[hbt!]
% \small
% \centering
% % \setlength{\tabcolsep}{3.9pt}
% \begin{tabular}{l|ccccccc}
%     \toprule
%     Model & Antonym & Length   & Negation & Numerical & Spelling & Word  & Average \\ 
%           &         & mismatch &          & reasoning & error    & overlap & \\
%     \midrule
%     Vanilla	   & 48.6 & 78.5 & 52.9 & 36.1 & 76.2 & 60.6 & 58.8 \\
%     \midrule
%     \MASK      & 41.3 & 71.2 & 51.7 & 33.5 & 70.2 & 53.7 & 53.6 \\
%     \KW        & 40.4 & 70.4 & 51.0 & 33.1 & 71.8 & 52.1 & 53.1 \\
%     % \RUBI      &  7.6 & 68.1 & 51.5 & 20.2 & 66.6 & 70.3 & 47.4 \\ 
%     \ETE	   & 47.7 & 77.4 & 52.8 & 36.9 & 75.1 & 56.8 & 57.8 \\
%     \ETEFocal  & 48.4 & 77.1 & 53.2 & 35.2 & 75.8 &	63.7 & 58.9 \\
%     \midrule
%     Ours - $\mathcal{P}=$ DropPremise    & 55.7 & 81.5 & 56.5 & 40.0 & 80.8 & 61.1 & 61.6 \\
%     Ours - $\mathcal{P}=$ DropHypothesis & 55.7 & 81.5 & 56.5 & 40.0 & 80.8 & 61.1 & 61.6 \\
%     Ours - $\mathcal{P}=$ HalfHalf	     & 58.6 & 81.9 & 56.3 & 40.0 & 80.1 & 59.3 & 62.1 \\
%     Ours - $\mathcal{P}=$ Shuffle	     & 58.6 & 81.9 & 56.3 & 40.0 & 80.1 & 59.3 & 62.1 \\
%     Ours - $\mathcal{P}=$ DropLayer	     & 57.1 & 81.5 & 56.5 & 40.2 & 80.6 & 60.7 & 60.3 \\
%     Ours - $\mathcal{P}=$ PerturbRep     & 56.3 & 81.4 & 57.4 & 44.4 & 80.6 & 59.8 & 62.0 \\
%     % \midrule
%     % \OursPoe   & 47.3 & 78.2 & 53.8 & 36.0 & 76.3 & 61.1 & 58.8 \\
%     % \OursFocal & 47.3 & 78.2 & 53.8 & 36.0 & 76.3 & 61.1 & 58.8 \\
%     % \OursCL    & 54.4 & 80.5 & 55.2 & 38.1 & 79.1 & 61.9 & 61.5 \\
%     \bottomrule
% \end{tabular}
% \caption{Detailed results on specific type of biases from stress test set~\citep{naik-etal-2018-stress}.}
% \vspace{-20pt}
% \label{tab:bias_type}
% \end{table*}


\section{Implementation Details}
For all datasets, we train all methods on the BERT-base~\citep{devlin-etal-2019-bert} checkpoint, with a 2e-5 learning rate with linear decay using AdamW~\citep{kingma2014adam} optimizer. The batch size is set to 32. For the baseline models, we follow their papers for the hyperparameter choices. All experiments on done on a single A100 GPU.
% We adopt $\lambda = 0.1$.

% For Contrastive Learning, we adopt $lambda = 0.1$. For Poe and Focal loss, we adopt $lambda = 0.01$.

% \subsection{Settings}
We implement the proposed perturbation as illustrated in Table~\ref{tab:aug_list} by randomly dropping $50\%$ of the tokens from each sentence, dropping all layers after the second layer (3--12), %$k=20\%$), 
and zeroing $m=90\%$ of the elements in the intact representation $f(x_i)$. Each branch-specific MLP consists of two linear layers with a ReLU activation function in between. 
We use $\lambda = 0.1$ in our experiments.


\section{Other Debiasing Objectives}
\label{sec:debias_obj}
The idea of existing debiasing objectives is based on the idea of adjusting the importance of training examples, i.e. their contribution to loss calculation. The importance of examples which the model fails the correctly predict is promoted while the importance of examples which the model correctly predicts is reduced. 

Product-of-Experts (PoE)~\citep{clark-etal-2019-dont,karimi-mahabadi-etal-2020-end,sanh2020learning} is one of the most commonly adopted debiasing objective, which takes dot product of the logits of the main model and the biased models. Debiasing Focal Loss~\citep{karimi-mahabadi-etal-2020-end} down-weights the main model based on how close the logits of the biased models is to 1. Confidence Regularization~\citep{utama-etal-2020-towards} reduced the loss scale of examples with a scaling mechanism.

\section{RoBERTa as Encoder}
We conducted experiments on RoBERTa-base~\citep{liu2019roberta} using the MNLI dataset to evaluate the efficacy of FairFlow more effectively. The results in Table~\ref{tab:roberta} shows that the performance of all models improved using RoBERTa-base as encoder. We also observe comparable gains to BERT as encoder in case of ID and Transfer settings and smaller gains in case of Stress and OOD settings, which can be attributed to the use of a more powerful encoder.


\begin{table*}
\small
\centering
% \setlength{\tabcolsep}{3.9pt}
% \renewcommand{\arraystretch}{0.9} 
\begin{tabular}{l|cccc|cccc|cc}
\toprule
\textbf{Model} & \multicolumn{4}{c|}{\textbf{MNLI} (Acc.)} & \multicolumn{4}{c|}{\textbf{QQP} (F1)} & \multicolumn{2}{c}{\textbf{PGR} (F1)} \\
               & ID   & Stress & OOD & Transfer & ID & Stress & OOD & Transfer & ID & Stress \\ \toprule
    \textbf{\FT}        & 84.4 & 55.8 & 60.7 & 80.1 & 89.1 & 59.3 & 40.8 & 61.8 & 67.1 & 54.3 \\
    \midrule
    \textbf{\MASK}      & 84.7 & 53.6 & 60.8 & 80.5 & 88.3 & 60.2 & 44.7 & 62.1 & 65.4 & 44.6 \\
    \textbf{\KW}        & 83.3 & 53.5 & 60.5 & 80.2 & 87.6 & 61.3 & 45.1 & 62.7 & 63.5 & 42.0 \\
    \textbf{\ETE}       & 83.8 & 57.8 & 66.3 & 80.1 & 89.2 & 58.9 & 42.5 & \underline{63.1} & 63.2 & 50.3 \\
    % \textbf{\RUBI}      & 71.1 & 47.4 & 60.2 & 50.6 & 85.0 & 47.3 & 28.4 & 48.5 & 58.6 & 39.7 \\ 
    \textbf{LWBC}	    & 83.2 & 58.3 & 60.2 & 80.7 & 89.6 & 73.2 & 49.2 & 67.4 & 66.5 & 53.2 \\
    \textbf{\IE}        & 84.5 & 60.1 & 67.2 & 79.8 & 84.6 & 57.3 & \underline{50.6} & 60.5 & 64.8 & 54.6 \\
    \textbf{\READ}      & 79.6 & 58.3 & \textbf{68.4} & 73.0 & 84.5 & 65.8 & 46.7 & 61.7 & 62.6 & 55.0 \\
    \midrule
    \textbf{\OursPoe}   & \underline{84.8} & 62.3 & 67.5 & \underline{81.0} & 89.2 & 77.5 & 48.9 & \underline{63.1} & \underline{67.4} & 55.6 \\
    \textbf{\OursFocal} & \underline{84.8} & \underline{62.8} & \underline{67.9} & 80.9 & \underline{89.6} & \underline{77.8} & 49.2 & \underline{63.1} & \textbf{67.7} & \textbf{56.1} \\
    \textbf{\OursCL}    & \textbf{84.9}    & \textbf{63.6} & \textbf{68.4} & \textbf{81.1} & \textbf{91.8} & \textbf{78.4} & \textbf{51.5} & \textbf{68.3} & \textbf{67.7} & \underline{55.8} \\ \bottomrule
  \end{tabular}
  \caption{Experimental results on three datasets using BERT as the base model. The best performance is in \textbf{bold} and the second best is \underline{underlined}. Note that \IE does not release their code. We tried our best to reproduce the results but failed on HANS, which is 5.2 points lower than the reported 72.4. This is potentially due to implementation and optimization details which the authors did not release.}
  \label{tab:bert}
\end{table*}

\begin{table*}[ht]
\small
\centering
\begin{tabular}{l|cccc|cccc|cc}
\toprule
\multirow{2}{2em}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{MNLI} (Acc.)} & \multicolumn{4}{c|}{\textbf{QQP} (F1)} & \multicolumn{2}{c}{\textbf{PGR} (F1)} \\
               & ID   & Stress & OOD & Transfer & ID & Stress & OOD & Transfer & ID & Stress \\
    \toprule
    \textbf{\FT}        & 88.1 & 75.3 & 66.4 & 81.0 & 92.2 & 63.5 & 44.7 & 68.3 & 69.3 & 57.1 \\
    \midrule
    \textbf{\MASK}      & 86.5 & 72.7 & 66.9 & 80.7 & 92.5 & 66.1 & 49.1 & 68.7 & 70.2 & 57.5 \\
    \textbf{\KW}        & 88.1 & 74.1 & 67.4 & 79.9 & 93.1 & 66.7 & 50.2 & 68.9 & 71.3 & 58.3 \\
    \textbf{\ETE}       & 88.3 & 72.6 & 69.5 & 80.7 & 92.4 & 66.4 & 50.3 & 68.5 & 70.5 & 57.9 \\
    \textbf{LWBC}       & 84.6 & 69.3 & 66.7 & 81.0 & 91.7 & 63.2 & 43.9 & 67.4 & 70.4 & 54.2 \\
    \textbf{\IE}        & 88.2 & 72.4 & 69.3 & 80.3 & 92.3 & 66.3 & 50.2 & 68.3 & 70.8 & 56.3 \\
    \textbf{\READ}      & 85.3 & 73.5 & 70.3 & 78.5 & 91.4 & 68.1 & 51.0 & 67.8 & 69.3 & 55.7 \\
    \midrule
    \textbf{\OursPoe}   & 88.3 & 76.1 & 70.2 & 81.4 & 92.5 & 66.7 & 50.6 & 68.3 & 70.8 & 58.0 \\
    \textbf{\OursFocal} & 88.2 & 76.7 & 70.3 & 81.4 & 92.7 & 67.8 & 51.3 & 68.7 & 71.1 & 58.3 \\
    \textbf{\OursCL}    & 88.3 & 77.2 & 70.4 & 81.2 & 93.3 & 68.4 & 51.8 & 68.6 & 71.4 & 58.3 \\
    \bottomrule
  \end{tabular}
  \caption{Results using RoBERTa~\citep{liu2019roberta} as the base model. The best performance is in \textbf{bold} and the second best is \underline{underlined}.}
  \label{tab:roberta}
\end{table*}



\begin{table*}
\small
\centering
% \setlength{\tabcolsep}{3.9pt}
% \renewcommand{\arraystretch}{0.9} 
\begin{tabular}{l|cccc|cccc|cc}
\toprule
\multirow{2}{2em}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{MNLI} (Acc.)} & \multicolumn{4}{c|}{\textbf{QQP} (F1)} & \multicolumn{2}{c}{\textbf{PGR} (F1)} \\
               & ID   & Stress & OOD & Transfer & ID & Stress & OOD & Transfer & ID & Stress \\ \toprule
    \textbf{\FT}        & 80.4 & 54.0 & 52.1 & 75.2 & 84.5 & 67.3 & 57.7 & 65.1 & 56.5 & 54.2 \\
    \midrule
    \textbf{\MASK}      & 79.3 & 52.8 & 51.6 & 73.7 & 83.6 & 67.5 & 57.3 & 74.9 & 56.7 & 53.2 \\
    \textbf{\KW}        & 80.7 & 55.1 & 52.8 & 75.1 & 85.7 & 67.4 & 58.3 & 77.2 & 58.3 & 55.1 \\
    \textbf{\ETE}       & 78.1 & 53.6 & 51.1 & 71.9 & 83.9 & 68.2 & 61.5 & 80.1 & 55.4 & 52.7 \\
    \textbf{LWBC}       & 74.5 & 50.8 & 51.0 & 71.4 & 80.2 & 61.0 & 55.8 & 75.4 & 53.2 & 51.0 \\
    \textbf{\IE}        & 79.7 & 52.9 & 51.8 & 74.3 & 86.1 & 66.9 & 58.2 & 76.2 & 57.1 & 53.8 \\
    \textbf{\READ}      & 77.5 & 52.7 & 51.5 & 73.8 & 85.2 & 66.4 & 63.3 & 75.2 & 57.3 & 52.6 \\
    \midrule
    \textbf{\OursPoe}   & 80.9 & 54.7 & 55.2 & 76.2 & 84.7 & 68.8 & 62.3 & 80.0 & 56.5 & 54.1 \\
    \textbf{\OursFocal} & 81.8 & 55.1 & 54.9 & 75.8 & 86.3 & 68.5 & 64.2 & 80.4 & 57.4 & 55.3 \\
    \textbf{\OursCL}    & 82.2 & 55.6 & 56.1 & 76.7 & 86.3 & 69.3 & 64.7 & 80.4 & 58.6 & 55.8 \\
    \bottomrule
  \end{tabular}
  \caption{Results using GPT-2 as the base model. The best performance is in \textbf{bold} and the second best is \underline{underlined}.}
  \label{tab:gpt2}
\end{table*}


\begin{table}
\small
\centering
% \setlength{\tabcolsep}{3.9pt}
% \renewcommand{\arraystretch}{0.9} 
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & \textbf{Avg. Acc ($\uparrow$)} & \textbf{Std. Acc ($\downarrow$)} \\
    \midrule
    \textbf{\FT}        & 60.1 & 9.3 \\ 
    \midrule
    \textbf{\MASK}      & 58.7 & 6.7 \\ 
    \textbf{\KW}        & 59.3 & 5.9 \\ 
    \textbf{\ETE}       & 60.0 & 6.1 \\
    \textbf{LSWC}       & 59.4 & 5.8 \\ 
    \textbf{\IE}        & 60.1 & 7.3 \\
    \textbf{\READ}      & 60.9 & 5.6 \\ 
    \midrule
    \textbf{\OursPoe}   & 63.8 & 5.7 \\ 
    \textbf{\OursFocal} & \underline{64.3} & \underline{5.2} \\
    \textbf{\OursCL}    & \textbf{64.7} & \textbf{5.1} \\ \bottomrule
  \end{tabular}
  \caption{Average performance and standard deviation on each type of stress test averaged across three architectures. The best performance is in \textbf{bold} and the second best is \underline{underlined}.}
  \label{tab:subset}
\end{table}


\section{Perturbation for Data Augmentation}
The explicit perturbation operators proposed in our framework offer a valuable opportunity for data augmentation. This can be particularly useful in tasks such as NLI. Consider the example $(x_i^p, x_i^h, y_i)$, where $x_i^p$ represents the premise, $x_i^h$ represents the hypothesis, and $y_i$ denotes the label. To augment the dataset, we create additional data samples by applying different perturbation operations, e.g., by dropping the premise: (`', $x_i^h$, not entailment), dropping the hypothesis: ($x_i^p$, `', not entailment), shuffling the data: ($\mathcal{P}_{Irr}(x_i^p)$, $\mathcal{P}_{Irr}(x_i^h)$, not entailment) and dropping parts of the input: ($\mathcal{P}_{Sub}(x_i^p)$, $\mathcal{P}_{Irr}(x_i^h)$, not entailment). 
The augmented examples can be added back to the original dataset to mitigate the effect of bias during fine-tuning and potentially enhance model's generalizability, leading to improved performance on existing debiasing methods (See Table~\ref{tab:res_aug}).

\begin{table*}[ht]
\small
\centering  
\begin{tabular}{l|p{.6cm}p{.6cm}p{.6cm}p{.9cm}}
\toprule
\multirow{2}{2em}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{MNLI} (Acc.)} \\
               & ID   & Stress & OOD & Transfer \\
    \toprule
    \textbf{\FT}         & 84.4 & 55.8 & 60.7 & 80.1 \\
    \textbf{\FT + Aug}   & 84.5 & 59.1 & 61.0 & 81.0 \\
    \midrule
    \textbf{\MASK}       & 84.7 & 53.6 & 60.8 & 80.5 \\
    \textbf{\MASK + Aug} & 85.6 & 55.4 & 62.2 & 81.1 \\
    \textbf{\KW}         & 83.3 & 53.5 & 60.5 & 80.2 \\
    \textbf{\KW + Aug}   & 85.1 & 56.2 & 60.8 & 81.0 \\
    \textbf{\ETE}        & 83.8 & 57.8 & 66.3 & 80.1 \\
    \textbf{\ETE + Aug}  & 84.8 & 61.1 & 66.2 & 80.6 \\
    \textbf{\IE}         & 84.5 & 60.1 & 65.7 & 79.8 \\
    \textbf{\IE + Aug}   & 85.6 & 60.8 & 66.4 & 80.7 \\
    \textbf{\READ}       & 79.6 & 58.3 & 68.4 & 73.0 \\
    \textbf{\READ + Aug} & 79.6 & 58.3 & 69.6 & 77.2 \\
    \bottomrule
  \end{tabular}
  \caption{Performance when applying data augmentation, which effectively improve existing debiasing methods. The best performance is in \textbf{bold}.}
  \label{tab:res_aug}
\end{table*}



% \section{Effect of balancing factor}
% We investigate the effect of the balancing hyperparameter $\lambda$ by varying its value, specifically $0.01, 0.05, 0.1, 0.5, 1$ on MNLI dataset and summarize the results in Table~\ref{tab:hyper}. 

% Our first discovery is that different tasks require different weights between the original branch and the biased branches. For 

% In addition, we find that our contrastive learning approach is more sensitive to the choice of $\lambda$, while other fusion techniques are less sensitive. We attribute this to the fact that in contrastive learning, the gradient of the bias branches is backpropagated to the backbone model. While in the rest of the fusion techniques, the losses of the bias branches do not affect the backbone model. 

% \section{Performance on specific shortcuts}
% \label{sec:detail_type_of_bias}
% We further present the finer-grained performance on different types of biases from the stress test sets~\citep{naik-etal-2018-stress} for MNLI in Table~\ref{tab:bias_type}. Overall, our method can outperform standard fine-tuning by 3.2\% absolute point and other baselines. \MASK~\citep{} and \KW~\citep{}

% We also notice that \RUBI, a visual QA model, has a significantly lower performance than our method and other baselines. Given the fact that \RUBI and \ETEPoe are very similar (\RUBI has an extra sigmoid function before PoE), we attribute this performance drop to  shows difference between multi-modal (VQA) and uni-modal (NLP) datasets. 


% \paragraph{Antonym} In these stress test examples, premise and hypothesis are the same except the verbs are antonyms of each other. Our model $\mathcal{P}=$``Shuffle'' and $\mathcal{P}=$``HalfHalf'' outperforming ``Vanilla'' by 10\% absolute point. The rest of the debiasing methods are even worse than ``Vanilla'', indicating such methods are still biased.

% \paragraph{Length mismatch} In these stress test examples, premise is significantly longer than hypothesis or the other way around. The minimum performance advantage over baselines is 1.9\%.

% \paragraph{Negation} In these stress test examples, negation words are inserted to the verbs in the hypothesis. The minimum performance advantage over baselines is 3.9\%. This shows that our model can be more attentive to details in the input and make better predictions based on that.

% \paragraph{Numerical reasoning} In these stress test examples, models are asked to perform NLI on numerical statements. The minimum performance advantage over baseline is 3.9\% absolute point.

% \paragraph{Spelling error} In these stress test examples, some words are mis-spelled, which may result in `<UNK>' tokens in the vocabulary. The minimum performance advantage over baseline is 4.9\% absolute point. This highlights that our model does not rely on specific words in the sentence to make predictions and is robust to spelling errors.

% \paragraph{Word overlap} In these stress test examples, premise and hypothesis have a large portion of lexical overlap. The minimum performance advantage over baselines is 1.9\%.


