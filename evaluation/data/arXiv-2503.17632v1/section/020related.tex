
\section{Related Work}

% \paragraph{Dataset bias}
% refers to the spurious correlations between features and labels in datasets. During training, models can learn these shortcut solutions in datasets rather than learning meaningful signals that contribute to the correct predictions for the task. Dataset biases can be specific, such as negation words~\cite{gururangan-etal-2018-annotation} in NLI tasks, or text-only inputs in visual question answering tasks~\cite{cadene2019rubi}. They may also be non-specific or of unknown type, such as biases that exist in weak models~\cite{sanh2020learning}.
%
% Previous work developed stress or out-of-domain test sets to evaluate models against shortcut solutions. For example, \citet{naik-etal-2018-stress} designed a set of rules to generated hard test examples using the original data, e.g., by appending irrelevant text to the data. \citet{mccoy-etal-2019-right} developed a controlled evaluation set called heuristic analysis for NLI systems (HANS) that contains fallible syntactic heuristics, illustrating that strong NLI models fail to achieve good performance on these cases.debiased focal

% The bias identification methods can be broadly classified into two categories, 1) \textit{explicit} identification, and 2) \textit{implicit} identification. 

% On the other hand, \textit{implicit} identification methods aim at identifying bias that we don't know \textit{a priori}. \citep{karimi-mahabadi-etal-2020-end} proposes to use an under-trained model as the bias identifier, while \citep{sanh2020learning} uses a two-layer Bert-tiny model, i.e. an under-parametrized model, as the bias identifier.

% \xhdr{Debias datasets}
% The following work try to debias from the dataset point of view.
\paragraph{Quantifying Bias}
Several works focus on understanding dataset bias and deibasing algorithms, including measurement of bias of specific words with statistical test~\citep{gardner-etal-2021-competency}, identification of biased and generation of non-biased samples with $z$-filtering~\citep{wu-etal-2022-generating}, identification of bias-encoding parameters~\citep{yu-etal-2023-unlearning}, when bias mitigation makes model less or more biased~\citep{ravichander-etal-2023-bias}, bias transfer from  other models~\citep{jin-etal-2021-transferability}, and  representation fairness~\citep{shen-etal-2022-representational}. 


\paragraph{Debiasing with Biased Models} These approaches model shortcuts from datasets, and use biased predictions as a reference to quantify bias in input data. Bias can be \emph{explicit bias} in NLI datasets~\citep{belinkov-etal-2019-dont,clark-etal-2019-dont,karimi-mahabadi-etal-2020-end,utama-etal-2020-mind}, and \emph{implicit bias} detected by weak models~\citep{ghaddar-etal-2021-end,sanh2020learning,meissner-etal-2022-debiasing,utama-etal-2020-towards,meissner-etal-2022-debiasing}. 
% These methods suffer from an incomplete source of biases, which may lead to suboptimal performance.
Ensemble techniques include Product-of-Experts (PoE)~\citep{10.1162/089976602760128018,sanh2020learning,cheng24c_interspeech} which takes element-wise multiplication of the logits, Debiased Focal Loss~\citep{karimi-mahabadi-etal-2020-end} and ConfReg~\citep{utama-etal-2020-mind} which both down-weight predictions based on the confidence of biased models.\looseness-1


\paragraph{Debiased Representations} Existing methods focus on weak-learner guided pruning~\citep{meissner-etal-2022-debiasing}, disentangling robust and spurious representations~\citep{gao-etal-2022-kernel}, 
% using \textit{isotropic} sentence embedding
decision boundaries~\citep{lyu2023feature}, and
attention patterns with PoE~\citep{wang-etal-2023-robust}, 
training biased models with one-vs-rest approach~\citep{jeon-etal-2023-improving}, and amplifying bias in training set with debiased test set~\citep{reif-schwartz-2023-fighting}.

% DCT formulates the debiasing task as learning a robust decision boundary between biased and non-biased examples, by pushing \textit{the closest biased and non-biased examples} away and pull \textit{the farthest biased and non-biased examples} close to each other. 
% READ assume that attention of the \texttt{[CLS]} token is a key source of spuriousness and use a biased model to regularize the attention of the target model.





\paragraph{Fairness and Toxicity} These approaches focus on protected variable such as race. Existing methods spans across counterfactual data augmentation~\citep{zmigrod-etal-2019-counterfactual,dinan-etal-2020-queens,barikeri-etal-2021-redditbias}, comparisons between network architectures~\citep{meade-etal-2022-empirical}, deibasing with counterfactual inference~\citep{qian-etal-2021-counterfactual}, adversarial training~\citep{madanagopal-caverlee-2023-bias}, prompt perturbation~\citep{guo-etal-2023-debias}, data balancing~\citep{han-etal-2022-balancing}, contrastive learning~\citep{cheng2021fairfil}, detecting toxic outputs~\citep{schick-etal-2021-self}, performance degradation incurred by debiasing methods~\citep{meade-etal-2022-empirical}, and benchmarks~\cite{nadeem-etal-2021-stereoset,hartvigsen-etal-2022-toxigen,sun-etal-2022-chapterbreak}. Social debiasing methods may underperform in OOD settings because OOD examples may not contain social stereotypes or biases.

% Compared to these methods, our method targets a broader spectrum of biases.

% CDA that replaces words with bias attributes with their counterparts (e.g. \textit{he} -> \textit{she}) and add the augmented sentences back to the training corpus.
% In contrast to existing debiasing methods that are restricted by the limited types of biases captured, 
% Despite significant progress, existing models often have a single view to dataset biases, often focus on specific types of biases, rely on specialized debiasing objectives, may sacrifice in-domain performance while debiasing, and are often tailored to specific NLP tasks. The present work overcomes these limitations by developing a new debiasing method that effectively models known dataset biases and do not suffer from in-domain performance drop when debiasing.

