% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").



@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{NIPS2017_3f5ee243,
 title = {Attention is All you Need},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2017}
}


@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{chen2022hts,
  title={HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection},
  author={Chen, Ke and Du, Xingjian and Zhu, Bilei and Ma, Zejun and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
  booktitle={International Conference on Acoustics, Speech and Signal Processing},
  year={2022}
}


@inproceedings{li2022learning,
  title={Learning to answer questions in dynamic audio-visual scenarios},
  author={Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2022}
}


@misc{Ultralytics2023yolov8,
  author       = {Ultralytics},
  title        = {YOLOv8},
  year         = {2023},
  howpublished = {\url{https://github.com/ultralytics/ultralytics}},
  note         = {Accessed: 2023-05-26}
}



@inproceedings{lin2023vision,
  title={Vision transformers are parameter-efficient audio-visual learners},
  author={Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{liu2024tackling,
  title={Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering},
  author={Liu, Xiulong and Dong, Zhikang and Zhang, Peng},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2024}
}

@article{fayek2020temporal,
  title={Temporal reasoning via audio question answering},
  author={Fayek, Haytham M. and Johnson, Justin},
  journal={Transactions on Audio, Speech, and Language Processing},
  year={2020}
}

@inproceedings{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018}
}

@inproceedings{yu2019activitynet,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{garcia2020knowit,
  title={KnowIT VQA: Answering knowledge-based questions about videos},
  author={Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{ravi2023vlc,
  title={VLC-BERT: visual question answering with contextualized commonsense knowledge},
  author={Ravi, Sahithya and Chinchure, Aditya and Sigal, Leonid and Liao, Renjie and Shwartz, Vered},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2023}
}

@inproceedings{yu2024self,
  title={Self-chained image-language model for video localization and question answering},
  author={Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{lipping2022clotho,
  title={Clotho-aqa: A crowdsourced dataset for audio question answering},
  author={Lipping, Samuel and Sudarsanam, Parthasaarathy and Drossos, Konstantinos and Virtanen, Tuomas},
  booktitle={European Signal Processing Conference},
  year={2022}
}

@inproceedings{sudarsanam2023attention,
  title={Attention-Based Methods For Audio Question Answering},
  author={Sudarsanam, Parthasaarathy and Virtanen, Tuomas},
  booktitle={European Signal Processing Conference},
  year={2023}
}

@inproceedings{li2023multi,
  title={Multi-Scale Attention for Audio Question Answering},
  author={Li, Guangyao and Xu, Yixin and Hu, Di},
  booktitle={Annual Conference of the International Speech Communication Association},
  year={2023}
}

@article{kong2023universal,
  title={Universal Source Separation with Weakly Labelled Data},
  author={Kong, Qiuqiang and Chen, Ke and Liu, Haohe and Du, Xingjian and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo and Plumbley, Mark D.},
  journal={arXiv preprint arXiv:2305.07447},
  year={2023}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={International Conference on Computer Vision},
  year={2015}
}

@inproceedings{tian2020unified,
  title={Unified multisensory perception: Weakly-supervised audio-visual video parsing},
  author={Tian, Yapeng and Li, Dingzeyu and Xu, Chenliang},
  booktitle={European Conference on Computer Vision},
  year={2020},
}

@inproceedings{zhao2019sound,
  title={The sound of motions},
  author={Zhao, Hang and Gan, Chuang and Ma, Wei-Chiu and Torralba, Antonio},
  booktitle={International Conference on Computer Vision},
  year={2019}
}

@inproceedings{zhao2018sound,
  title={The sound of pixels},
  author={Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={International Conference on Machine Learning},
  year={2011}
}

@inproceedings{srivastava2012multimodal,
  title={Multimodal learning with deep boltzmann machines},
  author={Srivastava, Nitish and Salakhutdinov, Russ R.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}

@inproceedings{yang2022avqa,
  title={Avqa: A dataset for audio-visual question answering on videos},
  author={Yang, Pinci and Wang, Xin and Duan, Xuguang and Chen, Hong and Hou, Runze and Jin, Cong and Zhu, Wenwu},
  booktitle={International Conference on Multimedia},
  year={2022}
}
@inproceedings{yun2021pano,
  title={Pano-avqa: Grounded audio-visual question answering on 360deg videos},
  author={Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee},
  booktitle={International Conference on Computer Vision},
  year={2021}
}

@inproceedings{duan2024cross,
  title={Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks},
  author={Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{azuma2022scanqa,
  title={Scanqa: 3d question answering for spatial scene understanding},
  author={Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki},
  booktitle={proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={19129--19139},
  year={2022}
}

@inproceedings{zhang2017attentive,
  title={Attentive interactive neural networks for answer selection in community question answering},
  author={Zhang, Xiaodong and Li, Sujian and Sha, Lei and Wang, Houfeng},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{rogers2020getting,
  title={Getting closer to AI complete question answering: A set of prerequisite real tasks},
  author={Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@article{budler2023review,
  title={Review of artificial intelligence-based question-answering systems in healthcare},
  author={Budler, Leona Cilar and Gosak, Lucija and Stiglic, Gregor},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year={2023},
  publisher={Wiley Online Library}
}

@article{zhuang2023toolqa,
  title={Toolqa: A dataset for llm question answering with external tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{saito2024unsupervised,
  title={Unsupervised llm adaptation for question answering},
  author={Saito, Kuniaki and Sohn, Kihyuk and Lee, Chen-Yu and Ushiku, Yoshitaka},
  journal={arXiv preprint arXiv:2402.12170},
  year={2024}
}

@article{wang2024healthq,
      title={HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare Conversations}, 
      author={Ziyu Wang and Hao Li and Di Huang and Amir M. Rahmani},
      journal={arXiv preprint arXiv:2409.19487},
      year={2024}
}

@inproceedings{huang2024audiogpt,
  title={Audiogpt: Understanding and generating speech, music, sound, and talking head},
  author={Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2024}
}

@article{shen2024hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@Inproceedings{copet2024simple,
  title={Simple and controllable music generation},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{dai2022missing,
  title={What is missing in deep music generation? a study of repetition and structure in popular music},
  author={Dai, Shuqi and Yu, Huiran and Dannenberg, Roger B},
  journal={arXiv preprint arXiv:2209.00182},
  year={2022}
}

@inproceedings{diao2023av,
  title={Av-maskenhancer: Enhancing video representations through audio-visual masked autoencoder},
  author={Diao, Xingjian and Cheng, Ming and Cheng, Shitong},
  booktitle={International Conference on Tools with Artificial Intelligence},
  year={2023}
}

@article{agostinelli2023musiclm,
  title={Musiclm: Generating music from text},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}

@article{lu2023musecoco,
  title={Musecoco: Generating symbolic music from text},
  author={Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  journal={arXiv preprint arXiv:2306.00110},
  year={2023}
}

@inproceedings{li2023progressive,
  title={Progressive Spatio-temporal Perception for Audio-Visual Question Answering},
  author={Li, Guangyao and Hou, Wenxuan and Hu, Di},
  booktitle={International Conference on Multimedia},
  year={2023}
}

@inproceedings{liu2023parameter,
  title={Parameter-Efficient Transfer Learning for Audio-Visual-Language Tasks},
  author={Liu, Hongye and Xie, Xianhai and Gao, Yang and Yu, Zhou},
  booktitle={International Conference on Multimedia},
  year={2023}
}

@inproceedings{fan2019heterogeneous,
  title={Heterogeneous memory enhanced multimodal attention model for video question answering},
  author={Fan, Chenyou and Zhang, Xiaofan and Zhang, Shu and Wang, Wensheng and Zhang, Chi and Huang, Heng},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@article{wei2022learning,
  title={Learning in audio-visual context: A review, analysis, and new perspective},
  author={Wei, Yake and Hu, Di and Tian, Yapeng and Li, Xuelong},
  journal={arXiv preprint arXiv:2208.09579},
  year={2022}
}


@inproceedings{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@inproceedings{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{liu2021opt,
  title={OPT: Omni-perception pre-trainer for cross-modal understanding and generation},
  author={Liu, Jing and Zhu, Xinxin and Liu, Fei and Guo, Longteng and Zhao, Zijia and Sun, Mingzhen and Wang, Weining and Lu, Hanqing and Zhou, Shiyu and Zhang, Jiajun and others},
  journal={arXiv preprint arXiv:2107.00249},
  year={2021}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{pfeiffer2020adapterfusion,
  title={AdapterFusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}


@misc{aubio,
  author = {Paul Brossier},
  title = {aubio: a library to label music and sounds},
  year = {2024},
  url = {https://github.com/aubio/aubio},
  note = {Accessed: 2024-05-26}
}

@inproceedings{kingma2015adam,
author = {Kingma, Diederick P and Ba, Jimmy},
title = {Adam: A method for stochastic optimization},
booktitle = {International Conference on Learning Representations},
year = {2015}
}

@INPROCEEDINGS{9053174,
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={International Conference on Acoustics, Speech and Signal Processing}, 
  title={Vggsound: A Large-Scale Audio-Visual Dataset}, 
  year={2020}
}