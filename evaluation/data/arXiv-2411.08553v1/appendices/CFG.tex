\section{\fewgen{}}% \ad{Don't add FewGen, start from CFG directly.}
\label{sec:fewgen}
Let us consider the case of binary classification with labels $\{0,1\}$ and corresponding verbalization $\{\mathbf{y}_0,\mathbf{y}_1\}$. \fewgen{}~\cite{gpt3} is a standard approach to generate an instance $\mathbf{x}$ for a label $\mathbf{y}$: construct a prompt $\Prompt$ that has some description of the classification task, few ICL example generations, optional instance attributes and the choice of label $\mathbf{y}\in\{\mathbf{y}_0,\mathbf{y}_1\}$, and task the LLM to generate $x$. For brevity, we only keep the dependence of $\Prompt$ on $\mathbf{y}$ and use the notation $\Prompt(\mathbf{y})$ to denote the \textit{prompt tokens}. Let $\mP$ denote the auto-regressive LLM probability distribution with vocabulary $\cV$. An instance corresponding to label $\mathbf{y}$ is sampled in \fewgen{} as 
\begin{equation}
\label{eq:std_sample}
    \mathbf{x}=(x_1,\cdots,x_n)\distas{}\mP(\cdot|\Prompt(\mathbf{y}))
\end{equation}


\section{CFG}
\label{sec:CFG}
In CFG decoding~\cite{sanchez2023stay}, output token distribution is tilted in order to ensure that the LLM generations satisfy a particular condition.  In particular, we construct a \textit{contrastive prompt} $\widebar{\Prompt}$, and choose a guidance strength $\gamma>0$. Then instead of \eqref{eq:std_sample}, $\mathbf{x}$ is sampled using a titled distribution $\tilde \mP$ where
\begin{align}
    \tilde \mP(\cdot)& \propto  \frac{\mP(\cdot|\Prompt(\mathbf{y}))^{\gamma+1}}{\mP(\cdot|\widebar{\Prompt})^{\gamma}}\nonumber \\
    &=\mP(\cdot|\Prompt(\mathbf{y}))\left[\frac{\mP(\cdot|\Prompt(\mathbf{y}))}{\mP(\cdot|\widebar{\Prompt})}\right]^\gamma\label{eq:2cfg_seq}
\end{align}
Suppose we choose $\widebar{\Prompt}=\Prompt(\bar{\mathbf{y}})$, the prompt corresponding to the complementary label $\bar{\mathbf{y}}$ of $\mathbf{y}$ (or it could be any other label different from $\mathbf{y}$ in case of multiclass scenario). Then in the above equation, we are up-weighing the sequences that likely under $\Prompt(\mathbf{y})$ but unlikely under $\bar{\mathbf{y}}$ using the ratio of the two probabilities. This is supposed to move the generations away from the complementary label $\bar{\mathbf{y}}$. Writing in terms of tokens, we sample the $i$-th token $x_i$ as follows
\begin{equation}
    \label{eq:2cfg-ar}
    x_i\distas{}\tilde \mP(\cdot|\mathbf{x}_{<i}) \propto \frac{\mP(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^{\gamma+1}}{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\mathbf{x}_{<i})^{\gamma}}
\end{equation}

\paragraph{Drawbacks:} We find two drawbacks in CFG:
\begin{enumerate}
    \item In equation \eqref{eq:2cfg-ar}, the same $\mathbf{x}_{<i}$ is fed as a continuation from both prompts $\Prompt(y)$ and $\Prompt(\mathbf{\bar{\mathbf{y}}})$. We posit that this leads to decrease in the effect on guidance as more tokens are generated. This is because even the generation $\mathbf{x}$ is expected to be more faithful to $\Prompt(\mathbf{y})$ than to $\Prompt(\mathbf{\bar{\mathbf{y}}})$. So even though $\Prompt(\mathbf{\bar{\mathbf{y}}})$ is sort of opposite to $\Prompt(\mathbf{y})$, feeding in the generations that are faithful to the latter would move the token distributions in the denominator closer to the numerator. This is shown in \autoref{fig:cfg_vs_corrsynth}.
    \item Only a single sequence is generated at the cost of increase in number of forward passes of the model by two-fold. So a natural $K$-way extension for $K$-class classification would incur $K^2$ forward passes through the model per token for generating a single token for each of the $K$-classes. 
\end{enumerate}


% \input{table/entropy_bleu_table}
