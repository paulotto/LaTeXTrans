\section{Method}
\label{sec:method}
In this section we describe our novel \corrsyn{} method of sampling from an LLM. Although it is a general technique, we choose to motivate it from the perspective of data synthesis for a text based supervised learning problem. Furthermore, we take inspiration from the classifier free guidance~\cite{ho2021classifierfree} method applied to text based LLMs \cite{sanchez2023stay}.



A ``good'' synthetic dataset generated through this paradigm must ensure that the conditional distribution of instances given any label must closely approximate that of the true distribution $P_{XY}$. This includes: i) correct semantic separation of labels, ii) preservation of intra-label semantic diversity and of course, iii) fluent and coherent generations. In order to achieve (i) and (ii) (without compromising on (iii)), we present a method in the flavor of decoding time guidance techniques \cite{li2023contrastive,o2023contrastive,sanchez2023stay,chuang2023dola}. In these works, at inference time, the token probability distribution is tilted by another distribution obtained either from a different LLM, or same LLM with a different prompt, or different layers of the same LLM. Next we will describe a standard setup in dataset synthesis \fewgen{} and then one of the guidance techniques called the classifier free guidance (CFG) from \cite{sanchez2023stay} and finally describe our method highlighting the differences.

\subsection{\fewgen{}}
Let us consider the case of binary classification with labels $\{0,1\}$ and corresponding verbalization $\{\mathbf{y}_0,\mathbf{y}_1\}$. \fewgen{}~\cite{ye2022zerogen} is a standard approach to generate an instance $\mathbf{x}$ for a label $\mathbf{y}$: construct a prompt $\Prompt$ that has some description of the classification task, few ICL example generations, optional instance attributes and the choice of label $\mathbf{y}\in\{\mathbf{y}_1,\mathbf{y}_2\}$, and task the LLM to generate $x$. For brevity, we only keep the dependence of $\Prompt$ on $\mathbf{y}$ and use the notation $\Prompt(\mathbf{y})$ to denote the \textit{prompt tokens}. Let $\mP$ denote the auto-regressive LLM probability distribution with vocabulary $\cV$. An instance corresponding to label $\mathbf{y}$ is sampled in \fewgen{} as 
\begin{equation}
\label{eq:std_sample}
    \mathbf{x}=(x_1,\cdots,x_n)\distas{}\mP(\cdot|\Prompt(\mathbf{y}))
\end{equation}


\subsection{CFG}
In CFG decoding~\cite{sanchez2023stay}, output token distribution is tilted in order to ensure that the LLM generations satisfy a particular condition.  In particular, we construct a \textit{contrastive prompt} $\widebar{\Prompt}$, and choose a guidance strength $\gamma>0$. Then instead of \eqref{eq:std_sample}, $\mathbf{x}$ is sampled using a titled distribution $\tilde \mP$ where
\begin{align}
    \tilde \mP(\cdot)& \propto  \frac{\mP(\cdot|\Prompt(\mathbf{y}))^{\gamma+1}}{\mP(\cdot|\widebar{\Prompt})^{\gamma}}\nonumber \\
    &=\mP(\cdot|\Prompt(\mathbf{y}))\left[\frac{\mP(\cdot|\Prompt(\mathbf{y}))}{\mP(\cdot|\widebar{\Prompt})}\right]^\gamma\label{eq:2cfg_seq}
\end{align}
Suppose we choose $\widebar{\Prompt}=\Prompt(\bar{\mathbf{y}})$, the prompt corresponding to the complementary label $\bar{\mathbf{y}}$ of $\mathbf{y}$ (or it could be any other label different from $\mathbf{y}$ in case of multiclass scenario). Then in the above equation, we are up-weighing the sequences that likely under $\Prompt(\mathbf{y})$ but unlikely under $\bar{\mathbf{y}}$ using the ratio of the two probabilities. This is supposed to move the generations away from the complementary label $\bar{\mathbf{y}}$. Writing in terms of tokens, we sample the $i$-th token $x_i$ as follows
\begin{equation}
    \label{eq:2cfg-ar}
    x_i\distas{}\tilde \mP(\cdot|\mathbf{x}_{<i}) \propto \frac{\mP(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^{\gamma+1}}{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\mathbf{x}_{<i})^{\gamma}}
\end{equation}

\paragraph{Drawbacks:} We find two drawbacks in CFG
\begin{enumerate}
    \item In the display in \eqref{eq:2cfg-ar}, the same $\mathbf{x}_{<i}$ is fed as a continuation from both prompts $\Prompt(y)$ and $\Prompt(\mathbf{\bar{\mathbf{y}}})$. We posit that this leads to decrease in the effect on guidance as more tokens are generated. This is because even the generation $\mathbf{x}$ is expected to be more faithful to $\Prompt(\mathbf{y})$ than to $\Prompt(\mathbf{\bar{\mathbf{y}}})$. So even though $\Prompt(\mathbf{\bar{\mathbf{y}}})$ is sort of opposite to $\Prompt(\mathbf{y})$, feeding in the generations that are faithful to the latter would move the token distributions in the denominator closer to the numerator.\sk{add plots to show this decrease?}
    \item Only a single sequence is generated at the cost of increase in number of forward passes of the model by two-fold. So a natural $K$-way extension for $K$-class classification would incur $K^2$ forward passes through the model per token for generating a single token for each of the $K$-classes. 
\end{enumerate}

\subsection{\corrsyn}
Given the drawbacks of CFG, we propose a method that aims to overcome these limitations and can generalize to multiclass, and serve as a framework to generate dependent sequences from an LLM. We first illustrate it for the case of generating data for binary classification. Suppose we want to generate two instances $\mbf{x},\bar{\mbf{x}}$ corresponding to labels $\mbf{y}, \bar{\mbf{y}}$ respectively. Then, instead of CFG, in \corrsyn we generate them together as follows. Let $0\leq \delta\leq \gamma$. Then
\begin{align}
    & x_i \distas{}  \tilde \mP_i(\cdot) \propto \frac{P(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^\gamma}{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\bar{\mathbf{x}}_{<i})^{\gamma-\delta}} \label{eq:2corr_eq1}\\
   & \bar{x}_i  \distas{} \tilde Q_i(\cdot) \propto \frac{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\bar{\mathbf{x}}_{<i})^\gamma}{\mP(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^{\gamma-\delta}} \label{eq:2corr_eq2}
\end{align}

 Notice the crucial change from CFG: in \eqref{eq:2corr_eq1} denominator, the conditioned partial sequence $\bar{\mbf{x}}_{<i}$ is actually expected to be faithful to $\Prompt(\bar{\mbf{y}})$, and thus the effect of guidance would persist even after many tokens. Furthermore, we actually generate two sequences together with two fold increase in the number of forward passes of the model. We also introduce another parameter $\delta$ to control the effect of the denominator. 

The sequences $\mbf{x}, \bar{\mbf{x}}$ generated auto-regressively using equations \eqref{eq:2corr_eq1} and \eqref{eq:2corr_eq2} are naturally anti-correlated: they tend to be far apart in the embedding space of the LLM. So we call the sequences $\mbf{x},\bar{\mbf{x}}$ to be contrastive to one another. We can use this property to control label separation as well as intra-class diversity when generating synthetic data for multiclass problems. 

There are two potential issues with our formulation: i) at the word level, the generated sequences may not be aligned with respect to the entities that are being generated, and ii) if one of the sequences hit end-of-sentence token, then the guidance provided by the denominator is not meaningful. In order to address these issues, we introduce a generalization in the next section

\subsection{$M$--\corrsyn}
\label{sec:M-corrsyn}
We provide a generalization of the binary \corrsyn{} to an $M$-way \corrsyn{}. Suppose we have $M$ prompts $\{\Prompt_1,\cdots,\Prompt_M\}$. We want to generate $M$ sequences $\{\mbf{x}_m:m\in [M]\}$ such that $\mbf{x}_m$ is faithful to $\Prompt_m$. Let $\gamma>0$ be the guidance, and let $0\leq \delta\leq \gamma$. We introduce $M^2$ weights $\{\gamma_{m,n}:m,n\in[M], \gamma_{m,m}=0\}$. We generate the $i$-th token of $\mbf{x}_m=(x_{m,1},\cdots,x_{m,n_m})$ for all $m$ as follows
\begin{align}
     x_{m,i}&\distas{}\tilde \mP_{m,i}(\cdot)\nonumber\\
    &\propto \frac{\mP(\cdot| \Prompt_m,\mbf{x}_{m,<i})^{\gamma}}{\prod_{n\neq m}\mP(\cdot| \Prompt_n,\mbf{x}_{n,<i})^{\gamma_{m,n}}}\label{eq:M-Corr-1}
\end{align}

Notice that there are potentially $M-1$ terms in the denominator. In order to gain some intuition on the method, we set some values $\gamma_{m,n}$.
\subsubsection{Uniform contrastive guidance}
\label{sec:unif_guidance}
We set a parameter $\delta$ that controls the total about of contrast guidance: for each $m$, $\sum_n \gamma_{m,n}=\gamma-\delta$.
At step $i$, let the active set $\cS_{i}=\{m\in[M]:x_{m,i-1}\neq \eos\}\}$ which captures the sequences which have not yet hit the EOS token. Let $M_{i,active}=|\cS_{i}|$ denote the number of such sequences. Then in uniform contrastive guidance we set 
$$\gamma_{m,n}=\begin{cases} \frac{\gamma-\delta}{M_{i,active}-1}&,\, m,n\in\cS_i\\
 0&,\, \mathrm{otherwise}\end{cases}$$
at stage/token $i$ (dependence of $\gamma_{m,n}$ on $i$ is suppressed). Thus equation \eqref{eq:M-Corr-1} becomes
\begin{align}
    x_{m,i}&\distas{} \tilde \mP_{m,i}(\cdot)\nonumber\\
    &\propto \frac{\mP(\cdot| \Prompt_m,\mbf{x}_{m,<i})^{\gamma}}{\prod_{\substack{n\in \cS_i \\n\neq  m}}\mP(\cdot| \Prompt_n,\mbf{x}_{n,<i})^{\frac{\gamma-\delta}{M_{i,active}-1}}}\label{eq:M-Corr-2}
\end{align}
Using uniform contrastive guidance, $M$-\corrsyn{} has a natural geometric mean interpretation that we discuss in appendix~\ref{sec:geometric}. 


\subsubsection{\corrsyn{} for $K$-class data generation}
Now we briefly describe how we use \corrsyn{} in data synthesis for $K$ class classification. Recall that in $K$-class classification problem over $\cX\times\cY$ we have classes $[K]$ with label verbalizations $\{\mbf{y}_1,\cdots,\mbf{y}_K\}$. To generates instances for each class, we create prompts as follows. Let $R\in\mathbb{N}$ be the repeat factor. In $M$-\corrsyn{}, we take $M=KR$, and prompts in $\{\Prompt_{m}:m=(k-1)R+r,\, 1\leq r\leq R\}$ correspond to class $k$ for all $k\in[K]$. For $m=(k-1)R+r$, prompt $\Prompt_{m}$ asks the LLM to generate instances for class $k$ contains positive ICL examples for that class\footnote{in few-shot setting; there would be no ICL in zero-shot}. These ICL examples differ across $r$. Thus in equation~\eqref{eq:M-Corr-1}, a generation for class $k$ is, potentially, contrasted against the remaining $R-1$ generations from the same class, as well as the $(K-1)R$ generations from other classes. Based on setting the weights $\gamma_{m,n}$ to be zero for either intra-label terms or cross label terms, we get three scenarios:
\begin{enumerate}
    \item \textbf{Cross-label \corrsyn{}}: When generating a sequence for class $k$ and $m=(k-1)R+r$, we set $\gamma_{m,n}=0$ for $n\in\{(k-1)R+r':r'\neq r\}$. So only terms belonging to classes $k'\neq k$ appear in the denominator of \eqref{eq:M-Corr-1}.
    \item \textbf{Intra-label \corrsyn{}}: When generating a sequence for class $k$ and $m=(k-1)R+r$, we set $\gamma_{m,n}=0$ for $n\in\{(k'-1)R+r':r'\in[R],k'\neq k\}$. So only terms belonging to class $k$ appear in the denominator of \eqref{eq:M-Corr-1}.
    \item \textbf{Hybrid \corrsyn{}}: Here, the denominator of \eqref{eq:M-Corr-1} contains terms that belong to the same class as well as those that belong to other classes.
\end{enumerate}

In Hybrid \corrsyn{}, we separately set the target guidance for each of the cross and intra label terms. That is, we fix two targets $\gamma_{intra}$ and $\gamma_{cross}$, and for any $k$, $r$, letting $m=(k-1)R+r$, we choose $\gamma_{m,n}$ such that
\begin{align}
    \gamma_{intra}&=\sum_{\substack{n\in\{(k-1)R+r':\\r'\neq r\}}}\gamma_{m,n},\,\forall k,r\\
    \gamma_{cross}&=\sum_{\substack{n\in\{(k'-1)R+r':\\ r'\in[R],k'\neq k\}}}\gamma_{m,n},\,\forall k,r
\end{align}
Then we uniformly split the target guidances $\gamma_{intra}$ and $\gamma_{cross}$ in respective groups. More details of $K$-class \corrsyn{} is given in appendix~\ref{sec:K-corrsyn}



% \paragraph{Geometric mean interpretation:} To gain intuition, let us assume that $S_i=[M]$ and hence $M_{i,active}=M$. Further let $\delta=0$. Recall that the geometric mean of $n$ non-negative reals $\{\alpha_1,\cdots,\alpha_n\}$ is given by
% \begin{equation}
%     GM(\{\alpha_i:i\in [n]\})=\left(\prod_{i=1}^n\alpha_i\right)^{\frac{1}{n}}
% \end{equation}
% Analogously we can define the geometric mean of $M$ probability distributions in a point-wise manner. Thus we can write \eqref{eq:M-Corr-2} as
% \begin{align}
%      &x_{m,i}\distas{}\tilde \mP_{m,i}(\cdot)\nonumber\\
%     &\propto \frac{\mP(\cdot| \Prompt_m,\mbf{x}_{m,<i})^{\gamma}}{GM\left(\{\mP(\cdot| \Prompt_n,\mbf{x}_{n,<i}):n\in \cS_i, n\neq m\}\right)^{\gamma}}\label{eq:M-Corr-3}
% \end{align}
%  Thus, in \corrsyn, the contrasting guidance signal is provided by a \textit{geometric ensemble} of token distributions obtained from contrasting prompts as well as corresponding contrasting sequences. We expect that this geometric ensemble contrast, when $M\gg 2$, to average out the signal from the contrast and mitigate the issue of non alignment of words or entities between sequences. 

%  \subsection{\corrsyn{} for $K$-class data generation}
%  In this section we describe how \corrsyn{} is applied to generate data for $K$-class text classification problem. Recall that in $K$-class classification problem over $\cX\times\cY$ we have classes $[K]$ with label verbalizations $\{\mbf{y}_1,\cdots,\mbf{y}_K\}$. To generates instances for each class, we create prompts as follows. Let $R\in\mathbb{N}$ be the repeat factor. For each class $\mbf{y}$ consider the, possibly empty, ICL examples sets $\cI_{\mbf{y},r}\subset \cX\times\cY$ for $r\in [R]$ which contain positive examples for $\mbf{y}$. We construct a set of $K\cdot R$ prompts $\{\Prompt_{k,r}:k\in[K],r\in[R]\}$ where $\Prompt_{k,r}=\Prompt(\mbf{y}_k,\cI_{\mbf{y}_k,r})$ is a prompt that asks the LLM to generate instances for the class ${\mbf{y_k}}$ and includes ICL examples in $\cI_{\mbf{y}_k,r}$. For brevity, we assume that no sequence hits $\eos$ until some pre-set max number of tokens has been reached. There are a couple of ways in which \corrsyn{} can be used. Here we describe just one of the ways.%We describe two ways in which \corrsyn{} can be used.

%  \subsubsection{Cross-label \corrsyn{}}
% Here we contrast the instance for a label $\mbf{y_k}$ with instances of all the other labels $\mbf{y}_{k'}$ where $k'\neq k$. Thus, assuming uniform contrastive guidance~\ref{sec:unif_guidance}, we generate instances $\{\mbf{x}_{k,r}:k\in[K], r\in[R]\}$ together in \textit{lockstep} as follows. At stage/token $i$ we have for every $k\in[k]$ and $r\in [R]$
%  \begin{align}
%  \begin{aligned}
%       & x_{k,r,i}\distas{} \tilde \mP_{k,r,i}(\cdot) \\
%       &\propto \frac{\mP(\cdot| \Prompt_{k,r},\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt_{k',r'},\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\gamma-\delta}}\\
%      &= \frac{\mP(\cdot| \Prompt(\mbf{y}_k,\cI_{\mbf{y}_k,r}),\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt(\mbf{y}_{k'},\cI_{\mbf{y}_{k'},r'}),\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\gamma-\delta}}\label{eq:K-Corr-cross-1}
%  \end{aligned}
%  \end{align}
%  \paragraph{Effect of repeat factor:} We include repeat factor because it will increase the number of contrast terms for taking the geometric mean. We expect that this would provide improved averaging and reduces the noise due to potential misalignment. \sk{What do experiments show?}


%  \subsubsection{Hybrid \corrsyn{}}
%  In the hybrid approach, we contrast the instance $\mbf{x}_{k,r}$ for a label $\mbf{y}_k$ with instances $\mbf{x}_{k,r'}$ of the same label (but with different repeat $r'\neq r$), as well as instances $\mbf{x}_{k',r'}$ for all the other labels (where $k'\neq k$, and $r'\in [R]$). We introduce another parameter $\beta\geq 0$ which controls how the contrast guidance is distributed among contrasts within the label and other labels. Within each group we use uniform constrastive guidance from \ref{sec:unif_guidance}. The instances are generated as follows. At stage/token $i$ we have for every $k\in[k]$ and $r\in [R]$
%  \begin{align}
%  \begin{aligned}
%       x_{k,r,i}&\distas{}\tilde \mP_{k,r,i}(\cdot) \\
%       &\propto \frac{\mP(\cdot| \Prompt_{k,r},\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt_{k,r'},\mbf{x}_{k,r',<i})\right\}_{\substack {r'\neq r}}\right)^{\frac{\gamma-\delta}{\beta+1}}GM\left(\left\{\mP(\cdot| \Prompt_{k',r'},\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\frac{\beta(\gamma-\delta)}{\beta+1}}}\label{eq:K-Corr-hybrid-1}
%  \end{aligned}
%  \end{align}
% As seen from the above display, the first term in the denominator gives contrast signal from generations with the class, in order to get good intra-label diversity. While the second term gives contrast signal from other classes and hence serves to increase class separation. 

\subsubsection{\corrsyn{} in Logits Space and Plausibility Constraint}
Although the \corrsyn{} method described using LLM token probability distribution, it is implemented in the space of model outputs, i.e., logits. That is, the next-token distribution is obtained by first computing the next-token logits as described in appendix~\ref{sec:logit_corrsyn}. It is equivalent\footnote{This is not fully equivalent to probability space version since taking logarithm gives us log-probabilities which are normalized version of logits. Experimentally we have not found significant impact of this normalization.} to taking logarithm of the \corrsyn{} sampling equations. 

Another issue we foresee is that the contrast terms in \corrsyn{} could sometimes up weigh some irrelevant tokens that are not plausible at all for the prompt/label under consideration. To mitigate this, we borrow the idea of plausibility constraint from \cite{li2023contrastive, o2023contrastive} to limit the space of tokens that can up weighted by contrast terms. For every token of every instance being generated corresponding to some label, we reduce the space of logits to those that have at least $\alpha$ fraction of the mode of the numerator distribution in \eqref{eq:M-Corr-1}. More details are in appendix~\ref{sec:plaus}.


 