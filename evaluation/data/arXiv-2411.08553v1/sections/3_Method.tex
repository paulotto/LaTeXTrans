\section{Method}
\label{sec:method}
Now we describe our novel \corrsyn{} method of sampling from an LLM. Although it is a general technique, we choose to motivate it from the perspective of data synthesis for a text based supervised learning problem. %Furthermore, we take inspiration from the classifier free guidance~\cite{ho2021classifierfree} method applied to text based LLMs \cite{sanchez2023stay}.
% A ``good'' synthetic dataset must ensure that the conditional distribution of instances given any label must closely approximate that of the true distribution $P_{XY}$. This includes: i) correct semantic separation of labels, ii) preservation of intra-label semantic diversity and of course, iii) fluent and coherent generations. In order to achieve (i) and (ii) (without compromising on (iii)), we present a method in the flavor of decoding time guidance techniques \cite{li2023contrastive,o2023contrastive,sanchez2023stay,chuang2023dola}. In these works, at inference time, the token probability distribution is tilted by another distribution obtained either from a different LLM, or same LLM with a different prompt, or different layers of the same LLM. In particular, we take inspiration from the classifier free guidance~\cite{ho2021classifierfree} method applied to text based LLMs \cite{sanchez2023stay}. 

% In the rest of this section, we describe how \corrsyn{} can be applied to synthesis of binary and multiclass datasets.

%Next we will describe a standard setup in dataset synthesis \fewgen{} and then one of the guidance techniques called the classifier free guidance (CFG) from \cite{sanchez2023stay} and finally describe our method highlighting the differences.


\subsection{\corrsyn}
\label{sec:corrsyn-intro}
Let us consider the case of binary classification with verbalized labels $\{\mathbf{y}_0,\mathbf{y}_1\}$. As is standard in dataset synthesis~\cite{ye2022zerogen,gpt3}, we create class-conditioned prompt $\Prompt(\mathbf{y})$ which describes the task using verbalization $\mathbf{y}\in\{\mathbf{y}_0,\mathbf{y}_1\}$, and prompt the LLM to generate continuations as our synthetic input $x$. In-context examples are used to guide the generations to follow the format specified in the prompt. %We only show the dependence of $\Prompt$ on $\mathbf{y}$ and use the notation $\Prompt(\mathbf{y})$ to denote the \textit{prompt tokens}. %Given the drawbacks of CFG, we propose a method that aims to overcome these limitations and can generalize to multiclass, and serve as a framework to generate dependent sequences from an LLM. We first illustrate it for the case of generating data for binary classification. 
Suppose we want to generate two instances $\mbf{x},\bar{\mbf{x}}$ corresponding to labels $\mbf{y}, \bar{\mbf{y}}$ respectively. In \corrsyn we generate them together as follows. Let $0\leq \delta\leq \gamma$. Then:
\begin{align}
    & x_i \distas{}  \tilde \mP_i(\cdot) \propto \frac{P(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^\gamma}{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\bar{\mathbf{x}}_{<i})^{\gamma-\delta}} \label{eq:2corr_eq1}\\
   & \bar{x}_i  \distas{} \tilde Q_i(\cdot) \propto \frac{\mP(\cdot|\Prompt(\mathbf{\bar{\mathbf{y}}}),\bar{\mathbf{x}}_{<i})^\gamma}{\mP(\cdot|\Prompt(\mathbf{y}),\mathbf{x}_{<i})^{\gamma-\delta}} \label{eq:2corr_eq2}
\end{align}
We hypothesize that the sequences $\mbf{x}, \bar{\mbf{x}}$ generated auto-regressively using equations \eqref{eq:2corr_eq1} and \eqref{eq:2corr_eq2} are naturally anti-correlated: they tend to be far apart in the embedding space of the LLM. This is because, when sampling a token  for a sequence, the plausible tokens for the contrasting sequences are weighted down. Furthermore, at token $i$, even if the numerator and denominator distributions in \eqref{eq:2corr_eq1} highlight different entities or parts of speech, we expect the overall semantic meaning to be weakly present in the individual token distributions due to the attention mechanism. Thus even at these tokens, we posit that the contrast provides a signal that moves the generated sequences apart. This reasoning is based on intuition that requires careful experiments to prove. Nonetheless, we will demonstrate this separation of sequences in our analysis in \autoref{sec:analysis}. So we call the sequences $\mbf{x},\bar{\mbf{x}}$ to be contrastive to one another. We can use this property to control label separation as well as intra-class diversity when generating synthetic instances.

\insection[]{Crucial change from CFG}: in denominator of \eqref{eq:2corr_eq1}, the conditioned partial sequence $\bar{\mbf{x}}_{<i}$ is actually expected to be faithful to $\Prompt(\bar{\mbf{y}})$, and thus the effect of guidance would persist even after many tokens. Additionally, we generate two sequences together, leading to a two fold increase in the number of forward passes compared to a single generation, whereas CFG would require four times more. We introduce another parameter $\delta$ which controls the strength of the denominator contrast. More details on CFG for dataset synthesis are in \autoref{sec:CFG}. 

%There are two potential issues with our formulation: i) at the word level, the generated sequences may not be aligned with respect to the entities that are being generated, and ii) if one of the sequences hit end-of-sentence token, then the guidance provided by the denominator is not meaningful. In order to address these issues, we introduce a generalization in the next section

\subsection{$M$--\corrsyn}
\label{sec:M-corrsyn}
Next, we generalize from binary to $M$-way contrastive generation. Suppose we have $M$ prompts $\{\Prompt_1,\cdots,\Prompt_M\}$. We want to generate $M$ sequences $\{\mbf{x}_m:m\in [M]\}$ such that $\mbf{x}_m$ is faithful to $\Prompt_m$. Let $\gamma>0$ be the guidance, and let $0\leq \delta\leq \gamma$. We introduce $M^2$ weights $\{\gamma_{m,n}:m,n\in[M], \gamma_{m,m}=0\}$. We generate the $i$-th token of $\mbf{x}_m=(x_{m,1},\cdots,x_{m,n_m}), \forall m$:
\begin{align}
     x_{m,i}&\distas{}\tilde \mP_{m,i}(\cdot)\nonumber\\
    &\propto \frac{\mP(\cdot| \Prompt_m,\mbf{x}_{m,<i})^{\gamma}}{\prod_{n \neq m} \mP(\cdot| \Prompt_n,\mbf{x}_{n,<i})^{\gamma_{m,n}}}\label{eq:M-Corr-1}
\end{align}
Next we describe our choice of $\gamma_{m,n}$. 
% There are potentially $M-1$ terms in the denominator. In order to gain some intuition on the method, we set some values $\gamma_{m,n}$.

\subsubsection{Uniform contrastive guidance}
\label{sec:unif_guidance}
We set a parameter $\delta$ that controls the total amount of contrast guidance: for each $m$, $\sum_n \gamma_{m,n}=\gamma-\delta$. Then, when generating $i$-th token for $\mbf{x}_m$, we set $\gamma_{m,n}=0$ for sequences $\mbf{x}_{n}$ that have already hit the EOS token. Then, we uniformly divide $\gamma-\delta$ among remaining $\gamma_{m,n}$\footnote{$\gamma_{m,n}$ also depends on the token index $i$; we suppress it.}. More details are in \aref{sec:app_unif_guidance}. Using uniform contrastive guidance, $M$-\corrsyn{} has a natural geometric mean interpretation that we discuss in \aref{sec:geometric}. 



\subsubsection{\corrsyn{} for $K$-class synthesis}
Now we briefly describe how we use \corrsyn{} in data synthesis for $K$ class classification. Recall that in $K$-class classification problem over $\cX\times\cY$ we have classes $[K]$ with label verbalizations $\{\mbf{y}_1,\cdots,\mbf{y}_K\}$. To generates instances for each class, we create prompts as follows. Let $R\in\mathbb{N}$ be the repeat factor. In $M$-\corrsyn{}, we take $M=KR$, and prompts in $\{\Prompt_{m}:m=(k-1)R+r,\, 1\leq r\leq R\}$ correspond to class $k$ for all $k\in[K]$. For $m=(k-1)R+r$, prompt $\Prompt_{m}$ asks the LLM to generate instances for class $k$ contains positive ICL examples for that class. These ICL examples differ across $r$. Thus in equation~\eqref{eq:M-Corr-1}, a generation for class $k$ is, potentially, contrasted against the remaining $R-1$ generations from the same class, as well as the $(K-1)R$ generations from other classes. Based on setting the weights $\gamma_{m,n}$ to be zero for either intra-label terms or cross label terms, we get three scenarios:
\insection[]{\corrsyn{} Cross-label}: When generating a sequence for class $k$ and $m=(k-1)R+r$, we set $\gamma_{m,n}=0$ for $n\in\{(k-1)R+r':r'\neq r\}$. So only terms belonging to classes $k'\neq k$ appear in the denominator of \eqref{eq:M-Corr-1}.\\
\insection[]{\corrsyn{} Intra-label}:  When generating a sequence for class $k$ and $m=(k-1)R+r$, we set $\gamma_{m,n}=0$ for $n\in\{(k'-1)R+r':r'\in[R],k'\neq k\}$. So only terms belonging to class $k$ appear in the denominator of \eqref{eq:M-Corr-1}.\\
\insection[]{\corrsyn{} Hybrid}: denominator of \eqref{eq:M-Corr-1} contains terms that belong to the same class as well as those that belong to other classes. We separately set the target guidance for each of the Cross- and Intra-label terms: we fix two targets $\gamma_{intra}$ and $\gamma_{cross}$ such the sum of $\gamma_{m,n}$ for Intra and Cross label terms are set to $\gamma_{intra}$ and $\gamma_{cross}$ respectively. Then we uniformly split the target guidances $\gamma_{intra}$ and $\gamma_{cross}$ in respective groups. More details of $K$-class \corrsyn{} is given in \aref{sec:K-corrsyn}


% \paragraph{Geometric mean interpretation:} To gain intuition, let us assume that $S_i=[M]$ and hence $M_{i,active}=M$. Further let $\delta=0$. Recall that the geometric mean of $n$ non-negative reals $\{\alpha_1,\cdots,\alpha_n\}$ is given by
% \begin{equation}
%     GM(\{\alpha_i:i\in [n]\})=\left(\prod_{i=1}^n\alpha_i\right)^{\frac{1}{n}}
% \end{equation}
% Analogously we can define the geometric mean of $M$ probability distributions in a point-wise manner. Thus we can write \eqref{eq:M-Corr-2} as
% \begin{align}
%      &x_{m,i}\distas{}\tilde \mP_{m,i}(\cdot)\nonumber\\
%     &\propto \frac{\mP(\cdot| \Prompt_m,\mbf{x}_{m,<i})^{\gamma}}{GM\left(\{\mP(\cdot| \Prompt_n,\mbf{x}_{n,<i}):n\in \cS_i, n\neq m\}\right)^{\gamma}}\label{eq:M-Corr-3}
% \end{align}
%  Thus, in \corrsyn, the contrasting guidance signal is provided by a \textit{geometric ensemble} of token distributions obtained from contrasting prompts as well as corresponding contrasting sequences. We expect that this geometric ensemble contrast, when $M\gg 2$, to average out the signal from the contrast and mitigate the issue of non alignment of words or entities between sequences. 

%  \subsection{\corrsyn{} for $K$-class data generation}
%  In this section we describe how \corrsyn{} is applied to generate data for $K$-class text classification problem. Recall that in $K$-class classification problem over $\cX\times\cY$ we have classes $[K]$ with label verbalizations $\{\mbf{y}_1,\cdots,\mbf{y}_K\}$. To generates instances for each class, we create prompts as follows. Let $R\in\mathbb{N}$ be the repeat factor. For each class $\mbf{y}$ consider the, possibly empty, ICL examples sets $\cI_{\mbf{y},r}\subset \cX\times\cY$ for $r\in [R]$ which contain positive examples for $\mbf{y}$. We construct a set of $K\cdot R$ prompts $\{\Prompt_{k,r}:k\in[K],r\in[R]\}$ where $\Prompt_{k,r}=\Prompt(\mbf{y}_k,\cI_{\mbf{y}_k,r})$ is a prompt that asks the LLM to generate instances for the class ${\mbf{y_k}}$ and includes ICL examples in $\cI_{\mbf{y}_k,r}$. For brevity, we assume that no sequence hits $\eos$ until some pre-set max number of tokens has been reached. There are a couple of ways in which \corrsyn{} can be used. Here we describe just one of the ways.%We describe two ways in which \corrsyn{} can be used.

%  \subsubsection{Cross-label \corrsyn{}}
% Here we contrast the instance for a label $\mbf{y_k}$ with instances of all the other labels $\mbf{y}_{k'}$ where $k'\neq k$. Thus, assuming uniform contrastive guidance~\ref{sec:unif_guidance}, we generate instances $\{\mbf{x}_{k,r}:k\in[K], r\in[R]\}$ together in \textit{lockstep} as follows. At stage/token $i$ we have for every $k\in[k]$ and $r\in [R]$
%  \begin{align}
%  \begin{aligned}
%       & x_{k,r,i}\distas{} \tilde \mP_{k,r,i}(\cdot) \\
%       &\propto \frac{\mP(\cdot| \Prompt_{k,r},\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt_{k',r'},\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\gamma-\delta}}\\
%      &= \frac{\mP(\cdot| \Prompt(\mbf{y}_k,\cI_{\mbf{y}_k,r}),\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt(\mbf{y}_{k'},\cI_{\mbf{y}_{k'},r'}),\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\gamma-\delta}}\label{eq:K-Corr-cross-1}
%  \end{aligned}
%  \end{align}
%  \paragraph{Effect of repeat factor:} We include repeat factor because it will increase the number of contrast terms for taking the geometric mean. We expect that this would provide improved averaging and reduces the noise due to potential misalignment. \sk{What do experiments show?}


%  \subsubsection{Hybrid \corrsyn{}}
%  In the hybrid approach, we contrast the instance $\mbf{x}_{k,r}$ for a label $\mbf{y}_k$ with instances $\mbf{x}_{k,r'}$ of the same label (but with different repeat $r'\neq r$), as well as instances $\mbf{x}_{k',r'}$ for all the other labels (where $k'\neq k$, and $r'\in [R]$). We introduce another parameter $\beta\geq 0$ which controls how the contrast guidance is distributed among contrasts within the label and other labels. Within each group we use uniform constrastive guidance from \ref{sec:unif_guidance}. The instances are generated as follows. At stage/token $i$ we have for every $k\in[k]$ and $r\in [R]$
%  \begin{align}
%  \begin{aligned}
%       x_{k,r,i}&\distas{}\tilde \mP_{k,r,i}(\cdot) \\
%       &\propto \frac{\mP(\cdot| \Prompt_{k,r},\mbf{x}_{k,r,<i})^{\gamma}}{GM\left(\left\{\mP(\cdot| \Prompt_{k,r'},\mbf{x}_{k,r',<i})\right\}_{\substack {r'\neq r}}\right)^{\frac{\gamma-\delta}{\beta+1}}GM\left(\left\{\mP(\cdot| \Prompt_{k',r'},\mbf{x}_{k',r',<i})\right\}_{\substack {k'\neq k\\ r'\in[R]}}\right)^{\frac{\beta(\gamma-\delta)}{\beta+1}}}\label{eq:K-Corr-hybrid-1}
%  \end{aligned}
%  \end{align}
% As seen from the above display, the first term in the denominator gives contrast signal from generations with the class, in order to get good intra-label diversity. While the second term gives contrast signal from other classes and hence serves to increase class separation. 

\subsubsection{Logits Space computation}
The \corrsyn{} method is implemented using vector arithmetic in the space of LLM outputs i.e. logits space. Complete details are in \aref{sec:logit_corrsyn}. Taking logarithm of the \corrsyn{} sampling equations gives us similar results\footnote{Caveat: taking logarithm gives us log-probabilities which are normalized version of logits. Experimentally, we have not found significant impact of this normalization.}.


\subsubsection{Plausibility Constraint ($\alpha$)}

The contrast terms in \corrsyn{} could sometimes upweight irrelevant tokens i.e. those which are not plausible conditioned on the prompt/label under consideration. To mitigate this, we borrow the idea of plausibility constraint from \cite{li2023contrastive, o2023contrastive} to limit the token up weighting space: by reducing the space of logits to those tokens that have at least $\alpha$ fraction of the mode of the numerator distribution in \eqref{eq:M-Corr-1}. We provide the complete formulation in \aref{sec:plaus}.
 