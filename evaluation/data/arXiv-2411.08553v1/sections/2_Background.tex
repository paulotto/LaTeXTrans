\section{Background}
\label{sec:background}

% \paragraph{Notation:} Let $\mathbb{N}$ and $\mathbb{R}$ denote the sets of natural numbers and real numbers respectively. For $n\in\mathbb{N}$, let $[n]=\{1,2,\cdots,n\}$.\ad{This can be removed, not needed.} \ad{Makeit $P_{\theta}$ or $P_{\text{LM}}$ or $\mathcal{M}_{\text{LM}}(.)$, mention it is a teacher model}
\paragraph{Notation:} For $n\in\mathbb{N}$, let $[n]=\{1,2,\cdots,n\}$. An LLM is defined through its vocabulary $\cV$ and the auto-regressive sequence distribution $\mP$ or equivalently the logits $\lg$. Let $\cV^*=\cup_{n\geq 1}\cV^n$ denote the space of all finite sequences of tokens from $\cV$. We denote sequences of tokens from $\cV$ using lower boldface letters like $\mathbf{u},\mathbf{v}$. For any sequence of tokens $\mathbf{w}=(w_1,\cdots,w_n)\in \cV^n$ from $\cV$, and any $j\in[n]$, let $\mathbf{w}_{<j}=(w_1,\cdots,w_{j-1})$ if $j>1$, else, it is an empty sequence. Similarly $\mathbf{w}_{\leq j}=(w_1,\cdots,w_j)$. For any two sequences $\mathbf{u},\mathbf{v}\in\cV^*$ let $(\mathbf{u},\mathbf{v})$ denote their concatenation. We denote by $\mP(\mathbf{v}|\mathbf{u})$ the conditional probability of generating $(\mathbf{u},\mathbf{v})$ given that $\mathbf{u}$ has already been generated i.e., probability that $\mathbf{v}$ is a continuation of $\mathbf{u}$ for a given $\mathbf{u}$. Furthermore, for any $\mathbf{u},\mathbf{v}\in \cV^*$, we use $\mP(\cdot|\mathbf{u},\mathbf{v})$ to denote the conditioning on the concatenation $(\mathbf{u},\mathbf{v})$. For any prompt $\Prompt\in\cV^*$ , and any $\mathbf{w}\in\cV^n$, the auto-regressive distribution $\mP$ satisfies
\begin{align*}
    &\mP(\mathbf{w}|\Prompt)=\nonumber\\
    &\mP(w_1|\Prompt)\prod_{j=2}^{n}\mP(w_j|\Prompt,w_1,\cdots,w_{j-1})
\end{align*}
When we describe natural language domains using $\cX$, $\cY$ we mean either in the sense of them containing natural language sentences or as subsets of $\cV^*$, it will be clear from the context.%So $\mathbf{x}\in \cX$ can mean a piece of text, or its corresponding tokenization. The prompt for an LLM will be denoted by $\Prompt$ which can refer to the prompt text or its tokenization (i.e., an element of $\cV^*$).

We consider dataset generation for text classification tasks. Suppose we have a multiclass text classification problem with $K$ classes as $[K]$ and input domain $\cX$. Let $\cY=\{\mbf{y}_1,\cdots,\mbf{y}_K\}$ be the space of label verbalizations for the $K$ classes i.e., $\mbf{y}_k$ is a textual description of label $k\in[K]$. A natural language example input is denoted as $\mbf{x}\in\cX$. So the learning problem is defined on $\cX\times \cY$: given a data generating distribution $P_{XY}$ on $\cX\times \cY$ the task is to learn a classifier $h:\cX\to \cY$ (using some training data) such that $\Ex{l(h(\mbf{x}),\mbf{y})}$ is minimized for a given loss function $l:\cY\times\cY\to \mathbb{R}$, where the expectation is taken with respect to $P_{XY}$. 

Given the rapid advancement of LLMs like GPT-4, Llama2, Mistral etc. we are interested in utilizing the world knowledge and reasoning capabilities of these large models to generate synthetic training data for the textual $K$-class classification problem. Similar to recent works in this domain \cite{ye2022zerogen,gao2022self,Meng2022GeneratingTD,meng2023tuning,yu2023regen,ye2022progen,yu2024large,guo2024generative}, we consider the setup of prompting teacher LLM with a prompt $\Prompt$ that includes a label $\mbf{y}\in\cY$, a few In-Context Learning (ICL) examples for the label $\mbf{y}$ and potentially any other instance dependent attributes, and the prompt tasks the LLM to generate a synthetic instance $\mbf{x}\in \cX$ whose true label is expected to be $\mbf{y}$ i.e., the aim is to generate $x\distas{}P_{X|Y=\mbf{y}}$. That is, we generate a synthetic dataset \synthd{}. A student language model (e.g., a BERT-style pre-trained encoder model \citep{devlin-etal-2019-bert}) is trained on \synthd{}. 

For the ICL examples, we assume that we have access to a \emph{seed set} of examples $\mathcal{D}_{\textsc{Seed}} = \{(\mbf{x}_1,\mbf{y}_1),\ldots,(\mbf{x}_n,\mbf{y}_n)\}$. For us, typically $n$ is such that we have around $50$ examples per class.\sk{Abhishek, is this correct?} We assume that $\mathcal{D}_{\textsc{Seed}}$ is not large enough to train an effective student, but instead a larger synthetic dataset $\mathcal{D}_{\textsc{Synth}}=\{ (\tilde{\mbf{x}}_i,\mbf{y}_i)\}_{i=1}^m$  will be needed.% \synthdataset{}

A standard approach to dataset synthesis is few shot generation i.e. \fewgen{} \cite{NEURIPS2020_1457c0d6,ye2022progen,Yehudai2024GenieAH}. For instance, consider a task of detecting a business news article. In order to synthesize a dataset for this task, we could prompt the LLM appropriately, include few ICL examples. The LLM might generate a fairly decent article. But when we sample a large number of generations we see that the there is lack of diversity: similar entities are repeated, popular topics are highlighted and potential stylistic differences from a human written text. These could affect the performance of a student model that is trained on such dataset.

A ``good'' synthetic dataset must ensure that the conditional distribution of instances given any label must closely approximate that of the true distribution $P_{XY}$. This includes: i) correct semantic separation of labels, ii) preservation of intra-label semantic diversity and of course, iii) fluent and coherent generations. In order to achieve (i) and (ii) (without compromising on (iii)), we present a method, \corrsyn{}, in the flavor of decoding time guidance techniques \cite{li2023contrastive,o2023contrastive,sanchez2023stay,chuang2023dola}. In these works, at inference time, the token probability distribution is tilted by another distribution obtained either from a different LLM, or same LLM with a different prompt, or different layers of the same LLM. In particular, we take inspiration from the classifier free guidance~\cite{ho2021classifierfree} method applied to text based LLMs \cite{sanchez2023stay}. 
\corrsyn{} aims to control i) diversity in generations, ii) similarity to human crafted gold dataset, iii) cross label separation and at the same time iv) improve the student performance. The core idea of our approach is to perform correlated or dependent sampling from the LLM i.e., multiple sequences are generated in parallel that have strong dependency between each other. \autoref{fig:corrsynth_high_level} illustrates our method. More details are given in \autoref{sec:method}. This method can be used in conjunction with other synthetic dataset generation approaches like retrieval augmented generation~\cite{lewis2020retrieval}.

