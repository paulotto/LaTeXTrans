\section{Limitations}
The scope of our experiments is restricted to a set of classification tasks over a few English domains of text. While we believe our approach can be applied to other languages, other domains, and tasks like question answering that go beyond classification, we have not validated this in this work. Furthermore, the scope of our formulation is restricted to supervised learning problems where there a well-defined or natural label space. Extensions to unsupervised tasks like datasets for pre-training is an interesting possibility to be explored. The introduction of new hyper-parameters in any method requires tuning, which increases costs. In our case a high value of $\delta$ with respect to the original guidance $\gamma$ (e.g. $\delta = 0.9*\gamma$, yields positive results for all guidance values). However, the tuning of the initial guidance parameter was subject to a heuristic search. Finally, our approach performs modifications to the generation process by performing correlated sampling in the logits space. This makes our approach infeasible to use with API-only teacher LMs such as GPT-4, Claude, Gemini, etc. 