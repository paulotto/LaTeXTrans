\section{Introduction}

Large Language Models (LLMs) have emerged as exceptionally powerful AI tools, demonstrating advanced capabilities in natural language understanding and complex reasoning. Recently, the rise of reasoning-focused LLMs, also referred to as reasoning-capable models or Large Reasoning Models (LRMs) \cite{xu2025towards} such as OpenAI o1 \cite{openai_learning_to_reason} and DeepSeek-R1 \cite{guo2025deepseek}, has significantly improved performance in System-2 reasoning domains \cite{li2025system}, particularly in challenging mathematics \cite{cobbe2021training,hendrycks2measuring} and programming tasks \cite{codeforces, chen2021evaluating}. Evolving from foundational pretrained models (e.g., LLaMA \cite{touvron2023llama,grattafiori2024llama}) trained with next-token prediction \cite{devlin2019bert}, these models typically leverage Chain-of-Thought (CoT) \cite{wei2022chain} reasoning chains to generate explicit, step-by-step reasoning sequences before arriving at a final answer, significantly improving their effectiveness in reasoning-intensive tasks.

Such reasoning abilities in LLMs are typically developed through supervised fine-tuning (SFT) and reinforcement learning (RL), which promote iterative and systematic problem-solving abilities. For instance, DeepSeek-R1 \cite{guo2025deepseek} undergoes multiple rounds of SFT and RL training, emphasizing structured thinking templates and rule-based reward mechanisms. In particular, the rule-based rewards provides precise and explicit feedback signals during training, effectively enhancing the general reasoning capabilities beyond the pretrained LLM.

However, while long CoT reasoning significantly boosts accuracy, step-by-step thinking mechanisms also lead to lengthy output responses, resulting in substantial computational overhead and increased reasoning time. For instance, the "overthinking problem" arises when answering a simple question~\cite{chen2024not} like, "\textit{what is the answer of 2 plus 3?}" Some reasoning models, especially smaller ones, can generate reasoning sequences spanning thousands of tokens. This verbosity significantly increases both inference costs and latency, limiting the practical application of reasoning models in computation-sensitive real-world scenarios, such as real-time autonomous driving systems, interactive conversational assistants, precision robotic control tasks, and large-scale online search engines.

Efficient reasoning, particularly the reduction of reasoning length, offers significant benefits in such regards, providing direct cost reduction and improved feasibility for real-world deployments. Recently, numerous studies \cite{luo2025o1, yeo2025demystifying, han2024token, ma2025cot, hao2024training} have explored ways to develop more concise reasoning paths, making efficient reasoning a rapidly evolving research area.

In this paper, we present the first structured survey systematically exploring the progress in efficient reasoning for LLMs. As illustrated in Figure \ref{fig:overview}, we categorize existing work into three key directions:  
\textit{(1) Model-based efficient reasoning}, which focuses on optimizing full-length reasoning models into more concise variants or directly training efficient reasoning models.  
\textit{(2) Reasoning output-based efficient reasoning}, which dynamically reduces reasoning steps and length during inference.  
\textit{(3) Input prompts-based efficient reasoning}, which enhances reasoning efficiency based on input properties such as difficulty or length control. 
Unlike model compression techniques such as quantization \cite{xiao2023smoothquant, frantar2023gptq, lin2024awq} or KV cache compression \cite{zhang2023h2o, liu2024kivi, shi2024keepcost, yuan2024lcbench, hao2025omnikv}, which focus on reducing model size for lightweight inference, efficient reasoning in LLMs emphasizes \textit{smart and concise reasoning} by optimizing the length of \textit{generated} reasoning sequences and reducing unnecessary thinking steps.

Overall, we provide a summary of the current key approaches to efficient reasoning, organizing them into the following categories:

\begin{itemize}
    \item Reinforcement Learning with Length-Based Reward Design (Section \ref{sec:rl})
    \item Supervised Fine-Tuning with Variable-Length CoT Data (Section \ref{sec:longshortdata})
    \item Compressing Reasoning Steps into Fewer Latent Representations (Section \ref{sec:latent})
    \item Dynamic Reasoning Paradigms During Inference (Section \ref{sec:dynamic})
    \item Prompt-Guided Efficient Reasoning (Section \ref{sec:prompts})
    \item Routing Prompts to Optimize Reasoning Efficiency (Section \ref{sec:routing})
\end{itemize}

Additionally, we explore other relevant topics, including:

\begin{itemize}
    \item Training Reasoning Models with Efficient Data (Section \ref{sec:lessdata})
    \item Reasoning Abilities of Small Language Models and Model Compression (Section \ref{sec:compression}) 
    \item Evaluation and Benchmarking of Efficient Reasoning Models (Section \ref{sec:evaluation})
\end{itemize}
