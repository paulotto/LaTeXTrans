\section{Applications and Discussion}

\subsection{Applications}

\paragraph{Autonomic Driving.} Efficient reasoning LLMs are able to greatly improve autonomic driving \cite{cui2024survey, xing2025openemma, xing2024autotrust, xing2025can} by helping them understand large amounts of sensor data in a human-like way. They make the cars better at making decisions, so the vehicles can plan for difficult driving situations and react quickly when unexpected events occur. By combining information from cameras, LiDAR, radar, and other sensors, these models help cars drive more safely, choose better routes, and assess risks as they happen. Moreover, because they can explain why they make certain decisions, both passengers and regulators feel more confident in the technology, and the cars can interact more smoothly with smart road systems.

\paragraph{Embodied AI.} Efficient reasoning LLMs make embodied AI \cite{duan2022survey} much smarter by helping robots and smart devices understand and react to the world around them. These models process lots of data from cameras, sensors, and other inputs in a way that resembles human thinking. This deep understanding means that a robot can quickly decide the best way to move, handle unexpected changes, and interact safely with people. For example, in a busy factory or a home setting, a robot using these models can navigate obstacles, adjust to new situations, and even explain its actions in simple terms. Altogether, efficient reasoning LLMs boost the reliability, safety, and usefulness of embodied AI systems in daily environments.

\paragraph{Healthcare.} Efficient reasoning LLMs would improve healthcare \cite{he2023survey} by helping doctors and researchers work with large amounts of medical data more easily. They can quickly analyze patient records, test results, and medical research to spot important trends and patterns that might be hard to see otherwise. This support can lead to faster and more accurate diagnoses, better treatment recommendations, and fewer mistakes. In addition, these models can break down complex medical information into plain language, making it easier for both medical professionals and patients to understand. Generally, efficient reasoning LLMs make healthcare processes smoother and more reliable, leading to better care and outcomes for patients.

\paragraph{Recommender System.} Efficient reasoning LLMs can greatly enhance recommender systems by enabling more accurate, personalized, and context-aware suggestions across various domains such as e-commerce, entertainment, and education. These models can reason over diverse and dynamic user behavior, preferences, and historical interactions to uncover subtle patterns and relationships that traditional models might overlook. By efficiently processing complex input data, LLMs can generate high-quality recommendations with fewer computational resources. For instance, in an online shopping platform, an efficient reasoning model can anticipate evolving user interests, adapt to seasonal trends, and explain recommendations in a user-friendly way. Overall, efficient reasoning LLMs improve the scalability, transparency, and responsiveness of recommender systems, leading to better user satisfaction and engagement. ReaRec \cite{tang2025think} proposes a latent reasoning framework for recommender systems. Inspired by the think-before-action paradigm in LLMs, ReaRec enables implicit multi-step reasoning at inference time, significantly improving performance, particularly for long-tail users and items. 

\subsection{Discussion}

\textbf{Improving Reasoning Ability.} From another perspective on efficiency, improving reasoning performance is an important topic \cite{chen2025inner, sui2025meta}. To prioritize promising avenues by discarding ineffective strategies early, Meta-Reasoner \cite{sui2025meta} leverages contextual multi-armed bandits for evaluating reasoning progress and selecting the optimal strategy. In each round, the LLM produces a new reasoning step, and the meta-reasoner evaluates its output and generates a progress report, the meta-reasoner uses contextual multi-arm bandit to choose the best guidance strategy for the reasoning step. ITT \cite{chen2025inner} treats each transformer layer as a step in an internal thinking process. By dynamically allocating extra processing to difficult tokens through adaptive routing, ITT enables smaller language models to achieve performance comparable to larger models while using fewer training resources. SyzygyoT~\cite{li2025syzygy} introduces Minimal Free Resolution (MFR), which is inspired by algebraic geometry, to break down complex tasks into logically complete and minimal subproblems. This decomposition enhances the structural efficiency of CoT reasoning, enabling more precise and efficient problem-solving.
 
\textbf{Safety of Efficient Reasoning.} Safety and efficiency in LLMs often pull in opposite directions, as optimizing one always leads to the performance degradation of the other. When enhancing safety, such as filtering harmful content, mitigating adversarial attacks, and enabling self-correction, the reasoning model typically requires additional computational resources and longer reasoning sequences, leading to increased inference costs and slower response times. Conversely, prioritizing efficiency by minimizing token usage and computational overhead may reduce the reasoning ability to self-reflect, verify its outputs, or defend against adversarial manipulations. This trade-off reflects the well-known principle that there is no ``free lunch'', making it crucial to strike a careful balance between safety and efficiency. \cite{kuo2025h} investigates the robustness of safety checks in large CoT reasoning models, revealing severe security flaws in commercial systems. They introduce the malicious-educator benchmark and demonstrate that with their hijacking Chain-of-Thought (H-CoT) attack, models can drastically reduce their refusal rates, leading to the generation of harmful content. \cite{li2025output} investigates the safety of long reasoning models. It is observed that while longer outputs enable self-correction and enhance safety, some attack strategies exploit extended generations. They propose a dynamic output length control via an RL-based method to maintain both reasoning quality and security. Balancing safety and efficiency in long reasoning models remains a challenging yet crucial area of investigation. \cite{kumar2025overthink} proposes an indirect prompt injection attack targeting reasoning LLMs applied to untrusted data sources and substantially degrades reasoning efficiency. SafeMLRM~\cite{fang2025safemlrm} provides a safety analysis of multi-modal large reasoning models (MLRMs).

\textbf{Efficient LLMs for Agentic AI.} Efficient reasoning is essential to the advancement of agentic AI systems~\cite{liu2025advances}, as it directly influences their decision-making speed, resource utilization, and overall effectiveness in real-world applications. Recent research efforts have extensively investigated methods for improving agent efficiency by optimizing internal reasoning processes. Notable approaches include merging multiple planning trees to reduce computational redundancy~\cite{hu2023tree}, as well as consolidating and structuring memory representations to enhance efficiency and adaptability in dynamic environments~\cite{hu2024hiagent}. CoA~\cite{pan2024chain} enhances the capability of LLMs in managing complex tasks, especially in scenarios demanding real-time or domain-specific knowledge, with an efficient verification module leveraging our MRFS framework to refine LLM-generated responses using retrieved information. These innovations collectively contribute toward the development of more responsive, scalable, and practical AI agents capable of operating effectively under constrained computational resources. DOWN~\cite{eo2025debatenecessaryadaptivemultiagent} proposes an adaptive multi‑agent debate framework that only triggers debate when the initial confidence of an agent is low, then uses the confidence‑weighted inputs of participating agents to collaboratively refine the final answer.

\textbf{RL vs. SFT, which is better?} When comparing RL (Section \ref{sec:rl}) and SFT (Section \ref{sec:longshortdata}) for creating efficient reasoning language models, the answer is unclear as each method has its own strengths. RL allows a model to learn by trial and error, rewarding it for satisfactory decisions, which can assist it find creative ways to solve problems in new situations. However, this approach can sometimes be unpredictable and require a lot of training. On the other hand, SFT teaches the model using carefully chosen efficient CoT examples constructed by either humans or models, leading to more consistent behavior and easier control. Yet, SFT might struggle when faced with challenges that are not covered in its training data. In practice, combining both methods might be a promising direction and potentially works best because it harnesses the creativity of RL and the reliability of SFT, resulting in a model that is both adaptable and stable.
