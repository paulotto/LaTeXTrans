\section{Evaluation and Benchmark}
\label{sec:evaluation}

Recent research has introduced innovative benchmarks and evaluation frameworks to systematically assess the reasoning capabilities of LLMs. As LLMs continue to advance in their ability to perform complex reasoning tasks, the need for rigorous, standardized evaluation metrics and frameworks has become increasingly important. 

\paragraph{\textbf{Inference-time Computation.}} \cite{parashar2025inferencetimecomputationsllmreasoning} develops Sys2Bench, which is a comprehensive suite designed to evaluate LLMs across five reasoning categories, including arithmetic, logical, commonsense, algorithmic, and planning tasks. This benchmark comprises eleven diverse datasets, covering various reasoning tasks. It includes GSM8K and AQuA for arithmetic problems, StrategyQA and HotPotQA for commonsense reasoning, ProntoQA for logical reasoning, Game of 24 and Bin Packing for algorithmic tasks, and BlocksWorld, Rubik’s Cube, TripPlan, and Calendar Plan for planning tasks. The study revealed that scaling inference-time computation alone has limitations, as no single technique consistently excels across all reasoning tasks, and this emphasizes the need for diverse approaches to enhance LLM reasoning capabilities. \cite{liu2025bag} examine how various commonly used strategies affect the reasoning capabilities of LLMs. Further, they present an extensive experimental benchmark involving six inference-time optimization techniques across eight reasoning-oriented tasks. \cite{liu20251bllmsurpass405b} investigates the impact of Test-Time Scaling (TTS) strategies on LLM performance, focusing on how policy models, process reward models, and problem difficulty influence TTS effectiveness. Their findings indicate that compute-optimal TTS strategies are highly dependent on these factors. The paper finds that, with appropriate TTS strategies, smaller models (e.g., a 1B parameter LLM) are able to outperform significantly larger models (e.g., a 405B parameter LLM) on complex reasoning tasks like MATH-500, and this underscores the importance of tailored TTS approaches in evaluating and enhancing LLM reasoning. Bag of Tricks~\cite{liu2025bagoftrick} investigates several often-overlooked techniques capable of enhancing the reasoning performance of LLMs. Furthermore, it benchmarks multiple inference-time computation methods within a predefined budget, enabling controlled token usage through a flexible N-sample strategy.

\paragraph{\textbf{Evaluating Overthinking.}} \cite{cuadron2025dangeroverthinkingexaminingreasoningaction} introduces a framework to systematically analyze the "overthinking" in LLMs, where models favor extended internal reasoning over necessary environmental interactions. By examining 4,018 trajectories in agentic tasks, the study identified patterns such as Analysis Paralysis, Rogue Actions, and Premature Disengagement. \cite{cuadron2025dangeroverthinkingexaminingreasoningaction} also proposed a novel ``overthinking score'' and showed a strong correlation between higher scores and decreased task performance. Mitigation strategies such as selecting solutions with lower overthinking scores can improve performance by 30\% and at the same time reduce computational overhead by 43\%. \cite{liu2024mind} uses six comprehensive tasks from the overthinking literature to guide the design of evaluation benchmarks for testing CoT failures in LLMs. It reveals that, in many cases where humans tend to fail due to excessive deliberation, LLMs employing CoT reasoning exhibit similar failure patterns. S1-Bench~\cite{zhang2025s1} evaluates large reasoning models on straightforward tasks aligned with System 1 thinking, focusing on intuitive and fast reasoning rather than the more complex, deliberative processes associated with System 2.


\paragraph{\textbf{Effect of Long CoT Reasoning.}} \cite{yeo2025demystifying} provides a comprehensive analysis of the mechanism underlying long CoT reasoning. In addition to presenting several key insights, they propose a reward design to enhance the stability of reasoning ability during training and reduce the CoT length, which is also shown in Section~\ref{sec:rl}. \cite{jin2024impact} reveals a strong relationship between the length of the reasoning chain and the effectiveness of model outputs. Models tend to perform better with extended reasoning steps, suggesting the CoT length is more crucial than accuracy for effective problem-solving. CriticalThinking~\cite{lee2025criticalthinkingkindscomplexity} investigates the optimal reasoning length of LLMs based on using deterministic finite automata (DFAs). MiP-Overthinking~\cite{fan2025missing} discovers that when the reasoning models get questions missing key information, they tend to keep ``thinking'' over and over instead of admitting they fail to answer. By studying this behavior, they show that current training methods push models to generate long, repetitive reasoning steps rather than recognize unsolvable problems and stop early.

\paragraph{\textbf{Effect of Compression on Reasoning Models.}} CompressionReasoning~\cite{zhang2025reasoning} benchmarks compression techniques including quantization, distillation, and pruning, on reasoning tasks. The results indicate that parameter count has a greater impact on knowledge retention than on reasoning ability, and that generating shorter outputs generally leads to improved performance. QuantRM~\cite{liu2025quantizationhurtsreasoningempirical} provides a benchmark evaluating weight quantization, KV‑cache quantization, and activation quantization across various algorithms and bit‑width configurations.