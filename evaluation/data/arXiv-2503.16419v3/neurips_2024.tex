\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[nonatbib, final]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib, final]{neurips_2024}

% \usepackage{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell} 

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}
\usepackage{cite}
\usepackage{forest}
\usetikzlibrary{shadows}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{multirow}

\newcommand{\TODOYS}[1]{\textcolor{red}{\textbf{TODO (YS): #1}}}

\newcolumntype{Y}{>{\setlength\hsize{2\hsize}}X}

\definecolor{hidden-red}{RGB}{205, 44, 36}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=hidden-red,linkcolor=hidden-red, urlcolor=hidden-red]{hyperref}

\usepackage{tcolorbox}
\newcommand{\insightbox}[1]{%
    \begin{tcolorbox}[colframe=black!60, colback=blue!5, boxrule=1pt, arc=4mm]
        \includegraphics[width=0.4cm]{figs/bulb.png}
        \textbf{\textit{#1}}
    \end{tcolorbox}
}

% added
\usepackage{amsmath}

% \title{Brevity is the Soul of Reasoning: A Survey on Efficient Reasoning for Large Language Models}
\title{Stop Overthinking: A Survey on \\ Efficient Reasoning for Large Language Models}
% \title{Stop Overthinking, Incentivize Fast Reasoning \TODOYS{check}: A Survey on Efficient Reasoning for Large Language Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
\textbf{Yang Sui,}$^{}$ \enspace
\textbf{Yu-Neng Chuang,}$^{}$ \enspace
\textbf{Guanchu Wang,}$^{}$ \enspace
\textbf{Jiamu Zhang,}$^{}$  \enspace
\textbf{Tianyi Zhang,}$^{}$ \enspace
\textbf{Jiayi Yuan,}$^{}$ \\
\textbf{Hongyi Liu,}$^{}$ \enspace
\textbf{Andrew Wen,}$^{}$ \enspace
\textbf{Shaochen (Henry) Zhong,}$^{}$ \enspace
\textbf{Hanjie Chen,}$^{}$ \enspace
\textbf{Xia Hu}$^{}$ \enspace \\ 
Department of Computer Science \\
$^{}$ Rice University \\
\texttt{yang.sui@rice.edu, xia.hu@rice.edu}
% \quad  $^2$XXX University 
% Authors
\\
\\
Project Website: {\href{https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs}{https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs}}
}


\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[2]{Project Lead.}
% \footnotetext[1]{Core Contribution.}
\renewcommand*{\thefootnote}{\arabic{footnote}}

% \TODOYS{Discussion, Saturday.}

% \TODOYS{Section 5 8 12 13, Sunday.}


% \TODOYS{Section 12 13.}
% \TODOYS{Figures.}

% \TODOYS{Background, Sunday.}

\begin{abstract}
    %
    Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. 
    %
    However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the ``overthinking phenomenon''.

    %
    \textbf{\textit{Efficient Reasoning}}, which seeks to optimize reasoning length while preserving reasoning capabilities, offers practical benefits such as reduced computational costs and improved responsiveness for real-world applications. 
    %
    Despite its potential, efficient reasoning remains in the early stages of research. 
    %
    
    %
    In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. 
    % 
    Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: \textbf{\textit{(1) model-based efficient reasoning}}, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; \textbf{\textit{(2) reasoning output-based efficient reasoning}}, which aims to dynamically reduce reasoning steps and length during inference; \textbf{\textit{(3) input prompts-based efficient reasoning}}, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. 
    %
    Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.
    % 
    We maintain a public repository to continuously track and update the latest research in this promising area.

\end{abstract}

% \input{sec/0_abstract}    

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline.pdf}
    % \vspace{2mm}
    \caption{The pipeline of developing efficient reasoning for LLMs. A reasoning model can be trained on the base model using SFT, RL, or a combination of both. While reasoning models demonstrate strong reasoning capabilities, they often suffer from the ``overthinking phenomenon'', generating unnecessarily lengthy reasoning steps. To improve efficiency, various methods can be applied to reduce redundant steps while maintaining accuracy, or to fine-tune non-reasoning models to incorporate efficient reasoning capabilities. This approach enables the model to answer questions with concise and effective reasoning steps. In this paper, we explore the latest progress in efficient reasoning for LLMs, aiming to provide insights that can guide future research and the development of reasoning-driven applications across various domains.}
    \label{fig:pipeline}
\end{figure}


\input{sec/1_introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \vspace{2mm}
    \caption{
    Overview of efficient reasoning methods, which can be summarized as model-oriented (Left: \textit{I}, \textit{II}) and reasoning output-oriented (Middle: \textit{III}, \textit{IV}), and input prompts-oriented (Right: \textit{V, VI}) methods. Specifically, (I) Reinforcement Learning with Length Reward Design (Section \ref{sec:rl}); (II) Supervised Fine-Tuning with Variable-Length CoT Data (Section \ref{sec:longshortdata}); (III) Compressing Reasoning Steps into Fewer Latent Representation (Section \ref{sec:latent}); (IV) Dynamic Reasoning Paradigm during Inference (Section \ref{sec:dynamic}); (V) Prompt-guided Efficient Reasoning (Section \ref{sec:prompts}); (VI) Routing Prompts to Optimize Reasoning Efficiency (Section \ref{sec:routing}); }

    \label{fig:overview}
\end{figure}

\input{plot/taxonomy}
\input{sec/2_background}
\input{sec/part1_RL}
\input{sec/part2_data}
\input{sec/part3_latent}
\input{sec/part4_dynamic}
\input{sec/part5_prompt}
\input{sec/part6_lessdata}
\input{sec/part8_smallmodel}
\input{sec/part7_evaluation}
\input{sec/part11_application}
\input{sec/5_conclusion}


% \begin{ack}
% .
% \end{ack}


{
\bibliographystyle{plain}
\bibliography{ref}
}

\end{document}