\section{Related Works}
\label{sec:related}
\subsection{Encoder-based MLLMs}
The dominant architecture of MLLMs has remained largely unchanged since its inception, comprising three components: a ViT \cite{clip, siglip, aimv2}, an LLM \cite{llama, gpt3, qwen2.5, vicuna}, and a connector to bridge modality gaps. Previous research has focused primarily on connector design, ranging from simple MLP layers \cite{llava, llava1_5, minigpt, minigptv2} to hierarchical feature fusion modules \cite{flamingo, llama3.2} or other complex architectures \cite{elysium, dynamicvlm, internvl, cambrian1}. Despite these innovations, fundamental limitations persist due to their reliance on external vision encoders. First, computational and memory overhead escalates dramatically when applying multiple vision encoders \cite{cambrian1} or scaling to larger ones \cite{cogvlm}. Second, fixed-resolution pre-training of ViTs forces MLLMs to employ workarounds like image tiling \cite{llava1_5, llavaov} or restricted square resolutions \cite{cogvlm, qwen}. Recent attempts \cite{pixtral, qwen2vl, aimv2} to train resolution-agnostic ViTs have remained impractical, in that they adopted massive proprietary data and opaque training procedures. These challenges have spurred interest in encoder-free architectures that could bypass ViTs entirely.
\subsection{Encoder-free MLLMs}
The pioneering work, Fuyu \cite{fuyu}, demonstrated the feasibility of training encoder-free models on interleaved image-text data, though at prohibitive computational costs with limited technical transparency. Subsequent approaches, such as EVE \cite{eve}, reduced the vision encoder parameters to a single Transformer block, aligning its output features with a ViT through distillation while updating all LLM parameters to learn about vision during the main training stage. However, these methods still struggle with conflicts between the LLM’s inherent language abilities and the new modality, i.e., vision. These conflicts arise from the coupled language and vision parameters, which exacerbate unstable training and lead to catastrophic forgetting of the original language abilities.
% Fuyu \cite{fuyu} pioneered such an approach by training on interleaved image-text data. However, this method required substantial computational resources and the detailed technic is unavalable. Subsequent efforts, such as EVE \cite{eve}, explored converting pre-trained LLMs into MLLMs by replacing ViTs with a single transformer block and aligning features via a distillation-like method. This can be understood as reducing the parameters of the vision encoder. During their main stage training, all LLM parameters are updated. Such methods have certain issues, where the modality confliction between a well-pretrained LLM and a new modality that LLM has never seen before remains a severe problem that can easily cause the training unstable, and even we can stablize the training, such method suffer from catastropic forgetting issue.

To overcome these problems, Mono-InternVL \cite{monointernvl} and EVEv2 \cite{evev2} proposed parameter decoupling strategies inspired by the MoE method \cite{moe}, duplicating LLM parameters for vision-specific processing while freezing its original weights. Despite successfully addressing forgetting issues and modality conflicts, these methods suffered from substantial memory overhead by doubling model parameters, compromising architectural simplicity. 
% This tension highlights a critical insight: preserving LLM integrity while integrating vision understanding requires strict parameter decoupling without persistent duplication. 
Our work addresses this by applying LoRA, which encodes vision while maintaining the language abilities of the LLM, and can be merged into the LLM without causing additional memory overhead.
%while maintaining modality alignment.


% \subsection{Encoder-based MLLM}
% The mainstream architecture of MLLMs follows the “Vision-Connector-LLM” framework, consisting of three key components: the vision encoder(s), the connector, and the LLM. Below, we examine these components in detail.
% \\
% \textbf{vision Encoder}
% The vision encoders used in MLLMs are often trained with contrastive learning loss on image-text tasks, such as CLIP. Additionally, there are other vision encoders trained on vision tasks using self-supervised or semi-supervised objectives. These pre-trained vision encoders provide strong prior knowledge in vision, making them widely adopted in MLLMs.
% \\
% \textbf{Connector}
% Due to the misalignment between the semantic spaces of vision encoders and LLMs, a connector is typically required to bridge this gap, which is one of the most distinctive functions of the connector. Its architecture can vary based on its additional functions, such as re-sampling to reduce the number of visual tokens.
% \\
% \textbf{LLM}
% The LLM often serves as the core component of the architecture, consuming the majority of computations and parameters. These models are typically developed by large companies or organizations due to the substantial amounts of data and training costs involved. The open community usually focuses on post-training enhancements or develops MLLMs based on existing LLMs.
% \subsection{\mono}
% We define \mono as utilizing a single Transformer, specifically an LLM, to perform multimodal tasks. This architecture requires only a few parameters to convert the dimensionality of data from other modalities, such as using a patchifier for vision, without the need for additional pre-trained models. The explorations of this kind of architecture emerges quite early, but soon struggles in several problems caused by the big gap between 