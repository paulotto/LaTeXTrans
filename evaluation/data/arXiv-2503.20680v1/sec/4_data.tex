\section{Data}
\label{sec:data}
\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.2} % 行间距
    \setlength{\tabcolsep}{2pt} % 减少列间距
    \small
    \begin{tabular}{l|l|c|c}
        \toprule
         Data Format & Dataset & \# Sample & Total \\
         \midrule
        \multirow{2}{*}{Image Caption} & DataComp29M-recap (ours) & 29M & \multirow{2}{*}{30.4M} \\
         & GLDv2-recap (ours) & 1.4M & \\
        \midrule
        \multirow{10}{*}{Text QA} & Infinity-Instruct-3M \cite{infinityinstruct} & 3.5M & \multirow{10}{*}{6.4M} \\
         & SmolTalk \cite{smoltalk} & 1.0M & \\
         & OpenOrca \cite{OpenOrca} & 994.0K & \\
         & MathInstruct \cite{mathinstruct} & 262.0K & \\
         & OrcaMath \cite{orcamath} & 200.0K & \\
         & MagpiePro (L3 ST) \cite{llavaov} & 150.0K & \\
         & WizardCoder \cite{wizardcoder} & 143.0K & \\
         & OpenCodeInterpreter \cite{opencodeinterpreter} & 66.0K & \\
         & MathQA \cite{mathqa} & 29.8K & \\
         & Dolly \cite{dolly} & 11.0K & \\
        \bottomrule
    \end{tabular}
    \caption{Data used in the pre-training stage of \model{}. We use a mixture of both image and text data to alleviate the forgetting issue in training.}
    \label{tab:data}
\end{table}

\subsection{Data collection and preprocessing}
We claim that the primary focus of this work is not on data engineering or filtration; therefore, we adopt a straightforward data collection and processing strategy. Following previous studies \cite{eve, evev2, monointernvl}, our pre-training framework utilized re-captioned data. Given the limited availability of open-source, large-scale re-captioned datasets, we employed Qwen2-VL-72B \cite{qwen2vl} to generate captions for images sampled from DataComp-1B \cite{datacomp}. From this raw dataset, we selected approximately 29 million images with a longer edge exceeding 448 pixels.

We recognize that this dataset lacks specific world knowledge, particularly regarding landmarks, celebrities, and artworks. To address the deficiency in landmark data, we supplemented our dataset with approximately 1.4 million images from the Google Landmarks Dataset v2 (GLDv2) \cite{googlelandmarksdatasetv2}. For other categories, no suitable million-scale datasets were available. Furthermore, due to potential ethical concerns, we chose not to collect such data. Consequently, we acknowledge that our method may underperform in these domains. However, this limitation can be mitigated in future works by integrating relevant datasets. 
% \subsection{Coarse-to-fine data sampling strategy}
% In preliminary experiments, we evaluated caption granularity, specifically standard and detailed captions, to optimize training stability and downstream performance. Our observations indicated that detailed captions introduced instability during early training stages, likely due to the challenges LLMs face in learning fine-grained visual perception, which hindered convergence. In contrast, standard captions improved stability but limited information richness, ultimately capping final performance. To address these trade-offs, we adopted a coarse-to-fine data sampling strategy: during the initial training steps, we exclusively utilized standard captions, and after a predetermined number of steps, we incorporated detailed captions as well. This approach conceptually aligns with the overall training process of modular MLLMs, where the pre-training of the ViT can be viewed as involving relatively coarse supervision, while alignment and fine-tuning \cite{sharegpt4v, llavaov} represent fine-grained supervision.

\subsection{Multimodal data mixture}

While \model{} decouples vision and language parameters, we have observed that extended caption-only training slightly degrades the LLM's instruction-following capability. To preserve this ability, we mixed text instruction data into the training data. As shown in Table \ref{tab:data}, our final mixture contained approximately 30M image-caption pairs and 6.4M text instruction samples. The text data were obtained directly from: Infinity-Instruction \cite{infinityinstruct}, SmolTalk \cite{smoltalk}, Cambrian-1 \cite{cambrian1}, and LLaVA-OneVison \cite{llavaov}.