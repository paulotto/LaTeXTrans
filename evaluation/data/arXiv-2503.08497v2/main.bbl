\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Barraco et~al.(2022)Barraco, Cornia, Cascianelli, Baraldi, and Cucchiara]{clip_captioning1}
Manuele Barraco, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, and Rita Cucchiara.
\newblock The unreasonable effectiveness of clip features for image captioning: an experimental analysis.
\newblock In \emph{proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4662--4670, 2022.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{food101}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In \emph{Computer vision--ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13}, pages 446--461. Springer, 2014.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem[Chen et~al.(2023)Chen, Yao, Song, Li, Rao, and Zhang]{plot}
Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang.
\newblock Plot: Prompt learning with optimal transport for vision-language models, 2023.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{dtd}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3606--3613, 2014.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale, 2021.

\bibitem[Fei-Fei et~al.(2004)Fei-Fei, Fergus, and Perona]{caltech101}
Li Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.
\newblock In \emph{2004 conference on computer vision and pattern recognition workshop}, pages 178--178. IEEE, 2004.

\bibitem[Gao et~al.(2024)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and Qiao]{clip-adapter}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{International Journal of Computer Vision}, 132\penalty0 (2):\penalty0 581--595, 2024.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 12\penalty0 (7):\penalty0 2217--2226, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{imagenet_r}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 8340--8349, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{imagenet_a}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 15262--15271, 2021{\natexlab{b}}.

\bibitem[Huang et~al.(2024)Huang, Jiang, Feng, Zhang, Wang, and Wang]{clip_medical1}
Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xinchao Wang, and Yanfeng Wang.
\newblock Adapting visual-language models for generalizable anomaly detection in medical images.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11375--11385, 2024.

\bibitem[Huang et~al.(2023)Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui, Mohammed, Patra, et~al.]{kosmos1}
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Barun Patra, et~al.
\newblock Language is not all you need: Aligning perception with language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 72096--72109, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{align}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International conference on machine learning}, pages 4904--4916. PMLR, 2021.

\bibitem[Khattak et~al.(2023{\natexlab{a}})Khattak, Rasheed, Maaz, Khan, and Khan]{maple}
Muhammad~Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Maple: Multi-modal prompt learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19113--19122, 2023{\natexlab{a}}.

\bibitem[Khattak et~al.(2023{\natexlab{b}})Khattak, Wasim, Naseer, Khan, Yang, and Khan]{promptsrc}
Muhammad~Uzair Khattak, Syed~Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad~Shahbaz Khan.
\newblock Self-regulating prompts: Foundational model adaptation without forgetting.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15190--15200, 2023{\natexlab{b}}.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{stanford_cars}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision workshops}, pages 554--561, 2013.

\bibitem[Lee et~al.(2023)Lee, Song, Suh, Choi, Lee, and Kim]{rpo}
Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo~J Kim.
\newblock Read-only prompt optimization for vision-language few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 1401--1411, 2023.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{prompt_tuning}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3045--3059, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.

\bibitem[Li and Liang(2021)]{prefix_tuning}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, Online, 2021. Association for Computational Linguistics.

\bibitem[Li et~al.(2024)Li, Li, Fu, Zhang, Wang, Chen, and Yang]{promptkd}
Zheng Li, Xiang Li, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, and Jian Yang.
\newblock Promptkd: Unsupervised prompt distillation for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26617--26626, 2024.

\bibitem[Lin et~al.(2024)Lin, Yin, Ping, Molchanov, Shoeybi, and Han]{vila}
Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han.
\newblock Vila: On pre-training for visual language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26689--26699, 2024.

\bibitem[Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{p_tuning}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock {P}-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 61--68, Dublin, Ireland, 2022. Association for Computational Linguistics.

\bibitem[Lu et~al.(2022)Lu, Liu, Zhang, Liu, and Tian]{proda}
Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian.
\newblock Prompt distribution learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5206--5215, 2022.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{fgvc_aircraft}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{clip_captioning2}
Ron Mokady, Amir Hertz, and Amit~H. Bermano.
\newblock Clipcap: Clip prefix for image captioning, 2021.

\bibitem[Nilsback and Zisserman(2008{\natexlab{a}})]{flowers102}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian conference on computer vision, graphics \& image processing}, pages 722--729. IEEE, 2008{\natexlab{a}}.

\bibitem[Nilsback and Zisserman(2008{\natexlab{b}})]{ucf101}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian conference on computer vision, graphics \& image processing}, pages 722--729. IEEE, 2008{\natexlab{b}}.

\bibitem[{\"O}zdemir and Akag{\"u}nd{\"u}z(2024)]{clip_answering1}
{\"O}vg{\"u} {\"O}zdemir and Erdem Akag{\"u}nd{\"u}z.
\newblock Enhancing visual question answering through question-driven image captions as prompts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1562--1571, 2024.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and Jawahar]{oxford_pets}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Peng et~al.(2023)Peng, Wang, Dong, Hao, Huang, Ma, and Wei]{kosmos2}
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{imagenetv2}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International conference on machine learning}, pages 5389--5400. PMLR, 2019.

\bibitem[Roy and Etemad(2024)]{coprompt}
Shuvendu Roy and Ali Etemad.
\newblock Consistency-guided prompt learning for vision-language models.
\newblock In \emph{ICLR}, 2024.

\bibitem[Tang et~al.(2021)Tang, Wang, Liu, Rao, Li, and Li]{clip_captioning3}
Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, and Xiu Li.
\newblock Clip4caption: Clip for video caption.
\newblock In \emph{Proceedings of the 29th ACM International Conference on Multimedia}, pages 4858--4862, 2021.

\bibitem[Tian et~al.(2024)Tian, Zou, Yang, and Zhang]{argue}
Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang.
\newblock Argue: Attribute-guided prompt tuning for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 28578--28587, 2024.

\bibitem[van~den Oord et~al.(2019)van~den Oord, Li, and Vinyals]{contrastive_learning}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding, 2019.

\bibitem[Vaswani(2017)]{transformer}
A Vaswani.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2023)Wang, Ge, Ding, Kankanhalli, and Shan]{clip_answering3}
Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan.
\newblock What makes for good visual tokenizers for large language models?, 2023.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{imagenet_sketch}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wang et~al.(2024)Wang, Jiang, Cheng, Li, and Zhao]{hpt}
Yubin Wang, Xinyang Jiang, De Cheng, Dongsheng Li, and Cairong Zhao.
\newblock Learning hierarchical prompt with structured linguistic knowledge for vision-language models.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pages 5749--5757, 2024.

\bibitem[Wang et~al.(2022)Wang, Wu, Agarwal, and Sun]{clip_medical3}
Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun.
\newblock {M}ed{CLIP}: Contrastive learning from unpaired medical images and text.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 3876--3887, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and Torralba]{sun397}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{2010 IEEE computer society conference on computer vision and pattern recognition}, pages 3485--3492. IEEE, 2010.

\bibitem[Xu et~al.(2024)Xu, Zhu, Shen, Chen, Liao, Chen, and Wang]{provp}
Chen Xu, Yuhan Zhu, Haocheng Shen, Boheng Chen, Yixuan Liao, Xiaoxin Chen, and Limin Wang.
\newblock Progressive visual prompt learning with contrastive feature re-formation.
\newblock \emph{International Journal of Computer Vision}, pages 1--16, 2024.

\bibitem[Yang et~al.(2024)Yang, Zhang, Wang, and Xie]{mma}
Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, and Xiaohua Xie.
\newblock Mma: Multi-modal adapter for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23826--23837, 2024.

\bibitem[Yao et~al.(2023)Yao, Zhang, and Xu]{kgcoop}
Hantao Yao, Rui Zhang, and Changsheng Xu.
\newblock Visual-language prompt tuning with knowledge-guided context optimization.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6757--6767, 2023.

\bibitem[Yao et~al.(2024)Yao, Zhang, and Xu]{tcp}
Hantao Yao, Rui Zhang, and Changsheng Xu.
\newblock Tcp: Textual-based class-aware prompt tuning for visual-language model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23438--23448, 2024.

\bibitem[Yao et~al.(2021)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and Xu]{filip}
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.
\newblock Filip: Fine-grained interactive language-image pre-training.
\newblock \emph{arXiv preprint arXiv:2111.07783}, 2021.

\bibitem[Ye et~al.(2023)Ye, Kong, Yao, Ren, and Jiang]{clip_answering2}
Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, and Xudong Jiang.
\newblock Video question answering using clip-guided visual-text attention.
\newblock In \emph{2023 IEEE International Conference on Image Processing (ICIP)}, pages 81--85. IEEE, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Fang, Gao, Li, Dai, Qiao, and Li]{tip-adapter}
Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free adaption of clip for few-shot classification.
\newblock In \emph{European conference on computer vision}, pages 493--510. Springer, 2022.

\bibitem[Zhao et~al.(2024{\natexlab{a}})Zhao, Wang, Jiang, Shen, Song, Li, and Miao]{metaprompt}
Cairong Zhao, Yubin Wang, Xinyang Jiang, Yifei Shen, Kaitao Song, Dongsheng Li, and Duoqian Miao.
\newblock Learning domain invariant prompt for vision-language models.
\newblock \emph{IEEE Transactions on Image Processing}, 2024{\natexlab{a}}.

\bibitem[Zhao et~al.(2024{\natexlab{b}})Zhao, Liu, Wu, Wang, Li, Wang, Teng, Liu, Cui, Wang, and Shen]{clip_medical2}
Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, and Dinggang Shen.
\newblock Clip in medical imaging: A comprehensive survey, 2024{\natexlab{b}}.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu]{cocoop}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16816--16825, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu]{coop}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0 (9):\penalty0 2337--2348, 2022{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Niu, Han, Wu, and Zhang]{prograd}
Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang.
\newblock Prompt-aligned gradient for prompt tuning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15659--15669, 2023.

\end{thebibliography}
