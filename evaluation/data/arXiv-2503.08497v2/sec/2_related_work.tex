\section{Related Work}
\label{sec:related work}
\subsection{Vision-Language Models}
Vision-language models (VLMs) have emerged as powerful tools for capturing rich multimodal representations, standing apart from traditional models that rely exclusively on visual or textual supervision. Recent advances in VLMs, such as CLIP \cite{clip}, ALIGN \cite{align}, FILIP \cite{filip}, KOSMOS \cite{kosmos1, kosmos2}, and VILA \cite{vila}, have demonstrated remarkable performance across a variety of tasks. These models typically learn joint image-language representations through self-supervised learning, leveraging large-scale architectures and massive collections of image-text pairs. For instance, CLIP is trained on a collection of 400 million image-text pairs, while ALIGN leverages an impressive 1.8 billion pairs. Although these pre-trained models excel at learning generalized representations, efficiently adapting them to specific downstream tasks remains a challenge.


\subsection{Efficient Transfer Learning}
Prompt learning methods have proven effective for adapting VLMs. CoOp \cite{coop} pioneers prompt learning \cite{prompt_tuning, prefix_tuning, p_tuning} by replacing fixed templates with learnable continuous vectors, enhancing flexibility but compromising CLIP's zero-shot and generalization capabilities. To address this, CoCoOp \cite{cocoop} incorporates visual cues to generate instance-specific prompts, improving generalization to class distribution shifts, while ProDA \cite{proda} learns prompt distributions to enhance adaptability. PLOT \cite{plot} uses optimal transport to align the vision and text modalities. KgCoOp \cite{kgcoop} retains general textual knowledge by minimizing divergence between learned and crafted prompts. ProGrad \cite{prograd} selectively updates gradients aligned with general knowledge, and RPO \cite{rpo} mitigates internal representation shifts using masked attention. Moving beyond text-focused approaches, MaPLe \cite{maple} integrates visual prompts mapped from text prompts through a coupling function, fostering cross-modal synergy. ProVP \cite{provp} employs single-modal visual prompts with contrastive feature re-formation to align prompted visual features with CLIP's distribution. PromptSRC \cite{promptsrc} employs a self-regularization strategy to mitigate overfitting, while MetaPrompt \cite{metaprompt} applies a meta-learning-based prompt tuning algorithm that encourages task-specific prompts to generalize across various domains or classes. TCP \cite{tcp} adapts textual knowledge into class-aware tokens, enhancing generalization capabilities.

Adapter-style learning methods represent another efficient pathway for VLM adaptation. CLIP-Adapter \cite{clip-adapter} uses lightweight adapters, implemented as two-layer MLPs, to refine CLIP's feature representations through cross-entropy optimization. Building on this, Tip-Adapter \cite{tip-adapter} caches training features to facilitate efficient similarity calculations between test and training features. However, both methods process image and text representations independently before prediction. Addressing this separation, MMA \cite{mma} integrates features across branches into a shared space, allowing for cross-branch gradient flow and enhanced coherence between modalities.

In addition to the aforementioned methods, several approaches \cite{coprompt, hpt, argue, promptkd} leverage large language models (LLMs) such as GPT-3 \cite{gpt3} for text augmentation or apply distillation over the entire dataset to improve performance. However, the increased computational requirements associated with these methods may place them beyond the intended scope of efficient transfer learning.