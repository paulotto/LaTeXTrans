\section{Conclusion}
In this work, we introduce the MMRL framework to enhance the generalization of VLMs when adapting to diverse downstream datasets. MMRL establishes a shared, unbiased representation space that bridges image and text modalities, promoting balanced multimodal learning while preserving the pre-trained knowledge encapsulated in class tokens. By strategically decoupling representation tokens from class tokens during inference, MMRL effectively mitigates overfitting risks and reinforces adaptability. Extensive evaluations confirm MMRLâ€™s capacity for an optimal balance between task-specific adaptation and generalization, setting a new benchmark for efficient transfer learning. 