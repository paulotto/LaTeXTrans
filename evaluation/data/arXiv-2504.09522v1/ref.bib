

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@InProceedings{WikiAtomicEdits,
  title = {{WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse}},
  author = {Faruqui, Manaal and Pavlick, Ellie and Tenney, Ian and Das, Dipanjan},
  booktitle = {Proc. of EMNLP},
  year = {2018}
}

@ARTICLE{palm,
       author = {{Chowdhery}, Aakanksha and {Narang}, Sharan and {Devlin}, Jacob and {Bosma}, Maarten and {Mishra}, Gaurav and {Roberts}, Adam and {Barham}, Paul and {Chung}, Hyung Won and {Sutton}, Charles and {Gehrmann}, Sebastian and {Schuh}, Parker and {Shi}, Kensen and {Tsvyashchenko}, Sasha and {Maynez}, Joshua and {Rao}, Abhishek and {Barnes}, Parker and {Tay}, Yi and {Shazeer}, Noam and {Prabhakaran}, Vinodkumar and {Reif}, Emily and {Du}, Nan and {Hutchinson}, Ben and {Pope}, Reiner and {Bradbury}, James and {Austin}, Jacob and {Isard}, Michael and {Gur-Ari}, Guy and {Yin}, Pengcheng and {Duke}, Toju and {Levskaya}, Anselm and {Ghemawat}, Sanjay and {Dev}, Sunipa and {Michalewski}, Henryk and {Garcia}, Xavier and {Misra}, Vedant and {Robinson}, Kevin and {Fedus}, Liam and {Zhou}, Denny and {Ippolito}, Daphne and {Luan}, David and {Lim}, Hyeontaek and {Zoph}, Barret and {Spiridonov}, Alexander and {Sepassi}, Ryan and {Dohan}, David and {Agrawal}, Shivani and {Omernick}, Mark and {Dai}, Andrew M. and {Sankaranarayana Pillai}, Thanumalayan and {Pellat}, Marie and {Lewkowycz}, Aitor and {Moreira}, Erica and {Child}, Rewon and {Polozov}, Oleksandr and {Lee}, Katherine and {Zhou}, Zongwei and {Wang}, Xuezhi and {Saeta}, Brennan and {Diaz}, Mark and {Firat}, Orhan and {Catasta}, Michele and {Wei}, Jason and {Meier-Hellstern}, Kathy and {Eck}, Douglas and {Dean}, Jeff and {Petrov}, Slav and {Fiedel}, Noah},
        title = "{PaLM: Scaling Language Modeling with Pathways}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2022,
        month = apr,
          eid = {arXiv:2204.02311},
        pages = {arXiv:2204.02311},
          doi = {10.48550/arXiv.2204.02311},
archivePrefix = {arXiv},
       eprint = {2204.02311},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220402311C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{palm2,
       author = {{Anil}, Rohan and {Dai}, Andrew M. and {Firat}, Orhan and {Johnson}, Melvin and {Lepikhin}, Dmitry and {Passos}, Alexandre and {Shakeri}, Siamak and {Taropa}, Emanuel and {Bailey}, Paige and {Chen}, Zhifeng and {Chu}, Eric and {Clark}, Jonathan H. and {El Shafey}, Laurent and {Huang}, Yanping and {Meier-Hellstern}, Kathy and {Mishra}, Gaurav and {Moreira}, Erica and {Omernick}, Mark and {Robinson}, Kevin and {Ruder}, Sebastian and {Tay}, Yi and {Xiao}, Kefan and {Xu}, Yuanzhong and {Zhang}, Yujing and {Hernandez Abrego}, Gustavo and {Ahn}, Junwhan and {Austin}, Jacob and {Barham}, Paul and {Botha}, Jan and {Bradbury}, James and {Brahma}, Siddhartha and {Brooks}, Kevin and {Catasta}, Michele and {Cheng}, Yong and {Cherry}, Colin and {Choquette-Choo}, Christopher A. and {Chowdhery}, Aakanksha and {Crepy}, Cl{\'e}ment and {Dave}, Shachi and {Dehghani}, Mostafa and {Dev}, Sunipa and {Devlin}, Jacob and {D{\'\i}az}, Mark and {Du}, Nan and {Dyer}, Ethan and {Feinberg}, Vlad and {Feng}, Fangxiaoyu and {Fienber}, Vlad and {Freitag}, Markus and {Garcia}, Xavier and {Gehrmann}, Sebastian and {Gonzalez}, Lucas and {Gur-Ari}, Guy and {Hand}, Steven and {Hashemi}, Hadi and {Hou}, Le and {Howland}, Joshua and {Hu}, Andrea and {Hui}, Jeffrey and {Hurwitz}, Jeremy and {Isard}, Michael and {Ittycheriah}, Abe and {Jagielski}, Matthew and {Jia}, Wenhao and {Kenealy}, Kathleen and {Krikun}, Maxim and {Kudugunta}, Sneha and {Lan}, Chang and {Lee}, Katherine and {Lee}, Benjamin and {Li}, Eric and {Li}, Music and {Li}, Wei and {Li}, YaGuang and {Li}, Jian and {Lim}, Hyeontaek and {Lin}, Hanzhao and {Liu}, Zhongtao and {Liu}, Frederick and {Maggioni}, Marcello and {Mahendru}, Aroma and {Maynez}, Joshua and {Misra}, Vedant and {Moussalem}, Maysam and {Nado}, Zachary and {Nham}, John and {Ni}, Eric and {Nystrom}, Andrew and {Parrish}, Alicia and {Pellat}, Marie and {Polacek}, Martin and {Polozov}, Alex and {Pope}, Reiner and {Qiao}, Siyuan and {Reif}, Emily and {Richter}, Bryan and {Riley}, Parker and {Castro Ros}, Alex and {Roy}, Aurko and {Saeta}, Brennan and {Samuel}, Rajkumar and {Shelby}, Renee and {Slone}, Ambrose and {Smilkov}, Daniel and {So}, David R. and {Sohn}, Daniel and {Tokumine}, Simon and {Valter}, Dasha and {Vasudevan}, Vijay and {Vodrahalli}, Kiran and {Wang}, Xuezhi and {Wang}, Pidong and {Wang}, Zirui and {Wang}, Tao and {Wieting}, John and {Wu}, Yuhuai and {Xu}, Kelvin and {Xu}, Yunhan and {Xue}, Linting and {Yin}, Pengcheng and {Yu}, Jiahui and {Zhang}, Qiao and {Zheng}, Steven and {Zheng}, Ce and {Zhou}, Weikang and {Zhou}, Denny and {Petrov}, Slav and {Wu}, Yonghui},
        title = "{PaLM 2 Technical Report}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = may,
          eid = {arXiv:2305.10403},
        pages = {arXiv:2305.10403},
          doi = {10.48550/arXiv.2305.10403},
archivePrefix = {arXiv},
       eprint = {2305.10403},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230510403A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{flan,
       author = {{Wei}, Jason and {Bosma}, Maarten and {Zhao}, Vincent Y. and {Guu}, Kelvin and {Yu}, Adams Wei and {Lester}, Brian and {Du}, Nan and {Dai}, Andrew M. and {Le}, Quoc V.},
        title = "{Finetuned Language Models Are Zero-Shot Learners}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2021,
        month = sep,
          eid = {arXiv:2109.01652},
        pages = {arXiv:2109.01652},
          doi = {10.48550/arXiv.2109.01652},
archivePrefix = {arXiv},
       eprint = {2109.01652},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210901652W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{hallu1,
       author = {{Wan}, Alexander and {Wallace}, Eric and {Shen}, Sheng and {Klein}, Dan},
        title = "{Poisoning Language Models During Instruction Tuning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.00944},
        pages = {arXiv:2305.00944},
          doi = {10.48550/arXiv.2305.00944},
archivePrefix = {arXiv},
       eprint = {2305.00944},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230500944W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{data1,
       author = {{Mecklenburg}, Nick and {Lin}, Yiyou and {Li}, Xiaoxiao and {Holstein}, Daniel and {Nunes}, Leonardo and {Malvar}, Sara and {Silva}, Bruno and {Chandra}, Ranveer and {Aski}, Vijay and {Yannam}, Pavan Kumar Reddy and {Aktas}, Tolga and {Hendry}, Todd},
        title = "{Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2024,
        month = mar,
          eid = {arXiv:2404.00213},
        pages = {arXiv:2404.00213},
          doi = {10.48550/arXiv.2404.00213},
archivePrefix = {arXiv},
       eprint = {2404.00213},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240400213M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{data2,
       author = {{Ovadia}, Oded and {Brief}, Menachem and {Mishaeli}, Moshik and {Elisha}, Oren},
        title = "{Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2023,
        month = dec,
          eid = {arXiv:2312.05934},
        pages = {arXiv:2312.05934},
          doi = {10.48550/arXiv.2312.05934},
archivePrefix = {arXiv},
       eprint = {2312.05934},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231205934O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{data3,
       author = {{Cohen}, Roi and {Geva}, Mor and {Berant}, Jonathan and {Globerson}, Amir},
        title = "{Crawling the Internal Knowledge-Base of Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jan,
          eid = {arXiv:2301.12810},
        pages = {arXiv:2301.12810},
          doi = {10.48550/arXiv.2301.12810},
archivePrefix = {arXiv},
       eprint = {2301.12810},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230112810C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{reversal,
       author = {{Golovneva}, Olga and {Allen-Zhu}, Zeyuan and {Weston}, Jason and {Sukhbaatar}, Sainbayar},
        title = "{Reverse Training to Nurse the Reversal Curse}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2024,
        month = mar,
          eid = {arXiv:2403.13799},
        pages = {arXiv:2403.13799},
          doi = {10.48550/arXiv.2403.13799},
archivePrefix = {arXiv},
       eprint = {2403.13799},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240313799G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{physics_LLMs,
       author = {{Allen-Zhu}, Zeyuan and {Li}, Yuanzhi},
        title = "{Physics of Language Models: Part 3.1, Knowledge Storage and Extraction}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2023,
        month = sep,
          eid = {arXiv:2309.14316},
        pages = {arXiv:2309.14316},
          doi = {10.48550/arXiv.2309.14316},
archivePrefix = {arXiv},
       eprint = {2309.14316},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230914316A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{priming_schacter,
author = {Tulving, Endel and Schacter, Daniel and Stark, Heather},
year = {1982},
month = {07},
pages = {336-342},
title = {Priming Effects in Word-Fragment Completion are Independent of Recognition Memory},
volume = {8},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
doi = {10.1037/0278-7393.8.4.336}
}

@article{priming_meyer,
author = {Meyer, David and Schvaneveldt, Roger},
year = {1971},
month = {10},
pages = {227-34},
title = {Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations},
volume = {90},
journal = {Journal of experimental psychology},
doi = {10.1037/h0031564}
}
@article{morris, title={Memory Transformation and Systems Consolidation}, volume={17}, DOI={10.1017/S1355617711000683}, number={5}, journal={Journal of the International Neuropsychological Society}, author={Winocur, Gordon and Moscovitch, Morris}, year={2011}, pages={766–780}} <div></div>

@article{akiko,
author = {Akiko Wagatsuma  and Teruhiro Okuyama  and Chen Sun  and Lillian M. Smith  and Kuniya Abe  and Susumu Tonegawa },
title = {Locus coeruleus input to hippocampal CA3 drives single-trial learning of a novel context},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {2},
pages = {E310-E316},
year = {2018},
doi = {10.1073/pnas.1714082115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1714082115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1714082115},
abstract = {The ability to remember a new place is crucial for survival. The locus coeruleus (LC) in the brain stem is known to respond to novel sensory stimuli and can facilitate hippocampus-dependent memory, although the circuit and the role that LC plays in novelty-associated memory is unknown. We performed circuit-specific optogenetic inhibition and found that the hippocampal CA3 subregion is the crucial target of LC projections during the encoding of a novel context. Furthermore, we show with activity-dependent labeling and in vivo calcium imaging that LC inputs are necessary to provide stable neuronal representations of the context. This study provides evidence that LC neuromodulation, especially to the CA3 subregion, plays a crucial role in memory formation of a new context. The memory for a new episode is formed immediately upon experience and can last up to a lifetime. It has been shown that the hippocampal network plays a fundamental role in the rapid acquisition of a memory of a one-time experience, in which the novelty component of the experience promotes the prompt formation of the memory. However, it remains unclear which neural circuits convey the novelty signal to the hippocampus for the single-trial learning. Here, we show that during encoding neuromodulatory input from locus coeruleus (LC) to CA3, but not CA1 or to the dentate gyrus, is necessary to facilitate novel contextual learning. Silencing LC activity during exposure to a novel context reduced subsequent reactivation of the engram cell ensembles in CA3 neurons and in downstream CA1 upon reexposure to the same context. Calcium imaging of the cells reactivated in both novel and familiar contexts revealed that suppression of LC inputs at the time of encoding resulted in more variable place fields in CA3 neurons. These results suggest that neuromodulatory input from LC to CA3 is crucial for the formation of a persistent memory in the hippocampus.}}
@ARTICLE{vaswani,
       author = {{Vaswani}, Ashish and {Shazeer}, Noam and {Parmar}, Niki and {Uszkoreit}, Jakob and {Jones}, Llion and {Gomez}, Aidan N. and {Kaiser}, Lukasz and {Polosukhin}, Illia},
        title = "{Attention Is All You Need}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.03762},
        pages = {arXiv:1706.03762},
          doi = {10.48550/arXiv.1706.03762},
archivePrefix = {arXiv},
       eprint = {1706.03762},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603762V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Uli_dataset,
       author = {{Elazar}, Yanai and {Kassner}, Nora and {Ravfogel}, Shauli and {Ravichander}, Abhilasha and {Hovy}, Eduard and {Sch{\"u}tze}, Hinrich and {Goldberg}, Yoav},
        title = "{Measuring and Improving Consistency in Pretrained Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2021,
        month = feb,
          eid = {arXiv:2102.01017},
        pages = {arXiv:2102.01017},
          doi = {10.48550/arXiv.2102.01017},
archivePrefix = {arXiv},
       eprint = {2102.01017},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210201017E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{geva_KE_dataset,
       author = {{Cohen}, Roi and {Biran}, Eden and {Yoran}, Ori and {Globerson}, Amir and {Geva}, Mor},
        title = "{Evaluating the Ripple Effects of Knowledge Editing in Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = jul,
          eid = {arXiv:2307.12976},
        pages = {arXiv:2307.12976},
          doi = {10.48550/arXiv.2307.12976},
archivePrefix = {arXiv},
       eprint = {2307.12976},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230712976C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{dileep,
       author = {{Swaminathan}, Sivaramakrishnan and {Dedieu}, Antoine and {Vasudeva Raju}, Rajkumar and {Shanahan}, Murray and {Lazaro-Gredilla}, Miguel and {George}, Dileep},
        title = "{Schema-learning and rebinding as mechanisms of in-context learning and emergence}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jun,
          eid = {arXiv:2307.01201},
        pages = {arXiv:2307.01201},
          doi = {10.48550/arXiv.2307.01201},
archivePrefix = {arXiv},
       eprint = {2307.01201},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230701201S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{diff_privacy,
       author = {{Andrew}, Galen and {Thakkar}, Om and {McMahan}, H. Brendan and {Ramaswamy}, Swaroop},
        title = "{Differentially Private Learning with Adaptive Clipping}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2019,
        month = may,
          eid = {arXiv:1905.03871},
        pages = {arXiv:1905.03871},
          doi = {10.48550/arXiv.1905.03871},
archivePrefix = {arXiv},
       eprint = {1905.03871},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190503871A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{portability,
       author = {{Yao}, Yunzhi and {Wang}, Peng and {Tian}, Bozhong and {Cheng}, Siyuan and {Li}, Zhoubo and {Deng}, Shumin and {Chen}, Huajun and {Zhang}, Ningyu},
        title = "{Editing Large Language Models: Problems, Methods, and Opportunities}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.13172},
        pages = {arXiv:2305.13172},
          doi = {10.48550/arXiv.2305.13172},
archivePrefix = {arXiv},
       eprint = {2305.13172},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230513172Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{overshadow_hallucination,
       author = {{Zhang}, Yuji and {Li}, Sha and {Liu}, Jiateng and {Yu}, Pengfei and {Fung}, Yi R. and {Li}, Jing and {Li}, Manling and {Ji}, Heng},
        title = "{Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2024,
        month = jul,
          eid = {arXiv:2407.08039},
        pages = {arXiv:2407.08039},
          doi = {10.48550/arXiv.2407.08039},
archivePrefix = {arXiv},
       eprint = {2407.08039},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240708039Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{grad_clip,
       author = {{Zhang}, Jingzhao and {He}, Tianxing and {Sra}, Suvrit and {Jadbabaie}, Ali},
        title = "{Why gradient clipping accelerates training: A theoretical justification for adaptivity}",
      journal = {arXiv e-prints},
     keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning},
         year = 2019,
        month = may,
          eid = {arXiv:1905.11881},
        pages = {arXiv:1905.11881},
          doi = {10.48550/arXiv.1905.11881},
archivePrefix = {arXiv},
       eprint = {1905.11881},
 primaryClass = {math.OC},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190511881Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{continualLLM1,
       author = {{Wu}, Tongtong and {Luo}, Linhao and {Li}, Yuan-Fang and {Pan}, Shirui and {Vu}, Thuy-Trang and {Haffari}, Gholamreza},
        title = "{Continual Learning for Large Language Models: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2024,
        month = feb,
          eid = {arXiv:2402.01364},
        pages = {arXiv:2402.01364},
          doi = {10.48550/arXiv.2402.01364},
archivePrefix = {arXiv},
       eprint = {2402.01364},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240201364W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{continualLLM2,
       author = {{Shi}, Haizhou and {Xu}, Zihao and {Wang}, Hengyi and {Qin}, Weiyi and {Wang}, Wenyuan and {Wang}, Yibin and {Wang}, Zifeng and {Ebrahimi}, Sayna and {Wang}, Hao},
        title = "{Continual Learning of Large Language Models: A Comprehensive Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
         year = 2024,
        month = apr,
          eid = {arXiv:2404.16789},
        pages = {arXiv:2404.16789},
          doi = {10.48550/arXiv.2404.16789},
archivePrefix = {arXiv},
       eprint = {2404.16789},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240416789S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{hallu_FT,
       author = {{Gekhman}, Zorik and {Yona}, Gal and {Aharoni}, Roee and {Eyal}, Matan and {Feder}, Amir and {Reichart}, Roi and {Herzig}, Jonathan},
        title = "{Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2024,
        month = may,
          eid = {arXiv:2405.05904},
        pages = {arXiv:2405.05904},
          doi = {10.48550/arXiv.2405.05904},
archivePrefix = {arXiv},
       eprint = {2405.05904},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240505904G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{hallu_review,
       author = {{Huang}, Lei and {Yu}, Weijiang and {Ma}, Weitao and {Zhong}, Weihong and {Feng}, Zhangyin and {Wang}, Haotian and {Chen}, Qianglong and {Peng}, Weihua and {Feng}, Xiaocheng and {Qin}, Bing and {Liu}, Ting},
        title = "{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = nov,
          eid = {arXiv:2311.05232},
        pages = {arXiv:2311.05232},
          doi = {10.48550/arXiv.2311.05232},
archivePrefix = {arXiv},
       eprint = {2311.05232},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231105232H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{hallu2,
       author = {{Yin}, Xunjian and {Huang}, Baizhou and {Wan}, Xiaojun},
        title = "{ALCUNA: Large Language Models Meet New Knowledge}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = oct,
          eid = {arXiv:2310.14820},
        pages = {arXiv:2310.14820},
          doi = {10.48550/arXiv.2310.14820},
archivePrefix = {arXiv},
       eprint = {2310.14820},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231014820Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{mcclelland_sys_con,
       author = {{McClelland}, James K. and {McNaughton}, Bruce K. and {O'Reilly}, Randall},
        title = "{Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory}",
      journal = {Psychological Review},
     keywords = {Computer Science - Computation and Language},
         year = 1995,
          doi = {10.1037/0033-295X.102.3.419}
}

@article{review_sys_con,
author = {Kudithipudi, Dhireesha and Aguilar-Simon, Mario and Babb, Jonathan and Bazhenov, Maxim and Blackiston, Douglas and Bongard, Josh and Brna, Andrew and Chakravarthi Raja, Suraj and Cheney, Nick and Clune, Jeff and Daram, Anurag and Fusi, Stefano and Helfer, Peter and Kay, Leslie and Ketz, Nicholas and Kira, Zsolt and Kolouri, Soheil and Krichmar, Jeff and Kriegman, Sam and Siegelmann, Hava},
year = {2022},
month = {03},
pages = {196-210},
title = {Biological underpinnings for lifelong learning machines},
volume = {4},
journal = {Nature Machine Intelligence},
doi = {10.1038/s42256-022-00452-0}
}

@article{mcnaughton_sys_con,
author = {Saxena, Rajat and Shobe, Justin and Mcnaughton, Bruce},
year = {2022},
month = {07},
pages = {},
title = {Learning in deep neural networks and brains with similarity-weighted interleaved learning},
volume = {119},
journal = {Proceedings of the National Academy of Sciences},
doi = {10.1073/pnas.2115229119}
}

@ARTICLE{poison2,
       author = {{Kurita}, Keita and {Michel}, Paul and {Neubig}, Graham},
        title = "{Weight Poisoning Attacks on Pre-trained Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2020,
        month = apr,
          eid = {arXiv:2004.06660},
        pages = {arXiv:2004.06660},
          doi = {10.48550/arXiv.2004.06660},
archivePrefix = {arXiv},
       eprint = {2004.06660},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200406660K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{poison3,
       author = {{Carlini}, Nicholas and {Jagielski}, Matthew and {Choquette-Choo}, Christopher A. and {Paleka}, Daniel and {Pearce}, Will and {Anderson}, Hyrum and {Terzis}, Andreas and {Thomas}, Kurt and {Tram{\`e}r}, Florian},
        title = "{Poisoning Web-Scale Training Datasets is Practical}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = 2023,
        month = feb,
          eid = {arXiv:2302.10149},
        pages = {arXiv:2302.10149},
          doi = {10.48550/arXiv.2302.10149},
archivePrefix = {arXiv},
       eprint = {2302.10149},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230210149C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{hooker_syscon,
       author = {{Hooker}, Sara and {Courville}, Aaron and {Clark}, Gregory and {Dauphin}, Yann and {Frome}, Andrea},
        title = "{What Do Compressed Deep Neural Networks Forget?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
         year = 2019,
        month = nov,
          eid = {arXiv:1911.05248},
        pages = {arXiv:1911.05248},
          doi = {10.48550/arXiv.1911.05248},
archivePrefix = {arXiv},
       eprint = {1911.05248},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{syscon,
author = {McClelland, James L.  and McNaughton, Bruce L.  and Lampinen, Andrew K. },
title = {Integration of new information in memory: new insights from a complementary learning systems perspective},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
volume = {375},
number = {1799},
pages = {20190637},
year = {2020},
doi = {10.1098/rstb.2019.0637},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2019.0637},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0637}
,
    abstract = { According to complementary learning systems theory, integrating new memories into the neocortex of the brain without interfering with what is already known depends on a gradual learning process, interleaving new items with previously learned items. However, empirical studies show that information consistent with prior knowledge can sometimes be integrated very quickly. We use artificial neural networks with properties like those we attribute to the neocortex to develop an understanding of the role of consistency with prior knowledge in putatively neocortex-like learning systems, providing new insights into when integration will be fast or slow and how integration might be made more efficient when the items to be learned are hierarchically structured. The work relies on deep linear networks that capture the qualitative aspects of the learning dynamics of the more complex nonlinear networks used in previous work. The time course of learning in these networks can be linked to the hierarchical structure in the training data, captured mathematically as a set of dimensions that correspond to the branches in the hierarchy. In this context, a new item to be learned can be characterized as having aspects that project onto previously known dimensions, and others that require adding a new branch/dimension. The projection onto the known dimensions can be learned rapidly without interleaving, but learning the new dimension requires gradual interleaved learning. When a new item only overlaps with items within one branch of a hierarchy, interleaving can focus on the previously known items within this branch, resulting in faster integration with less interleaving overall. The discussion considers how the brain might exploit these facts to make learning more efficient and highlights predictions about what aspects of new information might be hard or easy to learn. This article is part of the Theo Murphy meeting issue ‘Memory reactivation: replaying events past, present and future’. }
}


@ARTICLE{scaling2,
       author = {{Carlini}, Nicholas and {Ippolito}, Daphne and {Jagielski}, Matthew and {Lee}, Katherine and {Tramer}, Florian and {Zhang}, Chiyuan},
        title = "{Quantifying Memorization Across Neural Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
         year = 2022,
        month = feb,
          eid = {arXiv:2202.07646},
        pages = {arXiv:2202.07646},
          doi = {10.48550/arXiv.2202.07646},
archivePrefix = {arXiv},
       eprint = {2202.07646},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220207646C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{wundt,
author = {Graziano, Vincent and Glasmachers, Tobias and Schaul, Tom and Pape, Leo and Cuccu, Giuseppe and Leitner, Juxi and Schmidhuber, Jürgen},
year = {2011},
month = {01},
pages = {41-51},
title = {Artificial Curiosity for Autonomous Space Exploration},
journal = {Acta Futura}
}

@ARTICLE{glue,
       author = {{Wang}, Alex and {Pruksachatkun}, Yada and {Nangia}, Nikita and {Singh}, Amanpreet and {Michael}, Julian and {Hill}, Felix and {Levy}, Omer and {Bowman}, Samuel R.},
        title = "{SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2019,
        month = may,
          eid = {arXiv:1905.00537},
        pages = {arXiv:1905.00537},
          doi = {10.48550/arXiv.1905.00537},
archivePrefix = {arXiv},
       eprint = {1905.00537},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190500537W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{spearman,
    title = "Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity",
    author = "Reimers, Nils  and
      Beyer, Philip  and
      Gurevych, Iryna",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1009",
    pages = "87--96",
    abstract = "Semantic Textual Similarity (STS) is a foundational NLP task and can be used in a wide range of tasks. To determine the STS of two texts, hundreds of different STS systems exist, however, for an NLP system designer, it is hard to decide which system is the best one. To answer this question, an intrinsic evaluation of the STS systems is conducted by comparing the output of the system to human judgments on semantic similarity. The comparison is usually done using Pearson correlation. In this work, we show that relying on intrinsic evaluations with Pearson correlation can be misleading. In three common STS based tasks we could observe that the Pearson correlation was especially ill-suited to detect the best STS system for the task and other evaluation measures were much better suited. In this work we define how the validity of an intrinsic evaluation can be assessed and compare different intrinsic evaluation methods. Understanding of the properties of the targeted task is crucial and we propose a framework for conducting the intrinsic evaluation which takes the properties of the targeted task into account.",
}

@article{nature_hallucinations,
  title={Detecting hallucinations in large language models using semantic entropy},
  author={Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  journal={Nature},
  volume={630},
  number={8017},
  pages={625--630},
  year={2024},
  publisher={Nature Publishing Group UK London}
}


@ARTICLE{knowledge_injection,
       author = {{Ovadia}, Oded and {Brief}, Menachem and {Mishaeli}, Moshik and {Elisha}, Oren},
        title = "{Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2023,
        month = dec,
          eid = {arXiv:2312.05934},
        pages = {arXiv:2312.05934},
          doi = {10.48550/arXiv.2312.05934},
archivePrefix = {arXiv},
       eprint = {2312.05934},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231205934O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{finn_knowledge_injection,
       author = {{Mitchell}, Eric and {Lin}, Charles and {Bosselut}, Antoine and {Manning}, Christopher D. and {Finn}, Chelsea},
        title = "{Memory-Based Model Editing at Scale}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
         year = 2022,
        month = jun,
          eid = {arXiv:2206.06520},
        pages = {arXiv:2206.06520},
          doi = {10.48550/arXiv.2206.06520},
archivePrefix = {arXiv},
       eprint = {2206.06520},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220606520M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{RAG,
       author = {{Borgeaud}, Sebastian and {Mensch}, Arthur and {Hoffmann}, Jordan and {Cai}, Trevor and {Rutherford}, Eliza and {Millican}, Katie and {van den Driessche}, George and {Lespiau}, Jean-Baptiste and {Damoc}, Bogdan and {Clark}, Aidan and {de Las Casas}, Diego and {Guy}, Aurelia and {Menick}, Jacob and {Ring}, Roman and {Hennigan}, Tom and {Huang}, Saffron and {Maggiore}, Loren and {Jones}, Chris and {Cassirer}, Albin and {Brock}, Andy and {Paganini}, Michela and {Irving}, Geoffrey and {Vinyals}, Oriol and {Osindero}, Simon and {Simonyan}, Karen and {Rae}, Jack W. and {Elsen}, Erich and {Sifre}, Laurent},
        title = "{Improving language models by retrieving from trillions of tokens}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2021,
        month = dec,
          eid = {arXiv:2112.04426},
        pages = {arXiv:2112.04426},
          doi = {10.48550/arXiv.2112.04426},
archivePrefix = {arXiv},
       eprint = {2112.04426},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv211204426B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{llama,
       author = {{Touvron}, Hugo and {Martin}, Louis and {Stone}, Kevin and {Albert}, Peter and {Almahairi}, Amjad and {Babaei}, Yasmine and {Bashlykov}, Nikolay and {Batra}, Soumya and {Bhargava}, Prajjwal and {Bhosale}, Shruti and {Bikel}, Dan and {Blecher}, Lukas and {Canton Ferrer}, Cristian and {Chen}, Moya and {Cucurull}, Guillem and {Esiobu}, David and {Fernandes}, Jude and {Fu}, Jeremy and {Fu}, Wenyin and {Fuller}, Brian and {Gao}, Cynthia and {Goswami}, Vedanuj and {Goyal}, Naman and {Hartshorn}, Anthony and {Hosseini}, Saghar and {Hou}, Rui and {Inan}, Hakan and {Kardas}, Marcin and {Kerkez}, Viktor and {Khabsa}, Madian and {Kloumann}, Isabel and {Korenev}, Artem and {Singh Koura}, Punit and {Lachaux}, Marie-Anne and {Lavril}, Thibaut and {Lee}, Jenya and {Liskovich}, Diana and {Lu}, Yinghai and {Mao}, Yuning and {Martinet}, Xavier and {Mihaylov}, Todor and {Mishra}, Pushkar and {Molybog}, Igor and {Nie}, Yixin and {Poulton}, Andrew and {Reizenstein}, Jeremy and {Rungta}, Rashi and {Saladi}, Kalyan and {Schelten}, Alan and {Silva}, Ruan and {Smith}, Eric Michael and {Subramanian}, Ranjan and {Tan}, Xiaoqing Ellen and {Tang}, Binh and {Taylor}, Ross and {Williams}, Adina and {Kuan}, Jian Xiang and {Xu}, Puxin and {Yan}, Zheng and {Zarov}, Iliyan and {Zhang}, Yuchen and {Fan}, Angela and {Kambadur}, Melanie and {Narang}, Sharan and {Rodriguez}, Aurelien and {Stojnic}, Robert and {Edunov}, Sergey and {Scialom}, Thomas},
        title = "{Llama 2: Open Foundation and Fine-Tuned Chat Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jul,
          eid = {arXiv:2307.09288},
        pages = {arXiv:2307.09288},
          doi = {10.48550/arXiv.2307.09288},
archivePrefix = {arXiv},
       eprint = {2307.09288},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230709288T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{trans_ICL,
       author = {{Ahn}, Kwangjun and {Cheng}, Xiang and {Daneshmand}, Hadi and {Sra}, Suvrit},
        title = "{Transformers learn to implement preconditioned gradient descent for in-context learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = 2023,
        month = may,
          eid = {arXiv:2306.00297},
        pages = {arXiv:2306.00297},
          doi = {10.48550/arXiv.2306.00297},
archivePrefix = {arXiv},
       eprint = {2306.00297},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230600297A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{max_ICL,
       author = {{von Oswald}, Johannes and {Niklasson}, Eyvind and {Randazzo}, Ettore and {Sacramento}, Jo{\~a}o and {Mordvintsev}, Alexander and {Zhmoginov}, Andrey and {Vladymyrov}, Max},
        title = "{Transformers learn in-context by gradient descent}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
         year = 2022,
        month = dec,
          eid = {arXiv:2212.07677},
        pages = {arXiv:2212.07677},
          doi = {10.48550/arXiv.2212.07677},
archivePrefix = {arXiv},
       eprint = {2212.07677},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv221207677V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{gemma,
       author = {{Gemma Team} and {Mesnard}, Thomas and {Hardin}, Cassidy and {Dadashi}, Robert and {Bhupatiraju}, Surya and {Pathak}, Shreya and {Sifre}, Laurent and {Rivi{\`e}re}, Morgane and {Kale}, Mihir Sanjay and {Love}, Juliette and {Tafti}, Pouya and {Hussenot}, L{\'e}onard and {Sessa}, Pier Giuseppe and {Chowdhery}, Aakanksha and {Roberts}, Adam and {Barua}, Aditya and {Botev}, Alex and {Castro-Ros}, Alex and {Slone}, Ambrose and {H{\'e}liou}, Am{\'e}lie and {Tacchetti}, Andrea and {Bulanova}, Anna and {Paterson}, Antonia and {Tsai}, Beth and {Shahriari}, Bobak and {Le Lan}, Charline and {Choquette-Choo}, Christopher A. and {Crepy}, Cl{\'e}ment and {Cer}, Daniel and {Ippolito}, Daphne and {Reid}, David and {Buchatskaya}, Elena and {Ni}, Eric and {Noland}, Eric and {Yan}, Geng and {Tucker}, George and {Muraru}, George-Christian and {Rozhdestvenskiy}, Grigory and {Michalewski}, Henryk and {Tenney}, Ian and {Grishchenko}, Ivan and {Austin}, Jacob and {Keeling}, James and {Labanowski}, Jane and {Lespiau}, Jean-Baptiste and {Stanway}, Jeff and {Brennan}, Jenny and {Chen}, Jeremy and {Ferret}, Johan and {Chiu}, Justin and {Mao-Jones}, Justin and {Lee}, Katherine and {Yu}, Kathy and {Millican}, Katie and {Lowe Sjoesund}, Lars and {Lee}, Lisa and {Dixon}, Lucas and {Reid}, Machel and {Miku{\l}a}, Maciej and {Wirth}, Mateo and {Sharman}, Michael and {Chinaev}, Nikolai and {Thain}, Nithum and {Bachem}, Olivier and {Chang}, Oscar and {Wahltinez}, Oscar and {Bailey}, Paige and {Michel}, Paul and {Yotov}, Petko and {Chaabouni}, Rahma and {Comanescu}, Ramona and {Jana}, Reena and {Anil}, Rohan and {McIlroy}, Ross and {Liu}, Ruibo and {Mullins}, Ryan and {Smith}, Samuel L and {Borgeaud}, Sebastian and {Girgin}, Sertan and {Douglas}, Sholto and {Pandya}, Shree and {Shakeri}, Siamak and {De}, Soham and {Klimenko}, Ted and {Hennigan}, Tom and {Feinberg}, Vlad and {Stokowiec}, Wojciech and {Chen}, Yu-hui and {Ahmed}, Zafarali and {Gong}, Zhitao and {Warkentin}, Tris and {Peran}, Ludovic and {Giang}, Minh and {Farabet}, Cl{\'e}ment and {Vinyals}, Oriol and {Dean}, Jeff and {Kavukcuoglu}, Koray and {Hassabis}, Demis and {Ghahramani}, Zoubin and {Eck}, Douglas and {Barral}, Joelle and {Pereira}, Fernando and {Collins}, Eli and {Joulin}, Armand and {Fiedel}, Noah and {Senter}, Evan and {Andreev}, Alek and {Kenealy}, Kathleen},
        title = "{Gemma: Open Models Based on Gemini Research and Technology}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2024,
        month = mar,
          eid = {arXiv:2403.08295},
        pages = {arXiv:2403.08295},
          doi = {10.48550/arXiv.2403.08295},
archivePrefix = {arXiv},
       eprint = {2403.08295},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240308295G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{priming,
author = {Doyen, Stephane},
year = {2012},
month = {01},
pages = {},
title = {Behavioral priming: It’s all in the mind, but whose mind?},
volume = {7},
journal = {Plos One}
}

@ARTICLE{mitigatepoison,
       author = {{Wallace}, Eric and {Zhao}, Tony Z. and {Feng}, Shi and {Singh}, Sameer},
        title = "{Concealed Data Poisoning Attacks on NLP Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.12563},
        pages = {arXiv:2010.12563},
          doi = {10.48550/arXiv.2010.12563},
archivePrefix = {arXiv},
       eprint = {2010.12563},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201012563W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{neel_grok,
       author = {{Nanda}, Neel and {Chan}, Lawrence and {Lieberum}, Tom and {Smith}, Jess and {Steinhardt}, Jacob},
        title = "{Progress measures for grokking via mechanistic interpretability}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jan,
          eid = {arXiv:2301.05217},
        pages = {arXiv:2301.05217},
          doi = {10.48550/arXiv.2301.05217},
archivePrefix = {arXiv},
       eprint = {2301.05217},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230105217N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{
scaling,
title={Effect of scale on catastrophic forgetting in neural networks},
author={Vinay Venkatesh Ramasesh and Aitor Lewkowycz and Ethan Dyer},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=GhVS8_yPeEa}
}



@ARTICLE{scaling_mem,
       author = {{Biderman}, Stella and {Sai Prashanth}, USVSN and {Sutawika}, Lintang and {Schoelkopf}, Hailey and {Anthony}, Quentin and {Purohit}, Shivanshu and {Raff}, Edward},
        title = "{Emergent and Predictable Memorization in Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = apr,
          eid = {arXiv:2304.11158},
        pages = {arXiv:2304.11158},
          doi = {10.48550/arXiv.2304.11158},
archivePrefix = {arXiv},
       eprint = {2304.11158},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230411158B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{scaling_more_forget,
       author = {{Luo}, Yun and {Yang}, Zhen and {Meng}, Fandong and {Li}, Yafu and {Zhou}, Jie and {Zhang}, Yue},
        title = "{An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = aug,
          eid = {arXiv:2308.08747},
        pages = {arXiv:2308.08747},
          doi = {10.48550/arXiv.2308.08747},
archivePrefix = {arXiv},
       eprint = {2308.08747},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230808747L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{memory1,
       author = {{Geva}, Mor and {Schuster}, Roei and {Berant}, Jonathan and {Levy}, Omer},
        title = "{Transformer Feed-Forward Layers Are Key-Value Memories}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = dec,
          eid = {arXiv:2012.14913},
        pages = {arXiv:2012.14913},
          doi = {10.48550/arXiv.2012.14913},
archivePrefix = {arXiv},
       eprint = {2012.14913},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201214913G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{memory2,
       author = {{Geva}, Mor and {Caciularu}, Avi and {Wang}, Kevin Ro and {Goldberg}, Yoav},
        title = "{Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2022,
        month = mar,
          eid = {arXiv:2203.14680},
        pages = {arXiv:2203.14680},
          doi = {10.48550/arXiv.2203.14680},
archivePrefix = {arXiv},
       eprint = {2203.14680},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220314680G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ONLINE {wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
    
}

@ARTICLE{memory_neel,
       author = {{Nanda}, Neel and {Rajamanoharan}, Senthooran and {Kramár}, János and {Rohin}, Shah},
        title = "{Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level}",
         year = 2023,
        month = dec,
       adsurl = {https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB},
}


@ARTICLE{memory3,
       author = {{Roberts}, Adam and {Raffel}, Colin and {Shazeer}, Noam},
        title = "{How Much Knowledge Can You Pack Into the Parameters of a Language Model?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = feb,
          eid = {arXiv:2002.08910},
        pages = {arXiv:2002.08910},
          doi = {10.48550/arXiv.2002.08910},
archivePrefix = {arXiv},
       eprint = {2002.08910},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200208910R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{memory4,
       author = {{Geva}, Mor and {Bastings}, Jasmijn and {Filippova}, Katja and {Globerson}, Amir},
        title = "{Dissecting Recall of Factual Associations in Auto-Regressive Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = apr,
          eid = {arXiv:2304.14767},
        pages = {arXiv:2304.14767},
          doi = {10.48550/arXiv.2304.14767},
archivePrefix = {arXiv},
       eprint = {2304.14767},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230414767G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{bau2,
       author = {{Meng}, Kevin and {Sharma}, Arnab Sen and {Andonian}, Alex and {Belinkov}, Yonatan and {Bau}, David},
        title = "{Mass-Editing Memory in a Transformer}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2022,
        month = oct,
          eid = {arXiv:2210.07229},
        pages = {arXiv:2210.07229},
          doi = {10.48550/arXiv.2210.07229},
archivePrefix = {arXiv},
       eprint = {2210.07229},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv221007229M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{bau1,
       author = {{Meng}, Kevin and {Bau}, David and {Andonian}, Alex and {Belinkov}, Yonatan},
        title = "{Locating and Editing Factual Associations in GPT}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
         year = 2022,
        month = feb,
          eid = {arXiv:2202.05262},
        pages = {arXiv:2202.05262},
          doi = {10.48550/arXiv.2202.05262},
archivePrefix = {arXiv},
       eprint = {2202.05262},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220205262M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{patchscope,
       author = {{Ghandeharioun}, Asma and {Caciularu}, Avi and {Pearce}, Adam and {Dixon}, Lucas and {Geva}, Mor},
        title = "{Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2024,
        month = jan,
          eid = {arXiv:2401.06102},
        pages = {arXiv:2401.06102},
          doi = {10.48550/arXiv.2401.06102},
archivePrefix = {arXiv},
       eprint = {2401.06102},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240106102G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{ties_merge,
       author = {{Yadav}, Prateek and {Tam}, Derek and {Choshen}, Leshem and {Raffel}, Colin and {Bansal}, Mohit},
        title = "{TIES-Merging: Resolving Interference When Merging Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.01708},
        pages = {arXiv:2306.01708},
          doi = {10.48550/arXiv.2306.01708},
archivePrefix = {arXiv},
       eprint = {2306.01708},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230601708Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{pruning,
author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
title = {Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {241},
numpages = {124},
keywords = {sparsity, deep learning, performance, low memory, generalization}
}


@article{team2023gemini,
  title={Gemini: A family of highly capable multimodal models},
  author={{Gemini Team Google}},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}}

@inproceedings{levy_dataset,
    title = "Zero-Shot Relation Extraction via Reading Comprehension",
    author = "Levy, Omer  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Zettlemoyer, Luke",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1034",
    doi = "10.18653/v1/K17-1034",
    pages = "333--342",
    abstract = "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.",
}


@ARTICLE{locality,
       author = {{Hase}, Peter and {Bansal}, Mohit and {Kim}, Been and {Ghandeharioun}, Asma},
        title = "{Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
         year = 2023,
        month = jan,
          eid = {arXiv:2301.04213},
        pages = {arXiv:2301.04213},
          doi = {10.48550/arXiv.2301.04213},
archivePrefix = {arXiv},
       eprint = {2301.04213},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230104213H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}






















