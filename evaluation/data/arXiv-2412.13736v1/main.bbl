\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abacha et~al.(2019)Abacha, Hasan, Datla, Liu, Demner-Fushman, and M{\"u}ller}]{abacha2019vqa}
Asma~Ben Abacha, Sadid~A Hasan, Vivek~V Datla, Joey Liu, Dina Demner-Fushman, and Henning M{\"u}ller. 2019.
\newblock Vqa-med: Overview of the medical visual question answering task at imageclef 2019.
\newblock \emph{CLEF (working notes)}, 2(6).

\bibitem[{Banerjee et~al.(2021)Banerjee, Gokhale, Yang, and Baral}]{banerjee2020weaqa}
Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, and Chitta Baral. 2021.
\newblock Weaqa: Weak supervision via captions for visual question answering.
\newblock \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}.

\bibitem[{Ben~Abacha et~al.(2019)Ben~Abacha, Hasan, Datla, Demner-Fushman, and M{\"u}ller}]{ben2019vqa}
Asma Ben~Abacha, Sadid~A Hasan, Vivek~V Datla, Dina Demner-Fushman, and Henning M{\"u}ller. 2019.
\newblock Vqa-med: Overview of the medical visual question answering task at imageclef 2019.
\newblock In \emph{Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes}. 9-12 September 2019.

\bibitem[{Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko}]{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020.
\newblock End-to-end object detection with transformers.
\newblock In \emph{European conference on computer vision}, pages 213--229. Springer.

\bibitem[{Changpinyo et~al.(2022)Changpinyo, Kukliansy, Szpektor, Chen, Ding, and Soricut}]{changpinyo2022all}
Soravit Changpinyo, Doron Kukliansy, Idan Szpektor, Xi~Chen, Nan Ding, and Radu Soricut. 2022.
\newblock All you may need for vqa are image captions.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 1947--1963.

\bibitem[{Chen et~al.(2022)Chen, Li, and Wan}]{chen2022align}
Zhihong Chen, Guanbin Li, and Xiang Wan. 2022.
\newblock Align, reason and learn: Enhancing medical vision-and-language pre-training with knowledge.
\newblock In \emph{Proceedings of the 30th ACM International Conference on Multimedia}, pages 5152--5161.

\bibitem[{Eslami et~al.(2021)Eslami, de~Melo, and Meinel}]{eslami2021does}
Sedigheh Eslami, Gerard de~Melo, and Christoph Meinel. 2021.
\newblock Does clip benefit visual question answering in the medical domain as much as it does in the general domain?
\newblock \emph{arXiv preprint arXiv:2112.13906}.

\bibitem[{Eslami et~al.(2023)Eslami, Meinel, and de~Melo}]{eslami-etal-2023-pubmedclip}
Sedigheh Eslami, Christoph Meinel, and Gerard de~Melo. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-eacl.88} {{P}ub{M}ed{CLIP}: How much does {CLIP} benefit visual question answering in the medical domain?}
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2023}, pages 1181--1193, Dubrovnik, Croatia. Association for Computational Linguistics.

\bibitem[{Fedus et~al.(2022{\natexlab{a}})Fedus, Dean, and Zoph}]{fedus2022review}
William Fedus, Jeff Dean, and Barret Zoph. 2022{\natexlab{a}}.
\newblock A review of sparse expert models in deep learning.
\newblock \emph{arXiv preprint arXiv:2209.01667}.

\bibitem[{Fedus et~al.(2022{\natexlab{b}})Fedus, Zoph, and Shazeer}]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer. 2022{\natexlab{b}}.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23(120):1--39.

\bibitem[{Gai et~al.(2024)Gai, Zhou, Liu, Feng, Wu, and Liu}]{gai2024medthink}
Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng, Jian Wu, and Zuozhu Liu. 2024.
\newblock Medthink: Explaining medical visual question answering via multimodal decision-making rationale.
\newblock \emph{arXiv preprint arXiv:2404.12372}.

\bibitem[{Gong et~al.(2021)Gong, Chen, Liu, Yu, and Li}]{gong2021cross}
Haifan Gong, Guanqi Chen, Sishuo Liu, Yizhou Yu, and Guanbin Li. 2021.
\newblock Cross-modal self-attention with multi-task pre-training for medical visual question answering.
\newblock In \emph{Proceedings of the 2021 International Conference on Multimedia Retrieval}, pages 456--460.

\bibitem[{He et~al.(2020)He, Zhang, Mou, Xing, and Xie}]{he2020pathvqa}
Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020.
\newblock \href {https://arxiv.org/abs/2003.10286} {Pathvqa: 30000+ questions for medical visual question answering}.
\newblock \emph{Preprint}, arXiv:2003.10286.

\bibitem[{Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton}]{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton. 1991.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3(1):79--87.

\bibitem[{Khare et~al.(2021)Khare, Bagal, Mathew, Devi, Priyakumar, and Jawahar}]{khare2021mmbert}
Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, U~Deva Priyakumar, and CV~Jawahar. 2021.
\newblock Mmbert: multimodal bert pretraining for improved medical vqa.
\newblock In \emph{2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)}, pages 1033--1036. IEEE.

\bibitem[{Khashabi et~al.(2020)Khashabi, Min, Khot, Sabharwal, Tafjord, Clark, and Hajishirzi}]{khashabi2020unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020.
\newblock Unifiedqa: Crossing format boundaries with a single qa system.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 1896--1907.

\bibitem[{Kim et~al.(2018)Kim, Jun, and Zhang}]{kim2018bilinear}
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018.
\newblock Bilinear attention networks.
\newblock \emph{Advances in neural information processing systems}, 31.

\bibitem[{Lau et~al.(2018)Lau, Gayen, Ben~Abacha, and Demner-Fushman}]{lau2018dataset}
Jason~J Lau, Soumya Gayen, Asma Ben~Abacha, and Dina Demner-Fushman. 2018.
\newblock A dataset of clinically generated visual questions and answers about radiology images.
\newblock \emph{Scientific data}, 5(1):1--10.

\bibitem[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen}]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Li et~al.(2024)Li, Wong, Zhang, Usuyama, Liu, Yang, Naumann, Poon, and Gao}]{li2024llava}
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2024.
\newblock Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Liu et~al.(2021)Liu, Zhan, Xu, Ma, Yang, and Wu}]{liu2021slake}
Bo~Liu, Li-Ming Zhan, Li~Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. 2021.
\newblock Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering.
\newblock In \emph{2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)}, pages 1650--1654. IEEE.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Hu, Zhang, Feng, Hao, Lv, and Liu}]{liu2023parameter}
Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Yang Feng, Jin Hao, Junhui Lv, and Zuozhu Liu. 2023{\natexlab{a}}.
\newblock Parameter-efficient transfer learning for medical visual question answering.
\newblock \emph{IEEE Transactions on Emerging Topics in Computational Intelligence}.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Hu, Zhang, Gai, FENG, and Liu}]{liu2023chatgpt}
Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, YANG FENG, and Zuozhu Liu. 2023{\natexlab{b}}.
\newblock A chatgpt aided explainable framework for zero-shot medical image diagnosis.
\newblock In \emph{ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)}.

\bibitem[{Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and Kalyan}]{lu2022learn}
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:2507--2521.

\bibitem[{Lu et~al.(2023)Lu, Peng, Cheng, Galley, Chang, Wu, Zhu, and Gao}]{lu2023chameleon}
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023.
\newblock Chameleon: Plug-and-play compositional reasoning with large language models.
\newblock \emph{arXiv preprint arXiv:2304.09842}.

\bibitem[{Nguyen et~al.(2019)Nguyen, Do, Nguyen, Do, Tjiputra, and Tran}]{nguyen2019overcoming}
Binh~D Nguyen, Thanh-Toan Do, Binh~X Nguyen, Tuong Do, Erman Tjiputra, and Quang~D Tran. 2019.
\newblock Overcoming data limitation in medical visual question answering.
\newblock In \emph{International Conference on Medical Image Computing and Computer-Assisted Intervention}, pages 522--530. Springer.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga et~al.}]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al. 2019.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{Pelka et~al.(2018)Pelka, Koitka, R{\"u}ckert, Nensa, and Friedrich}]{pelka2018radiology}
Obioma Pelka, Sven Koitka, Johannes R{\"u}ckert, Felix Nensa, and Christoph~M Friedrich. 2018.
\newblock Radiology objects in context (roco): a multimodal image dataset.
\newblock In \emph{Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3}, pages 180--189. Springer.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21(1):5485--5551.

\bibitem[{Ren and Zhou(2020)}]{ren2020cgmvqa}
Fuji Ren and Yangyang Zhou. 2020.
\newblock Cgmvqa: A new classification and generative model for medical visual question answering.
\newblock \emph{IEEE Access}, 8:50626--50636.

\bibitem[{Shazeer et~al.(2016)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}]{shazeer2016outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2016.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Song et~al.(2022)Song, Dong, Zhang, Liu, and Wei}]{song2022clip}
Haoyu Song, Li~Dong, Weinan Zhang, Ting Liu, and Furu Wei. 2022.
\newblock Clip models are few-shot learners: Empirical studies on vqa and visual entailment.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 6088--6100.

\bibitem[{Tiong et~al.(2022{\natexlab{a}})Tiong, Li, Li, Savarese, and Hoi}]{tiong2022plug}
Anthony Meng~Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven~C.H. Hoi. 2022{\natexlab{a}}.
\newblock \href {https://aclanthology.org/2022.findings-emnlp.67} {Plug-and-play {VQA}: Zero-shot {VQA} by conjoining large pretrained models with zero training}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 951--967, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Tiong et~al.(2022{\natexlab{b}})Tiong, Li, Li, Savarese, and Hoi}]{tiong-etal-2022-plug}
Anthony Meng~Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven~C.H. Hoi. 2022{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2022.findings-emnlp.67} {Plug-and-play {VQA}: Zero-shot {VQA} by conjoining large pretrained models with zero training}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 951--967, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2022)Wang, Xiao, Codella, Yang, Chen, Zhou, Chang, Dai, You, and Yuan}]{wang2022clip}
Zhecan Wang, Bin Xiao, Noel Codella, Jianwei Yang, Yen-Chun Chen, Luowei Zhou, Shih-Fu Chang, Xiyang Dai, Haoxuan You, and Lu~Yuan. 2022.
\newblock Clip-td: Clip targeted distillation for vision-language tasks.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz et~al.}]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al. 2020.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations}, pages 38--45.

\bibitem[{Zhan et~al.(2020)Zhan, Liu, Fan, Chen, and Wu}]{zhan2020medical}
Li-Ming Zhan, Bo~Liu, Lu~Fan, Jiaxin Chen, and Xiao-Ming Wu. 2020.
\newblock Medical visual question answering via conditional reasoning.
\newblock In \emph{Proceedings of the 28th ACM International Conference on Multimedia}, pages 2345--2354.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao}]{zhang2023llama}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu~Qiao. 2023{\natexlab{a}}.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
\newblock \emph{arXiv preprint arXiv:2303.16199}.

\bibitem[{Zhang et~al.(2024)Zhang, Liu, Li, Dong, Fu, and Wu}]{zhang2024scalable}
Ruiyuan Zhang, Jiaxiang Liu, Zexi Li, Hao Dong, Jie Fu, and Chao Wu. 2024.
\newblock Scalable geometric fracture assembly via co-creation space among assemblers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 7269--7277.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Zhang, Li, Zhao, Karypis, and Smola}]{zhang2023multimodal}
Zhuosheng Zhang, Aston Zhang, Mu~Li, Hai Zhao, George Karypis, and Alex Smola. 2023{\natexlab{b}}.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2302.00923}.

\bibitem[{Zheng et~al.(2023)Zheng, Yang, Tang, Zhou, and Yang}]{zheng2023ddcot}
Ge~Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. 2023.
\newblock Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\end{thebibliography}
