% \begin{table}[h]
%     \centering
%     \footnotesize
%     \setlength{\tabcolsep}{1.9pt}
%     \begin{tabular}{@{}c l | c  c | c c c c| c@{}}
%     \toprule
%     & \multirow{2}{*}{\centering \textbf{Benchmark {\tiny (Metric)}}} & {\textbf{DeepSeek}} & {\textbf{DeepSeek}} & \textbf{Qwen2.5} & \textbf{LLaMA-3.1}  & \textbf{Claude-3.5-}  & \textbf{GPT-4o}& \textbf{DeepSeek} \\
%     & & \textbf{V2-0506}& \textbf{V2.5-0905} & \textbf{72B-Inst.} & \textbf{405B-Inst.} & \textbf{Sonnet-1022}  & \textbf{0513} & \textbf{V3} \\
%     \midrule
%     \multirow{8}{*}{English}&   MMLU {\tiny (EM)}  & 78.2 & 80.6 & 85.3& \textbf{88.6} & \textbf{88.3}&87.2 & \textbf{88.5} \\
%      & MMLU-Redux {\tiny (EM)} &77.9 & 80.3 &  85.6 &86.2& \textbf{88.9}& 88.0 & \textbf{89.1}\\
%     & MMLU-Pro {\tiny (EM)} & 58.5 &  66.2 & 71.6 &73.3 & \textbf{78.0} & 72.6 & 75.9\\
%     & DROP {\tiny (3-shot F1)} &83.0 & 87.8 & 76.7 & 88.7 & 88.3 & 83.7 & \textbf{91.}\\
%     & IF-Eval {\tiny (Prompt Strict)} &57.7 & 80.6 & 84.1 & 86.0 & \textbf{86.5} & 84.3 & 86.1\\
%     & GPQA-Diamond {\tiny (Pass@1)} & 35.3 & 41.3& 49.0 & 51.1& \textbf{65.0} & 49.9 & 59.1\\
%     & SimpleQA {\tiny (Correct)} & 9.0 & 10.2 & 9.1 & 17.1& 28.4 & \textbf{38.2}& 24.9\\
%      & FRAMES {\tiny (Acc.)} & 66.9 & 65.4 & 69.8 & 70.0 & 72.5 & \textbf{80.5} & 73.3 \\
%      & LongBench v2 {\tiny (Acc.)} & 31.6 & 35.4 & 39.4 & 36.1 & 41.0 & 48.1 &\textbf{48.7} \\
%     \midrule
%     \multirow{5}{*}{Code} & HumanEval-Mul {\tiny (Pass@1)} & 69.3 &77.4 & 77.3 & 77.2 & {81.7} &80.5&\textbf{82.6}\\
%     & LiveCodeBench {\tiny (Pass@1-COT)} &18.8 & 29.2 & 31.1 & 28.4 & 36.3& 33.4& \textbf{40.5} \\
%     & LiveCodeBench {\tiny (Pass@1)} &20.3 & 28.4 & 28.7 & 30.1 & 32.8& 34.2& \textbf{37.6} \\
%     & Codeforces {\tiny (Percentile)} & 17.5 & 35.6 & 24.8 & 25.3 & 20.3 & 23.6 & \textbf{51.6} \\
%     & SWE Verified {\tiny (Resolved)} &-&22.6& 23.8 & 24.5 & \textbf{50.8}&38.8&42.0\\
%     & Aider-Edit {\tiny (Acc.)} & 60.3& 71.6 & 65.4 & 63.9 & \textbf{84.2} &72.9&79.7 \\
%     & Aider-Polyglot {\tiny (Acc.)}  & -& 18.2 & 7.6 & 5.8 & 45.3&16.0&\textbf{49.6} \\
%     \midrule
%     \multirow{3}{*}{Math} & AIME 2024 {\tiny (Pass@1)} & 4.6 & 16.7 & 23.3 & 23.3 & 16.0 & 9.3 & \textbf{39.2} \\
%     & MATH-500 {\tiny (EM)} &  56.3 & 74.7 & 80.0 & 73.8 & 78.3 & 74.6&\textbf{90.2} \\
%     & CNMO 2024 {\tiny (Pass@1)} & 2.8 & 10.8 & 15.9& 6.8& 13.1 & 10.8 &\textbf{43.2} \\
%     \midrule
%     \multirow{3}{*}{Chinese} & CLUEWSC {\tiny (EM)} & 89.9& 90.4 & \textbf{91.4} & 84.7 & 85.4 & 87.9 & 90.9\\
%     & C-Eval {\tiny (EM)} & 78.6& 79.5 & 86.1 & 61.5 & 76.7 & 76.0 & \textbf{86.5}\\
%      & C-SimpleQA {\tiny (Correct)}  & 48.5& 54.1 & 48.4 & 50.4 & 51.3 & 59.3 & \textbf{64.8}\\
%     \bottomrule
%     \end{tabular}
%     \caption{
%         Main long perf.
%         % Comparison between \dsviii{} and other representative chat models. 
%         % All models are evaluated in a configuration that limits the output length to 8K.
%         % Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results.
%         % \dsviii{} stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. 
%     }
%     \label{tab:long_perf}
% \end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{@{}c l | c c | c c c@{}}
    \toprule
    & \multirow{3}{*}{\centering \textbf{Benchmark {\tiny (Metric)}}} 
    & \multicolumn{2}{c|}{\textbf{Language-only Model}} 
    & \multicolumn{3}{c}{\textbf{Vision-Language Model}} \\ 
    & & \textbf{QwQ-32B} & \textbf{OpenAI} & \textbf{QVQ-72B} & \textbf{OpenAI} & \textbf{Kimi} \\
    & & \textbf{Preview} & \textbf{o1-mini} & \textbf{Preview} & \textbf{o1} & \textbf{k1.5}  \\
    \midrule
    
    \multirow{4}{*}{Reasoning}
    & MATH-500 {\tiny (EM)} & 90.6 & 90 & - & 94.8 & \textbf{96.2} \\
    & AIME 2024 {\tiny (Pass@1)} & 50 & 56.7 & - & 74.4 & \textbf{77.5} \\
    & Codeforces {\tiny (Percentile)} & 62 & 88 & - & \textbf{94} & \textbf{94} \\
    & LiveCodeBench  {\tiny (Pass@1)} & 40.6 & 53.1 & - & \textbf{67.2} & 62.5 \\
    \midrule
    
    \multirow{3}{*}{Vision} 
    & MathVista-Test {\tiny (Pass@1)} & - & - & 71.4 & 71 & \textbf{74.9} \\
    & MMMU-Val {\tiny (Pass@1)} & - & - & 70.3 & \textbf{77.3} & 70 \\
    & MathVision-Full {\tiny (Pass@1)} & - & - & 35.9 & - & \textbf{38.6} \\
    % & Mensa-Norway-35 {\tiny (Acc@5)} & - & - & 28.5 & 22.9 & 38.3 \\
 
    \bottomrule
    \end{tabular}
    \caption{Performance of Kimi k1.5 long-CoT and  flagship open-source and proprietary models.}
    \label{tab:long_perf}
\end{table}