\begin{table}[h]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{@{}c l | c c c | c c c  c@{}}
    \toprule
    & \multirow{3}{*}{\centering \textbf{Benchmark {\tiny (Metric)}}} 
    & \multicolumn{3}{c|}{\textbf{Language-only Model}} 
    & \multicolumn{4}{c}{\textbf{Vision-Language Model}} \\ 
    & & \textbf{Qwen2.5} & \textbf{LLaMA-3.1} & \textbf{DeepSeek} & \textbf{Qwen2-VL} & \textbf{Claude-3.5-} & \textbf{GPT-4o}  & \textbf{Kimi} \\
    & & \textbf{72B-Inst.} & \textbf{405B-Inst.} & \textbf{V3}  & & \textbf{Sonnet-1022} & \textbf{0513} & \textbf{k1.5} \\
    \midrule
    
    \multirow{4}{*}{Text}
    & MMLU {\tiny (EM)}   & 85.3 & \textbf{88.6} & 88.5 & - & 88.3 & 87.2 & 87.4 \\
    & IF-Eval {\tiny (Prompt Strict)}  & 84.1 & 86.0 & 86.1 & - & 86.5 & 84.3 & \textbf{87.2} \\
    & CLUEWSC  {\tiny (EM)} & 91.4 & 84.7 & 90.9 & - & 85.4 & 87.9 & \textbf{91.7} \\
    & C-Eval  {\tiny (EM)}  & 86.1 & 61.5 & 86.5 & - & 76.7 & 76.0 & \textbf{88.3} \\
    \midrule
    
    \multirow{4}{*}{Reasoning} 
    & MATH-500 {\tiny (EM)} & 80.0 & 73.8 & 90.2 & - & 78.3 & 74.6 & \textbf{94.6} \\
    & AIME 2024 {\tiny (Pass@1)} & 23.3 & 23.3 & 39.2 & - & 16.0 & 9.3 & \textbf{60.8} \\
    & HumanEval-Mul {\tiny (Pass@1)}  & 77.3 & 77.2 & \textbf{82.6} & - & 81.7 & 80.5 & 81.5 \\
    & LiveCodeBench {\tiny (Pass@1)} & 31.1 & 28.4 & 40.5 & - & 36.3 & 33.4 & \textbf{47.3} \\
    \midrule
    
    \multirow{3}{*}{Vision} 
    & MathVista-Test {\tiny (Pass@1)} & - & - & - & 69.7 & 65.3 & 63.8 & \textbf{70.1} \\
    & MMMU-Val {\tiny (Pass@1)} & - & - & - & 64.5 & 66.4 & \textbf{69.1} & 68.0 \\
    & MathVision-Full {\tiny (Pass@1)} & - & - & - & 26.6 & \textbf{35.6} & 30.4 & 31.0 \\
 
    \bottomrule
    \end{tabular}
    \vspace{1em}
    \caption{Performance of Kimi k1.5 short-CoT and flagship open-source and proprietary models. VLM model performance were obtained from the OpenCompass benchmark platform (https://opencompass.org.cn/).}
    \label{tab:short_perf}
\end{table}