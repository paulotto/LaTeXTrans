\section{Conclusion}
\label{sec:conclusion}
In this work, we propose IDEA, a novel feature learning framework for multi-modal object ReID.
%
We first construct three text-enhanced multi-modal object ReID benchmarks using MLLMs, providing a structured caption generation pipeline.
%
With the generated text, the Inverted Multi-modal Feature Extractor (IMFE) leverages semantic guidance from inverted texts while reducing fusion conflicts.
%
Additionally, the Cooperative Deformable Aggregation (CDA) adaptively aggregates discriminative local features with global information.
%
Experiments on three public ReID benchmarks demonstrate the effectiveness of our method.
%
\\
{\small
\textbf{Acknowledgements.}
This work was supported in part by the National Natural Science Foundation of China (No. 62101092, 62476044, 62388101) and Open Project of Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, Anhui University (No. MMC202102, MMC202407).} % 这里结束 \small 的作用范围
\vspace*{-4mm} 