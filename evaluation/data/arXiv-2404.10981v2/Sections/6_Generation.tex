\subsection{Enhancing}
Enhancing methods are strategies aimed at improving the quality and relevance of generated outputs by integrating retrieved content in various ways. These methods differ in how they combine, aggregate, or refine retrieved information, offering multiple approaches to enrich the final output. Broadly, these techniques can be grouped into three categories: enhancing with queries, enhancing with ensemble approaches, and enhancing with feedback loops.

\paragraph{Enhance with Query}
This approach integrates the retrieved documents with the original query, enabling the generator to leverage both sources in producing the final output. By combining the query with the retrieved content, the generation process ensures that the response remains closely aligned with the user’s intent while being enriched by relevant information. The focus here is on the seamless fusion of the query and context, allowing the generated output to maintain both relevance and completeness. For instance, the RETRO \cite{borgeaud2022improving} model enhances generation by integrating retrieved text chunks with the user’s query using a chunked cross-attention mechanism, where relevant information from the retrieved neighbors is directly injected into the generation process. This method involves first retrieving similar document chunks based on the query and then using a cross-attention module to align and combine these chunks with the input sequence during generation. In-Context RALM \cite{ram2023incontext} takes a comparable approach, directly prepending the retrieved documents to the input query. In this way, the language model can generate responses conditioned on both the query and the retrieved content without requiring changes to the model's architecture. Both examples illustrate a straightforward yet effective method: concatenating the query and retrieved documents into a single input sequence that the LLMs process together, yielding outputs that are contextually enhanced.

\paragraph{Enhance with Ensemble}
When multiple sources are synthesized, the generation process can achieve a more coherent and well-rounded response. Rather than relying solely on a single source, this approach aggregates information from various documents, allowing the generator to reconcile conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output. The ensemble process can manifest in different ways: it may involve combining insights from several sources into a unified narrative, or generating multiple candidate outputs and choosing the best one based on criteria like consistency, relevance, or factual accuracy. An instance of this strategy is seen in FiD \cite{izacard2021leveraging}, which encodes multiple retrieved passages independently before fusing them in the decoder to create a coherent answer. By treating each passage separately during encoding and then merging them during decoding, the model effectively combines evidence from multiple sources. Meanwhile, in REPLUG \cite{shi2023replug}, an ensemble approach is adopted where each retrieved document is independently prepended to the query and processed separately. The outputs are then aggregated, with relevance scores guiding the weighting of each document’s contribution. Through this process, the model capitalizes on diverse information across several sources, leading to improvements in answer accuracy, coverage, and scalability as more data becomes available.

\paragraph{Enhance with Feedback}
In contrast to approaches that process retrieved information in a single pass, this method introduces iterative refinement into the generation process by incorporating feedback loops. Initially, the generator produces a draft response, which is then evaluated and adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on factual accuracy and fluency. This iterative approach aims to incrementally improve the output by identifying and correcting errors or fine-tuning content to better align with quality standards, ultimately producing a polished and reliable response. PRCA \cite{yang2023prca} offers an example by positioning itself between the retriever and generator, distilling retrieved information based on feedback from the generator. This distilled information serves as a reward model to guide context optimization, leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which details should be emphasized or downplayed. DSP \cite{khattab2022demonstratesearchpredict}, on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant passages, and updates the context in subsequent steps—each stage building on the last to refine the final output. Feedback-driven enhancements are also evident in models like Selfmem \cite{cheng2023lift}, which focus on generating self-memory. The model first produces an unbounded pool of outputs and then selects the most relevant one as memory for the next generation, guided by metrics like BLEU or ROUGE. Finally, RECITE \cite{shi2023replug} integrates feedback by generating multiple recitations from the model’s internal knowledge and using self-consistency techniques to aggregate the outputs. By introducing diversity in the recitations and leveraging passage hints during generation, this approach selects the best content through majority voting. Together, these methods demonstrate how feedback loops and iterative refinements can lead to outputs that are not only more accurate but also increasingly coherent and contextually grounded as they evolve.

\subsection{Customization}
Customization focuses on tailoring content to the user's personality and needs. It involves adjusting the output either to align with specific knowledge retrieved during earlier stages (content alignment) or to adapt the generated response to meet the user’s preferences, context, or audience needs (contextual adaptation).

In LAPDOG \cite{huang2023learning}, customization is achieved primarily through content alignment by integrating persona profiles with external stories to enrich the context used for generation. The story retriever identifies relevant narratives based on the persona, expanding the limited profiles with additional information. The generator then combines this enriched knowledge with the dialogue history, ensuring that responses align closely with the persona’s traits and background. This approach allows for a nuanced understanding of the user’s personality, making the output more engaging and contextually appropriate.

On the other hand, PersonaRAG \cite{zerhoudi2024personarag} emphasizes real-time adaptation by customizing generated content based on dynamic user profiles, session behavior, and ongoing feedback. A multi-agent system continuously analyzes user interactions to refine responses, ensuring alignment with the user’s preferences and context. By integrating personalized insights at each step, the system can adjust its output to suit specific informational needs and situational contexts. This level of responsiveness allows the system to evolve in line with the user’s changing requirements, creating more relevant and targeted responses.

ERAGent \cite{shi2024eragent} also focuses on customization but through the use of a Personalized LLM Reader, which adapts responses using user-specific profiles. This module integrates rewritten questions, filtered knowledge, and user preferences to tailor responses according to both content relevance and user needs. For instance, it takes into account preferences like environmental consciousness or dietary restrictions, ensuring that the generated content is not only aligned with retrieved knowledge but also personalized to the user’s particular values and requirements. This deep level of customization ensures that the output is both relevant and personally meaningful, enhancing user engagement.

ROPG \cite{salemi2024optimization} proposes a dynamic pre- and post-generation retriever selection model, enhancing personalization by aligning the retrieval process with both the input context and the user’s preferences. The pre-generation model determines which retrieval strategy—such as recency-based, keyword matching, or semantic retrieval—is most appropriate before generation begins. By tailoring the retrieval process in this way, the model ensures that the documents retrieved from the user profile closely match the current input, thereby aligning the content with relevant user-specific knowledge. Following this, the post-generation model evaluates the outputs generated by different retrieval strategies and selects the most personalized result. This selection is guided by feedback from the generated content, which is then used to adjust future retrievals. By combining content alignment (through pre-generation retrieval) with contextual adaptation (through post-generation evaluation), this approach offers a comprehensive solution for customization within RAG.