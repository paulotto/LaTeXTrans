\begin{table}
	\centering
	\resizebox{\linewidth}{!}{%
		\begin{tblr}{
				cells = {m},
				cell{2}{1} = {r=3}{},
				cell{2}{2} = {r=3}{},
				cell{2}{5} = {r=3}{},
				cell{5}{1} = {r=3}{},
				cell{5}{2} = {r=3}{},
				cell{5}{4} = {r=3}{},
				cell{5}{5} = {r=3}{},
				cell{8}{1} = {r=2}{},
				cell{8}{2} = {r=2}{},
				cell{8}{5} = {r=2}{},
				cell{10}{1} = {r=4}{},
				cell{10}{2} = {r=4}{},
				cell{10}{5} = {r=4}{},
				cell{14}{1} = {r=4}{},
				cell{14}{2} = {r=4}{},
				cell{14}{4} = {r=4}{},
				cell{14}{5} = {r=4}{},
				cell{18}{1} = {r=3}{},
				cell{18}{2} = {r=3}{},
				cell{18}{5} = {r=3}{},
				cell{21}{1} = {r=2}{},
				cell{21}{2} = {r=2}{},
				cell{21}{5} = {r=2}{},
				vlines,
				hline{1-2,5,8,10,14,18,21,23} = {-}{},
				hline{3-4,9,11-13,19-20,22} = {3-4}{},
				hline{6-7,15-17} = {3}{},
			}
			\textbf{Evaluation Framework} & \textbf{Aspects} & \textbf{Methods} & \textbf{Metrics} & \textbf{Datasets}\\
			RAGAS \cite{es2023ragas} & Quality of RAG Systems & Context Relevance & Extracted Sentences / Total Sentences & WikiEval\footnote{https://huggingface.co/datasets/explodinggradients/WikiEval}\\
			&  & Answer Relevance & Average Cosine Similarity & \\
			&  & Faithfulness & Supported Statements / Total Statements & \\
			ARES \cite{Saad2023ARES} & Improving RAGAS & Context Relevance & Confidence Intervals & {KILT \cite{petroni2021kilt} \\SuperGLUE \cite{wang2019superglue} }\\
			&  & Answer Relevance &  & \\
			&  & Answer Faithfulness &  & \\
			RECALL  \cite{liu2023recall} & Counterfactual Robustness & Response Quality & {Accuracy (QA)\\BLEU, ROUGE-L (Generation)} & {EventKG \cite{gottschalk2018eventkg} \\UJ \cite{huang2022understanding} }\\
			&  & Robustness & {Misleading Rate (QA)\\Mistake Reappearance Rate (Generation)} & \\
			RGB \cite{chen2024benchmarking}& Impact of RAG on LLMs & Noise Robustness & Accuracy & Synthetic\\
			&  & Negative Rejection & Rejection Rate & \\
			&  & Information Integration & Accuracy & \\
			&  & Counterfactual Robustness & {Error Detection Rate \\Error Correction Rate} & \\
			MIRAGE \cite{xiong2024benchmarking}& RAG in Medical QA & Zero-Shot Learning & Accuracy & {MMLU-Med \cite{hendrycks2021measuring}\\MedQA-US \cite{jin2021what}\\MedMCQA \cite{pal2022medmcqa}\\PubMedQA \cite{jin2019pubmedqa}\\BioASQ-Y/N \cite{tsatsaronis2015overview}}\\
			&  & Multi-Choice Evaluation &  & \\
			&  & Retrieval-Augmented Generation &  & \\
			&  & Question-Only Retrieval &  &  \\
			eRAG \cite{salemi2024evaluating} & Retrieval Quality in RAG & Downstream Task & Accuracy, ROUGE & KILT\\
			&  & Set-based & Precision, Recall, Hit Rate & \\
			&  & Ranking & MAP, MRR, NDCG & \\
			BERGEN \cite{rau2024bergen} & Standardizing RAG Experiments & Surface-Based & EM, F1, Precision, Recall & QA Datasets \cite{kwiatkowski2019natural, joshi2017triviaqa}\\
			&  & Semantic & BEM \cite{bulian2022tomayto}, LLMeval \cite{rau2024bergen}& 
		\end{tblr}
	}
	\caption{The Comparison of Different RAG Evaluation Frameworks.}
	\label{tab:evaluation}
\end{table}

To assess how effectively language models can generate more accurate, relevant, and robust responses by leveraging external knowledge, the evaluation of RAG systems has emerged as a crucial research focus. Given the rising popularity of dialogue-based interactions, much recent work has concentrated on evaluating RAG models' performance on such downstream tasks using established metrics like Exact Match (EM) and F1 scores. These metrics have been applied across a wide array of datasets, including TriviaQA \cite{joshi2017triviaqa}, HotpotQA \cite{yang2018hotpotqa}, FEVER \cite{thorne2018fever}, Natural Questions (NQ) \cite{kwiatkowski2019natural}, Wizard of Wikipedia (WoW) \cite{dinan2019wizard}, and T-REX \cite{elsahar2018trex}, which are often used to benchmark the effectiveness of retrieval and generation components in knowledge-intensive tasks.

While downstream task evaluations provide valuable insights, they fail to address the multifaceted challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has proposed various frameworks and benchmarks that aim to evaluate these systems from multiple perspectives, considering not only the quality of the generated text but also the relevance of retrieved documents and the system’s resilience to misinformation, as shown in Table \ref{tab:evaluation}. These evaluations include metrics that assess noise robustness, negative prompting, information integration, and counterfactual robustness, all of which reflect the complex challenges RAG systems face in real-world applications. The ongoing development of comprehensive evaluation frameworks and metrics is essential for advancing the field, broadening the applicability of RAG systems, and ensuring that they meet the demands of an increasingly dynamic and complex information landscape \cite{yu2024evaluation}.

\subsection{Retrieval-based Aspect}
In information retrieval, standard metrics such as Mean Average Precision (MAP), Precision, Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) \cite{radlinski2010comparing, reimers2019sentencebert, nogueira2019multistage} have traditionally been used to evaluate the relevance of retrieved documents to a given query. These metrics are essential in assessing the effectiveness of traditional information retrieval systems, where the primary goal is to measure how well the retrieved documents match the user’s query.

When applied to RAG systems, these retrieval-based metrics extend their focus to consider how the retrieved information contributes to the quality of the generated output. In this context, Accuracy becomes a crucial metric, assessing how precisely the retrieved documents provide correct information for answering queries. Additionally, Rejection Rate \cite{chen2024benchmarking}, which measures the system’s ability to decline answering when no relevant information is available, has emerged as a key indicator of responsible output generation. Similarly, Error Detection Rate \cite{chen2024benchmarking} evaluates the model’s capability to identify and filter out incorrect or misleading information, ensuring that the generation process is based on trustworthy sources.

Another important consideration is Context Relevance, which assesses the alignment of retrieved documents with the specific query, emphasizing the need for content directly relevant to the generation task’s context. Faithfulness \cite{es2023ragas} is also critical in determining whether the generated text accurately reflects the information found in the retrieved documents, thereby minimizing the risk of generating misleading or incorrect content.

The eRAG framework \cite{salemi2024evaluating} introduces a more refined approach to evaluating retrieval quality in RAG systems by focusing on individual documents rather than the entire retrieval process. It operates by feeding each document in the retrieval list into the LLM alongside the query and evaluating the generated output against downstream task metrics such as Accuracy. The document-level scores are then aggregated using ranking metrics like MAP to produce a single evaluation score. This focus on document-level contributions offers a more precise assessment of retrieval quality while being significantly more computationally efficient than traditional end-to-end evaluations.

Notably, eRAG demonstrates that its document-level evaluation correlates more strongly with downstream RAG performance compared to conventional methods like human annotations or provenance labels. This correlation underscores that the LLM, as the primary consumer of the retrieved results, is the most reliable judge of retrieval performance \cite{salemi2024evaluating}. Regardless of the retrieval model or the number of retrieved documents, eRAG consistently outperforms other evaluation approaches, indicating that directly evaluating how each document supports the LLM’s output is the most effective way to measure retrieval quality in RAG systems.

\subsection{Generation-based Aspect}
The evaluation of text produced by large language models involves analyzing performance across a range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy, and alignment with ground-truth data. Metrics like BLEU \cite{10.3115/1073083.1073135} and ROUGE-L \cite{lin-2004-rouge} are often used to measure fluency, similarity to human-produced text, and the overlap with reference summaries, respectively, providing insights into how well the generated content captures key ideas and phrases.

In addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap with ground-truth data are evaluated using EM and F1 scores, which respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized.

Beyond these standard evaluation techniques, more specialized criteria have been introduced to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate \cite{liu2023recall} have been developed to measure a model’s ability to avoid generating incorrect or misleading content. Other advanced metrics include Answer Relevance \cite{es2023ragas}, which assesses the precision of responses to queries, Kendall’s tau \cite{Saad2023ARES}, used for evaluating the accuracy of system rankings, and Micro-F1 \cite{Saad2023ARES}, which fine-tunes accuracy evaluation in tasks involving multiple correct answers. Prediction Accuracy further complements these by directly measuring how closely the generated responses align with the expected answers, offering a clear measure of a system’s effectiveness in producing accurate content.


\begin{table}[htb]
	\centering
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} 
			\hline
			\multirow{2}{*}{\textbf{Research}} & \multirow{2}{*}{\textbf{Year}} & \multicolumn{2}{c|}{\textbf{Retrieval Source}} & \multirow{2}{*}{\textbf{Multi-hop}} & \multirow{2}{*}{\textbf{Training}} & \multicolumn{3}{c|}{\textbf{Pre-Retrieval}} & \textbf{Retrieval} & \multicolumn{2}{c|}{\textbf{Post-Retrieval}} & \multicolumn{2}{c|}{\textbf{Generation}} \\ 
			\cline{3-4}\cline{7-14}
			&  & \textbf{Internal} & \textbf{External} &  &  & \textbf{Indexing} & \textbf{Query Manipulation} & \textbf{Data Modification} & \textbf{Search \& Ranking} & \textbf{Re-Ranking} & \textbf{Filtering} & \textbf{Enhancing} & \textbf{Customization} \\ 
			\hline
			REALM \cite{guu2020retrieval} & 2020 &  & \Checkmark &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  &  \\ 
			\hline
			kNN-LMs \cite{khandelwal2020generalization} & 2020 & \Checkmark & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			RAG \cite{lewis2020retrievalaugmented} & 2020 &  & \Checkmark &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  &  \\ 
			\hline
			FiD \cite{izacard2021leveraging} & 2021 &  & \Checkmark &  &  &  &  &  & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			Webgpt \cite{nakano2021webgpt} & 2021 &  & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark &  & \Checkmark &  & \Checkmark & \Checkmark &  \\ 
			\hline
			Re2G \cite{glass2022reg} & 2022 & \Checkmark &  & \Checkmark & \Checkmark &  &  &  &  & \Checkmark &  &  &  \\ 
			\hline
			RETRO \cite{borgeaud2022improving} & 2022 &  & \Checkmark & \Checkmark & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  &  \\ 
			\hline
			DSP \cite{khattab2022demonstratesearchpredict} & 2022 &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  & \Checkmark &  \\ 
			\hline
			CoK \cite{li2024chainofknowledge} & 2023 &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  &  &  \\ 
			\hline
			IRCOT \cite{trivedi2023interleaving} & 2023 &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  &  & \Checkmark &  \\ 
			\hline
			ITRG \cite{feng2024retrievalgeneration} & 2023 & \Checkmark & \Checkmark & \Checkmark &  &  &  &  & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			PKG \cite{luo2023augmented} & 2023 & \Checkmark &  &  &  &  &  &  &  &  &  &  & \\ 
			\hline
			RA-DIT \cite{lin2024radit} & 2023 &  & \Checkmark & \Checkmark & \Checkmark &  &  & \Checkmark & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			Self-RAG \cite{asai2024selfrag} & 2023 &  & \Checkmark &  & \Checkmark &  &  &  &  &  & \Checkmark &  & \\ 
			\hline
			SURGE \cite{kang2023knowledge} & 2023 & \Checkmark &  &  &  &  &  &  & \Checkmark &  &  &  & \\ 
			\hline
			FiD-TF \cite{berchansky2023optimizing} & 2023 &  & \Checkmark &  &  &  &  &  &  & \Checkmark & \Checkmark &  &  \\ 
			\hline
			PRCA \cite{yang2023prca} & 2023 &  & \Checkmark &  & \Checkmark &  &  &  & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			REPLUG \cite{shi2023replug} & 2023 &  & \Checkmark &  & \Checkmark &  &  &  &  &  &  & \Checkmark & \\ 
			\hline
			AAR \cite{yu2023augmentationadapted} & 2023 &  & \Checkmark &  & \Checkmark &  &  &  & \Checkmark &  &  &  &  \\ 
			\hline
			Query2doc \cite{wang2023querydoc} & 2023 & \Checkmark &  &  &  &  & \Checkmark &  &  &  &  &  &  \\ 
			\hline
			Step-Back \cite{zheng2024take} & 2023 &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  &  &  &  \\ 
			\hline
			ITER-RETGEN \cite{shao2023enhancing} & 2023 &  & \Checkmark & \Checkmark &  &  &  &  & \Checkmark & \Checkmark &  &  &  \\ 
			\hline
			RECITE \cite{sun2023recitationaugmented} & 2023 & \Checkmark &  & \Checkmark & \Checkmark &  &  & \Checkmark &  &  &  & \Checkmark &  \\ 
			\hline
			PROMPTAGATOR \cite{dai2023promptagator} & 2023 & \Checkmark &  & \Checkmark &  &  & \Checkmark &  &  & \Checkmark & \Checkmark &  &  \\ 
			\hline
			UPRISE \cite{cheng2023uprise} & 2023 & \Checkmark &  & \Checkmark & \Checkmark &  &  & \Checkmark & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			GENREAD \cite{yu2023generate} & 2023 & \Checkmark &  &  &  &  &  & \Checkmark &  &  &  & \Checkmark &  \\ 
			\hline
			LAPDOG \cite{huang2023learning} & 2023 & & \Checkmark & & \Checkmark & & \Checkmark & & \Checkmark & \Checkmark & & \Checkmark & \Checkmark \\
			\hline
			KnowledGPT \cite{wang2023knowledgpt} & 2023 &  & \Checkmark & \Checkmark &  &  & \Checkmark & \Checkmark &  &  &  &  &  \\ 
			\hline
			Selfmem \cite{cheng2023lift} & 2023 &  & \Checkmark & \Checkmark & \Checkmark &  &  &  &  & \Checkmark &  & \Checkmark &  \\ 
			\hline
			MEMWALKER \cite{chen2023walking} & 2023 &  & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  &  & \Checkmark &  \\ 
			\hline
			RECOMP \cite{xu2024recomp} & 2023 &  & \Checkmark &  & \Checkmark &  &  &  &  &  & \Checkmark &  &  \\ 
			\hline
			Rewrite-Retrieve-Read \cite{ma2023query} & 2023 &  & \Checkmark &  & \Checkmark &  & \Checkmark &  &  &  &  &  &  \\ 
			\hline
			Atlas \cite{ma2023query} & 2023 &  & \Checkmark & \Checkmark & \Checkmark & \Checkmark &  &  & \Checkmark & \Checkmark &  &  &  \\ 
			\hline
			DKS-RAC \cite{huang2023retrieval} & 2023 &  & \Checkmark & \Checkmark & \Checkmark &  &  &  &  & \Checkmark & \Checkmark &  &  \\ 
			\hline
			In-Context RALM \cite{ram2023incontext} & 2023 &  & \Checkmark &  &  &  &  &  &  & \Checkmark &  &  &  \\ 
			\hline
			Fid-light \cite{hofstätter2023fidlight} & 2023 &  & \Checkmark & \Checkmark &  &  &  &  &  & \Checkmark &  &  &  \\ 
			\hline
			FLARE \cite{jiang2023active} & 2023 &  & \Checkmark &  &  &  & \Checkmark &  & \Checkmark &  &  &  &  \\
			\hline
			Chameleon \cite{jiang2023chameleon} & 2023 & & \Checkmark & & \Checkmark & \Checkmark & & & \Checkmark & & & & \\
			\hline
			ERAGent \cite{shi2024eragent} & 2024 & \Checkmark & \Checkmark &&\Checkmark& &\Checkmark&&\Checkmark&&\Checkmark&\Checkmark&\Checkmark \\
			\hline
			PipeRAG \cite{jiang2024piperag} & 2024 & & \Checkmark & \Checkmark & \Checkmark & \Checkmark & & & & & & \Checkmark & \\
			\hline
			GenRT \cite{xu2024listaware} & 2024 & \Checkmark & & & \Checkmark & & & & & \Checkmark & \Checkmark & & \\
			\hline
			PersonaRAG \cite{zerhoudi2024personarag} & 2024 & \Checkmark & \Checkmark & \Checkmark & \Checkmark & & \Checkmark & & \Checkmark & \Checkmark & & \Checkmark & \Checkmark \\
			\hline
			CRAG \cite{yan2024corrective} & 2024 & \Checkmark & \Checkmark & & \Checkmark & & & \Checkmark & & \Checkmark & \Checkmark & \Checkmark & \\
			\hline
			IMRAG \cite{yang2024imrag} & 2024 & & \Checkmark & \Checkmark & \Checkmark & & \Checkmark & & & \Checkmark & & \Checkmark & \\
			\hline
			AiSAQ \cite{tatsuno2024aisaq} & 2024 & \Checkmark & & & & \Checkmark & & & \Checkmark & \Checkmark & & & \\
			\hline
			ROPG \cite{salemi2024optimization} & 2024 & \Checkmark & & & \Checkmark & & & & \Checkmark & & & \Checkmark & \Checkmark \\
			\hline
			RQ-RAG \cite{chan2024rqrag} & 2024 & & \Checkmark & \Checkmark & \Checkmark & & \Checkmark & & & \Checkmark & & & \\
			\hline
			PlanRAG \cite{lee2024planrag} & 2024 & & \Checkmark & \Checkmark & & & \Checkmark & & & \Checkmark & & \Checkmark & \\
			\hline
			RARG \cite{yue2024evidencedriven} & 2024 & & \Checkmark & & \Checkmark & & & \Checkmark & \Checkmark & & & \Checkmark & \\
			\hline 
			DRAGIN \cite{su2024dragin} & 2024 & & \Checkmark & \Checkmark & & & \Checkmark & & \Checkmark & & & \Checkmark & \\
			\hline
			LRUS-CoverTree \cite{ma2024reconsidering} & 2024 & \Checkmark & & & \Checkmark & \Checkmark & & & \Checkmark & \Checkmark & & & \\
			\hline
		\end{tabular}
	}
	\caption{The comprehensive summary of RAG studies. A \Checkmark in the ``Multi-hop'' column signifies that the research involves multiple search rounds. Similarly, a \Checkmark in the ``Training'' column indicates that the study included training phases. It is important to note that in this context, ``Training'' encompasses both initial model training and fine-tuning processes.}
	\label{tab:appendixb}
\end{table}