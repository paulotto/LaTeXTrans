\begin{figure}[t]
	\centering
	\tikzset{
		root/.style   = {align=center, fill=f_beige, text width=6cm, font=\Huge},
		xnode/.style = {align=center, fill=blue!15, text width=6cm, font=\Huge},
		tnode/.style = {align=center, fill=pink!50, text width=5cm, font=\huge},
		pnode/.style = {align=center, fill=pink!50, text width=5cm, font=\LARGE},
		edge from parent/.style={draw=black, very thick, edge from parent fork south},
	}
	\resizebox{\textwidth}{!}{%
		\begin{forest}
			for tree={
				%        	forked edges,
				grow=south,
				growth parent anchor=south,
				parent anchor=south,
				child anchor=north,
				draw,
				minimum height = 1.2cm,
				rounded corners=10pt,
				drop shadow,
				node options={align=center,},
				text width=3cm,
				l sep=10mm,
				s sep=3mm,
				edge={very thick, draw=black}, % Make lines thicker and black
				% Align nodes at the same level
				tier/.wrap pgfmath arg={tier #1}{level()},
				% Customize the edge path
				edge path={
					\noexpand\path [\forestoption{edge}, very thick]
					(!u.parent anchor) -- +(0,-15pt) -|   
					(.child anchor)\forestoption{edge label};
				},
			}
			[RAG, root, for tree={parent anchor=south}
			[Pre-Retrieval, xnode, for tree={parent anchor=south}
			[Indexing, tnode, for tree={parent anchor=south}
			[{REALM \cite{guu2020retrieval}; \\ kNN-LMs \cite{khandelwal2020generalization}; \\RAG \cite{lewis2020retrievalaugmented}; \\Webgpt \cite{nakano2021webgpt}; \\RETRO \cite{borgeaud2022improving}; \\MEMWALKER \cite{chen2023walking}; \\Atlas \cite{ma2023query}; \\ Chameleon \cite{jiang2023chameleon}; \\ AiSAQ \cite{tatsuno2024aisaq}; \\PipeRAG \cite{jiang2024piperag}; \\LRUS-CoverTree \cite{ma2024reconsidering}
			}, pnode]]
			[Query Manipulation, tnode, for tree={parent anchor=south}
			[{Webgpt \cite{nakano2021webgpt}; \\DSP \cite{khattab2022demonstratesearchpredict};\\ CoK \cite{li2024chainofknowledge};\\ IRCOT \cite{trivedi2023interleaving};\\ Query2doc \cite{wang2023querydoc};\\ Step-Back \cite{zheng2024take};\\ PROMPTAGATOR \cite{dai2023promptagator};\\ KnowledGPT \cite{wang2023knowledgpt}; \\Rewrite-Retrieve-Read \cite{ma2023query}; \\FLARE \cite{jiang2023active}; \\RQ-RAG \cite{chan2024rqrag}; \\RARG \cite{yue2024evidencedriven}; \\DRAGIN \cite{su2024dragin}
			}, pnode]]
			[Data Modification, tnode, for tree={parent anchor=south}
			[{RA-DIT \cite{lin2024radit}; \\RECITE \cite{sun2023recitationaugmented}; \\UPRISE \cite{cheng2023uprise}; \\GENREAD \cite{yu2023generate}; \\KnowledGPT \cite{wang2023knowledgpt}; \\Selfmem \cite{cheng2023lift}; \\RARG \cite{yue2024evidencedriven}
			}, pnode] ]
			]
			[Retrieval, xnode, for tree={parent anchor=south}
			[Search \& Ranking, tnode, for tree={parent anchor=south}
			[{REALM \cite{guu2020retrieval}; \\kNN-LMs \cite{khandelwal2020generalization}; \\RAG \cite{lewis2020retrievalaugmented}; \\FiD \cite{izacard2021leveraging}; \\Webgpt \cite{nakano2021webgpt}; \\RETRO \cite{borgeaud2022improving}; \\ITRG \cite{feng2024retrievalgeneration}; \\RA-DIT \cite{lin2024radit}; \\SURGE \cite{kang2023knowledge}; \\PRCA \cite{yang2023prca}; \\AAR \cite{yu2023augmentationadapted}; \\ITER-RETGEN \cite{shao2023enhancing}; \\UPRISE \cite{cheng2023uprise}; \\MEMWALKER \cite{chen2023walking}; \\Atlas \cite{ma2023query}; \\FLARE \cite{jiang2023active}; \\PlanRAG \cite{lee2024planrag}
			}, pnode] ]]
			[Post-Retrieval, xnode, for tree={parent anchor=south}
			[Re-Ranking, tnode, for tree={parent anchor=south}
			[{Re2G \cite{glass2022reg}; \\DSP \cite{khattab2022demonstratesearchpredict}; \\CoK \cite{li2024chainofknowledge}; \\FiD-TF \cite{berchansky2023optimizing}; \\ITER-RETGEN \cite{shao2023enhancing}; \\PROMPTAGATOR \cite{dai2023promptagator}; \\Selfmem \cite{cheng2023lift}; \\DKS-RAC \cite{huang2023retrieval}; \\In-Context RALM \cite{ram2023incontext}; \\Fid-light \cite{hofstätter2023fidlight}; \\GenRT \cite{xu2024listaware}
			}, pnode]]
			[Filtering, tnode, for tree={parent anchor=south}
			[{Webgpt \cite{nakano2021webgpt}; \\Self-RAG \cite{asai2024selfrag}; \\FiD-TF \cite{berchansky2023optimizing}; \\PROMPTAGATOR \cite{dai2023promptagator}; \\RECOMP \cite{xu2024recomp}; \\DKS-RAC \cite{huang2023retrieval}; \\CoK \cite{li2024chainofknowledge}; \\FILCO \cite{wang2023learning}; \\BlendFilter \cite{wang2024blendfilter}; \\CRAG \cite{yan2024corrective}
			}, pnode] ] ] 
			[Generation, xnode, for tree={parent anchor=south}
			[Enhancing, tnode, for tree={parent anchor=south}
			[{FiD \cite{izacard2021leveraging}; \\Webgpt \cite{nakano2021webgpt}; \\DSP \cite{khattab2022demonstratesearchpredict}; \\IRCOT \cite{trivedi2023interleaving}; \\ITRG \cite{feng2024retrievalgeneration}; \\RA-DIT \cite{lin2024radit}; \\PRCA \cite{yang2023prca}; \\RECITE \cite{sun2023recitationaugmented}; \\UPRISE \cite{cheng2023uprise}; \\GENREAD \cite{yu2023generate}; \\Selfmem \cite{cheng2023lift}; \\MEMWALKER \cite{chen2023walking}; \\Atlas \cite{ma2023query}
			}, pnode]]
			[Customization, tnode, for tree={parent anchor=south}
			[{LAPDOG \cite{huang2023learning}; \\PersonaRAG \cite{zerhoudi2024personarag}; \\ERAGent \cite{shi2024eragent}; \\ROPG \cite{salemi2024optimization}
			}, pnode] ]] 
			]
		\end{forest}
	}
	\caption{Taxonomy tree of RAG’s core techniques}
	\label{fig:rag tax}
	
\end{figure}

\subsection{Indexing}
One of the most commonly used indexing structures in traditional information retrieval systems is the inverted index. This structure associates documents with words to form a vocabulary list, allowing users to quickly locate references where a specific word appears within a collection of documents. The vocabulary list here refers to the set of all unique words present in the document collection, while the reference includes the documents where the word appears, along with the word's position and weight within those documents. However, traditional indexing structures struggle to retrieve documents that are semantically related to a user's query but do not contain the exact query terms.

To address this limitation, retrieval methods using dense vectors generated by deep learning models have become the preferred choice. These vectors, also known as embeddings, capture the semantic meaning of words and documents, allowing for more flexible and accurate retrieval. Dense vector-based indexing methods can be categorized into three main types: graphs, product quantization (PQ) \cite{jégou2011product}, and locality-sensitive hashing (LSH) \cite{datar2004localitysensitive}. Since generating dense vectors with large language models requires substantial resources, and the document collections to be searched are typically vast, the core strategy of these indexing methods is based on approximate nearest neighbor search (ANNS) \cite{DBLP:journals/jacm/AryaMNSW98}. This approach significantly speeds up the search process at the cost of a slight reduction in search accuracy. 

\paragraph{Graph} Using graphs to build indexes is a common practice in RAG. By indexing vectors with a graph structure, the range of nodes where distances need to be computed during retrieval can be limited to a local subgraph, thereby enhancing search speed. Several prominent methods and tools have been developed using this approach. For example, k-nearest neighbor language models kNN-LMs \cite{khandelwal2020generalization}, as demonstrated by Khandelwal et al., integrate the kNN algorithm with pre-trained language models. This method employs a datastore created from collections of texts to dynamically retrieve contextually relevant examples, enhancing model performance without requiring additional training. FAISS \cite{8733051}, a tool widely adopted for indexing in many studies \cite{khandelwal2020generalization, lewis2020retrievalaugmented, khattab2022demonstratesearchpredict}, integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation \cite{8594636} to further speed up retrieval \cite{lewis2020retrievalaugmented}. WebGPT \cite{nakano2021webgpt} showcases another practical application by utilizing the Bing API\footnote{https://www.microsoft.com/en-us/bing/apis/bing-web-search-api} for indexing based on actual user search histories, which illustrates the potential of integrating real-world user data into the retrieval process. Additionally, other methods like MEMWALKER \cite{chen2023walking} introduces innovative approaches to overcome limitations such as context window size in large language models. It creates a memory tree from input text, segmenting the text into smaller pieces and summarizing these segments into a hierarchical structure. Moreover, LRUS-CoverTree method \cite{ma2024reconsidering} designed another tree structure for k-Maximum Inner-Product Search (k-MIPS) and achieves performance comparable with significantly lower index construction time. These techniques facilitate efficient indexing and management of large information volumes, demonstrating the versatility and effectiveness of graph-based approaches.

\paragraph{Product Quantization} PQ is one of the most representative methods for handling large-scale data. It accelerates searches by segmenting vectors and then clustering each part for quantization. Unlike graph-based methods, which speed up searches by reducing the number of vectors for distance calculation, PQ achieves faster searches by reducing the time spent on calculating word distances. Several implementations of PQ have emerged in RAG, each improving its efficiency and scalability in different ways. PipeRAG \cite{jiang2024piperag} integrates PQ within a pipeline-parallelism framework to enhance retrieval-augmented generation by optimizing retrieval intervals. Chameleon system \cite{jiang2023chameleon} leverages PQ in a disaggregated accelerator environment to balance memory usage and retrieval speed in RAG tasks. AiSAQ \cite{tatsuno2024aisaq} introduces an all-in-storage ANNS method that offloads PQ vectors from DRAM to storage, drastically reducing memory usage while maintaining high recall. It demonstrates that even with billion-scale datasets, memory usage can be minimized to around 10 MB with only minor latency increases, making it a highly scalable solution for RAG systems. 

\paragraph{Locality-sensitive Hashing} The core idea of LSH is to place similar vectors into the same hash bucket with high probability. LSH uses hash functions that map similar vectors to the same or nearby hash values, making it easier to find approximate nearest neighbours. In LSH, when a query vector is hashed, the system quickly retrieves candidate vectors that share the same hash value. This method reduces the dimensionality of the problem and can be implemented efficiently, but it may introduce some inaccuracies due to the hashing process itself. While LSH is rarely used in RAG systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over the slight loss in accuracy.

\subsection{Query Manipulation}
Query manipulation is pivotal in enhancing the effectiveness and accuracy of modern IR systems. By refining users' original queries, it addresses challenges such as ambiguous phrasing and vocabulary mismatches between the query and target documents. This process involves more than merely replacing words with synonyms; it requires a deep understanding of user intent and the context of the query, particularly in complex tasks like RAG. Effective query manipulation significantly boosts retrieval performance, which in turn can greatly impact the quality of generated outputs. The three primary approaches to query manipulation are query expansion, query reformulation, and prompt-based rewriting.

\paragraph{Query Expansion} Query expansion involves augmenting the original query with additional terms or phrases that are related to or synonymous with the query terms. In the context of LLMs, query expansion can be more sophisticated, utilizing the model's extensive knowledge to generate contextually relevant expansions. This technique aims to improve recall by ensuring that the retrieval process captures a broader range of relevant documents, accommodating different terminologies or expressions. Techniques such as synonym expansion, semantic similarity, or leveraging external knowledge bases are commonly employed in query expansion. For example, the method described in FiD \cite{izacard2021leveraging} expands the query by retrieving a wider range of passages using both sparse and dense retrieval techniques, enabling the model to aggregate evidence from multiple sources and thereby improving the accuracy and robustness of generated answers. A more advanced form of query expansion is demonstrated in Query2doc \cite{wang2023querydoc}, where pseudo-documents generated by LLMs enhance the original query, effectively bridging the gap between the user's input and the corpus information, which benefits both sparse and dense retrieval systems. Similarly, KnowledGPT \cite{wang2023knowledgpt} broadens the scope of information accessed during retrieval by leveraging external knowledge bases, further refining the retrieval process. The RARG \cite{yue2024evidencedriven} framework uses an evidence-driven query expansion approach, incorporating a wide array of supporting documents to generate informed and accurate counter-misinformation responses.

\paragraph{Query Reformulation} Query reformulation involves rephrasing or restructuring the original query to enhance its effectiveness. This might include making the wording more specific, removing vague terms, or adjusting the syntax to better align with the retrieval system's requirements. With LLMs, query reformulation can be dynamically driven by understanding the user's intent and context, allowing for more precise modifications that lead to improved retrieval results. This reformulation process can also be informed by past queries or user interactions, adapting the query to better fit the specific retrieval task. For instance, the RQ-RAG \cite{chan2024rqrag} model represents an advanced form of query reformulation by rewriting, decomposing, and disambiguating queries, making it particularly effective in scenarios that demand complex query handling. This approach ensures that the refined query better matches the needed context, improving the relevance of retrieved information. Rewrite-Retrieve-Read framework \cite{ma2023query} adjusts the original query to optimize the retrieval process, allowing the system to more effectively leverage retrieved data for generating accurate responses. Additionally, FLARE \cite{jiang2023active} exemplifies query reformulation through its active retrieval-augmented generation approach, which iteratively refines the query based on a feedback loop between retrieval and generation, thereby enhancing the accuracy and relevance of the retrieved information.

\paragraph{Prompt-based Rewriting} Prompt-based rewriting, particularly in the context of LLMs, represents an innovative approach where the original query is embedded within a larger prompt or context to guide the LLM's response. This technique harnesses the model's ability to understand and generate language within a specific context, effectively rewriting the query to align with the desired output. Prompt-based rewriting is especially powerful in scenarios where the retrieval process is integrated into a generative workflow, allowing the system to adapt the query to various stages of retrieval and generation. This approach may also involve dynamic prompts that evolve based on interaction, further refining the retrieval process. For example, Step-Back \cite{zheng2024take} refines the query context through carefully crafted prompts that guide the LLM's reasoning process, ensuring that the outputs are more aligned with the user's intent, particularly in complex reasoning tasks. The CoK \cite{li2024chainofknowledge} method focuses on dynamically adapting the knowledge source and using prompts to rewrite the context in which a query is interpreted. This approach leverages prompt-based rewriting to enable the LLM to effectively integrate and ground its responses based on various heterogeneous knowledge sources. Additionally, Promptagator \cite{dai2023promptagator} discusses using prompt-based techniques to adapt and rewrite the query to better align with the retrieval system's expectations, particularly in few-shot learning scenarios. These prompts guide the model in generating or refining the query to optimize retrieval results.

\subsection{Data Modification}
Document modification techniques play a critical role in enhancing retrieval performance, particularly when integrated with LLMs. These techniques can be broadly categorized into Internal Data Augmentation and External Data Enrichment. Internal Data Augmentation focuses on maximizing the value of existing information within documents or models, while External Data Enrichment introduces supplementary data from outside sources to fill gaps, provide additional context, or broaden the scope of the content.

\paragraph{Internal Data Augmentation}
Internal Data Augmentation leverages information already present within documents or taps into the inherent knowledge embedded in LLMs. Techniques like paraphrasing, where content is rewritten for improved readability or multiple perspectives, and summarization, which condenses information while retaining core content, are commonly employed. Other methods involve generating supplementary content or explanations that are contextually related without introducing external data. For instance, RECITE \cite{sun2023recitationaugmented} utilizes a model’s internal memory to recite relevant information before generating responses, thus enhancing performance in tasks like closed-book question answering without external data. KnowledGPT \cite{wang2023knowledgpt} similarly refines the internal knowledge embedded within LLMs, optimizing its use during generation. GENREAD \cite{yu2023generate} further demonstrates how pre-existing knowledge within LLMs can be used to generate context that enhances task performance, bypassing the need for external sources. In another example, the Selfmem \cite{cheng2023lift} framework allows the model to iteratively use its own outputs as memory in subsequent generation tasks. By selecting and utilizing the best internal outputs as memory, this approach boosts model performance without depending on external memory resources.

\paragraph{External Data Enrichment}
External Data Enrichment enhances document content by incorporating new information from external sources, enriching the overall context and accuracy. This process can involve integrating facts, data, or contextual knowledge from external datasets or knowledge bases. For example, RA-DIT \cite{lin2024radit} augments input prompts during fine-tuning by leveraging large datasets like Wikipedia and CommonCrawl, enhancing the model’s capability in knowledge-intensive tasks. The dual instruction tuning technique optimizes both the language model and the retriever to more effectively incorporate retrieved information. UPRISE \cite{cheng2023uprise} demonstrates how retrieving prompts from diverse task datasets improves model generalization in zero-shot scenarios by enriching the context during inference. Additionally, RARG \cite{yue2024evidencedriven} exemplifies external data enrichment by integrating scientific evidence from academic databases to strengthen responses countering misinformation. This method involves a two-stage retrieval pipeline that identifies and ranks relevant documents, which are then used to support and enhance the factual accuracy of generated responses.
