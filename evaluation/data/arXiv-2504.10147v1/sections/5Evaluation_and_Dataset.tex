\section{Evaluation and Dataset}\label{sec:evaluation&dataset}
\input{sections/table_dataset}
In the evolving landscape of personalization, from RAG to advanced Agent-based systems, the evaluation of models relies heavily on diverse datasets and metrics tailored to specific tasks. This survey categorizes metrics into several key types: Textual Quality metrics (\eg BLEU, ROUGE, METEOR) assess the fluency and coherence of generated outputs; Information Retrieval metrics (\eg MAP, MRR, Recall) evaluate the accuracy and relevance of retrieved information; Classification metrics (\eg Accuracy, F1) measure task-specific correctness; Regression metrics (\eg MAE, RMSE) quantify prediction errors; and Other metrics (\eg Fluency, Pass@k) address domain-specific or task-unique aspects like plausibility or executability. These metrics span pre-retrieval, retrieval, generation, and agent-based personalization approaches, reflecting their varied objectives. To provide a comprehensive overview, we compile an extensive list of datasets across these fields, as detailed in Table~\ref{tab:datasets}. These datasets, paired with their respective metrics, enable researchers to benchmark and refine personalized systems, from enhancing query rewriting to enabling autonomous agents in physical and virtual environments.