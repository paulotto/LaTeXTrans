\vspace{-5px}\section{Background}\label{sect:background}\vspace{-5px}

% \subsection{Parallel Inference Strategies}\label{sect:background_parallel_infernece}

Recent works propose a large number of frameworks for parallel reasoning and tool use that vary across several axes: how the parallel instances are organized together, what they exchange, and how often \citep{TTSsurvey}. In this section, we give a brief summary of these methods.

\paragraph{Discussion \& aggregation.} The simplest way to parallelize chain-of-thought reasoning is Self-Consistency~\citep{Wang2022SelfConsistencyIC}, where multiple LLM instances reason independently, then vote on the final solution. This approach was later extended in~\cite{du2024improving}, who replace independent voting with text-based communication rounds. Subsequent works in this field combine multiple LLM types~\citep{wang2024mixture} and scales to more agents~\cite{li2024moreagents}. Another line of work introduces specialized ``roles'' such as the Debugger~\citep{talebirad2023multiagent}, Examiner~\citep{cohen-etal-2023-lm}, Math Teacher~\citep{kong-etal-2024-better}, Judge~\citep{chen-etal-2024-reconcile}, and others, to further augment reasoning.

This type of role-based discussion was shown to greatly improve LLM reasoning factuality for certain tasks~\citep{Wang2022SelfConsistencyIC,du2024improving}, and can even enable multiple weaker LLM agents to collectively outperform state-of-the-art single-agent systems~\citep{wang2024mixture}. However, this improvement is not unique to multiple agents and can be offset with better single-agent prompting~\citep{wang-etal-2024-rethinking-bounds,muennighoff2025s1}. Additionally, these approaches do not necessarily accelerate reasoning,  because at least some of the agents have to solve the entire problem sequentially, and process (re-encode) each other's progress. This creates additional computational overhead, which presents challenges for both runtime and memory efficiency~\cite{wang2024mixture,du2024improving}.

\paragraph{Parallelism for efficiency.} A different line of work leverages multiple LLMs to solve tasks faster in parallel, such as Skeleton-of-Thought (SoT)~\citep{ning2024skeletonofthought}. SoT begins by running a single LLM to outline a plan for solving the problem with independent sub-tasks, then launches parallel LLM instances for each sub-task. Subsequent works propose more complex parallelism strategies such as dynamic parallel tree search~\citep{ding2025dynamicparalleltreesearch} or a single agent spawning asynchronous sub-tasks that are done by background LLM ``threads''~\citep{jin2025learningpromisescalinglanguage}. For problems that involve function calling, these functions can also run in parallel~\citep{kim2024llm,gim2024asynchronousllmfunctioncalling}.

These techniques are known to substantially accelerate inference for problems that fit their type of parallelism. However, we argue that this is also their main limitation: by imposing a specific parallelism strategy, these methods can harm reasoning for problems that do not fit their framework. For instance, when solving a complex reasoning problem, it is often the case that the initial plan turns out to be wrong or incomplete~\citep{muennighoff2025s1,deepseek_r1}, which conflicts with SoT-like methods~\citep{ning2024skeletonofthought,yu2025accelerateparallelizablereasoningparallel} that follow a fixed plan-execute-aggregate schedule. Furthermore, some of the sub-tasks may turn out to be more complicated than originally intended and take up more work, which would cause methods like PASTA~\cite{jin2025learningpromisescalinglanguage} to wait for that single task, whereas a more sophisticated reasoner could adjust the plan to work better in parallel. Note that each individual issue can be amended with a yet another, more complicated parallelism framework, but the sheer number of such cases makes us doubt if this is the right approach. In this work, we propose to allow multiple LLM instances to see each other's progress immediately and interact explicitly or implicitly to devise their own collaboration strategy and adjust it as necessary. We show that, perhaps surprisingly, existing reasoning LLMs already have the ability to leverage this. 

