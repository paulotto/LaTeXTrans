@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@article{EntinSerfaty1999,
  author  = {Entin, Elliot E. and Serfaty, Daniel},
  title   = {Adaptive Team Coordination},
  journal = {Human Factors},
  volume  = {41},
  number  = {2},
  pages   = {312--325},
  year    = {1999}
}

@book{Hutchins1995,
  author    = {Hutchins, Edwin},
  title     = {Cognition in the Wild},
  year      = {1995},
  publisher = {MIT Press}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}


@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@article{malinovskii2024pv,
  title={PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression},
  author={Malinovskii, Vladimir and Mazur, Denis and Ilin, Ivan and Kuznedelev, Denis and Burlachenko, Konstantin and Yi, Kai and Alistarh, Dan and Richtarik, Peter},
  journal={arXiv preprint arXiv:2405.14852},
  year={2024}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{he2024zipcache,
title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
author={He, Yiming and Zhang, Lutao and Wu, Wei and Liu, Jianqiao and Zhou, Hong and Zhuang, Bohan},
journal={arXiv preprint arXiv:2405.14256},
year={2024}
}
@article{yue2024wkvquant,
title={WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More},
author={Yue, Yixin and Yuan, Zongxiang and Duanmu, Haotian and Zhou, Shiyi and Wu, Jian and Nie, Liqiang},
journal={arXiv preprint arXiv:2402.12065},
year={2024}
}
@article{hooper2024kvquant,
title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
author={Hooper, Connor and Kim, Sehoon and Mohammadzadeh, Hamed and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
journal={arXiv preprint arXiv:2401.18079},
year={2024}
}
@article{liu2024kivi,
title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
author={Liu, Zechun and Yuan, Jiayi and Jin, Hao and Zhong, Shiji and Xu, Zhiquan and Braverman, Vladimir and Chen, Bin and Hu, Xiaolong},
journal={arXiv preprint arXiv:2402.02750},
year={2024}
}
@article{zandieh2024qjl,
title={QJL: 1-bit Quantized JL Transform for KV Cache Quantization with Zero Overhead},
author={Zandieh, Alireza and Daliri, Mohammad and Han, Isabella},
journal={arXiv preprint arXiv:2406.03482},
year={2024}
}
@misc{openai_arc_prize_o3,
  title = {OpenAI's New o3 System Scores Breakthrough on ARC-AGI-Pub},
  author = {{ARC Prize Foundation}},
  year = {2024},
  url = {https://arcprize.org/blog/oai-o3-pub-breakthrough},
  note = {Accessed: 2025.03.28}
}

@misc{AnthropicClaude3.7Sonnet,
  title = {Claude 3.7 Sonnet and Claude Code},
  author = {Anthropic},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-7-sonnet},
  note = {Accessed: 2025.04.02}
}
@article{Bai2024LongWriterU1,
  title={LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs},
  author={Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.07055},
  url={https://api.semanticscholar.org/CorpusID:271859903}
}
@article{yang2024mikv,
title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
author={Yang, Jun Young and Kim, Byungseok and Bae, Jonghyun and Kwon, Byungju and Park, Gyuwan and Yang, Eunho and Kwon, Soon Ju and Lee, Dongjoo},
journal={arXiv preprint arXiv:2402.18096},
year={2024}
}
@article{dong2024qaq,
title={QAQ: Quality-Adaptive Quantization for LLM KV Cache},
author={Dong, Shuai and Cheng, Wei and Qin, Jian and Wang, Wenjie},
journal={arXiv preprint arXiv:2403.04643},
year={2024}
}
@article{duanmu2024skvq,
title={SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models},
author={Duanmu, Haotian and Yuan, Zongxiang and Li, Xianan and Duan, Jianxin and Zhang, Xianming and Lin, Di},
journal={arXiv preprint arXiv:2405.06219},
year={2024}
}
@article{kang2024gear,
title={GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM},
author={Kang, Hyojin and Zhang, Qiang and Kundu, Sourav and Jeong, Gyeongchan and Liu, Zechun and Krishna, Tushar and Zhao, Tianyi},
journal={arXiv preprint arXiv:2403.05527},
year={2024}
}
@article{liu2024intactkv,
title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
author={Liu, Rui and Bai, Haihao and Lin, Hongyu and Li, Yifu and Gao, Haoyuan and Xu, Zhen and Hou, Le and Yao, Jiacheng and Yuan, Ce},
journal={arXiv preprint arXiv:2403.01241},
year={2024}
}
@article{ashkboos2024quarot,
title={QuaRot: Outlier-free 4-bit Inference in Rotated LLMs},
author={Ashkboos, Saleh and Mohtashami, Amir and Croci, Matteo Lukas and Li, Bojian and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
journal={arXiv preprint arXiv:2404.00456},
year={2024}
}
@article{chang2024palu,
title={Palu: Compressing KV-Cache with Low-Rank Projection},
author={Chang, Che-Chun and Lin, Wei-Chen and Lin, Chi-Yuan and Chen, Cheng-Yi and Hu, Yi-Fang and Wang, Po-Sen and Huang, Nien-Chih and Ceze, Luis and Wu, Kuan-Chieh},
journal={arXiv preprint arXiv:2407.21118},
year={2024}
}
@article{zhang2024zdc,
title={Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference},
author={Zhang, Zhewei and Shen, Haoran},
journal={arXiv preprint arXiv:2408.04107},
year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{gray1992vector,
  title={Vector quantization},
  author={Gray, Robert M and Neuhoff, David L},
  journal={IEEE Transactions on Information Theory},
  volume={44},
  number={6},
  pages={2325--2383},
  year={1998},
  publisher={IEEE}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@book{reinsel1998multivariate,
  title={Multivariate Reduced-Rank Regression},
  author={Reinsel, Gregory C. and Velu, R. Dennis},
  year={1998},
  publisher={Springer},
  address={New York},
  isbn={978-1-4757-2853-8},
  doi={10.1007/978-1-4757-2853-8},
  url={https://link.springer.com/book/10.1007/978-1-4757-2853-8}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{li2024survey,
  title={A Survey on Large Language Model Acceleration based on KV Cache Management},
  author={Li, Haoyang and Li, Yiming and Tian, Anxin and Tang, Tianhao and Xu, Zhanchao and Chen, Xuejia and Hu, Nicole and Dong, Wei and Li, Qing and Chen, Lei},
  journal={arXiv preprint arXiv:2412.19442},
  year={2024}
}

@article{li2024scbench,
  title={Scbench: A kv cache-centric analysis of long-context methods},
  author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
  journal={arXiv preprint arXiv:2412.10319},
  year={2024}
}

@article{malinovskii2024pushing,
  title={Pushing the Limits of Large Language Model Quantization via the Linearity Theorem},
  author={Malinovskii, Vladimir and Panferov, Andrei and Ilin, Ivan and Guo, Han and Richt{\'a}rik, Peter and Alistarh, Dan},
  journal={arXiv preprint arXiv:2411.17525},
  year={2024}
}
@inproceedings{
tseng2024qtip,
title={{QTIP}: Quantization with Trellises and Incoherence Processing},
author={Albert Tseng and Qingyao Sun and David Hou and Christopher De Sa},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=7sdkLVuYCU}
}

@article{tseng2024quip,
  title={Quip\#: Even better LLM quantization with hadamard incoherence and lattice codebooks},
  author={Tseng, Albert and Chee, Jerry and Sun, Qingyao and Kuleshov, Volodymyr and De Sa, Christopher},
  journal={arXiv preprint arXiv:2402.04396},
  year={2024}
}


@article{yang2024kvsharer,
  title={Kvsharer: Efficient inference via layer-wise dissimilar KV cache sharing},
  author={Yang, Yifei and Cao, Zouying and Chen, Qiguang and Qin, Libo and Yang, Dongjie and Zhao, Hai and Chen, Zhi},
  journal={arXiv preprint arXiv:2410.18517},
  year={2024}
}

@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{xu2024think,
  title={Think: Thinner key cache by query-driven pruning},
  author={Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2407.21018},
  year={2024}
}


@InProceedings{Alistarh-EF2018,
  author    = {Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Khirirat, Sarit and Konstantinov, Nikola and Renggli, C\'{e}dric},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {The convergence of sparsified gradient methods},
  year      = {2018},
}

@Article{biased2020,
  author  = {Aleksandr Beznosikov and Samuel Horv\'{a}th and Peter Richt\'{a}rik and Mher Safaryan},
  title   = {On Biased Compression for Distributed Learning},
  journal = {arXiv:2002.12410},
  year    = {2020},
}

@Article{PCDM,
  author  = {Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
  journal = {Mathematical Programming},
  title   = {Parallel coordinate descent methods for big data optimization},
  year    = {2016},
  number  = {1-2},
  pages   = {433--484},
  volume  = {156},
}

@Article{SSD:Kozak2019,
  author  = {Kozak, David and Becker, Stephen and Doostan, Alireza and Tenorio, Luis},
  title   = {Stochastic Subspace Descent},
  journal = {arXiv preprint arXiv:1904.01145},
  year    = {2019},
}

@Article{SDA,
  author  = {Gower, Robert Mansel and Richt\'{a}rik, Peter},
  journal = {arXiv:1512.06890},
  title   = {Stochastic dual ascent for solving linear systems},
  year    = {2015},
}

@article{luo1992convergence,
  title={On the convergence of the coordinate descent method for convex differentiable minimization},
  author={Luo, Zhi-Quan and Tseng, Paul},
  journal={Journal of Optimization Theory and Applications},
  volume={72},
  number={1},
  pages={7--35},
  year={1992},
  publisher={Springer}
}


@inproceedings{HalpernFixedPoint,
title={Exact Optimal Accelerated Complexity for Fixed-Point Iterations},
author = {Jisun Park and Ernest K. Ryu},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
year = {2022}
}

@article{dettmers2023spqr,
  title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}

@article{dettmers2023qlora,
  title={{QLoRA}: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{Qian1999OnTM,
  title={On the momentum term in gradient descent learning algorithms},
  author={N. Qian},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={1999},
  volume={12 1},
  pages={
          145-151
        }
}
% https://huggingface.co/tiiuae/falcon-40b
@misc{yao2023comprehensive,
      title={A Comprehensive Study on Post-Training Quantization for Large Language Models}, 
      author={Zhewei Yao and Cheng Li and Xiaoxia Wu and Stephen Youn and Yuxiong He},
      year={2023},
      eprint={2303.08302},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}



@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  year={2012}
}

@article{dettmers20168bit,
  title={8-bit approximations for parallelism in deep learning},
  author={Dettmers, Tim},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{seetharaman2020autoclip,
  title={AutoClip: Adaptive gradient clipping for source separation networks},
  author={Seetharaman, Prem and Wichern, Gordon and Pardo, Bryan and Le Roux, Jonathan},
  booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{brock2021high,
  title={High-Performance Large-Scale Image Recognition Without Normalization},
  author={Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen},
  journal={arXiv preprint arXiv:2102.06171},
  year={2021}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@misc{nagel2016cc,
  title={Cc-news},
  author={Nagel, Sebastian},
  year={2016}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{gokaslan2019openwebtext,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron and Cohen, Vanya},
  journal={urlhttp://Skylion007. github. io/OpenWebTextCorpus},
  year={2019}
}



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{ott2018scaling,
  title={Scaling neural machine translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1806.00187},
  year={2018}
}

@article{lewis2021base,
  title={BASE Layers: Simplifying Training of Large, Sparse Models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2103.16716},
  year={2021}
}

@article{zhang2022opt,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{chen2020statistical,
  title={A statistical framework for low-bitwidth training of deep neural networks},
  author={Chen, Jianfei and Gai, Yu and Yao, Zhewei and Mahoney, Michael W and Gonzalez, Joseph E},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={883--894},
  year={2020}
}

@article{lin2020towards,
  title={Towards fully 8-bit integer inference for the transformer model},
  author={Lin, Ye and Li, Yanyang and Liu, Tengbo and Xiao, Tong and Liu, Tongran and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2009.08034},
  year={2020}
}

@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}

@inproceedings{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  booktitle={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)},
  pages={36--39},
  year={2019},
  organization={IEEE}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{dettmers2022optimizers,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={9th International Conference on Learning Representations, ICLR},
  year={2022}
}

@article{artetxe2021efficient,
  title={Efficient Large Scale Language Modeling with Mixtures of Experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}



@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}

@article{PQ,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@article{jin2022f8net,
  title={F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization},
  author={Jin, Qing and Ren, Jian and Zhuang, Richard and Hanumante, Sumant and Li, Zhengang and Chen, Zhiyu and Wang, Yanzhi and Yang, Kaiyuan and Tulyakov, Sergey},
  journal={arXiv preprint arXiv:2202.05239},
  year={2022}
}

@inproceedings{Zhang2020TernaryBERTDU,
  title={TernaryBERT: Distillation-aware Ultra-low Bit BERT},
  author={Wei Zhang and Lu Hou and Yichun Yin and Lifeng Shang and Xiao Chen and Xin Jiang and Qun Liu},
  booktitle={EMNLP},
  year={2020}
}


@article{Bai2021BinaryBERTPT,
  title={BinaryBERT: Pushing the Limit of BERT Quantization},
  author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael R. Lyu and Irwin King},
  journal={ArXiv},
  year={2021},
  volume={abs/2012.15701}
}


@inproceedings{Rastegari2016xnor,
  author    = {Mohammad Rastegari and
               Vicente Ordonez and
               Joseph Redmon and
               Ali Farhadi},
  editor    = {Bastian Leibe and
               Jiri Matas and
               Nicu Sebe and
               Max Welling},
  title     = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural
               Networks},
  booktitle = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
               The Netherlands, October 11-14, 2016, Proceedings, Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {9908},
  pages     = {525--542},
  publisher = {Springer},
  year      = {2016},
  url       = {https://doi.org/10.1007/978-3-319-46493-0\_32},
  doi       = {10.1007/978-3-319-46493-0\_32},
  timestamp = {Wed, 25 Sep 2019 18:11:12 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/RastegariORF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and Léonard, Nicolas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}


@inproceedings{courbariaux2015binaryconnect,
  author    = {Matthieu Courbariaux and
               Yoshua Bengio and
               Jean{-}Pierre David},
  editor    = {Corinna Cortes and
               Neil D. Lawrence and
               Daniel D. Lee and
               Masashi Sugiyama and
               Roman Garnett},
  title     = {BinaryConnect: Training Deep Neural Networks with binary weights during
               propagations},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada},
  pages     = {3123--3131},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/CourbariauxBD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{courbariaux2016bit1,
  author    = {Matthieu Courbariaux and
               Yoshua Bengio},
  title     = {BinaryNet: Training Deep Neural Networks with Weights and Activations
               Constrained to +1 or -1},
  journal   = {CoRR},
  volume    = {abs/1602.02830},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02830},
  eprinttype = {arXiv},
  eprint    = {1602.02830},
  timestamp = {Mon, 13 Aug 2018 16:46:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CourbariauxB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{wang2018training8bit,
  author    = {Naigang Wang and
               Jungwook Choi and
               Daniel Brand and
               Chia{-}Yu Chen and
               Kailash Gopalakrishnan},
  editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {7686--7695},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/335d3d1cd7ef05ec77714a215134914c-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/WangCBCG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2019hybrid8bit,
  author    = {Xiao Sun and
               Jungwook Choi and
               Chia{-}Yu Chen and
               Naigang Wang and
               Swagath Venkataramani and
               Vijayalakshmi Srinivasan and
               Xiaodong Cui and
               Wei Zhang and
               Kailash Gopalakrishnan},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Hybrid 8-bit Floating Point {(HFP8)} Training and Inference for Deep
               Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {4901--4910},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/SunCCWVSCZG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@inproceedings{cambier2020shiftsqueeze,
  author    = {L{\'{e}}opold Cambier and
               Anahita Bhiwandiwalla and
               Ting Gong and
               Oguz H. Elibol and
               Mehran Nekuii and
               Hanlin Tang},
  title     = {Shifted and Squeezed 8-bit Floating Point format for Low-Precision
               Training of Deep Neural Networks},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=Bkxe2AVtPS},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/CambierBGENT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ren2021zerooffload,
      title={ZeRO-Offload: Democratizing Billion-Scale Model Training}, 
      author={Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
      year={2021},
      eprint={2101.06840},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{yao2023deepspeedchat,
      title={DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}, 
      author={Zhewei Yao and Reza Yazdani Aminabadi and Olatunji Ruwase and Samyam Rajbhandari and Xiaoxia Wu and Ammar Ahmad Awan and Jeff Rasley and Minjia Zhang and Conglong Li and Connor Holmes and Zhongzhu Zhou and Michael Wyatt and Molly Smith and Lev Kurilenko and Heyang Qin and Masahiro Tanaka and Shuai Che and Shuaiwen Leon Song and Yuxiong He},
      year={2023},
      eprint={2308.01320},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{griewank2000algorithm,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{Widrow199030YO,
  title={30 years of adaptive neural networks: perceptron, Madaline, and backpropagation},
  author={Bernard Widrow and Michael A. Lehr},
  journal={Proc. IEEE},
  year={1990},
  volume={78},
  pages={1415-1442},
  url={https://api.semanticscholar.org/CorpusID:195704643}
}

@Techreport{Rosenblatt_1957_6098,

  author = {Rosenblatt, F.},

 address = {Ithaca, New York},

 institution = {Cornell Aeronautical Laboratory},

 month = {January},

 number = {85-460-1},

 title = {The perceptron - A perceiving and recognizing automaton},

 year = {1957},

 title_with_no_special_chars = {The Perceptron  A perceiving and recognizing automaton}

}

@article{rvq,
  title={Approximate nearest neighbor search by residual vector quantization},
  author={Chen, Yongjian and Guan, Tao and Wang, Cheng},
  journal={Sensors},
  volume={10},
  number={12},
  pages={11259--11273},
  year={2010},
  publisher={Molecular Diversity Preservation International (MDPI)}
}

@misc{hinton_coursera,
  title={ Neural networks for machine learning, coursera (Video Lectures).},
  author={Hinton, Geoffrey},
  year={2012}
}
@inproceedings{freund1998large,
  title={Large margin classification using the perceptron algorithm},
  author={Freund, Yoav and Schapire, Robert E},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={209--217},
  year={1998}
}


@inproceedings{NIPS2017_7a98af17,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{benallal2024cosmopedia,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {Cosmopedia},
  month = February,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/cosmopedia}
}


@misc{tseng2024quipsharp,
      title={QuIP\#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks}, 
      author={Albert Tseng and Jerry Chee and Qingyao Sun and Volodymyr Kuleshov and Christopher De Sa},
      year={2024},
      eprint={2402.04396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{deepspeed,
author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
title = {ZeRO: memory optimizations toward training trillion parameter models},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {20},
numpages = {16},
location = {Atlanta, Georgia},
series = {SC '20}
}

@misc{ma2024era,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{huang2024billm,
  title={BiLLM: Pushing the Limit of Post-Training Quantization for LLMs},
  author={Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2402.04291},
  year={2024}
}

@misc{shang2023pbllm,
      title={PB-LLM: Partially Binarized Large Language Models}, 
      author={Yuzhang Shang and Zhihang Yuan and Qiang Wu and Zhen Dong},
      year={2023},
      eprint={2310.00034},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2024good,
      title={How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study}, 
      author={Wei Huang and Xudong Ma and Haotong Qin and Xingyu Zheng and Chengtao Lv and Hong Chen and Jie Luo and Xiaojuan Qi and Xianglong Liu and Michele Magno},
      year={2024},
      eprint={2404.14047},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2024dbllm,
      title={DB-LLM: Accurate Dual-Binarization for Efficient LLMs}, 
      author={Hong Chen and Chengtao Lv and Liang Ding and Haotong Qin and Xiabin Zhou and Yifu Ding and Xuebo Liu and Min Zhang and Jinyang Guo and Xianglong Liu and Dacheng Tao},
      year={2024},
      eprint={2402.11960},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{egiazarian2024extreme,
  title={Extreme Compression of Large Language Models via Additive Quantization},
  author={Egiazarian, Vage and Panferov, Andrei and Kuznedelev, Denis and Frantar, Elias and Babenko, Artem and Alistarh, Dan},
  journal={arXiv preprint arXiv:2401.06118},
  year={2024}
}

@inproceedings{flute2024,
  title={Fast Matrix Multiplications for Lookup Table-Quantized LLMs},
  author={Guo, Han and Brandon, William and Cholakov, Radostin and Ragan-Kelley, Jonathan and Xing, Eric and Kim, Yoon},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={12419--12433},
  year={2024}
}

@article{shao2023omniquant,
  title={Omniquant: Omnidirectionally calibrated quantization for large language models},
  author={Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2308.13137},
  year={2023}
}

@article{wang2023bitnet,
  title={Bitnet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@article{van2024gptvq,
  title={GPTVQ: The Blessing of Dimensionality for LLM Quantization},
  author={van Baalen, Mart and Kuzmin, Andrey and Nagel, Markus and Couperus, Peter and Bastoul, Cedric and Mahurin, Eric and Blankevoort, Tijmen and Whatmough, Paul},
  journal={arXiv preprint arXiv:2402.15319},
  year={2024}
}

@inproceedings{aq,
  title={Additive quantization for extreme vector compression},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={931--938},
  year={2014}
}


@INPROCEEDINGS{vq1,

  author={Burton, D. and Shore, J. and Buck, J.},

  booktitle={ICASSP '83. IEEE International Conference on Acoustics, Speech, and Signal Processing}, 

  title={A generalization of isolated word recognition using vector quantization}, 

  year={1983},

  volume={8},

  number={},

  pages={1021-1024},

  doi={10.1109/ICASSP.1983.1171915}}


@ARTICLE{vq2,

  author={Gray, R.},

  journal={IEEE ASSP Magazine}, 

  title={Vector quantization}, 

  year={1984},

  volume={1},

  number={2},

  pages={4-29},

  doi={10.1109/MASSP.1984.1162229}}


@inproceedings{lsq,
  title={Revisiting additive quantization},
  author={Martinez, Julieta and Clement, Joris and Hoos, Holger H and Little, James J},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={137--153},
  year={2016},
  organization={Springer}
}
@inproceedings{lsq++,
  title={LSQ++: Lower running time and higher recall in multi-codebook quantization},
  author={Martinez, Julieta and Zakhmi, Shobhit and Hoos, Holger H and Little, James J},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={491--506},
  year={2018}
}

@article{besag1986statistical,
  title={On the statistical analysis of dirty pictures},
  author={Besag, Julian},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={48},
  number={3},
  pages={259--279},
  year={1986},
  publisher={Oxford University Press}
}


@article{together2023redpajama,
	title   = {RedPajama: an Open Dataset for Training Large Language Models},
	author  = {Maurice Weber and Daniel Y. Fu and Quentin Anthony and Yonatan Oren and Shane Adams and Anton Alexandrov and Xiaozhong Lyu and Huu Nguyen and Xiaozhe Yao and Virginia Adams and Ben Athiwaratkun and Rahul Chalamala and Kezhen Chen and Max Ryabinin and Tri Dao and Percy Liang and Christopher Ré and Irina Rish and Ce Zhang},
	journal = {NeurIPS Datasets and Benchmarks Track},
	year    = 2024,
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@inproceedings{maddness2021,
  title={Multiplying matrices without multiplying},
  author={Blalock, Davis and Guttag, John},
  booktitle={International Conference on Machine Learning},
  pages={992--1004},
  year={2021},
  organization={PMLR}
}

@article{McCarter2022LookupsAN,
  title={Look-ups are not (yet) all you need for deep learning inference},
  author={Calvin McCarter and Nicholas Dronen},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.05808},
  url={https://api.semanticscholar.org/CorpusID:250491319}
}

@article{FernndezMarqus2023AreWT,
  title={Are We There Yet? Product Quantization and its Hardware Acceleration},
  author={Javier Fern{\'a}ndez-Marqu{\'e}s and Ahmed F. AbouElhamayed and Nicholas Donald Lane and Mohamed Saleh Abdelfattah},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18334},
  url={https://api.semanticscholar.org/CorpusID:258967539}
}

@inproceedings{norouzi13,
  title={Cartesian k-means},
  author={Norouzi, Mohammad and Fleet, David J},
  booktitle={Proceedings of the IEEE Conference on computer Vision and Pattern Recognition},
  pages={3017--3024},
  year={2013}
}

@ARTICLE{competitveq,

  author={Ozan, Ezgi Can and Kiranyaz, Serkan and Gabbouj, Moncef},

  journal={IEEE Transactions on Knowledge and Data Engineering}, 

  title={Competitive Quantization for Approximate Nearest Neighbor Search}, 

  year={2016},

  volume={28},

  number={11},

  pages={2884-2894},

  doi={10.1109/TKDE.2016.2597834}}



@inproceedings{compositeq,
  title={Composite quantization for approximate nearest neighbor search},
  author={Zhang, Ting and Du, Chao and Wang, Jingdong},
  booktitle={International Conference on Machine Learning},
  pages={838--846},
  year={2014},
  organization={PMLR}
}




@article{opq,
  title={Optimized product quantization},
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={36},
  number={4},
  pages={744--755},
  year={2013},
  publisher={IEEE}
}




@inproceedings{drumond2018hybridblock,
  author    = {Mario Drumond and
               Tao Lin and
               Martin Jaggi and
               Babak Falsafi},
  editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Training DNNs with Hybrid Block Floating Point},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {451--461},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html},
  timestamp = {Tue, 01 Jun 2021 10:12:08 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/DrumondLJF18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{arc_allenai,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{vig2019multiscale,
  title={A multiscale visualization of attention in the transformer model},
  author={Vig, Jesse},
  journal={arXiv preprint arXiv:1906.05714},
  year={2019}
}

@inproceedings{zhu2017ternary,
  author    = {Chenzhuo Zhu and
               Song Han and
               Huizi Mao and
               William J. Dally},
  title     = {Trained Ternary Quantization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=S1\_pAu9xl},
  timestamp = {Fri, 20 Nov 2020 16:16:07 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhuHMD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{li2019bit4,
  author    = {Rundong Li and
               Yan Wang and
               Feng Liang and
               Hongwei Qin and
               Junjie Yan and
               Rui Fan},
  title     = {Fully Quantized Network for Object Detection},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2019, Long Beach, CA, USA, June 16-20, 2019},
  pages     = {2810--2819},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Li\_Fully\_Quantized\_Network\_for\_Object\_Detection\_CVPR\_2019\_paper.html},
  doi       = {10.1109/CVPR.2019.00292},
  timestamp = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/LiWLQYF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}

@misc{bitsandbytes, 
  title={Accessible large language models via k-bit quantization for PyTorch.},
  author={Dettmers, Tim and von Koeller, Titus},
  url={https://github.com/TimDettmers/bitsandbytes},
}



@article{fbgemm,
  title={FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference},
  author={Khudia, Daya and Huang, Jianyu and Basu, Protonu and Deng, Summer and Liu, Haixin and Park, Jongsoo and Smelyanskiy, Mikhail},
  journal={arXiv preprint arXiv:2101.05615},
  year={2021}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{gong2019softquant,
  author    = {Ruihao Gong and
               Xianglong Liu and
               Shenghu Jiang and
               Tianxiang Li and
               Peng Hu and
               Jiazhen Lin and
               Fengwei Yu and
               Junjie Yan},
  title     = {Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit
               Neural Networks},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {4851--4860},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00495},
  doi       = {10.1109/ICCV.2019.00495},
  timestamp = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/GongLJLHLYY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{guo2024lqlora,
      title={LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning}, 
      author={Han Guo and Philip Greengard and Eric P. Xing and Yoon Kim},
      year={2024},
      eprint={2311.12023},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{choi2019bit2,
  author    = {Jungwook Choi and
               Swagath Venkataramani and
               Vijayalakshmi Srinivasan and
               Kailash Gopalakrishnan and
               Zhuo Wang and
               Pierce Chuang},
  editor    = {Ameet Talwalkar and
               Virginia Smith and
               Matei Zaharia},
  title     = {Accurate and Efficient 2-bit Quantized Neural Networks},
  booktitle = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford,
               CA, USA, March 31 - April 2, 2019},
  publisher = {mlsys.org},
  year      = {2019},
  url       = {https://proceedings.mlsys.org/book/268.pdf},
  timestamp = {Thu, 18 Jun 2020 15:48:01 +0200},
  biburl    = {https://dblp.org/rec/conf/mlsys/ChoiVSGWC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{mellempudi2019bit8,
  author    = {Naveen Mellempudi and
               Sudarshan Srinivasan and
               Dipankar Das and
               Bharat Kaul},
  title     = {Mixed Precision Training With 8-bit Floating Point},
  journal   = {CoRR},
  volume    = {abs/1905.12334},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12334},
  eprinttype = {arXiv},
  eprint    = {1905.12334},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-12334.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}}

@article{qin2020survey1bit,
  author    = {Haotong Qin and
               Ruihao Gong and
               Xianglong Liu and
               Xiao Bai and
               Jingkuan Song and
               Nicu Sebe},
  title     = {Binary Neural Networks: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2004.03333},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.03333},
  eprinttype = {arXiv},
  eprint    = {2004.03333},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03333.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{courbariaux2014training,
  title={Training deep neural networks with low precision multiplications},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal={arXiv preprint arXiv:1412.7024},
  year={2014}
}


@inproceedings{ilharco-etal-2020-high,
    title = "High Performance Natural Language Processing",
    author = "Ilharco, Gabriel  and
      Ilharco, Cesar  and
      Turc, Iulia  and
      Dettmers, Tim  and
      Ferreira, Felipe  and
      Lee, Kenton",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-tutorials.4",
    doi = "10.18653/v1/2020.emnlp-tutorials.4",
    pages = "24--27",
    abstract = "Scale has played a central role in the rapid progress natural language processing has enjoyed in recent years. While benchmarks are dominated by ever larger models, efficient hardware use is critical for their widespread adoption and further progress in the field. In this cutting-edge tutorial, we will recapitulate the state-of-the-art in natural language processing with scale in perspective. After establishing these foundations, we will cover a wide range of techniques for improving efficiency, including knowledge distillation, quantization, pruning, more efficient architectures, along with case studies and practical implementation tricks.",
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{jacob2017quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. arXiv e-prints, art},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={arXiv preprint arXiv:1712.05877},
  year={2017}
}

@inproceedings{zhao2021distribution,
  title={Distribution adaptive int8 quantization for training cnns},
  author={Zhao, Kang and Huang, Sida and Pan, Pan and Li, Yinghan and Zhang, Yingya and Gu, Zhenyu and Xu, Yinghui},
  booktitle={Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{chen2020improved,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{li20211,
  title={1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed},
  author={Li, Conglong and Awan, Ammar Ahmad and Tang, Hanlin and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2104.06069},
  year={2021}
}

@article{tang20211,
  title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed},
  author={Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
  journal={arXiv preprint arXiv:2102.02888},
  year={2021}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{huang2018gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:1811.06965},
  year={2018}
}

@article{harlap2018pipedream,
  title={Pipedream: Fast and efficient pipeline parallel dnn training},
  author={Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
  journal={arXiv preprint arXiv:1806.03377},
  year={2018}
}

@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={arXiv preprint arXiv:1707.04585},
  year={2017}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{pudipeddi2020training,
  title={Training large neural networks with constant memory using a new execution algorithm},
  author={Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  journal={arXiv preprint arXiv:2002.05645},
  year={2020}
}

@article{rajbhandari2021zero,
  title={ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  journal={arXiv preprint arXiv:2104.07857},
  year={2021}
}
  
  @article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}


@article{greenwald2001space,
  title={Space-efficient online computation of quantile summaries},
  author={Greenwald, Michael and Khanna, Sanjeev},
  journal={ACM SIGMOD Record},
  volume={30},
  number={2},
  pages={58--66},
  year={2001},
  publisher={ACM New York, NY, USA}
}

@inproceedings{govindaraju2005fast,
  title={Fast and approximate stream mining of quantiles and frequencies using graphics processors},
  author={Govindaraju, Naga K and Raghuvanshi, Nikunj and Manocha, Dinesh},
  booktitle={Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
  pages={611--622},
  year={2005}
}

@article{dunning2019computing,
  title={Computing extremely accurate quantiles using t-digests},
  author={Dunning, Ted and Ertl, Otmar},
  journal={arXiv preprint arXiv:1902.04023},
  year={2019}
}

@inproceedings{chen2001quantile,
  title={Quantile and histogram estimation},
  author={Chen, E Jack and Kelton, W David},
  booktitle={Proceeding of the 2001 Winter Simulation Conference (Cat. No. 01CH37304)},
  volume={1},
  pages={451--459},
  year={2001},
  organization={IEEE}
} 

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}


@inproceedings{wenzek-etal-2020-ccnet,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.494",
    pages = "4003--4012",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{machavcek2014results,
  title={Results of the WMT14 metrics shared task},
  author={Mach{\'a}{\v{c}}ek, Matou{\v{s}} and Bojar, Ond{\v{r}}ej},
  booktitle={Proceedings of the Ninth Workshop on Statistical Machine Translation},
  pages={293--301},
  year={2014}
}

@article{sennrich2016edinburgh,
  title={Edinburgh neural machine translation systems for wmt 16},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1606.02891},
  year={2016}
}

@misc{gonzalez2002digital,
  title={Digital image processing},
  author={Gonzalez, Rafael C and Woods, Richard E and others},
  year={2002},
  publisher={Prentice hall Upper Saddle River, NJ}
}



@article{hyndman1996sample,
  title={Sample quantiles in statistical packages},
  author={Hyndman, Rob J and Fan, Yanan},
  journal={The American Statistician},
  volume={50},
  number={4},
  pages={361--365},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}



@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}


@inproceedings{dong2019hawq,
  title={Hawq: Hessian aware quantization of neural networks with mixed-precision},
  author={Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={293--302},
  year={2019}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}


@article{horvath2023stochastic,
  title={Stochastic distributed learning with gradient quantization and double-variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Richt{\'a}rik, Peter and Stich, Sebastian},
  journal={Optimization Methods and Software},
  volume={38},
  number={1},
  pages={91--106},
  year={2023},
  publisher={Taylor \& Francis}
}

@inproceedings{cai2020zeroq,
  title={Zeroq: A novel zero shot quantization framework},
  author={Cai, Yaohui and Yao, Zhewei and Dong, Zhen and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13169--13178},
  year={2020}
}

@misc{optimum-quanto,
  title = {Optimum-Quanto: A PyTorch Quantization Backend for Optimum},
  author = {HuggingFace},
  year = {2024},
  howpublished = {\url{https://github.com/huggingface/optimum-quanto}},
  note = {Accessed: 2025-01-28}
}


@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023},
  url={https://arxiv.org/abs/2305.13245}
}

@misc{chen2025prefixquanteliminatingoutliersprefixed,
      title={PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization}, 
      author={Mengzhao Chen and Yi Liu and Jiahao Wang and Yi Bin and Wenqi Shao and Ping Luo},
      year={2025},
      eprint={2410.05265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05265}, 
}

@article{xiao2024efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2024}
}


@inproceedings{gong2019differentiable,
  title={Differentiable soft quantization: Bridging full-precision and low-bit neural networks},
  author={Gong, Ruihao and Liu, Xianglong and Jiang, Shenghu and Li, Tianxiang and Hu, Peng and Lin, Jiazhen and Yu, Fengwei and Yan, Junjie},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4852--4861},
  year={2019}
}

@inproceedings{zhang2018lq,
  title={Lq-nets: Learned quantization for highly accurate and compact deep neural networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={365--382},
  year={2018}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{gholami2021survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2103.13630},
  year={2021}
}

@misc{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}



@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01861},
  year={2022}
}

@inproceedings{zeroquantv2,
  title={Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation},
  author={Yao, Zhewei and Wu, Xiaoxia and Li, Cheng and Youn, Stephen and He, Yuxiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={19377--19385},
  year={2024}
}


@article{bondarenko2021understanding,
  title={Understanding and overcoming the challenges of efficient transformer quantization},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2109.12948},
  year={2021}
}

@article{zhao2021automatic,
  title={Automatic Mixed-Precision Quantization Search of BERT},
  author={Zhao, Changsheng and Hua, Ting and Shen, Yilin and Lou, Qian and Jin, Hongxia},
  journal={arXiv preprint arXiv:2112.14938},
  year={2021}
}

@inproceedings{yao2021hawq,
  title={Hawq-v3: Dyadic neural network quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle={International Conference on Machine Learning},
  pages={11875--11886},
  year={2021},
  organization={PMLR}
}


@inproceedings{luo-etal-2021-positional,
    title = "Positional Artefacts Propagate Through Masked Language Model Embeddings",
    author = "Luo, Ziyang  and
      Kulmizev, Artur  and
      Mao, Xiaoxi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.413",
    doi = "10.18653/v1/2021.acl-long.413",
    pages = "5312--5327",
    abstract = "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa{'}s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders{'} raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance.",
}

@article{puccetti2022outliers,
  title={Outliers Dimensions that Disrupt Transformers Are Driven by Frequency},
  author={Puccetti, Giovanni and Rogers, Anna and Drozd, Aleksandr and Dell'Orletta, Felice},
  journal={arXiv preprint arXiv:2205.11380},
  year={2022}
}

@article{gao2019representation,
  title={Representation degeneration problem in training natural language generation models},
  author={Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1907.12009},
  year={2019}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{luo2020positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  journal={arXiv preprint arXiv:2011.04393},
  year={2020}
}

@article{timkey2021all,
  title={All bark and no bite: Rogue dimensions in transformer language models obscure representational quality},
  author={Timkey, William and van Schijndel, Marten},
  journal={arXiv preprint arXiv:2109.04404},
  year={2021}
}

@article{wei2022outlier,
  title={Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={arXiv preprint arXiv:2209.13325},
  year={2022}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}


@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{scao2022bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{dettmers2022llm,
  title        = {{LLM}.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author       = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  year         = 2022,
  journal      = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022},
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{micikevicius2022fp8,
  title={FP8 Formats for Deep Learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@inproceedings{DBLP:conf/aaai/piqa2020,
  author    = {Yonatan Bisk and
               Rowan Zellers and
               Ronan LeBras and
               Jianfeng Gao and
               Yejin Choi},
  title     = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {7432--7439},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6239},
  timestamp = {Mon, 07 Mar 2022 16:58:16 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{DBLP:conf/acl/hellaswag2019,
  author    = {Rowan Zellers and
               Ari Holtzman and
               Yonatan Bisk and
               Ali Farhadi and
               Yejin Choi},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4791--4800},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1472},
  doi       = {10.18653/v1/p19-1472},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/cacm/winogrande2021,
  author    = {Keisuke Sakaguchi and
               Ronan Le Bras and
               Chandra Bhagavatula and
               Yejin Choi},
  title     = {WinoGrande: an adversarial winograd schema challenge at scale},
  journal   = {Commun. {ACM}},
  volume    = {64},
  number    = {9},
  pages     = {99--106},
  year      = {2021},
  url       = {https://doi.org/10.1145/3474381},
  doi       = {10.1145/3474381},
  timestamp = {Mon, 20 Sep 2021 17:52:06 +0200},
  biburl    = {https://dblp.org/rec/journals/cacm/SakaguchiBBC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paperno-etal-2016-lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}

@article{xiao2022smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}



@article{jain2020trained,
  title={Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks},
  author={Jain, Sambhav and Gural, Albert and Wu, Michael and Dick, Chris},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={112--128},
  year={2020}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019}
}

@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@article{rusci2020memory,
  title={Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers},
  author={Rusci, Manuele and Capotondi, Alessandro and Benini, Luca},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={326--335},
  year={2020}
}


@article{gong2014compressing,
  title={Compressing deep convolutional networks using vector quantization},
  author={Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
  journal={arXiv preprint arXiv:1412.6115},
  year={2014}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{choi2016towards,
  title={Towards the limit of network quantization},
  author={Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  journal={arXiv preprint arXiv:1612.01543},
  year={2016}
}

@inproceedings{wu2016quantized,
  title={Quantized convolutional neural networks for mobile devices},
  author={Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4820--4828},
  year={2016}
}

@inproceedings{park2017weighted,
  title={Weighted-entropy-based quantization for deep neural networks},
  author={Park, Eunhyeok and Ahn, Junwhan and Yoo, Sungjoo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5456--5464},
  year={2017}
}

@article{hou2016loss,
  title={Loss-aware binarization of deep networks},
  author={Hou, Lu and Yao, Quanming and Kwok, James T},
  journal={arXiv preprint arXiv:1611.01600},
  year={2016}
}

@inproceedings{leng2018extremely,
  title={Extremely low bit neural network: Squeeze the last bit out with admm},
  author={Leng, Cong and Dou, Zesheng and Li, Hao and Zhu, Shenghuo and Jin, Rong},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{jaszczur2021sparse,
  title={Sparse is enough in scaling transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9895--9907},
  year={2021}
}


@Misc{dongarra2022,
  author =   {Jack Dongarra},
  title =    {A Not So Simple Matter of Software},
  howpublished = {\url{https://www.youtube.com/watch?v=cSO0Tc2w5Dg}},
  month =    {November},
  day =          16,
  year =     2022
}

@Misc{falcon2023,
  author =   {{TII UAE}},
  title =    {The {Falcon} Family of Large Language Models},
  howpublished = {\url{https://huggingface.co/tiiuae/falcon-40b}},
  month =    {May},
  year =     2023
}

@Misc{refinedweb2023,
  author =   {{TII UAE}},
  title =    {The {Refined} {Web} Dataset},
  howpublished = {\url{https://huggingface.co/datasets/tiiuae/falcon-refinedweb}},
  month =    {May},
  year =     2023
}

%https://huggingface.co/datasets/tiiuae/falcon-refinedweb


@article{jia2019dissecting,
  title={Dissecting the NVidia Turing T4 GPU via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Smith, Jeffrey and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1903.07486},
  year={2019}
}

@article{rosenfeld2019constructive,
  title={A constructive prediction of the generalization error across scales},
  author={Rosenfeld, Jonathan S and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
  journal={arXiv preprint arXiv:1909.12673},
  year={2019}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{openai2023gpt,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv},
  year={2023}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dettmers2022case,
  title={The case for 4-bit precision: k-bit Inference Scaling Laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.09720},
  year={2022}
}

@misc{miao2023specinfer,
      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, 
      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
      year={2023},
      eprint={2305.09781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


%%%%%%%%%%%%%%%%%%%%
%%% Dan's sparsity references
%%%%%%%%%%%%%%%%%%%%


@string{TPAMI = "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"}
@string{IJCV = "International Journal of Computer Vision (IJCV)"}
@string{JMLR = "Journal of Machine Learning Research (JMLR)"}
@string{ML = "Machine Learning"}
@string{PR = "Pattern Recognition"}
@string{PRL = "Pattern Recognition Letters"}
@string{TNN = "IEEE Transactions on Neural Networks (TNN)"}

@string{CVPR = "Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{ECCV = "European Conference on Computer Vision (ECCV)"}
@string{ICCV = "International Conference on Computer Vision (ICCV)"}
@string{BMVC = "British Machine Vision Conference (BMVC)"}

@string{NeurIPS = "Conference on Neural Information Processing Systems (NeurIPS)"}
@string{NIPS = "Conference on Neural Information Processing Systems (NeurIPS)"}
@string{ICML = "International Conference on Machine Learning (ICML)"}
@string{ICLR = "International Conference on Learning Representations (ICLR)"}
@string{COLT = "Workshop on Computational Learning Theory (COLT)"}
@string{UAI = "Uncertainty in Artificial Intelligence (UAI)"}
@string{ECML = "European Conference on Marchine Learning (ECML)"}
@string{AISTATS = "International Conference on Artificial Intelligence and Statistics (AISTATS)"}
@string{KDD = "International conference on Knowledge Discovery and Data Mining (KDD)"}
@string{EUROCRYPT = "International Conference on the Theory and Applications of Cryptographic Techniques (Eurocrypt)"}

@string{WILEY = "Wiley"},
@string{MIT = "The MIT Press"},
@string{CAMBRIDGE = "Cambridge University Press"},
@string{SPRINGER = "Springer"},
@string{KLUWER = "Kluwer Academic Press"}
@string{ELSEVIER = "Elsevier"}
@string{OXFORD = "Oxford University Press"},




@incollection{parallel_scan,
  added-at = {2009-09-10T14:36:22.000+0200},
  author = {Harris, Mark and Sengupta, Shubhabrata and Owens, John D.},
  biburl = {https://www.bibsonomy.org/bibtex/2af00089a47e6be14a796740e12ffc6fe/gregoryy},
  booktitle = {GPU Gems 3},
  chapter = 39,
  editor = {Nguyen, Hubert},
  interhash = {4f92b11742376af7621df70d49f37772},
  intrahash = {af00089a47e6be14a796740e12ffc6fe},
  keywords = {imported},
  month = {August},
  owner = {gregor},
  pages = {851--876},
  publisher = {Addison Wesley},
  timestamp = {2009-09-10T14:36:48.000+0200},
  title = {Parallel Prefix Sum (Scan) with {CUDA}},
  year = 2007
}
% Sparsity survey
@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={arXiv preprint arXiv:2102.00554},
  year={2021}
}

@article{david2020tensorflow,
  title={{TensorFlow Lite Micro: Embedded machine learning on TinyML systems}},
  author={David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and others},
  journal={arXiv preprint arXiv:2010.08678},
  year={2020}
}

@misc{weidinger2021ethical,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Sparse training methods

@article{bellec2017deep,
  title={Deep rewiring: Training very sparse deep networks},
  author={Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  journal=ICLR,
  year={2018}
}



@article{courbariaux2016binarized,
  title={Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1602.02830},
  year={2016}
}

@inproceedings{chen2018tvm,
  title={{TVM}: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
  pages={578--594},
  year={2018}
}

@article{NVIDIASparse,
  title={Accelerating Sparse Deep Neural Networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}

@unpublished{vanholder2016efficient,
  title={Efficient inference with {TensorRT}},
  author={Vanholder, Han},
  year={2017},
  Note={{NVIDIA GTC} On-Demand. Slides available at  https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt}
}


@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}


@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle=ICML,
  year={2020}
}

@article{evci2018mean,
  title={Mean Replacement Pruning},
  author={Evci, Utku and Le Roux, Nicolas and Castro, Pablo and Bottou, Leon},
  year={2018}
}


@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle=ICML,
  year={2020}
}


@inproceedings{jayakumar2020top,
  title={{Top-KAST}: {Top-K} always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  booktitle=NeurIPS,
  year={2020}
}


@inproceedings{lin2019dynamic,
  title={Dynamic Model Pruning with Feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  booktitle=ICLR,
  year={2019}
}

@misc{NM,
title={{NeuralMagic DeepSparse Inference Engine}},
author={DeepSparse},
year={2021},
url={https://github.com/neuralmagic/deepsparse}
}

@misc{graphcore,
title={{Graphcore Poplar SDK 2.0}},
author={Graphcore},
year={2021},
url={https://github.com/graphcore/poplibs}
}



@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle=CVPR,
  year={2020}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle=ICLR,
  year={2019}
}

@article{frankle2019stabilizing,
  title={Stabilizing the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle=ICML,
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{wortsman2019discovering,
  title={Discovering Neural Wirings},
  author={Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  journal=NeurIPS,
  volume={32},
  pages={2684--2694},
  year={2019}
}

@article{laurencconbigscience,
  title={The {BigScience} Corpus: A 1.6 {TB} Composite Multilingual Dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and others}, 
  year={2022}
}

@inproceedings{2017-dong,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno Jialin},
  booktitle=NeurIPS,
  year={2017}
}

@inproceedings{strubell2020energy,
  title={Energy and policy considerations for modern deep learning research},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={13693--13696},
  year={2020}
}

@inproceedings{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Khirirat, Sarit and Konstantinov, Nikola and Renggli, C{\'e}dric},
  booktitle=NeurIPS,
  year={2018}
}

@article{shi2019understanding,
  title={Understanding top-k sparsification in distributed deep learning},
  author={Shi, Shaohuai and Chu, Xiaowen and Cheung, Ka Chun and See, Simon},
  journal={arXiv preprint arXiv:1911.08772},
  year={2019}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{mohtashami2021simultaneous,
  title={Simultaneous Training of Partially Masked Neural Networks},
  author={Mohtashami, Amirkeivan and Jaggi, Martin and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2106.08895},
  year={2021}
}

@article{kurtic2022optimal,
  title={The {Optimal BERT Surgeon}: Scalable and Accurate Second-Order Pruning for Large Language Models},
  author={Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  journal={arXiv preprint arXiv:2203.07259},
  year={2022}
}

@inproceedings{iofinova2022well,
  title={How Well Do Sparse {ImageNet} Models Transfer?},
  author={Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
  booktitle=CVPR,
  year={2022}
}


@inproceedings{alistarh2016qsgd,
	title={{QSGD}: Randomized Quantization for Communication-Efficient Stochastic Gradient Descent},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	booktitle=NeurIPS,
	year={2017}
}

@article{liang2021pruning,
  title={Pruning and quantization for deep neural network acceleration: A survey},
  author={Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  journal={Neurocomputing},
  volume={461},
  pages={370--403},
  year={2021},
  publisher={Elsevier}
}


% Post training pruning

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle=NeurIPS,
  year={1990}
}


@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE International Conference on Neural Networks},
  year={1993}
}



@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle=ICML,
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@inproceedings{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  booktitle=ICML,
  year={2019}
}

@inproceedings{wang2019eigendamage,
  title={Eigendamage: Structured pruning in the {K}ronecker-factored eigenbasis},
  author={Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  booktitle=ICML,
  year={2019}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural networks},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  booktitle=NeurIPS,
  year={2015}
}

@inproceedings{zhou2021learning,
  title={Learning {N:M} Fine-grained Structured Sparse Neural Networks From Scratch},
  author={Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
  booktitle=ICLR,
  year={2021}
}


% General IHT References

@article{blumensath2008iterative,
  title={Iterative thresholding for sparse approximations},
  author={Blumensath, Thomas and Davies, Mike E},
  journal={Journal of Fourier Analysis and Applications},
  volume={14},
  number={5-6},
  pages={629--654},
  year={2008},
  publisher={Springer}
}

@article{foucart2011hard,
  title={Hard thresholding pursuit: an algorithm for compressive sensing},
  author={Foucart, Simon},
  journal={SIAM Journal on Numerical Analysis},
  volume={49},
  number={6},
  pages={2543--2563},
  year={2011},
  publisher={SIAM}
}

@incollection{foucart2012sparse,
  title={Sparse recovery algorithms: sufficient conditions in terms of restricted isometry constants},
  author={Foucart, Simon},
  booktitle={Approximation Theory XIII: San Antonio 2010},
  pages={65--77},
  year={2012},
  publisher={Springer}
}

@inproceedings{yuan2014gradient,
  title={Gradient hard thresholding pursuit for sparsity-constrained optimization},
  author={Yuan, Xiaotong and Li, Ping and Zhang, Tong},
  booktitle=ICML,
  pages={127--135},
  year={2014},
  organization={PMLR}
}

@inproceedings{jain2014iterative,
  title={On iterative hard thresholding methods for high-dimensional {M}-estimation},
  author={Jain, Prateek and Tewari, Ambuj and Kar, Purushottam},
  booktitle=NeurIPS,
  pages={685--693},
  year={2014}
}

@inproceedings{axiotis2020sparse,
  title={Sparse Convex Optimization via Adaptively Regularized Hard Thresholding},
  author={Axiotis, Kyriakos and Sviridenko, Maxim},
  booktitle=ICML,
  pages={452--462},
  year={2020},
  organization={PMLR}
}

% IHT Methods

@article{jin2016training,
  title={Training skinny deep neural networks with iterative hard thresholding methods},
  author={Jin, Xiaojie and Yuan, Xiaotong and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1607.05423},
  year={2016}
}


@article{han2016dsd,
  title={{DSD}: Dense-sparse-dense training for deep neural networks},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  journal=ICLR,
  year={2017}
}


% Datasets

@inproceedings{PTB,
  title={The Penn treebank: Annotating predicate argument structure},
  author={Marcus, Mitch and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  booktitle={Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994},
  year={1994}
}


@article{cifar100,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}



@article{wikitext103,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}




@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal=ICLR,
  year={2017}
}

@article{hagiwara1994,
	title = {A simple and effective method for removal of hidden units and weights},
	journal = {Neurocomputing},
	volume = {6},
	number = {2},
	pages = {207 - 218},
	year = {1994},
	note = {Backpropagation, Part IV},
	issn = {0925-2312},
	author = {Masafumi Hagiwara},
}

@article{lee2018snip,
  title={{SNIP}: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal=ICLR,
  year={2019}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal=NeurIPS,
  volume={33},
  year={2020}
}

% Models, architectures

@inproceedings{zagoruyko2016wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle=BMVC,
  year={2016}
}


@article{hooker2020characterising,
  title={Characterising bias in compressed models},
  author={Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  journal={arXiv preprint arXiv:2010.03058},
  year={2020}
}

@article{howard2017mobilenets,
  title={{MobileNets}: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  year={2016}
}

@inproceedings{dai2019transformer,
  title={{Transformer-XL: Attentive language models beyond a fixed-length context}},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  year={2019}
}

@inproceedings{lamb,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle=ICLR,
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal=ICLR,
  year={2015}
}


% MISC

@misc{wandb,
title = {{Experiment Tracking with Weights and Biases}},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}


@misc{exllamav2,
title = {{ExLlamaV2. A fast inference library for running LLMs locally on modern consumer-class GPUs}},
note = {Open-source library developed by \@turboderp and controbutors.},
url={https://github.com/turboderp/exllamav2},
author = {ExLlama2 contributors.},
}


@incollection{pytorch,
  title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  publisher = {Neural Information Processing Systems Foundation},
  year = {2019}
}

@inproceedings{bahdanau2014neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year={2015},
  url={https://arxiv.org/abs/1409.0473}
}

@misc{abdin2024phi3,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Parul Chopra and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Dan Iter and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Chen Liang and Weishung Liu and Eric Lin and Zeqi Lin and Piyush Madan and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Xia Song and Masahiro Tanaka and Xin Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Michael Wyatt and Can Xu and Jiahang Xu and Sonali Yadav and Fan Yang and Ziyi Yang and Donghan Yu and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pytorchdistributed,
      title={PyTorch Distributed: Experiences on Accelerating Data Parallel Training}, 
      author={Shen Li and Yanli Zhao and Rohan Varma and Omkar Salpekar and Pieter Noordhuis and Teng Li and Adam Paszke and Jeff Smith and Brian Vaughan and Pritam Damania and Soumith Chintala},
      year={2020},
      eprint={2006.15704},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the {P}olyak-{{\L}}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@inproceedings{haroush2020knowledge,
  title={The knowledge within: Methods for data-free model compression},
  author={Haroush, Matan and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2020}
}


@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle=ICML,
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{candes2006near,
  title={Near-optimal signal recovery from random projections: Universal encoding strategies?},
  author={Candes, Emmanuel J and Tao, Terence},
  journal={IEEE transactions on information theory},
  volume={52},
  number={12},
  pages={5406--5425},
  year={2006},
  publisher={IEEE}
}

@article{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1412.6544},
  year={2014}
}

@inproceedings{shevchenko2020landscape,
  title={Landscape connectivity and dropout stability of {SGD} solutions for over-parameterized neural networks},
  author={Shevchenko, Alexander and Mondelli, Marco},
  booktitle=ICML,
  pages={8773--8784},
  year={2020},
  organization={PMLR}
}

%%%%%%%%%%%%%%%%%%%%
%%% End of Dan's reference list
%%%%%%%%%%%%%%%%%%%%



@article{aflalo2020knapsack,
  title={Knapsack Pruning with Inner Distillation},
  author={Aflalo, Yonathan and Noy, Asaf and Lin, Ming and Friedman, Itamar and Zelnik, Lihi},
  journal={arXiv preprint arXiv:2002.08258},
  year={2020}
}

@inproceedings{wu2020constraint,
  title={Constraint-Aware Importance Estimation for Global Filter Pruning under Multiple Resource Constraints},
  author={Wu, Yu-Cheng and Liu, Chih-Ting and Chen, Bo-Ying and Chien, Shao-Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={686--687},
  year={2020}
}

@inproceedings{yao2021hawq,
  title={{HAWQ-v3}: Dyadic Neural Network Quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle=ICML,
  year={2021}
}

@inproceedings{hubara2021accurate,
  title={Accurate Post Training Quantization With Small Calibration Sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle=ICML,
  year={2021}
}

@article{hubara2020improving
,
  title={Improving post training neural quantization: Layer-wise calibration and integer programming},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2006.10518},
  year={2020}
}


@inproceedings{he2018amc,
  title={{AMC}: {AutoML} for Model Compression and Acceleration on Mobile Devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{liebenwein2021compressing,
  title={Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition},
  author={Liebenwein, Lucas and Maalouf, Alaa and Feldman, Dan and Rus, Daniela},
  booktitle=NeurIPS,
  year={2021}
}

@article{markov2021project,
  title={Project {CGX}: Scalable Deep Learning on Commodity {GPUs}},
  author={Markov, Ilia and Ramezani, Hamidreza and Alistarh, Dan},
  journal={arXiv preprint arXiv:2111.08617},
  year={2021}
}

@inproceedings{cai2019once,
  title={{Once-for-All}: Train One Network and Specialize it for Efficient Deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  booktitle=ICLR,
  year={2019}
}

@inproceedings{hubara2021accelerated,
  title={Accelerated Sparse Neural Training: A Provable and Efficient Method to find {N:M} Transposable Masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Seffi and Soudry, Daniel},
  booktitle=NeurIPS,
  year={2021}
}

@article{li2016pruning,
  title={Pruning Filters for Efficient Convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@inproceedings{singh2020woodfisher,
  title={{WoodFisher}: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2020}
}

@inproceedings{frantar2021m,
  title={{M-FAC}: Efficient Matrix-Free Approximations of Second-Order Information},
  author={Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{nagel2020up,
  title={Up or Down? {A}daptive Rounding for Post-Training Quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle=ICML,
  year={2020}
}

@inproceedings{liu2021group,
  title={Group Fisher Pruning for Practical Network Compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle=ICML,
  year={2021}
}

@InProceedings{
    pmlr-v119-kurtz20a, 
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, 
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, 
    booktitle = ICML,
    year = {2020}
}

@inproceedings{sgk_sc2020,
  author    = {Trevor Gale and Matei Zaharia and Cliff Young and Erich Elsen},
  title     = {Sparse {GPU} Kernels for Deep Learning},
  booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  year      = {2020},
}

@article{dave2021hardware,
  title={Hardware Acceleration of Sparse and Irregular Tensor Computations of {ML} Models: A Survey and Insights},
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE},
  volume={109},
  number={10},
  pages={1706--1752},
  year={2021},
  publisher={IEEE}
}

@article{2020-sanh,
	title={Movement Pruning: Adaptive Sparsity by Fine-Tuning}, 
	author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
	year={2020},
	journal={arXiv preprint arXiv:2005.07683}
}

@inproceedings{peste2021ac,
  title={{AC/DC}: Alternating Compressed/DeCompressed Training of Deep Neural Networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{he2019filter,
  title={Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle=CVPR,
  year={2019}
}

@article{jayakumar2021top,
  title={{Top-KAST}: {Top-K} Always Sparse Training},
  author={Jayakumar, Siddhant M and Pascanu, Razvan and Rae, Jack W and Osindero, Simon and Elsen, Erich},
  journal={arXiv preprint arXiv:2106.03517},
  year={2021}
}

@inproceedings{he2017channel,
  title={Channel Pruning for Accelerating Very Deep Neural Networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle=ICCV,
  year={2017}
}

@inproceedings{ashok2018n2n,
  title={{N2N} Learning: Network to Network Compression via Policy Gradient Reinforcement Learning},
  author={Ashok, Anubhav and Rhinehart, Nicholas and Beainy, Fares and Kitani, Kris M},
  booktitle=ICLR,
  year={2018}
}

@inproceedings{yang2021netadaptv2,
  title={{NetAdaptV2}: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization},
  author={Yang, Tien-Ju and Liao, Yi-Lun and Sze, Vivienne},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{real2019regularized,
  title={Regularized Evolution for Image Classifier Architecture Search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={{BERT}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019}
}



@inproceedings{deng2009imagenet,
  title={{ImageNet}: A Large-Scale Hierarchical Image Database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  pages={248--255},
  year={2009},
  organization={IEEE}
}


@inproceedings{marcel2010torchvision,
  title={Torchvision the Machine-Vision Package of Torch},
  author={Marcel, S{\'e}bastien and Rodriguez, Yann},
  booktitle={ACM International Conference on Multimedia},
  year={2010}
}



@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2016}
}

@article{pool2021channel,
  title={Channel Permutations for {N:M} Sparsity},
  author={Pool, Jeff and Yu, Chong},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@inproceedings{schwarz2021powerpropagation,
  title={Powerpropagation: A sparsity inducing weight reparameterisation},
  author={Schwarz, Jonathan and Jayakumar, Siddhant and Pascanu, Razvan and Latham, Peter and Teh, Yee},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{lagunas21block,
    title = "Block Pruning For Faster Transformers",
    author = "Lagunas, Fran{\c{c}}ois  and
      Charlaix, Ella  and
      Sanh, Victor  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    pages = "10619--10629",
}

@inproceedings{li2021brecq,
  title={{BRECQ}: Pushing the Limit of Post-Training Quantization by Block Reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  booktitle=ICLR,
  year={2021}
}

@inproceedings{cai2018proxylessnas,
  title={{ProxylessNAS}: Direct Neural Architecture Search on Target Task and Hardware},
  author={Cai, Han and Zhu, Ligeng and Han, Song},
  booktitle=ICLR,
  year={2018}
}

@article{nagel2021white,
  title={A White Paper on Neural Network Quantization},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  year={2021}
}

@misc{deepsparse,
  author = "NeuralMagic",
  address = "NeuralMagic, Inc.",
  title = "{DeepSparse}",
  year = "2022",
  url = "https://github.com/neuralmagic/deepsparse"
}

@misc{yolov5,
  author = "Glenn Jocher",
  address = "Ultralytics",
  title = "{YOLOv5}",
  year = "2022.",
  howpublished = "https://github.com/ultralytics/yolov5"
}

@inproceedings{carreira2018learning,
  title={{“Learning-Compression”} algorithms for neural net pruning},
  author={Carreira-Perpin{\'a}n, Miguel A and Idelbayev, Yerlan},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{idelbayev2021beyond,
  title={Beyond {FLOPs} in low-rank compression of neural networks: Optimizing device-specific inference runtime},
  author={Idelbayev, Yerlan and Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  year={2021}
}

@misc{qin2024accurate,
      title={Accurate LoRA-Finetuning Quantization of LLMs via Information Retention}, 
      author={Haotong Qin and Xudong Ma and Xingyu Zheng and Xiaoyang Li and Yang Zhang and Shouda Liu and Jie Luo and Xianglong Liu and Michele Magno},
      year={2024},
      eprint={2402.05445},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{liebenwein2019provable,
  title={Provable Filter Pruning for Efficient Neural Networks},
  author={Liebenwein, Lucas and Baykal, Cenk and Lang, Harry and Feldman, Dan and Rus, Daniela},
  booktitle=ICLR,
  year={2019}
}

@article{baykal2019sipping,
  title={{SiPPing} neural networks: Sensitivity-informed provable pruning of neural networks},
  author={Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
  journal={arXiv preprint arXiv:1910.05422},
  year={2019}
}

@article{parnami2021pruning,
  title={Pruning Attention Heads of Transformer Models Using {A*} Search: A Novel Approach to Compress Big {NLP} Architectures},
  author={Parnami, Archit and Singh, Rahul and Joshi, Tarun},
  journal={arXiv preprint arXiv:2110.15225},
  year={2021}
}

@inproceedings{huang2020rethinking,
  title={Rethinking the Pruning Criteria for Convolutional Neural Network},
  author={Huang, Zhongzhan and Wang, Xinjiang and Luo, Ping},
  booktitle=NeurIPS,
  year={2021}
}

@article{jiao2019survey,
  title={A survey of deep learning-based object detection},
  author={Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
  journal={IEEE Access},
  volume={7},
  pages={128837--128868},
  year={2019}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with {AlphaFold}},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and {Huffman} coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle=ICLR,
  year={2016}
}

@article{zhang2015accelerating,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015}
}

@inproceedings{dubey2018coreset,
  title={Coreset-based neural network compression},
  author={Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2018}
}

@inproceedings{tan2019efficientnet,
  title={{EfficientNet}: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle=ICML,
  year={2019}
}

@inproceedings{sui2021chip,
  title={{CHIP}: CHannel Independence-based Pruning for Compact Neural Networks},
  author={Sui, Yang and Yin, Miao and Xie, Yi and Phan, Huy and Aliari Zonouz, Saman and Yuan, Bo},
  booktitle=NeurIPS,
  year={2021}
}

@article{huang2021rethinking,
  title={Rethinking the Pruning Criteria for Convolutional Neural Network},
  author={Huang, Zhongzhan and Shao, Wenqi and Wang, Xinjiang and Lin, Liang and Luo, Ping},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@inproceedings{liu2018rethinking,
  title={Rethinking the Value of Network Pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  booktitle=ICLR,
  year={2018}
}

@inproceedings{lagunas2021block,
  title={Block Pruning For Faster Transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021}
}

@misc{onebit,
      title={OneBit: Towards Extremely Low-bit Large Language Models}, 
      author={Yuzhuang Xu and Xu Han and Zonghan Yang and Shuo Wang and Qingfu Zhu and Zhiyuan Liu and Weidong Liu and Wanxiang Che},
      year={2024},
      eprint={2402.11295},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wanghaq,
  title={{HAQ}: Hardware-aware Automated Quantization with Mixed Precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle=CVPR,
  year=2019
}

@inproceedings{2017-wu,
	author={Yuhuai Wu and Elman Mansimov and Roger B. Grosse and Shun Liao and Jimmy Ba},
	title={Second-order Optimization for Deep Reinforcement Learning using {K}ronecker-factored Approximation},
	year={2017},
	booktitle=NeurIPS,
}

@article{doi:10.1162/089976698300017746,
	author = {Amari, Shun-ichi},
	title = {Natural Gradient Works Efficiently in Learning},
	journal = {Neural Computation},
	volume = {10},
	number = {2},
	pages = {251-276},
	year = {1998},
	doi = {10.1162/089976698300017746},
	
	URL = { 
	https://doi.org/10.1162/089976698300017746
	
	},
	eprint = { 
	https://doi.org/10.1162/089976698300017746
	
	}
}

@article{frantar2022spdy,
  title={{SPDY:} {A}ccurate Pruning with Speedup Guarantees},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2201.13096},
  year={2022}
}

@inproceedings{wang2020towards,
  title={Towards accurate post-training network quantization via bit-split and stitching},
  author={Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian},
  booktitle=ICML,
  year={2020},
}


@article{nahshan2021loss,
  title={Loss aware post-training quantization},
  author={Nahshan, Yury and Chmiel, Brian and Baskin, Chaim and Zheltonozhskii, Evgenii and Banner, Ron and Bronstein, Alex M and Mendelson, Avi},
  journal={Machine Learning},
  volume={110},
  number={11},
  pages={3245--3262},
  year={2021},
  publisher={Springer}
}

@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={International Conference on Computer Vision Workshop (ICCVW)},
  year={2019}
}

@inproceedings{2015-martens,
	title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
	author={James Martens and Roger Grosse},
	year={2015},
	booktitle=ICML
}

@inproceedings{grosse2016kroneckerfactored,
	title={A {K}ronecker-factored approximate {F}isher matrix for convolution layers},
	author={Roger Grosse and James Martens},
	year={2016},
	booktitle=ICML
}

@inproceedings{yang2020automatic,
  title={Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach},
  author={Yang, Haichuan and Gui, Shupeng and Zhu, Yuhao and Liu, Ji},
  booktitle=CVPR,
  year={2020}
}

@inproceedings{lin2014microsoft,
  title={Microsoft {COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle=ECCV,
  year={2014}
}

@article{chmiel2022optimal,
  title={Optimal Fine-Grained {N:M} sparsity for Activations and Neural Gradients},
  author={Chmiel, Brian and Hubara, Itay and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2203.10991},
  year={2022}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle=ICCV,
  year={2019}
}

@inproceedings{banner2019post,
  title={Post training 4-bit quantization of convolutional networks for rapid-deployment},
  author={Banner, Ron and Nahshan, Yury and Soudry, Daniel},
  booktitle=NeurIPS,
  year={2019}
}

@article{frantar2022obc,
  title={{Optimal Brain Compression}: A Framework for Accurate Post-Training Quantization and Pruning},
  author={Frantar, Elias and Sidak Pal Singh and Alistarh, Dan},
  journal={arXiv preprint arXiv:2208.11580},
  note={Accepted to NeurIPS 2022, to appear.},
  year={2022}
}


@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle=NeurIPS,
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{C4,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67}
}

@inproceedings{bender2021dangers, title={On the dangers of stochastic parrots: Can language models be too big?}, author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},   booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency}, pages={610--623}, year={2021} } 


@article{li2020gpt,
  title={Demistifying GPT-3},
  author={Li, Chuan},
  journal={Lambda Cloud Blog},
  year={2021}, 
  note={\url{https://lambdalabs.com/blog/demystifying-gpt-3/}}
}

@article{wu2022extreme,
  title={Extreme Compression for Pre-trained Transformers Made Simple and Efficient},
  author={Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01859},
  year={2022}
}




@article{park2022nuqmm,
  title={{nuQmm}: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2022}
}

@inproceedings{strom22_interspeech,
  author={Nikko Strom and Haidar Khan and Wael Hamza},
  title={{Squashed Weight Distribution for Low Bit Quantization of Deep Models}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={3953--3957},
  doi={10.21437/Interspeech.2022-50}
}

@article{ahmadian2023intriguing,
  title={Intriguing Properties of Quantization at Scale},
  author={Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and Venkitesh, Bharat and Gou, Stephen and Blunsom, Phil and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2305.19268},
  year={2023}
}

@article{wortsman2023stable,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2304.13013},
  year={2023}
}


%%%%%%
%zeroShot Datasets
%%%%%%

% LAMBADA:
@article{paperno2016lambada,
  title={The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

% PIQA:
@inproceedings{tata2003piqa,
  title={{PiQA}: An algebra for querying protein data sets},
  author={Tata, Sandeep and Patel, Jignesh M},
  booktitle={International Conference on Scientific and Statistical Database Management},
  year={2003}
}

% ARC:
@article{boratko2018systematic,
  title={A systematic classification of knowledge, reasoning, and context within the {ARC} dataset},
  author={Boratko, Michael and Padigela, Harshit and Mikkilineni, Divyendra and Yuvraj, Pritish and Das, Rajarshi and McCallum, Andrew and Chang, Maria and Fokoue-Nkoutche, Achille and Kapanipathi, Pavan and Mattei, Nicholas and others},
  journal={arXiv preprint arXiv:1806.00358},
  year={2018}
}

% StoryCloze
@inproceedings{mostafazadeh2017lsdsem,
  title={Lsdsem 2017 shared task: The story cloze test},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@article{tao2022compression,
  title={Compression of Generative Pre-trained Language Models via Quantization},
  author={Tao, Chaofan and Hou, Lu and Zhang, Wei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Luo, Ping and Wong, Ngai},
  journal={arXiv preprint arXiv:2203.10705},
  year={2022}
}

@article{frantar2023massive,
  title={Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}

@inproceedings{gorbachev2019openvino,
  title={Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference},
  author={Gorbachev, Yury and Fedorov, Mikhail and Slavutin, Iliya and Tugarev, Artyom and Fatekhov, Marat and Tarkan, Yaroslav},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@article{zheng2022alpa,
  title={Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2201.12023},
  year={2022}
}

@article{dao2022flashattention,
  title={{FlashAttention}: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}
@inproceedings{reddi2020mlperf,
  title={Mlperf inference benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}

@inproceedings{gale2020sparse,
  title={Sparse GPU kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@misc{chee2023quip,
      title={QuIP: 2-Bit Quantization of Large Language Models With Guarantees}, 
      author={Jerry Chee and Yaohui Cai and Volodymyr Kuleshov and Christopher De Sa},
      year={2023},
      eprint={2307.13304},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{kim2023squeezellm,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}

@misc{lee2024owq,
      title={OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models}, 
      author={Changhun Lee and Jungyu Jin and Taesu Kim and Hyungjun Kim and Eunhyeok Park},
      year={2024},
      eprint={2306.02272},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2024lqer,
      title={LQER: Low-Rank Quantization Error Reconstruction for LLMs}, 
      author={Cheng Zhang and Jianyi Cheng and George A. Constantinides and Yiren Zhao},
      year={2024},
      eprint={2402.02446},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{guo2016quantization,
  title={Quantization based fast inner product search},
  author={Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  booktitle={Artificial intelligence and statistics},
  pages={482--490},
  year={2016},
  organization={PMLR}
}


@Article{Zhou2017,
author={Zhou, Shu-Chang
and Wang, Yu-Zhi
and Wen, He
and He, Qin-Yao
and Zou, Yu-Heng},
title={Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks},
journal={Journal of Computer Science and Technology},
year={2017},
month={Jul},
day={01},
volume={32},
number={4},
pages={667-682},
issn={1860-4749},
doi={10.1007/s11390-017-1750-y},
url={https://doi.org/10.1007/s11390-017-1750-y}
}

@misc{li2017performance,
      title={Performance Guaranteed Network Acceleration via High-Order Residual Quantization}, 
      author={Zefan Li and Bingbing Ni and Wenjun Zhang and Xiaokang Yang and Wen Gao},
      year={2017},
      eprint={1708.08687},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{Chen_Wang_Pan_2019, title={Deep Neural Network Quantization via Layer-Wise Optimization Using Limited Training Data}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4206}, DOI={10.1609/aaai.v33i01.33013329}, abstractNote={&lt;p&gt;The advancement of deep models poses great challenges to real-world deployment because of the limited computational ability and storage space on edge devices. To solve this problem, existing works have made progress to prune or quantize deep models. However, most existing methods rely heavily on a supervised training process to achieve satisfactory performance, acquiring large amount of labeled training data, which may not be practical for real deployment. In this paper, we propose a novel layer-wise quantization method for deep neural networks, which only requires limited training data (1% of original dataset). Specifically, we formulate parameters quantization for each layer as a discrete optimization problem, and solve it using Alternative Direction Method of Multipliers (ADMM), which gives an efficient closed-form solution. We prove that the final performance drop after quantization is bounded by a linear combination of the reconstructed errors caused at each layer. Based on the proved theorem, we propose an algorithm to quantize a deep neural network layer by layer with an additional weights update step to minimize the final error. Extensive experiments on benchmark deep models are conducted to demonstrate the effectiveness of our proposed method using 1% of CIFAR10 and ImageNet datasets. Codes are available in: https://github.com/csyhhu/L-DNQ&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Shangyu and Wang, Wenya and Pan, Sinno Jialin}, year={2019}, month={Jul.}, pages={3329-3336} 
}

@misc{wanda,
      title={A Simple and Effective Pruning Approach for Large Language Models}, 
      author={Mingjie Sun and Zhuang Liu and Anna Bair and J. Zico Kolter},
      year={2024},
      eprint={2306.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{admmpruning,
      title={Fast and Optimal Weight Update for Pruned Large Language Models}, 
      author={Vladimír Boža},
      year={2024},
      eprint={2401.02938},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

%<found by chatgpt deepsearch>
@inproceedings{zhou2024selfdiscover,
  title = "{SELF-DISCOVER}: Large Language Models Self-Compose Reasoning Structures",
  author = "Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven",
  editor = "Globerson, Amir and Mackey, Lester and Belgrave, Danielle and Fan, Angela and Paquet, Ulrich and Tomczak, Jakub M. and Zhang, Cheng",
  booktitle = "Advances in Neural Information Processing Systems 37 (NeurIPS 2024)",
  address = "Vancouver, BC, Canada",
  month = dec,
  year = "2024"
}

@article{li2024moreagents,
  title = "More Agents Is All You Need",
  author = "Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng",
  journal = "Transactions on Machine Learning Research",
  year = "2024"
}

@inproceedings{chen-etal-2024-reconcile,
  title = "{R}e{C}oncile: Round-Table Conference Improves Reasoning via Consensus among Diverse {LLM}s",
  author = "Chen, Justin and Saha, Swarnadeep and Bansal, Mohit",
  editor = "Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.acl-long.381/",
  doi = "10.18653/v1/2024.acl-long.381",
  pages = "7066--7085",
  abstract = "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a {\\textquoteleft}discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4\\% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8\\% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance."
}

@inproceedings{kong-etal-2024-better,
  title = "Better Zero-Shot Reasoning with Role-Play Prompting",
  author = "Kong, Aobo and Zhao, Shiwan and Chen, Hao and Li, Qicheng and Qin, Yong and Sun, Ruiqi and Zhou, Xin and Wang, Enzhi and Dong, Xiaohang",
  editor = "Duh, Kevin and Gomez, Helena and Bethard, Steven",
  booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
  month = jun,
  year = "2024",
  address = "Mexico City, Mexico",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.naacl-long.228/",
  doi = "10.18653/v1/2024.naacl-long.228",
  pages = "4099--4113",
  abstract = "Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs’ reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5\\% to 63.8\\%, and on Last Letter from 23.8\\% to 84.2\\%. Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to “think step by step”, our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process. This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting."
}

@inproceedings{wang-etal-2024-rethinking-bounds,
  title = "Rethinking the Bounds of {LLM} Reasoning: Are Multi-Agent Discussions the Key?",
  author = "Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu",
  editor = "Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.acl-long.331/",
  doi = "10.18653/v1/2024.acl-long.331",
  pages = "6106--6131",
  abstract = "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same best performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observed that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion. Our code can be found in \\url{https://github.com/HKUST-KnowComp/LLM-discussion}."
}

@article{talebirad2023multiagent,
  title = "Multi-Agent Collaboration: Harnessing the Power of Intelligent {LLM} Agents",
  author = "Talebirad, Yashar and Nadiri, Amirhossein",
  journal = "CoRR",
  volume = "abs/2306.03314",
  year = "2023"
}

@inproceedings{cohen-etal-2023-lm,
  title = "{LM} vs {LM}: Detecting Factual Errors via Cross Examination",
  author = "Cohen, Roi and Hamri, May and Geva, Mor and Globerson, Amir",
  editor = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.emnlp-main.778/",
  doi = "10.18653/v1/2023.emnlp-main.778",
  pages = "12621--12640",
  abstract = "A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs for capturing factual errors."
}
%</bibtex found by ChatGPT>

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@misc{ye2025limoreasoning,
      title={LIMO: Less is More for Reasoning}, 
      author={Yixin Ye and Zhen Huang and Yang Xiao and Ethan Chern and Shijie Xia and Pengfei Liu},
      year={2025},
      eprint={2502.03387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.03387}, 
}

@inproceedings{
ning2024skeletonofthought,
title={Skeleton-of-Thought: Prompting {LLM}s for Efficient Parallel Generation},
author={Xuefei Ning and Zinan Lin and Zixuan Zhou and Zifu Wang and Huazhong Yang and Yu Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mqVgBbNCm9}
}

@misc{ding2025dynamicparalleltreesearch,
      title={Dynamic Parallel Tree Search for Efficient LLM Reasoning}, 
      author={Yifu Ding and Wentao Jiang and Shunyu Liu and Yongcheng Jing and Jinyang Guo and Yingjie Wang and Jing Zhang and Zengmao Wang and Ziwei Liu and Bo Du and Xianglong Liu and Dacheng Tao},
      year={2025},
      eprint={2502.16235},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.16235}, 
}

@misc{yu2025accelerateparallelizablereasoningparallel,
      title={Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence}, 
      author={Yijiong Yu},
      year={2025},
      eprint={2503.20533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.20533}, 
}

@misc{jin2025learningpromisescalinglanguage,
      title={Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding}, 
      author={Tian Jin and Ellie Y. Cheng and Zack Ankner and Nikunj Saunshi and Blake M. Elias and Amir Yazdanbakhsh and Jonathan Ragan-Kelley and Suvinay Subramanian and Michael Carbin},
      year={2025},
      eprint={2502.11517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.11517}, 
}

@misc{gim2024asynchronousllmfunctioncalling,
      title={Asynchronous LLM Function Calling}, 
      author={In Gim and Seung-seob Lee and Lin Zhong},
      year={2024},
      eprint={2412.07017},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.07017}, 
}

@inproceedings{kim2024llm,
  title={An llm compiler for parallel function calling},
  author={Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W and Keutzer, Kurt and Gholami, Amir},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}



@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@misc{kurtic2023sparse,
      title={Sparse Fine-tuning for Inference Acceleration of Large Language Models}, 
      author={Eldar Kurtic and Denis Kuznedelev and Elias Frantar and Michael Goin and Dan Alistarh},
      year={2023},
      eprint={2310.06927},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{frantar2023qmoe,
  title={QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2310.16795},
  year={2023}
}

@inproceedings{spallanzani2022training,
  title={Training quantised neural networks with ste variants: the additive noise annealing algorithm},
  author={Spallanzani, Matteo and Leonardi, Gian Paolo and Benini, Luca},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={470--479},
  year={2022}
}


@article{helwegen2019latent,
  title={Latent weights do not exist: Rethinking binarized neural network optimization},
  author={Helwegen, Koen and Widdicombe, James and Geiger, Lukas and Liu, Zechun and Cheng, Kwang-Ting and Nusselder, Roeland},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{shekhovtsov2021reintroducing,
  title={Reintroducing straight-through estimators as principled methods for stochastic binary networks},
  author={Shekhovtsov, Alexander and Yanush, Viktor},
  booktitle={DAGM German Conference on Pattern Recognition},
  pages={111--126},
  year={2021},
  organization={Springer}
}


@article{yin2019understanding,
  title={Understanding straight-through estimator in training activation quantized neural nets},
  author={Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
  journal={arXiv preprint arXiv:1903.05662},
  year={2019}
}

@inproceedings{huh2023straightening,
  title={Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks},
  author={Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={14096--14113},
  year={2023},
  organization={PMLR}
}



@article{polino2018model,
  title={Model compression via distillation and quantization},
  author={Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  journal={arXiv preprint arXiv:1802.05668},
  year={2018}
}


@article{mises1929praktische,
  title={Praktische Verfahren der Gleichungsaufl{\"o}sung.},
  author={Mises, RV and Pollaczek-Geiringer, Hilda},
  journal={ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift f{\"u}r Angewandte Mathematik und Mechanik},
  volume={9},
  number={1},
  pages={58--77},
  year={1929},
  publisher={Wiley Online Library}
}

@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


% Pruning
@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}



@article{impact1_Weidinger2021EthicalAS,
  title={Ethical and social risks of harm from Language Models},
  author={Laura Weidinger and John F. J. Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zachary Kenton and Sande Minnich Brown and William T. Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William S. Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.04359},
  url={https://api.semanticscholar.org/CorpusID:244954639}
}
@article{impact2_Weidinger2022TaxonomyOR,
  title={Taxonomy of Risks posed by Language Models},
  author={Laura Weidinger and Jonathan Uesato and Maribeth Rauh and Conor Griffin and Po-Sen Huang and John F. J. Mellor and Amelia Glaese and Myra Cheng and Borja Balle and Atoosa Kasirzadeh and Courtney Biles and Sande Minnich Brown and Zachary Kenton and William T. Hawkins and Tom Stepleton and Abeba Birhane and Lisa Anne Hendricks and Laura Rimell and William S. Isaac and Julia Haas and Sean Legassick and Geoffrey Irving and Iason Gabriel},
  journal={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249872629}
}
@inproceedings{impact3_bender,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{meta_reasoning_gao2024,
  title={Meta reasoning for large language models},
  author={Gao, Peizhong and Xie, Ao and Mao, Shaoguang and Wu, Wenshan and Xia, Yan and Mi, Haipeng and Wei, Furu},
  journal={arXiv preprint arXiv:2406.11698},
  year={2024}
}

@inproceedings{wang-etal-2024-meta,
    title = "Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models",
    author = "Wang, Yiming  and
      Zhang, Zhuosheng  and
      Zhang, Pei  and
      Yang, Baosong  and
      Wang, Rui",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.34/",
    doi = "10.18653/v1/2024.findings-acl.34",
    pages = "622--643",
    abstract = "Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique."
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}



@inproceedings{NIPS2011_218a0aef,
 author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},
 volume = {24},
 year = {2011}
}

@misc{hogwild_exclamation_mark,
  author = {{Stanford HAI}},
  title = {How a “Crazy Idea” Overturned the Conventional Rules of Machine Learning},
  year = {2023},
  url = {https://hai.stanford.edu/news/how-crazy-idea-overturned-conventional-rules-machine-learning},
  note = {Accessed: [Insert Date]}
}


@article{Wang2022SelfConsistencyIC,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed H. Chi and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.11171},
  url={https://api.semanticscholar.org/CorpusID:247595263}
}

@inproceedings{wang2024mixture,
  title={Mixture-of-agents enhances large language model capabilities},
  author={Wang, Junlin and Jue, WANG and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}


@inproceedings{
du2024improving,
title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
booktitle={Forty-first International Conference on Machine Learning},
year={2023},
url={https://openreview.net/forum?id=zj7YuTE4t8}
}

@misc{impact4_zhuo2023redteamingchatgptjailbreaking,
      title={Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity}, 
      author={Terry Yue Zhuo and Yujin Huang and Chunyang Chen and Zhenchang Xing},
      year={2023},
      eprint={2301.12867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.12867}, 
}

@article{impact5_Cui2024RiskTM,
  title={Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems},
  author={Tianyu Cui and Yanling Wang and Chuanpu Fu and Yong Xiao and Sijia Li and Xinhao Deng and Yunpeng Liu and Qinglin Zhang and Ziyi Qiu and Peiyang Li and Zhixing Tan and Junwu Xiong and Xinyu Kong and Zujie Wen and Ke Xu and Qi Li},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.05778},
  url={https://api.semanticscholar.org/CorpusID:266933337}
}

@article{impact6_Sheng2021SocietalBI,
  title={Societal Biases in Language Generation: Progress and Challenges},
  author={Emily Sheng and Kai-Wei Chang and P. Natarajan and Nanyun Peng},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.04054},
  url={https://api.semanticscholar.org/CorpusID:234337004}
}

@article{impact7_Durmus2023TowardsMT,
  title={Towards Measuring the Representation of Subjective Global Opinions in Language Models},
  author={Esin Durmus and Karina Nyugen and Thomas Liao and Nicholas Schiefer and Amanda Askell and Anton Bakhtin and Carol Chen and Zac Hatfield-Dodds and Danny Hernandez and Nicholas Joseph and Liane Lovitt and Sam McCandlish and Orowa Sikder and Alex Tamkin and Janel Thamkul and Jared Kaplan and Jack Clark and Deep Ganguli},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.16388},
  url={https://api.semanticscholar.org/CorpusID:259275051}
}


% latest

@article{cot_wei_2022,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zero_shot_cot_Kojima2022LargeLM,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.11916},
  url={https://api.semanticscholar.org/CorpusID:249017743}
}
@article{auto_cot_Zhang2022AutomaticCO,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03493},
  url={https://api.semanticscholar.org/CorpusID:252762275}
}

@inproceedings{challenging_bigbench_solved_with_cot_Suzgun2022ChallengingBT,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Mirac Suzgun and Nathan Scales and Nathanael Scharli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252917648}
}

@misc{beeching2024scalingtesttimecompute,
      title={Scaling test-time compute with open models},
      author={Edward Beeching and Lewis Tunstall and Sasha Rush},
      url={https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute},
}
@article{scaling_test_time_snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}
@article{verify_step_by_step,
  title={Let's Verify Step by Step},
  author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.20050},
  url={https://api.semanticscholar.org/CorpusID:258987659}
}
#TODO
@article{Huang2022LargeLM,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11610},
  url={https://api.semanticscholar.org/CorpusID:253080328}
}
@article{three_of_thought,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10601},
  url={https://api.semanticscholar.org/CorpusID:258762525}
}
@inproceedings{graph_of_thought,
  title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
  author={Maciej Besta and Nils Blach and Ale{\vs} Kub{\'i}{\vc}ek and Robert Gerstenberger and Lukas Gianinazzi and Joanna Gajda and Tomasz Lehmann and Michal Podstawski and Hubert Niewiadomski and Piotr Nyczyk and Torsten Hoefler},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261030303}
}
@misc{deepseek_r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
@misc{qwq32b,
    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},
    url = {https://qwenlm.github.io/blog/qwq-32b/},
    author = {{Qwen Team}},
    month = {March},
    year = {2025}
}

@article{qwen2.5,
      title={Qwen2.5 Technical Report}, 
      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      journal={arXiv preprint arXiv:2412.15115},
      year={2024}
}

@article{Schick2023ToolformerLM,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Timo Schick and Jane Dwivedi-Yu and Roberto Dess{\`i} and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04761},
  url={https://api.semanticscholar.org/CorpusID:256697342}
}
@article{Qin2023ToolLLMFL,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Yujia Qin and Shi Liang and Yining Ye and Kunlun Zhu and Lan Yan and Ya-Ting Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Marc H. Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.16789},
  url={https://api.semanticscholar.org/CorpusID:260334759}
}
@article{Yao2022ReActSR,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03629},
  url={https://api.semanticscholar.org/CorpusID:252762395}
}
@article{Shen2023HuggingGPTSA,
  title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
  author={Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yue Ting Zhuang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17580},
  url={https://api.semanticscholar.org/CorpusID:257833781}
}

@misc{openai_o1,
      title={OpenAI o1 System Card}, 
      author={OpenAI and : and Aaron Jaech and Adam Kalai and Adam Lerer and Adam Richardson and Ahmed El-Kishky and Aiden Low and Alec Helyar and Aleksander Madry and Alex Beutel and Alex Carney and Alex Iftimie and Alex Karpenko and Alex Tachard Passos and Alexander Neitz and Alexander Prokofiev and Alexander Wei and Allison Tam and Ally Bennett and Ananya Kumar and Andre Saraiva and Andrea Vallone and Andrew Duberstein and Andrew Kondrich and Andrey Mishchenko and Andy Applebaum and Angela Jiang and Ashvin Nair and Barret Zoph and Behrooz Ghorbani and Ben Rossen and Benjamin Sokolowsky and Boaz Barak and Bob McGrew and Borys Minaiev and Botao Hao and Bowen Baker and Brandon Houghton and Brandon McKinzie and Brydon Eastman and Camillo Lugaresi and Cary Bassin and Cary Hudson and Chak Ming Li and Charles de Bourcy and Chelsea Voss and Chen Shen and Chong Zhang and Chris Koch and Chris Orsinger and Christopher Hesse and Claudia Fischer and Clive Chan and Dan Roberts and Daniel Kappler and Daniel Levy and Daniel Selsam and David Dohan and David Farhi and David Mely and David Robinson and Dimitris Tsipras and Doug Li and Dragos Oprica and Eben Freeman and Eddie Zhang and Edmund Wong and Elizabeth Proehl and Enoch Cheung and Eric Mitchell and Eric Wallace and Erik Ritter and Evan Mays and Fan Wang and Felipe Petroski Such and Filippo Raso and Florencia Leoni and Foivos Tsimpourlas and Francis Song and Fred von Lohmann and Freddie Sulit and Geoff Salmon and Giambattista Parascandolo and Gildas Chabot and Grace Zhao and Greg Brockman and Guillaume Leclerc and Hadi Salman and Haiming Bao and Hao Sheng and Hart Andrin and Hessam Bagherinezhad and Hongyu Ren and Hunter Lightman and Hyung Won Chung and Ian Kivlichan and Ian O'Connell and Ian Osband and Ignasi Clavera Gilaberte and Ilge Akkaya and Ilya Kostrikov and Ilya Sutskever and Irina Kofman and Jakub Pachocki and James Lennon and Jason Wei and Jean Harb and Jerry Twore and Jiacheng Feng and Jiahui Yu and Jiayi Weng and Jie Tang and Jieqi Yu and Joaquin Quiñonero Candela and Joe Palermo and Joel Parish and Johannes Heidecke and John Hallman and John Rizzo and Jonathan Gordon and Jonathan Uesato and Jonathan Ward and Joost Huizinga and Julie Wang and Kai Chen and Kai Xiao and Karan Singhal and Karina Nguyen and Karl Cobbe and Katy Shi and Kayla Wood and Kendra Rimbach and Keren Gu-Lemberg and Kevin Liu and Kevin Lu and Kevin Stone and Kevin Yu and Lama Ahmad and Lauren Yang and Leo Liu and Leon Maksin and Leyton Ho and Liam Fedus and Lilian Weng and Linden Li and Lindsay McCallum and Lindsey Held and Lorenz Kuhn and Lukas Kondraciuk and Lukasz Kaiser and Luke Metz and Madelaine Boyd and Maja Trebacz and Manas Joglekar and Mark Chen and Marko Tintor and Mason Meyer and Matt Jones and Matt Kaufer and Max Schwarzer and Meghan Shah and Mehmet Yatbaz and Melody Y. Guan and Mengyuan Xu and Mengyuan Yan and Mia Glaese and Mianna Chen and Michael Lampe and Michael Malek and Michele Wang and Michelle Fradin and Mike McClay and Mikhail Pavlov and Miles Wang and Mingxuan Wang and Mira Murati and Mo Bavarian and Mostafa Rohaninejad and Nat McAleese and Neil Chowdhury and Neil Chowdhury and Nick Ryder and Nikolas Tezak and Noam Brown and Ofir Nachum and Oleg Boiko and Oleg Murk and Olivia Watkins and Patrick Chao and Paul Ashbourne and Pavel Izmailov and Peter Zhokhov and Rachel Dias and Rahul Arora and Randall Lin and Rapha Gontijo Lopes and Raz Gaon and Reah Miyara and Reimar Leike and Renny Hwang and Rhythm Garg and Robin Brown and Roshan James and Rui Shu and Ryan Cheu and Ryan Greene and Saachi Jain and Sam Altman and Sam Toizer and Sam Toyer and Samuel Miserendino and Sandhini Agarwal and Santiago Hernandez and Sasha Baker and Scott McKinney and Scottie Yan and Shengjia Zhao and Shengli Hu and Shibani Santurkar and Shraman Ray Chaudhuri and Shuyuan Zhang and Siyuan Fu and Spencer Papay and Steph Lin and Suchir Balaji and Suvansh Sanjeev and Szymon Sidor and Tal Broda and Aidan Clark and Tao Wang and Taylor Gordon and Ted Sanders and Tejal Patwardhan and Thibault Sottiaux and Thomas Degry and Thomas Dimson and Tianhao Zheng and Timur Garipov and Tom Stasi and Trapit Bansal and Trevor Creech and Troy Peterson and Tyna Eloundou and Valerie Qi and Vineet Kosaraju and Vinnie Monaco and Vitchyr Pong and Vlad Fomenko and Weiyi Zheng and Wenda Zhou and Wes McCabe and Wojciech Zaremba and Yann Dubois and Yinghai Lu and Yining Chen and Young Cha and Yu Bai and Yuchen He and Yuchen Zhang and Yunyun Wang and Zheng Shao and Zhuohan Li},
      year={2024},
      eprint={2412.16720},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.16720}, 
}

@misc{googledeepmind2025gemini25thinking,
  author = {{Google DeepMind}},
  title = {{Gemini 2.5: Our Newest Gemini Model with Thinking}},
  howpublished = {\url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking}},
  year = {2025},
  note = {Accessed: 2025-04-07},
}

@article{TTSsurvey,
  author={Zhang, Qiyuan and Lyu, Fuyuan and Sun, Zexu and Wang, Lei and Zhang, Weixu and Guo, Zhihan and Wang, Yufei and King, Irwin and Liu, Xue and Ma, Chen},
  title={What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models},
  journal={arXiv preprint arXiv:2503.24235},
  year={2025}
}


@inproceedings{streamingllm,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle=ICLR,
  year={2024}
}

@misc{rope_scaling,
      title={YaRN: Efficient Context Window Extension of Large Language Models}, 
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00071}, 
}

@article{cai2024medusa,
  title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  author={Cai, Tianle and Li, Xinyun and Wang, Zhiruo and Wang, Yuhuai and Song, Dawn},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{li2024eagle,
  title={EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={31147--31162},
  year={2024},
  organization={PMLR}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}