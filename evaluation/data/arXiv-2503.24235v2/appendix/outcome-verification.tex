
\section{Detailed Outcome Verification Methods}
\label{app:outcome_verification}

This appendix expands on the outcome verification techniques employed at test time in LLMs. Unlike training-time methods (\eg, RL fine-tuning), these techniques operate on the fly during inference, often by generating multiple solutions and using a \textit{proposer–verifier} framework.

\subsection{Verifier Model-Based Scoring}
The verifier, which is typically trained using human feedback or supervised data (\eg, as in~\cite{cobbe2021training,lambert2024rewardbenchevaluatingrewardmodels}), scores each candidate based on its expected correctness or quality. Variants include i) pairwise comparison verifiers~\citep{liu2025pairjudgermperformbestofn}, where candidates are compared against each other to determine a winner, ii) weighted voting systems~\citep{wettig2024quratingselectinghighqualitydata,li2024dnaevalenhancinglargelanguage} that use the verifier’s scores to combine outputs, iii) LLM-based verifiers that prompt LLM to perform evaluation instruction, like LLM-as-a-Judge~\citep{NEURIPS2023_91f18a12,zhang2025crowd,zhang2025reviseval}, LLM-based Evaluator~\citep{liu2023gevalnlgevaluationusing,xu2023instructscore,jiang2024tigerscorebuildingexplainablemetric}, Critic-based Model~\citep{gao2024llmcriticshelpcatch,mcaleese2024llmcriticshelpcatch}.

\subsection{Self-Consistency and Voting Mechanisms}
Self-consistency techniques generate multiple independent reasoning chains and choose the final answer based on majority voting~\cite{wang2023selfconsistency}. The underlying assumption is that if several chains converge on the same answer, that answer is more likely to be correct. Some approaches~\citep{taubenfeld2025confidenceimprovesselfconsistencyllms,mahmud2025enhancingllmcodegeneration} also incorporate confidence scores or soft-voting schemes to mitigate noise in individual outputs. In place of multiple samples from one model, one can also have multiple models~\citep{wan2025mammrefinerecipeimprovingfaithfulness,wu2025hiddenstrengthdisagreementunraveling,wang2025talkstructurallyacthierarchically,feng2024diverseagententropyquantifyingblackboxllm,chen2024reconcileroundtableconferenceimproves}: if a majority (or consensus) of these ``agents'' agree on an answer, trust it; if they diverge, it may trigger further scrutiny. This is effectively an ensemble vote. 

\subsection{Tool-Assisted and Heuristic Verification}
In domains like code generation or mathematical problem-solving, outcome verification can be implemented via direct execution or rule-based checks. For example, candidate programs are executed on sample test cases to ensure they produce correct results, while in math tasks, answers can be validated by plugging them back into the original equations. These approaches serve as an external check on the LLM's internal reasoning.

\paragraph{Execution-Based Verification.} In programming tasks, the ultimate test of correctness is running the code~\citep{tian2025codehaluinvestigatingcodehallucinations,ni2024nextteachinglargelanguage,yang2024exploringunleashingpowerlarge,ni2023leverlearningverifylanguagetocode}. For math problems, a simple heuristic is to verify the answer by plugging it back into the original equation or problem constraints. Similarly, if a puzzle answer must satisfy certain conditions, those can be programmatically checked.

\paragraph{Fact-Checking via Retrieval.} In open-domain QA or tasks that risk factual errors, search engines or knowledge bases serve as powerful verifiers~\citep{wei2024longformfactualitylargelanguage,vladika2024improvinghealthquestionanswering,asai2023selfraglearningretrievegenerate,peng2023checkfactstryagain}. An LLM may draft an answer, but then the system issues search queries (based on the answer’s claims) to find supporting evidence. If the retrieved documents contradict the LLM’s answer, the answer is likely incorrect and can be rejected or revised. Some frameworks generate answers in a ``closed-book'' fashion, then do a \textit{post-hoc} retrieval to validate facts. This idea overlaps with Retrieval-Augmented Generation~\citep{salemi2024searchenginemachinesunified}, but the focus is on post-generation validation – essentially checking if the answer aligns with external truth. 

\paragraph{Rule-Based Filters.} In some applications, simple heuristic filters~\citep{bai2022constitutionalaiharmlessnessai,NEURIPS2023_0764db11,weber2024redpajamaopendatasettraining} can automatically reject bad outputs. For a dialogue system, one might have a list of forbidden answers (certain unsafe or nonsensical replies) and if the model outputs one, the system can either regenerate or adjust it. These aren’t ``outcome-based'' in terms of correctness, but they verify the output against predefined rules of form and content.
