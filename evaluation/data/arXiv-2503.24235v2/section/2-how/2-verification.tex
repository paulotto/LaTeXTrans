
\subsubsection{Verification}
\label{subsec:verification}
Verifying the correctness and consistency of LLM during the test-time scaling is also crucial. The verification process plays an important role in the test-time scaling, as a solid verification process can be adapted to: 
\begin{itemize}
    \item directly selects the output sample among various ones, under the \textit{Parallel Scaling} paradigm;
    \item guides the stimulation process and determines when to stop, under the \textit{Sequential Scaling} paradigm;
    \item serves as the criteria in the search process, which we will discuss in Section~\ref{subsec:search};
    \item determines what sample to aggregate and how to aggregate them, e.g., weights, which we will discuss in Section~\ref{subsec:aggregation}.
\end{itemize}
Usually, there are two types of verifications, as shown below:

\paragraph{Outcome Verification.}
%Output verification approaches directly verify the sample outcome. \citet{cobbe2021training} initially proposes to train additional verifiers instead of fine-tuning models on mathematical tasks. 
Outcome verification plays a crucial role in ensuring the correctness and consistency of generated outputs. Common approaches include using a separate verifier model to score multiple candidate answers (\eg,\citet{cobbe2021training}), employing self-consistency, voting mechanisms~\citep{wang2023selfconsistency} and discriminator LM~\citep{chen2024tree} and leveraging tool-assisted~\citep{gou2024critic} or heuristic checks~\citep{deepseek-r1} in domains such as math and code generation. 
For specific task problems, such as trip planning, functional scoring~\citep{lee2025evolvingdeeperllmthinking} is also adopted for verifying the proposed plans. 
Instead of formulating the outcome verification as a classification problem, \citet{zhang2025generativeverifiersrewardmodeling} exploits the generative ability of LLM and proposes to reformulate the outcome verification process as a next-token prediction task.
\citet{li2025learningreasonfeedbacktesttime} formulate the feedback utilization as an optimization problem and adaptive propagate information between samples.

Apart from single criteria, certain outcome verification approaches verify the quality of the simulated samples from multiple perspectives. 
\citet{liu2023plan} conducts both (i) passive verification from external tools and (ii) active verification via a rethinking mechanism to justify each sample.
\citet{zhang2024wrongofthoughtintegratedreasoningframework} follows a similar idea and proposes to verify each sample from three aspects: Assertion, Process, and Result.
\citet{lifshitz2025multiagent} further extends the number of verification agents to an arbitrary number and decouples the semantic criteria with verification agents. 
\citet{parmar2025plangenmultiagentframeworkgenerating} and \citet{saadfalcon2024archonarchitecturesearchframework} also propose a verification agent to score each sample considering various factors, respectively. \citet{saadfalcon2024archonarchitecturesearchframework} additionally proposes a unit test-based verification approach.
We provide a detailed technical categorization in the Appendix~\ref{app:outcome_verification}.
%Table~\ref{tab:outcome_verification_methods} summarizes the main outcome-based verification methods, categorizing them by approach and providing brief descriptions.

% Output verification can also be extended to guide the decoding process during stimulation~\citep{yu2024ovm}.

\paragraph{Process Verification.}
Process verification approaches verify the sample outcomes and the process of obtaining such an outcome. They are commonly adopted in tasks with formal, deductive processes, such as reasoning, coding, or mathematics. They are also known as the process reward model (PRM) or state verification. \citet{lightman2023let} processes to train a PRM as a step-level verification on mathematical tasks. \citet{yao2023tree} processes an LM-based state verifier as guidelines for searching the samples under the tree structure. \citet{zhang2024chain} further tunes these preference data into LLM and enables CoT structure during test time. Instead of training an external verifier, \citet{xie2023selfevaluation} prompts the same LM to evaluate the current step given all previous ones. \citet{hosseini2024vstartrainingverifiersselftaught} proposes to train the verifier with both accurate and inaccurate generated data.
Although LM-based process verifiers can be easily integrated, they may yield unreliable verification, especially for complex problems with long processes. \citet{ling2023deductive} decomposes the verification process in a deductive manner. Hence, the verifier only needs to verify a few statements within the long thought chain. \citet{yu2024siamselfimprovingcodeassistedmathematical} is based on similar intuition but instead focuses on code-aided mathematical reasoning tasks with the critic model iteratively. \citet{li2025startselftaughtreasonertools} instead relies on the external toolbox, such as code interpreters, to verify the process. 


\begin{table*}[!htbp]
    \centering
    % \rowcolors{1}{}{gray!10}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{lll}
    \toprule
        \rowcolor{gray!10}
        \textbf{Category} & \textbf{Approach} & \textbf{Approach} Description \\
    \midrule
        \multirow{11}{*}{\textbf{Outcome}}
        & Naive ORM~\citep{cobbe2021training} & Naively process to train solution-level and token-level verifiers on labeled-dataset \\
        & OVM~\citep{yu2024ovm} & Train a value model under outcome supervision for guided decoding \\
        & Heuristic~\citep{deepseek-r1} & Heuristic check for domain-specific problems \\
        & Functional~\citep{lee2025evolvingdeeperllmthinking} & Functional scoring for task-specific problems \\
        & Bandit~\citep{sui2025metareasonerdynamicguidanceoptimized} & Train a bandit algorithm to learn how to verify \\
        & Generative Verifier~\citep{zhang2025generativeverifiersrewardmodeling} & Exploit the generative ability of LLM-based verifiers via reformulating the verification \\
        & Self-Reflection Feedback~\citep{li2025learningreasonfeedbacktesttime} & formulate the feedback utilization as an optimization problem and solve during test-time \\
        & Discriminator~\citep{chen2024tree} & SFT a domain-specific LM as a discriminator \\
        & Unit Test~\citep{saadfalcon2024archonarchitecturesearchframework} & Verify each sample as unit tests \\
        & XoT~\citep{liu2023plan} & Passive verification from external tools and Activate verification via re-thinking \\
        & WoT~\citep{zhang2024wrongofthoughtintegratedreasoningframework} & Multi-Perspective Verification on three aspects: Assertion, Process, and Result \\
        & Multi-Agent Verifiers~\citep{lifshitz2025multiagent} & Multi-Perspective Verification without explicit semantic meanings \\
    \midrule
        \rowcolor{gray!10}
        & Naive PRM~\citep{lightman2023let} & SFT an LM as a PRM on each reasoning step over mathematical tasks \\
        \rowcolor{gray!10}
        & State Verifier~\citep{yao2023tree} & SFT an LM as a state verifier and evaluate states either independently or jointly \\
        \rowcolor{gray!10}
        & Deductive PRM~\citep{ling2023deductive} & Deductively verify a few statements in the process \\ 
        \rowcolor{gray!10}
        & Self-Evaluation~\citep{xie2023selfevaluation} & Prompting the same LM to evaluate the current step given previous ones \\
        \rowcolor{gray!10}
        & PoT~\citep{chen2023program} & delegate computation steps to an external language interpreter \\
        \rowcolor{gray!10}
        & Tool~\citep{li2025startselftaughtreasonertools} & Relies on external toolbox for verification \\
        \rowcolor{gray!10}\multirow{-7}{*}{\textbf{Process}}
        & V-STaR~\citep{hosseini2024vstartrainingverifiersselftaught} & Verifier trained on both accurate and inaccurate self-generated data \\
    \bottomrule
    \end{tabular}}
    \caption{Summary of Certain Verification Techniques.}
    \label{tab:verification}
\end{table*}

