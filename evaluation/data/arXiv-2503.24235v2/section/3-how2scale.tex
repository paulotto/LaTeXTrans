\section{How to Scale}
\label{sec:how2scale}

\subsection{Tuning-based Approaches}
\label{subsec:tuning}

To activate a model’s ability to devote cost at test time, directly tuning its parameters is an effective strategy. This includes two approaches:
1) Supervised Finetuning (SFT): Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to imitate and internalize structured reasoning patterns, effectively learning to think through complex problems. By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time. 
2) Reinforcement Learning (RL): By leveraging feedback from a reward model on inference tasks, the policy model is automatically updated. Although no supervised data is introduced, the model autonomously generates long CoT reasoning while ensuring reliable answers.
We divide the RL for internal scaling works into two perspectives. The reward model-based methods and the reward model-free methods. 

\subsubsection{Supervised Finetuning (SFT)}
\label{subsubsec:sft}
Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to internalize structured reasoning patterns and effectively ``think'' through complex problems. By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time. This will include two subsections: (1) Imitation, describing techniques like MCTS used to generate CoT-style demonstrations for fine-tuning, and (2) Distillation, summarizing how student models are trained using outputs from stronger models (\eg, o1, R1).

\paragraph{Imitation}
A prominent approach to enhancing LLM reasoning via SFT is to generate long CoT demonstrations using test-time ``planner'' algorithms and then fine-tune the model to imitate those demonstrations. For example, STaR~\citep{zelikman2022star} uses the model itself to generate step-by-step solutions for a given problem and filters for correct outcomes, treating the verified solutions as new demonstrations to fine-tune. More structured search has been applied to generate even higher-quality traces: ReST-MCTS~\citep{zhang2024rest} integrates an MCTS planner (guided by a learned value model) to explore the space of possible reasoning steps; the model is subsequently fine-tuned on these search-generated traces, \ie, it learns to imitate the successful reasoning trajectories discovered by the planner.

\paragraph{Distillation}
While the imitation approach uses a model’s own intermediate outputs for improvement, distillation techniques aim to transfer the capabilities of a stronger model (or ensemble of models) into a target model via supervised learning. As reported by~\citet{muennighoff2025s1,li2025llmseasilylearnreason}, a 32B model trained on a curated sample set generated by a top-tier reasoner was able to solve competition-level math problems nearly as well as the teacher, indicating successful distillation of reasoning.

\paragraph{Warmup}
SFT warmup~\citep{luong2024reftreasoningreinforcedfinetuning} refers to an initial SFT phase applied to an LLM after its unsupervised pretraining but before other post-training steps like RL. This stage stabilizes subsequent training by providing a well-initialized model that adapts better to preference optimization and avoids instability due to ungrounded behavior~\citep{zeng2025itoolboostingtooluse}. Effective SFT warmup is characterized by several key elements: (i) the use of high-quality, task-relevant datasets~\citep{luong2024reftreasoningreinforcedfinetuning}; (ii) short duration; (iii) a tailored learning rate schedule~\citep{pareja2024unveilingsecretrecipeguide}. Technically, SFT warmup is often integrated with methods like rejection sampling~\citep{pareja2024unveilingsecretrecipeguide}—which uses warm-started models to generate high-quality data for further training.

\subsubsection{Reinforcement Learning (RL)}
\label{subsubsec:rl}

\paragraph{Reward model-free.} Recent advancements in RL and preference optimization have significantly enhanced the performance of large language models, particularly in reasoning and problem-solving tasks. A key innovation in this domain is the introduction of RL with verifiable reward by DeepSeek R1~\citep{deepseek-r1}, which leverages rule-based reward mechanisms to optimize models efficiently and reliably. This approach has sparked growing interest among researchers working on large models, as it addresses challenges such as sparse rewards and training instability by providing dense feedback for policy optimization.
%==
Several methods have been developed to improve exploration and accuracy in reasoning tasks through preference optimization. For instance, cDPO~\citep{lin2024critical}, CPL~\citep{wang2024cpl}, Focused-DPO~\citep{zhang2025focused}, DAPO~\citep{liu2024improvingmultistepreasoningabilities}, and RFTT~\citep{zhang2025reasoning} prioritize critical or error-prone areas, enhancing internal scaling and reasoning accuracy. Additionally, Selective DPO~\citep{gao2025principled} emphasizes the importance of aligning data difficulty with model capacity by filtering out overly challenging examples, further refining the training process.
%==
VC-PPO~\citep{yuan2025s} investigates the failure of PPO for the long CoT task and uses a pre-trained value model to achieve better results. 
Light-R1~\citep{wen2025lightxi} proposes a curriculum training framework for increasing data difficulty combined with multi-staged post-training. 
SimPO~\citep{meng2024simpo} uses the average log probability of a sequence as the implicit reward and removes the reference model in DPO.

In the realm of mathematical problem-solving, DQO~\citep{ji2024enhancing} and OREO~\citep{wang2024offline} propose novel value function optimization techniques, demonstrating improvements in model performance. 
DAPO~\citep{yu2025dapo} leverages dynamic sampling for large-scale RL systems. 
%==
These advancements are complemented by a range of open-source training frameworks that have equipped researchers and developers with tools to optimize training and enhance inference. Early frameworks like SimpleRL~\citep{zeng2025simplerl} and DeepScaler~\citep{deepscaler2025} quickly replicated the technology stack of DeepSeek R1. Furthermore, SimpleRL-Zoo~\citep{zeng2025simplerlzoo} presents more experimental details about SimpleRL.  Others, such as X-R1~\citep{xr12025} and TinyZero~\citep{tinyzero}, focus on delivering an intuitive and cost-effective user experience. Notably, Open-Reasoner-Zero~\citep{OpenReasonerZero2025} replicated the DeepSeek R1-zero training scheme using a 32B model, achieving comparable performance.
%==
Further advancements in RL for internal scaling have been facilitated by frameworks like OpenR~\citep{wang2024openr}, OpenRLHF~\citep{hu2024openrlhf}, OpenR1~\citep{openr1}, Logic-RL~\citep{xie2025logic} and AReaL\citep{areal2025}. These frameworks have enhanced the replication of internal scaling and, through open-source sharing, accelerated academic research progress. 
%=
The above developments not only address key challenges in RL but also pave the way for more efficient and reliable model training and deployment.



\paragraph{Reward model-based.}
With a Bradley-Terry model~\citep{zheng2023secrets} optimized by human preference as the reward model, PPO~\citep{schulman2017proximalpolicyoptimizationalgorithms} stands as one of the most influential algorithms with its efficiency and stability and is widely used for internal scaling. 
Building upon PPO, ReMax~\citep{li2023remax} introduces variance reduction techniques along with REINFORCE~\citep{sutton1999policy} and RLOO~\citep{ahmadian2024back} methods. This eliminates the need for additional value models in PPO, reduces over four hyperparameters, lowers GPU memory usage, and speeds up the training process.
%==
GRPO~\citep{shao2024deepseekmath} replaces traditional value models with improved sampling strategies. This significantly accelerates the learning process and achieves performance comparable to GPT-4 in mathematics.  REINFORCE++~\citep{hu2025reinforce++} further simplifies GRPO and enhances its training.
DVPO~\citep{huang2025lean} presents a streamlined framework, substituting the reward model with a pre-trained global value model and removing the dependency between the actor and critic. PRIME~\citep{cui2025process} integrates the SFT model as a PRM within a unified RL framework, allowing online updates through policy rollouts and outcome labels via implicit process rewards. SPPD~\citep{yi2025sppd} utilizes process preference learning with a dynamic value margin for self-training.
Recently, several works have focused on other challenges of existing reward model-based methods. UGDA~\citep{sun2025uncertain} leverages the uncertainty and influence of samples during PPO training and iteratively refines the reward model. VinePPO~\citep{kazemnejad2024vineppo} exploits the flexibility of language environments to compute unbiased Monte Carlo-based estimates, avoiding the need for large value networks. LCPO~\citep{aggarwal2025l1} focuses on optimizing accuracy and adherence to user-specified length constraints for reasoning tasks. Rest-MCTS*~\citep{zhang2024rest} uses tree-search-based RL to bypass per-step manual annotation typically required for training process rewards.
These advancements and refinements in algorithms continue to drive the field of reinforcement learning for internal scaling, offering more effective tools and methods for solving complex problems.


%What’s Behind PPO’s Collapse in Long-CoT? Value Optimization Holds the Secret

%Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and

%DAPO: An Open-Source LLM Reinforcement Learning System at Scale
\subsection{Inference-based Approaches}
\label{subsec:inference}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/TTS-how.png}
    \caption{A Visual Map and Comparison: From \textit{What to Scale} to \textit{How to Scale}.}
    \label{fig:illustration}
\end{figure}

Unlike training-based approaches, which adjust the model’s parameters offline, inference-based approaches dynamically adjust computation during deployment. This paradigm includes four essential components: (i) \textit{Stimulation}, which encourages the model to generate longer or multiple candidate outputs; (ii) \textit{Verification}, which filters or scores outputs based on correctness or other criteria; (iii) \textit{Search}, which systematically explores the sample space; and (iv) \textit{Aggregation}, which consolidates multiple outputs into the final output. These four components are often used in combination to allocate test-time computation more effectively and boost performance on complex reasoning tasks. In the following sections, we provide detailed discussions of each component.

\input{section/2-how/1-stimulation}

\input{section/2-how/2-verification}

\input{section/2-how/3-search}

\input{section/2-how/4-aggregation}

