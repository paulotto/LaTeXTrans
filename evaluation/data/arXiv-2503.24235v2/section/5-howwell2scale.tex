\section{How Well to Scale}
\label{sec:howwell2scale}

In this section, we classify the metrics used in evaluating the test-time scaling methods into four high-level dimensions: 
\textbf{Performance}, \textbf{Controllability}, \textbf{Scalability}, and \textbf{Efficiency}.
Each dimension captures an essential aspect critical to assessing test-time scaling approaches.

\subsection{Performance}
\label{subsec:performance}
Performance metrics assess the correctness of generated solutions.

\paragraph{Pass@1.} 
Pass@1 is one of the most widely used metrics for evaluating the correctness of a model’s first output attempt~\citep{deepseek-r1, li2025stesttimescaling, snell2024scaling, xie2025logic, kimi-k1.5, yang2025towards, yang2025reasonflux, hou2025advancing}. 
It measures the proportion of problems where the model’s first generated solution is correct. 
A correct solution means the one that exactly matches the ground-truth answer or passes all required validation checks, such as the exact answer match in mathematical benchmarks and private unit tests in coding tasks. 
Pass@1 is frequently used in tasks such as mathematical reasoning and coding benchmarks. 
In mathematical reasoning tasks such as \textbf{AIME 2024}~\citep{aime25} and \textbf{MATH-500}~\citep{zhang2024rest}, Pass@1 measures the percentage of exact matches between the model's answer and the ground truth.
In coding benchmarks such as \textbf{LiveCodeBench}~\citep{jain2025livecodebench} and \textbf{HumanEval-Mul}, Pass@1 evaluates the code correctness against hidden test cases. 

\paragraph{Pass@k (Coverage).} 
Pass@k extends Pass@1 by measuring whether at least one of the model’s $k$ sampled outputs is correct~\citep{brown2024large, snell2024scaling, li2025stesttimescaling}. Formally, Pass@k can be estimated using the unbiased estimator from~\citet{chen2021evaluating}:
\[
\text{Pass@k} = \frac{1}{n} \sum_{i=1}^{n} \left(1 - \frac{\binom{N - C_i}{k}}{\binom{N}{k}}\right),
\]
where $n$ is the number of problems, $N$ is the total number of samples per problem, and $C_i$ is the number of correct samples for the $i$-th problem. 
Pass@k is widely adopted in program synthesis and formal theorem-proving tasks, such as \textbf{CodeContests}~\citep{Li2022competition} and \textbf{SWE-bench Lite}~\citep{jimenez2024swebench}.

\paragraph{Cons@k (Consensus@k).}
Cons@k measures the majority vote correctness from $k$ independently sampled outputs~\citep{deepseek-r1, zeng2025revisiting}. Given $k$ responses generated by a model for a given problem, the majority-voted prediction is the most frequent answer. The answer is then compared against the ground truth. 
Cons@k is frequently used alongside pass@1 to assess the benefit of leveraging multiple samples. 
Larger values of $k$ (\emph{e.g.}, 16, 64) typically improve answer stability and accuracy but at the cost of increased compute.
This metric is especially valuable in tasks where single generations may be noisy or uncertain, and ensemble strategies can improve robustness. Cons@k has been widely adopted in mathematical reasoning benchmarks such as \textbf{AIME 2024}~\citep{aime25} and \textbf{MATH-500}~\citep{zhang2024rest}.

\paragraph{Arena-based Evaluation (Pairwise Win Rate).}
In addition to accuracy-oriented metrics, some studies adopt pairwise comparison metrics, where model outputs are compared against baselines using human or LLM-based judges~\citep{deepseek-r1, hou2025advancing}.
For instance, \textbf{LC-Winrate}~\citep{dubois2024lengthcontrolledalpacaevalsimpleway} adjusts win rates to control for response length, while \textbf{ArenaHard GPT-4 Judge}~\citep{li2024crowdsourceddatahighqualitybenchmarks} uses GPT-4-Turbo to score outputs from open-ended tasks.
These pairwise evaluation methods are especially common in generation tasks where qualitative assessments (\emph{e.g.}, fluency, coherence) matter.

\paragraph{Task-Specific Metrics.}
Certain domains employ specialized metrics. For example, \textbf{Codeforces} \textbf{Percentile} and \textbf{Elo Rating} are used to measure coding capabilities under competitive programming settings~\citep{deepseek-r1, kimi-k1.5}.
Percentile indicates how well a model performs relative to other participants, while Elo Rating reflects relative skill under tournament-based evaluations.


\subsection{Efficiency}
\label{subsec:efficiency}
Efficiency metrics assess the computational and resource cost, offering insights into the practical deployment of test-time scaling methods.

\paragraph{Token Cost.}
Token cost measures the total number of tokens generated during inference, including intermediate reasoning steps and final outputs~\citep{welleck2024decoding, brown2024large, hou2025advancing, yang2025towards,xu2025softcotsoftchainofthoughtefficient,wang2025makepennycountdifficultyadaptive,aytes2025sketchofthoughtefficientllmreasoning}. This metric is especially important, as verbose reasoning typically leads to higher token consumption. Reducing token cost while maintaining performance is crucial for inference efficiency, particularly when operating under fixed computational budgets or API pricing constraints. 
In addition, inference efficiency metrics such as latency and throughput are critical in real-world applications, especially for high-throughput systems~\citep{welleck2024decoding}.

\paragraph{FLOPs-based Efficiency Analysis.}
FLOPs-based compute analysis has been widely adopted to quantify computational cost~\citep{kaplan2020scalinglawsneurallanguage, snell2024scaling, wu2024inference, teng2025atom}. Several recent works~\citep{snell2024scaling, wu2024inference} benchmark test-time scaling strategies, such as adaptive revisions and verifier-based search, against model scaling by plotting accuracy versus total inference FLOPs. This FLOPs-based evaluation can be used to determine whether inference-time methods outperform larger models under equivalent compute budgets.

\paragraph{Underthinking score.}
The \emph{underthinking score}~\citep{wang2025thoughtsplaceunderthinkingo1like} quantifies the inefficiency of a model when it initially generates a correct thought but fails to follow through to a correct final answer. It measures how early in the response the first correct thought appears, relative to the total length of the response, in cases where the final answer is incorrect.

Formally, the underthinking score $\xi_{\mathrm{UT}}$ is defined as:
\begin{equation}
\xi_{\mathrm{UT}} = \frac{1}{N} \sum_{i=1}^{N} \left(1 - \frac{\hat{T}_i}{T_i} \right)
\end{equation}

\begin{itemize}
    \item $N$: Number of incorrect responses in the test set.
    \item $T_i$: Total number of tokens in the $i$-th incorrect response.
    \item $\hat{T}_i$: Number of tokens from the beginning of the response up to and including the first correct thought.
\end{itemize}

If no correct thought exists in the response, then $\hat{T}_i = T_i$, indicating the model failed to meaningfully engage with the problem, and the score for that instance is zero (i.e., not underthinking).

A high $\xi_{\mathrm{UT}}$ value indicates greater inefficiency, where useful insights appear early but are not pursued, reflecting strong underthinking behavior.

\paragraph{KV Cache Size.}
The \emph{KV cache size}~\citep{hooper2025etsefficienttreesearch} refers to the total memory footprint required to store the Key-Value cache across all trajectories and time steps during the inference-time search process. As each unique generation path requires its own KV cache, methods with low KV sharing across trajectories tend to consume significantly more memory and incur higher latency.
By promoting KV cache sharing among trajectories, ETS reduces the total KV cache size, thereby improving throughput. For instance, ETS achieves up to $1.8\times$ KV cache reduction compared to REBASE, leading to $1.4\times$ faster inference on NVIDIA H100 GPUs, \emph{without compromising accuracy}.


\subsection{Controllability}
\label{subsec:controllability}
Controllability metrics evaluate whether inference-time methods can consistently adhere to pre-defined resource constraints such as compute budgets or output length targets.

\paragraph{Control Metric}. 
\citet{muennighoff2025s1} propose \textbf{Control} as a formal metric to quantify adherence to a specified compute budget range. It measures the fraction of test-time compute values that stay within given upper and lower bounds:
\[
\text{Control} = \frac{1}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} \mathbb{I}(a_{\min} \leq a \leq a_{\max}),
\]
where $\mathcal{A}$ is the set of observed compute values such as thinking
tokens, and $\mathbb{I}(\cdot)$ is the indicator function. A score of 100\% denotes perfect adherence to the compute budget across all tasks.
Additionally, \citet{hou2025advancing} and \citet{yang2025towards} report experiments where models are evaluated under fixed token budgets, \emph{e.g.}, {1024, 2048, 4096}, to examine how well models meet pre-specified length or token constraints during reasoning.
Moreover, \citet{xie2025logic} and \citet{teng2025atom} impose explicit constraints on maximum output lengths to ensure inference-time stability and prevent output truncation. 

\paragraph{Length Deviation Metrics.}
Mean Deviation from Target Length and RMSE of Length Deviation are introduced to quantify a model’s ability to control output length~\citep{aggarwal2025l1}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Mean Deviation from Target Length} quantifies the average relative difference between the generated output length and the target length:
    \[
    \text{Mean Deviation} = \mathbb{E}_{x \sim D}\left[\frac{|n_{\text{generated}} - n_{\text{gold}}|}{n_{\text{gold}}}\right],
    \]
    where $n_{\text{generated}}$ is the model's output length and $n_{\text{gold}}$ is the target length.
    \item \textbf{Root Mean Squared Error (RMSE) of Length Deviation} captures the variance in length control:
    \[
    \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N \left(\frac{n_{\text{generated}, i} - n_{\text{gold}, i}}{n_{\text{gold}, i}}\right)^2}.
    \]
\end{itemize}
Lower values for both metrics indicate more stable and precise length control across samples.

\paragraph{\boldmath$k$--$\epsilon$ Controllability.} 
\citet{bhargava2024whatsmagicwordcontrol} propose \textbf{$k$--$\epsilon$ controllability} as a formal metric to characterize the prompt-based steerability of language models. Unlike metrics focused on compute or length constraints, this metric quantifies whether a model can be guided to produce a target output within a bounded prompt length and allowable deviation. Formally, a model is said to be $(k, \epsilon)$-controllable for a target output $y$ if there exists a prompt $p$ with $|p| \leq k$ such that the model outputs $y$ with probability at least $1 - \epsilon$:
\[
\Pr[\text{LLM}(p) = y] \geq 1 - \epsilon.
\]
By evaluating across different values of $k$ and $\epsilon$, one can map out the controllability landscape of a model. In practice, \citet{bhargava2024whatsmagicwordcontrol} measures this property on tasks such as next-token prediction in WikiText, finding that over 97\% of targets are reachable with a prompt of at most 10 tokens and an error tolerance $\epsilon \leq 0.05$. This metric provides a theoretical lens for quantifying how easily a model's outputs can be controlled via prompt design. While not directly tied to resource constraints, $k$--$\epsilon$ controllability offers valuable insight into the model's test-time responsiveness and has been used to compare inherent steerability across model families and sizes.

\subsection{Scalability}
\label{subsec:scalability}
Scalability metrics measure how effectively test-time scaling methods can leverage increased compute (e.g., token budgets, samples, inference steps) to improve performance.


\paragraph{Scaling Metric}
\citet{muennighoff2025s1} propose the \textbf{Scaling} metric, capturing the average slope of performance gains as compute increases:
\[
\text{Scaling} = \frac{1}{\binom{|\mathcal{A}|}{2}} \sum_{\substack{a, b \in \mathcal{A} \\ b > a}} \frac{f(b) - f(a)}{b - a}.
\]
This metric quantifies how effectively models improve accuracy or pass rates with additional computation.

\paragraph{Scaling Curves (Accuracy vs. Compute).}
Scaling curves are used to visualize how metrics such as accuracy, pass rate, or EM improve as token budgets, iteration depth, or the number of samples increase~\citep{aggarwal2025l1, teng2025atom, wu2024inference}. These plots help reveal diminishing returns and performance saturation at higher compute budgets.



