\section{Where to Scale} 
\label{sec:where2scale}
% Domains \& tasks
% TTS can substantially enhance LLMs' performance across diverse real-world scenarios. We systematically categorize these scenarios into representative domains, detailing the characteristic challenges, critical evaluation criteria, and representative benchmarks that illustrate the practical value of TTS.


% \begin{table*}[ht]
% \caption{Summary of Benchmarks}
% \label{tab:benchmark-summary-1}
% \begin{adjustbox}{width=\textwidth,keepaspectratio}
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Benchmark} & \textbf{Size} & \textbf{Evaluation Criteria} & \textbf{Example Task} & \textbf{Key Features} & \textbf{Type} \\
% \midrule
% \multicolumn{6}{c}{\textbf{Reasoning-intensive Tasks}} \\
% \midrule
% FrontierMath~\citep{glazer2024frontiermath} & Hundreds & Exact match & Algebraic geometry & High complexity & \multirow{8}{*}{Math} \\
% MATH~\citep{cobbe2021training} & 12.5K & Exact match & AMC/AIME-style & Structured reasoning &  \\
% NuminaMath~\citep{numina_math_datasets} & 860K & Exact match, CoT & Olympiad-level math & Annotated reasoning &  \\
% OmniMath~\citep{gao2025omnimath} & 4.4K & Accuracy & Math Olympiads & Advanced reasoning &  \\
% GSM8K~\citep{zhang2024rest} & 8.5K & Accuracy & Grade-school math & Natural-language solutions &  \\
% rStar-Math~\citep{guan2025rstarmath} & 747K & Pass@1 accuracy & Competition math & Iterative refinement &  \\
% ReST-MCTS~\citep{zhang2024rest} & Varied & Accuracy & Multi-step reasoning & Reward-guided search &  \\
% s1~\citep{muennighoff2025s1} & 1K & Accuracy & Math/science tasks & Controlled compute &  \\
% \midrule
% USACO~\citep{shi2024can} & 307 & Pass@1 & Olympiad coding & Creative algorithms & \multirow{4}{*}{Code} \\
% AlphaCode~\citep{Li2022competition} & Thousands & Solve rate & Competitive coding & Complex algorithms &  \\
% LiveCodeBench~\citep{jain2025livecodebench} & 511 & Pass@1 & Real-time coding & Live evaluation &  \\
% SWE-bench~\citep{jimenez2024swebench} & 2.3K & Resolution rate & GitHub issues & Multi-file edits &  \\
% \midrule
% \multicolumn{6}{c}{\textbf{Domain-specific \& Expert Tasks}} \\
% \midrule
% GPQA~\citep{rein2024gpqa} & 448 & Accuracy & Graduate STEM & Domain expertise & \multirow{4}{*}{Science} \\
% OlympicArena~\citep{huang2024olympicarena} & 11.1K & Accuracy & Multidisciplinary tasks & Multimodal reasoning &  \\
% OlympiadBench~\citep{he2024olympiadbench} & 8.4K & Accuracy & Math/Physics Olympiads & Expert multimodal tasks &  \\
% TheoremQA~\citep{chen2023theoremqa} & 800 & Accuracy & Theorem-based STEM & Theoretical application &  \\
% \midrule
% MedQA~\citep{jin2020diseasedoespatienthave} & 1.3K & Accuracy & Clinical diagnostics & Medical accuracy & Medical \\
% \midrule
% \multicolumn{6}{c}{\textbf{Knowledge-intensive Tasks}} \\
% \midrule
% SimpleQA~\citep{wei2024measuringshortformfactualitylarge} & 4.3K & Accuracy & Short queries & Factual correctness & \multirow{3}{*}{QA} \\
% C-SimpleQA~\citep{he2024chinesesimpleqachinesefactuality} & 3K & Accuracy & Chinese queries & Cultural relevance &  \\
% FRAMES~\citep{krishna2025factfetchreasonunified} & 824 & Accuracy & Multi-hop queries & Source aggregation &  \\
% \midrule
% \multicolumn{6}{c}{\textbf{Agentic Tasks}} \\
% \midrule
% WebShop~\citep{yao2023webshop} & 1.18M & Task success & Online shopping & Real-world interaction & \multirow{4}{*}{Agentic} \\
% WebArena~\citep{zhou2023webarena} & Varied & Task completion & Web navigation tasks & Adaptive decision-making &  \\
% SciWorld~\citep{wang2022sciworld} & 30 tasks & Task-specific scores & Scientific experiments & Interactive simulation &  \\
% TextCraft~\citep{prasad2024adaptasneededdecompositionplanning} & Varied & Success rate & Task decomposition & Iterative planning &  \\
% \midrule
% \multicolumn{6}{c}{\textbf{General-purpose \& Open-ended Tasks}} \\
% \midrule
% AGIEval~\citep{zhong2024agieval} & 8K & Accuracy & College exams & Human-centric reasoning & \multirow{7}{*}{General} \\
% MMLU-Pro~\citep{wang2024mmlupro} & 12K & Accuracy & Multidisciplinary tests & Deep reasoning complexity &  \\
% C-Eval~\citep{huang2023ceval} & 13.9K & Accuracy & Chinese exams & Multidisciplinary reasoning &  \\
% Gaokao~\citep{gaokao} & Varied & Accuracy & Chinese college exams & Broad knowledge & \\
% Kaoyan~\citep{kaoyan} & Varied & Accuracy & Graduate entry exams & Specialized knowledge &  \\
% CMMLU~\citep{li2024cmmlumeasuringmassivemultitask} & Varied & Accuracy & Multi-task Chinese eval & Comprehensive coverage &  \\
% LongBench~\citep{bai2024longbenchbilingualmultitaskbenchmark} & Varied & Accuracy & Bilingual multi-task eval & Long-form reasoning &  \\
% \midrule
% IF-Eval~\citep{zhou2023instructionfollowing} & 541 & Accuracy & Instruction adherence & Objective evaluation & \multirow{4}{*}{Open-ended} \\
% ArenaHard~\citep{li2024crowdsourceddatahighqualitybenchmarks} & 500 & Human preference & Open-ended creativity & Human alignment &  \\
% Chatbot Arena~\citep{zheng2023judgingllmasajudgemtbenchchatbot} & Varied & Human alignment & Chatbot quality & User-aligned responses &  \\
% AlpacaEval2.0~\citep{dubois2024lengthcontrolledalpacaevalsimpleway} & 805 & Win rate & Chatbot responses & Debiased evaluation &  \\
% \midrule
% \multicolumn{6}{c}{\textbf{Multimodal Tasks}} \\
% \midrule
% MMMU~\citep{yue2024mmmu} & 11.5K & Accuracy & Multimodal expert tasks & Multidisciplinary integration & \multirow{9}{*}{Vision} \\
% MathVista~\citep{lu2024mathvista} & 6.1K & Accuracy & Visual math reasoning & Visual-math integration &  \\
% MATH-Vision~\citep{wang2024measuring} & 3K & Accuracy & Visual math problems & Multimodal math reasoning &  \\
% LLAVA-Wild~\citep{liu2023visual} & Varied & GPT-4 score & Visual QA & Complex visuals & \\
% MM-Vet~\citep{yu2024mm} & Varied & GPT-4 evaluation & Integrated multimodal & Multi-capability eval &  \\
% MMBench~\citep{liu2024mmbench} & 3.2K & Accuracy & Diverse multimodal & Fine-grained eval &  \\
% CVBench~\citep{tong2024cambrian} & Varied & Accuracy & Vision tasks & High-quality eval &  \\
% MMStar~\citep{chen2024we} & 1.5K & Accuracy & Vision-critical QA & Visual reliance &  \\
% CHAIR~\citep{rohrbach2018object} & Varied & Hallucination rate & Image captioning & Object hallucination &  \\

% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table*}

% \subsection{Reasoning-intensive Tasks}

% Reasoning-intensive tasks require structured, explicit, multi-step reasoning, precision, and rigorous correctness verification. These tasks challenge LLMs' ability to systematically decompose problems, iteratively refine solutions, and verify intermediate reasoning steps.

% \paragraph{Mathematical Reasoning}

% Mathematical tasks involve complex computations, logical inference, and iterative verification. Key challenges for TTS methods include generating accurate step-by-step solutions, effectively verifying intermediate steps, and handling intricate reasoning logic. Representative benchmarks include AIME 2024~\citep{aime25}, MATH-500~\citep{zhang2024rest}, AMC 2023~\citep{guan2025rstarmath}, and OlympiadBench~\citep{he2024olympiadbench}. These datasets span advanced competition-level math problems, emphasizing precise and explicit reasoning skills.

% \paragraph{Programming \& Code Generation}
% Coding tasks demand syntactic accuracy, executable correctness, and iterative debugging. Challenges for TTS methods lie in generating correct implementations, debugging code iteratively, and efficiently exploring multiple coding solutions. Representative datasets include Codeforces~\citep{codeforce}, SWE-bench~\citep{jimenez2024swebench}, and LiveCodeBench~\citep{jain2025livecodebench}, each providing expert-level coding challenges that require rigorous logical thinking and implementation accuracy.


% \paragraph{Game Playing and Strategic Reasoning}
% Strategic reasoning tasks involve adaptive planning, interactive decision-making, and complex multi-round reasoning. TTS methods must efficiently perform iterative search, adaptive inference, and dynamic interactions. A representative benchmark is SysBench~\citep{aime25}, evaluating models' strategic reasoning in interactive tasks.

% \subsection{Domain-specific \& Expert Tasks}
% Tasks in this category involve specialized professional knowledge, requiring LLMs to reason accurately within specific fields like science and medicine.

% \paragraph{Scientific Reasoning}

% Scientific problems typically require multi-domain knowledge integration across physics, chemistry, biology, and other disciplines. TTS methods must demonstrate broad knowledge synthesis, multi-step reasoning, and accurate factual verification. Notable benchmarks include GPQA Diamond~\citep{rein2024gpqa}, MR-Ben~\citep{zeng2024mrben}, and MMLU-Pro~\citep{wang2024mmlupro}, focusing on advanced scientific reasoning and integrated domain knowledge.


% \paragraph{Medical Reasoning}
% Medical tasks involve diagnostic decision-making, clinical reasoning, and precise medical knowledge. The key challenge for TTS here is ensuring reliable, accurate reasoning mimicking medical professionals’ decision logic. Representative datasets include JAMA Clinical Challenge~\citep{chen2025benchmarkinglargelanguagemodels}, Medbullets~\citep{chen2025benchmarkinglargelanguagemodels}, and MedQA~\citep{jin2020diseasedoespatienthave}. These benchmarks critically assess reasoning LLMs’ capabilities in diagnosis, treatment planning, and medical decision accuracy.



% \subsection{Knowledge-intensive Tasks}

% Knowledge-intensive tasks require LLMs to retrieve and synthesize factual knowledge from external sources, ensuring accuracy and reducing hallucinations. TTS challenges center around effective retrieval-augmented reasoning, iterative verification, and multi-source aggregation. Representative benchmarks include SimpleQA~\citep{wei2024measuringshortformfactualitylarge}, C-SimpleQA~\citep{he2024chinesesimpleqachinesefactuality}, and FRAMES~\citep{krishna2025factfetchreasonunified}, emphasizing factual correctness and retrieval-based reasoning.

% \subsection{Agentic Tasks}

% Agentic tasks involve realistic and interactive environments, requiring complex planning, iterative reasoning, and effective tool utilization. TTS methods face challenges such as optimal stepwise planning, adaptive decision-making, tool integration, and iterative refinement. Representative benchmarks include WebShop~\citep{yao2023webshop}, WebArena~\citep{zhou2023webarena}, SciWorld~\citep{wang2022sciworld}, and TextCraft~\citep{prasad2024adaptasneededdecompositionplanning}. These datasets provide realistic interactive scenarios, emphasizing iterative decision-making and effective tool usage.

% \subsection{General-purpose \& Open-ended Tasks}
% These tasks require broad, general-purpose reasoning capabilities, creativity, and subjective evaluation of outputs. TTS methods must enhance output diversity, quality, and coherence, balancing between creativity and correctness. Representative benchmarks include AGIEval~\citep{zhong2024agieval}, AlpacaEval2.0~\citep{dubois2024lengthcontrolledalpacaevalsimpleway}, ArenaHard~\citep{li2024crowdsourceddatahighqualitybenchmarks}, IF-Eval~\citep{zhou2023instructionfollowing}, and C-Eval~\citep{huang2023ceval}, which collectively evaluate subjective, open-ended, and general-purpose reasoning.

% \subsection{Multimodal Tasks}

% Multimodal reasoning tasks demand effective cross-modal integration, iterative reasoning between modalities, and robust verification across visual and textual inputs. TTS methods face challenges in modality fusion, iterative multimodal reasoning, and handling ambiguity across modalities. Representative benchmarks include MMMU~\citep{yue2024mmmu}, MathVista~\citep{lu2024mathvista}, MathVision~\citep{wang2024measuring}, CMMaTH~\citep{li2025cmmath}, and PGPS9K~\citep{zhang2023multimodalneuralgeometricsolver}, each testing multimodal reasoning across visual and textual modalities.

\TTS can substantially enhance LLMs' performance across diverse real-world scenarios. We systematically categorize these scenarios into representative domains, detailing the characteristic challenges, critical evaluation criteria, and representative benchmarks that illustrate the practical value of TTS. Here, we also list a brief summary of various benchmarks in Table~\ref{tab:benchmark-summary-1}.

\subsection{Reasoning-intensive Tasks}
\label{sec:reasoning}

Reasoning-intensive tasks require structured, explicit, multi-step reasoning, precision, and rigorous correctness verification. These tasks challenge LLMs' ability to systematically decompose problems, iteratively refine solutions, and verify intermediate reasoning steps.

\paragraph{Mathematical Reasoning}

Mathematical tasks involve complex computations, logical inference, and iterative verification. Key challenges for \TTS methods include generating accurate step-by-step solutions, effectively verifying intermediate steps, and handling intricate reasoning logic. Representative benchmarks include AIME 2024~\citep{aime25}, MATH-500~\citep{zhang2024rest}, AMC 2023~\citep{guan2025rstarmath}, and OlympiadBench~\citep{he2024olympiadbench}. These datasets span advanced competition-level math problems, emphasizing precise and explicit reasoning skills.

\paragraph{Programming \& Code Generation}
Coding tasks demand syntactic accuracy, executable correctness, and iterative debugging. Challenges for \TTS methods lie in generating correct implementations, debugging code iteratively, and efficiently exploring multiple coding solutions. Representative datasets include Codeforces~\citep{codeforce}, SWE-bench~\citep{jimenez2024swebench}, and LiveCodeBench~\citep{jain2025livecodebench}, each providing expert-level coding challenges that require rigorous logical thinking and implementation accuracy.


\paragraph{Game Playing and Strategic Reasoning}
Strategic reasoning tasks involve adaptive planning, interactive decision-making, and complex multi-round reasoning. \TTS methods must efficiently perform iterative search, adaptive inference, and dynamic interactions. A representative benchmark is SysBench~\citep{aime25}, which evaluates models' strategic reasoning in interactive tasks.

%\subsection{Domain-specific \& Expert Tasks}
%Tasks in this category involve specialized professional knowledge, requiring LLMs to reason accurately within specific fields like science and medicine.

\paragraph{Scientific Reasoning}
Scientific problems typically require multi-domain knowledge integration across physics, chemistry, biology, and other disciplines. \TTS methods must demonstrate broad knowledge synthesis, multi-step reasoning, and accurate factual verification. Notable benchmarks include GPQA Diamond~\citep{rein2024gpqa} and MR-Ben~\citep{zeng2024mrben}, focusing on advanced scientific reasoning and integrated domain knowledge.


\paragraph{Medical Reasoning}
Medical tasks involve diagnostic decision-making, clinical reasoning, and precise medical knowledge. The key challenge for \TTS here is ensuring reliable, accurate reasoning that mimics medical professionals’ decision logic. Representative datasets include JAMA Clinical Challenge~\citep{chen2025benchmarkinglargelanguagemodels}, Medbullets~\citep{chen2025benchmarkinglargelanguagemodels}, and MedQA~\citep{jin2020diseasedoespatienthave}. These benchmarks critically assess reasoning LLMs’ capabilities in diagnosis, treatment planning, and medical decision accuracy.



\subsection{Others}
\label{sec:generalpurpose}
These tasks require broad, general-purpose reasoning capabilities, creativity, and subjective evaluation of outputs. 

\paragraph{General}
To achieve general objectives, many efforts have collected numerous official, public datasets that are challenging for humans but are not exclusive to any particular domain. Representative benchmarks include AGIEval~\citep{zhong2024agieval}, MMLU-Pro~\citep{wang2024measuring}, and Gaokao~\citep{guan2025rstarmath}. These benchmarks may cover multiple aspects of language models and aim to test their general performance.

\paragraph{Open-Ended Tasks}
\TTS methods must enhance output diversity, quality, and coherence, balancing creativity and correctness. Representative benchmarks include AlpacaEval2.0~\citep{dubois2024lengthcontrolledalpacaevalsimpleway}, ArenaHard~\citep{li2024crowdsourceddatahighqualitybenchmarks}, IF-Eval~\citep{zhou2023instructionfollowing}, and C-Eval~\citep{huang2023ceval}, which collectively evaluate subjective, open-ended, and general-purpose reasoning.


\paragraph{Agentic Tasks}
Agentic tasks involve realistic and interactive environments, requiring complex planning, iterative reasoning, and effective tool utilization. \TTS methods face challenges such as optimal stepwise planning, adaptive decision-making, tool integration, and iterative refinement. Representative benchmarks include WebShop~\citep{yao2023webshop}, WebArena~\citep{zhou2023webarena}, SciWorld~\citep{wang2022sciworld}, and TextCraft~\citep{prasad2024adaptasneededdecompositionplanning}. These datasets provide realistic interactive scenarios, emphasizing iterative decision-making and effective tool usage. Recent advances in scaling LLM-driven autonomous agents center on improved planning, memory, and self-optimization techniques. For example, ARMAP~\citep{chen2025scalingautonomousagentsautomatic} automatically learns a reward model from unlabeled environment interactions to score and guide an agent's actions, circumventing the need for human-labeled feedback and improving multi-step decision-making.


\paragraph{Knowledge-intensive Tasks}
Knowledge-intensive tasks require LLMs to retrieve and synthesize factual knowledge from external sources, ensuring accuracy and reducing hallucinations. \TTS challenges center around effective retrieval-augmented reasoning, iterative verification, and multi-source aggregation. Representative benchmarks include SimpleQA~\citep{wei2024measuringshortformfactualitylarge}, C-SimpleQA~\citep{he2024chinesesimpleqachinesefactuality}, and FRAMES~\citep{krishna2025factfetchreasonunified}, emphasizing factual correctness and retrieval-based reasoning.

\paragraph{Evaluation Tasks}

Evaluation tasks require LLMs to act as judges, also known as Generative Reward Models (GRMs), to conduct comprehensive and in-depth quality assessments of the candidate responses, thus ensuring reliable evaluation results. Representative benchmarks in this field include RewardBench~\citep{lambert2024rewardbenchevaluatingrewardmodels}, JudgeBench~\citep{tan2025judgebenchbenchmarkevaluatingllmbased}, RMBench~\citep{liu2024rmbenchbenchmarkingrewardmodels}, PPE~\citep{frick2024evaluaterewardmodelsrlhf}, and RMB~\citep{zhou2025rmbcomprehensivelybenchmarkingreward}. Recent research~\citep{kim2025scalingevaluationtimecomputereasoning} has demonstrated that \TTS effectively enhances the evaluation reasoning capabilities of LLMs. For instance, CCE~\citep{zhang2025crowd} scales the evaluation by comparing the candidate responses with other crowd-generated responses, enabling TTS evaluation effects. EvalPlan~\citep{saha2025learningplanreason} achieves deeper evaluation by first generating a specific evaluation plan tailored to the candidate responses. SPCT~\citep{liu2025inferencetimescalinggeneralistreward} goes a step further by employing RL to generate evaluation principles, further activating the \TTS potential. Additionally, JudgeLRM~\citep{chen2025judgelrmlargereasoningmodels} has validated that training using the R1 method can effectively enhance the performance of RMs, while MAV~\citep{lifshitz2025multiagent} introduces multiple aspect verifiers.  Notably, improving evaluator accuracy in Out-of-Distribution scenarios remains a critical issue, like Reward Hacking~\citep{skalse2025definingcharacterizingrewardhacking,shen2025exploringdatascalingtrends}, worthy of deeper exploration. 


\paragraph{Multimodal Tasks}
Multimodal reasoning tasks demand effective cross-modal integration, iterative reasoning between modalities, and robust verification across visual and textual inputs. \TTS methods face challenges in modality fusion, iterative multimodal reasoning, and handling ambiguity across modalities. Representative benchmarks include MMMU~\citep{yue2024mmmu}, MathVista~\citep{lu2024mathvista}, MathVision~\citep{wang2024measuring}, CMMaTH~\citep{li2025cmmath}, and PGPS9K~\citep{zhang2023multimodalneuralgeometricsolver}, each testing multimodal reasoning across visual and textual modalities.

%\subsection{Summary of Benchmarks}




\input{section/table-where}