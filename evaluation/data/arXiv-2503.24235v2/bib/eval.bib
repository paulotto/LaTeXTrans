@article{teng2025atom,
  title={Atom of thoughts for markov llm test-time scaling},
  author={Teng, Fengwei and Yu, Zhaoyang and Shi, Quan and Zhang, Jiayi and Wu, Chenglin and Luo, Yuyu},
  journal={arXiv preprint arXiv:2502.12018},
  year={2025}
}

@article{jin2025search,
  title={Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning},
  author={Jin, Bowen and Zeng, Hansi and Yue, Zhenrui and Wang, Dong and Zamani, Hamed and Han, Jiawei},
  journal={arXiv preprint arXiv:2503.09516},
  year={2025}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}

@article{welleck2024decoding,
  title={From decoding to meta-generation: Inference-time algorithms for large language models},
  author={Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:2406.16838},
  year={2024}
}

@article{zhang2024scaling,
  title={Scaling llm inference with optimized sample compute allocation},
  author={Zhang, Kexun and Zhou, Shang and Wang, Danqing and Wang, William Yang and Li, Lei},
  journal={arXiv preprint arXiv:2410.22480},
  year={2024}
}

@article{li2025llms,
  title={LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!},
  author={Li, Dacheng and Cao, Shiyi and Griggs, Tyler and Liu, Shu and Mo, Xiangxi and Patil, Shishir G and Zaharia, Matei and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2502.07374},
  year={2025}
}

@inproceedings{yang2025reasonflux,
  title={ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates}, 
  author={Ling Yang and Zhaochen Yu and Bin Cui and Mengdi Wang},
  year={2025},
  eprint={2502.06772},
  booktitle={arXiv},
  url={https://arxiv.org/abs/2502.06772}, 
}

@article{bi2024forest,
  title={Forest-of-thought: Scaling test-time compute for enhancing LLM reasoning},
  author={Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe},
  journal={arXiv preprint arXiv:2412.09078},
  year={2024},
  url={https://arxiv.org/abs/2412.09078}, 
}

@article{yang2025towards,
  title={Towards thinking-optimal scaling of test-time compute for llm reasoning},
  author={Yang, Wenkai and Ma, Shuming and Lin, Yankai and Wei, Furu},
  journal={arXiv preprint arXiv:2502.18080},
  year={2025}
}

@article{zeng2025revisiting,
  title={Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?},
  author={Zeng, Zhiyuan and Cheng, Qinyuan and Yin, Zhangyue and Zhou, Yunhua and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2502.12215},
  year={2025}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}