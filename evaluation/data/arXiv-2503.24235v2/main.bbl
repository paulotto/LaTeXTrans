\begin{thebibliography}{308}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aggarwal and Welleck(2025)}]{aggarwal2025l1}
Pranjal Aggarwal and Sean Welleck. 2025.
\newblock \href {http://arxiv.org/abs/2503.04697} {L1: Controlling how long a reasoning model thinks with reinforcement learning}.
\newblock In \emph{arXiv}.

\bibitem[{Ahmadian et~al.(2024)Ahmadian, Cremer, Gall{\'e}, Fadaee, Kreutzer, Pietquin, {\"U}st{\"u}n, and Hooker}]{ahmadian2024back}
Arash Ahmadian, Chris Cremer, Matthias Gall{\'e}, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet {\"U}st{\"u}n, and Sara Hooker. 2024.
\newblock \href {https://aclanthology.org/2024.acl-long.662/} {Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 12248--12267.

\bibitem[{aider(2025)}]{aider}
aider. 2025.
\newblock \href {https://aider.chat/} {Aider}.

\bibitem[{AntResearch-RL-Lab(2025)}]{areal2025}
AntResearch-RL-Lab. 2025.
\newblock Areal: Ant reasoning rl.
\newblock \url{https://github.com/inclusionAI/AReaL}.

\bibitem[{Arora et~al.(2023)Arora, Singh, and .}]{arora2023have}
Daman Arora, Himanshu~Gaurav Singh, and Mausam . 2023.
\newblock \href {https://openreview.net/forum?id=YHWXlESeS8} {Have {LLM}s advanced enough? a challenging problem solving benchmark for large language models}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Asai et~al.(2023)Asai, Wu, Wang, Sil, and Hajishirzi}]{asai2023selfraglearningretrievegenerate}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
\newblock \href {https://openreview.net/forum?id=hSyW5go0v8} {Self-rag: Learning to retrieve, generate, and critique through self-reflection}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Aytes et~al.(2025)Aytes, Baek, and Hwang}]{aytes2025sketchofthoughtefficientllmreasoning}
Simon~A. Aytes, Jinheon Baek, and Sung~Ju Hwang. 2025.
\newblock \href {http://arxiv.org/abs/2503.05179} {Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching}.
\newblock In \emph{arXiv}.

\bibitem[{Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan}]{bai2022constitutionalaiharmlessnessai}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022.
\newblock \href {http://arxiv.org/abs/2212.08073} {Constitutional ai: Harmlessness from ai feedback}.
\newblock In \emph{arXiv}.

\bibitem[{Bai et~al.(2024)Bai, Lv, Zhang, Lyu, Tang, Huang, Du, Liu, Zeng, Hou, Dong, Tang, and Li}]{bai2024longbenchbilingualmultitaskbenchmark}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024.
\newblock \href {http://arxiv.org/abs/2308.14508} {Longbench: A bilingual, multitask benchmark for long context understanding}.
\newblock In \emph{arXiv}.

\bibitem[{Bespoke(2025)}]{bespoke_stratos}
Bespoke. 2025.
\newblock Bespoke-stratos: The unreasonable effectiveness of reasoning distillation.
\newblock www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation.
\newblock Accessed: 2025-01-22.

\bibitem[{Besta et~al.(2024)Besta, Blach, Kubicek, Gerstenberger, Podstawski, Gianinazzi, Gajda, Lehmann, Niewiadomski, Nyczyk, and Hoefler}]{Besta2024graph}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock \emph{AAAI Conference on Artificial Intelligence}, page 17682–17690.

\bibitem[{Bhargava et~al.(2024)Bhargava, Witkowski, Looi, and Thomson}]{bhargava2024whatsmagicwordcontrol}
Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, and Matt Thomson. 2024.
\newblock \href {http://arxiv.org/abs/2310.04444} {What's the magic word? a control theory of llm prompting}.
\newblock In \emph{arXiv}.

\bibitem[{Bi et~al.(2024)Bi, Han, Liu, Tang, and Wang}]{bi2024forest}
Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. 2024.
\newblock \href {https://arxiv.org/abs/2412.09078} {Forest-of-thought: Scaling test-time compute for enhancing llm reasoning}.
\newblock \emph{arXiv preprint arXiv:2412.09078}.

\bibitem[{Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, Ré, and Mirhoseini}]{brown2024large}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V. Le, Christopher Ré, and Azalia Mirhoseini. 2024.
\newblock \href {http://arxiv.org/abs/2407.21787} {Large language monkeys: Scaling inference compute with repeated sampling}.
\newblock In \emph{arXiv}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{brown2020languagemodelsfewshotlearners}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href {http://arxiv.org/abs/2005.14165} {Language models are few-shot learners}.
\newblock In \emph{arXiv}.

\bibitem[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang}]{bubeck2023sparksartificialgeneralintelligence}
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco~Tulio Ribeiro, and Yi~Zhang. 2023.
\newblock \href {http://arxiv.org/abs/2303.12712} {Sparks of artificial general intelligence: Early experiments with gpt-4}.
\newblock In \emph{arXiv}.

\bibitem[{Chaffin et~al.(2022)Chaffin, Claveau, and Kijak}]{chaffin2022ppl}
Antoine Chaffin, Vincent Claveau, and Ewa Kijak. 2022.
\newblock \href {https://aclanthology.org/2022.naacl-main.215/} {Ppl-mcts: Constrained textual generation through discriminator-guided mcts decoding}.
\newblock In \emph{NAACL 2022-Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 1--15.

\bibitem[{Chakraborty et~al.(2025)Chakraborty, Bhatt, Sehwag, Ghosal, Qiu, Wang, Manocha, Huang, Koppel, and Ganesh}]{chakraborty2025collab}
Souradip Chakraborty, Sujay Bhatt, Udari~Madhushani Sehwag, Soumya~Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, and Sumitra Ganesh. 2025.
\newblock \href {https://openreview.net/forum?id=7ohlQUbTpp} {Collab: Controlled decoding using mixture of agents for {LLM} alignment}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Chen et~al.(2024{\natexlab{a}})Chen, Liao, Li, and Fan}]{chenalphamath}
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=VaXnxQ3UKo} {Alphamath almost zero: Process supervision without process}.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Chen et~al.(2025{\natexlab{a}})Chen, Fang, Singla, and Dredze}]{chen2025benchmarkinglargelanguagemodels}
Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2402.18060} {Benchmarking large language models on answering and explaining challenging medical questions}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2024{\natexlab{b}})Chen, Saha, and Bansal}]{chen2024reconcileroundtableconferenceimproves}
Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2309.13007} {Reconcile: Round-table conference improves reasoning via consensus among diverse llms}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2024{\natexlab{c}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin et~al.}]{chen2024we}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, et~al. 2024{\natexlab{c}}.
\newblock Are we on the right way for evaluating large vision-language models?
\newblock \emph{arXiv preprint arXiv:2403.20330}.

\bibitem[{Chen et~al.(2024{\natexlab{d}})Chen, Davis, Hanin, Bailis, Stoica, Zaharia, and Zou}]{chen2024are}
Lingjiao Chen, Jared~Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024{\natexlab{d}}.
\newblock \href {https://openreview.net/forum?id=m5106RRLgx} {Are more {LLM} calls all you need? towards the scaling properties of compound {AI} systems}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Chen et~al.(2025{\natexlab{b}})Chen, Hu, Zou, Wu, Wang, Hooi, and He}]{chen2025judgelrmlargereasoningmodels}
Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2504.00050} {{JudgeLRM}: Large reasoning models as a judge}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2025{\natexlab{c}})Chen, Qin, Liu, Peng, Guan, Wang, Hu, Zhou, Gao, and Che}]{chen2025reasoningerasurveylong}
Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te~Gao, and Wanxiang Che. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2503.09567} {Towards reasoning era: A survey of long chain-of-thought for reasoning large language models}.

\bibitem[{Chen et~al.(2024{\natexlab{e}})Chen, Jiang, Lin, Kwok, and Zhang}]{chen2024routerdcquerybasedrouterdual}
Shuhao Chen, Weisen Jiang, Baijiong Lin, James~T. Kwok, and Yu~Zhang. 2024{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2409.19886} {{RouterDC}: Query-based router by dual contrastive learning for assembling large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2025{\natexlab{d}})Chen, Koenig, and Dilkina}]{chen2025iterativedeepeningsamplinglarge}
Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2025{\natexlab{d}}.
\newblock \href {http://arxiv.org/abs/2502.05449} {Iterative deepening sampling for large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2023{\natexlab{a}})Chen, Ma, Wang, and Cohen}]{chen2023program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen. 2023{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=YfZ4ZPt8zd} {Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks}.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Chen et~al.(2023{\natexlab{b}})Chen, Yin, Ku, Lu, Wan, Ma, Xu, Wang, and Xia}]{chen2023theoremqa}
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2023.emnlp-main.489/} {{T}heorem{QA}: A theorem-driven question answering dataset}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, pages 7889--7901.

\bibitem[{Chen et~al.(2024{\natexlab{f}})Chen, Lin, Sch{\"a}rli, and Zhou}]{chen2024teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou. 2024{\natexlab{f}}.
\newblock \href {https://openreview.net/forum?id=KuPixIqPiq} {Teaching large language models to self-debug}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Chen et~al.(2024{\natexlab{g}})Chen, Chen, and Zhou}]{chen2024braininspiredtwostageapproachenhancing}
Yezeng Chen, Zui Chen, and Yi~Zhou. 2024{\natexlab{g}}.
\newblock \href {http://arxiv.org/abs/2403.00800} {Brain-inspired two-stage approach: Enhancing mathematical reasoning by imitating human thought processes}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2025{\natexlab{e}})Chen, Chen, Sun, Liu, and Gan}]{chen2025scalingautonomousagentsautomatic}
Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. 2025{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2502.12130} {Scaling autonomous agents via automatic reward modeling and planning}.
\newblock In \emph{arXiv}.

\bibitem[{Chen et~al.(2024{\natexlab{h}})Chen, White, Mooney, Payani, Su, and Sun}]{chen2024tree}
Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu~Su, and Huan Sun. 2024{\natexlab{h}}.
\newblock \href {https://aclanthology.org/2024.acl-long.738/} {When is tree search useful for {LLM} planning? it depends on the discriminator}.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, pages 13659--13678.

\bibitem[{Cheng et~al.(2025)Cheng, Liu, Wang, Gu, Lu, Zhang, Dong, Tang, Wang, and Huang}]{cheng2025sparselfplaytreesearchrefinement}
Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. 2025.
\newblock \href {http://arxiv.org/abs/2412.11605} {Spar: Self-play with tree-search refinement to improve instruction-following in large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Chollet(2019)}]{chollet2019measureintelligence}
François Chollet. 2019.
\newblock \href {http://arxiv.org/abs/1911.01547} {On the measure of intelligence}.
\newblock In \emph{arXiv}.

\bibitem[{Choudhury(2025)}]{choudhury2025processrewardmodelsllm}
Sanjiban Choudhury. 2025.
\newblock \href {http://arxiv.org/abs/2502.10325} {Process reward models for llm agents: Practical framework and directions}.
\newblock In \emph{arXiv}.

\bibitem[{CMS(2025)}]{cnmo}
CMS. 2025.
\newblock \href {https://www.cms.org.cn/Home/comp/comp/cid/12.html} {Chinese national high school mathematics olympiad}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{codeforce(2025)}]{codeforce}
codeforce. 2025.
\newblock \href {https://codeforces.com/} {Codeforces}.

\bibitem[{Coulom(2006)}]{coulom2006efficient}
R{\'e}mi Coulom. 2006.
\newblock Efficient selectivity and backup operators in monte-carlo tree search.
\newblock In \emph{International conference on computers and games}, pages 72--83. Springer.

\bibitem[{Cui et~al.(2025)Cui, Yuan, Wang, Wang, Li, He, Fan, Yu, Xu, Chen et~al.}]{cui2025process}
Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et~al. 2025.
\newblock \href {https://arxiv.org/abs/2502.01456} {Process reinforcement through implicit rewards}.
\newblock \emph{arXiv preprint arXiv:2502.01456}.

\bibitem[{DeepSeek-AI(2025)}]{deepseek-r1}
DeepSeek-AI. 2025.
\newblock \href {http://arxiv.org/abs/2501.12948} {Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning}.
\newblock In \emph{arXiv}.

\bibitem[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto}]{dubois2024lengthcontrolledalpacaevalsimpleway}
Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori~B. Hashimoto. 2024.
\newblock \href {http://arxiv.org/abs/2404.04475} {Length-controlled alpacaeval: A simple way to debias automatic evaluators}.
\newblock In \emph{arXiv}.

\bibitem[{Edward~Beeching(2024)}]{beenching2024scaling}
Sasha~Rush Edward~Beeching, Lewis~Tunstall. 2024.
\newblock \href {https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute} {Scaling test-time compute with open models}.

\bibitem[{Evans(1984)}]{Evans1984-EVAHAA}
Jonathan Evans. 1984.
\newblock Heuristic and analytic processes in reasoning.
\newblock \emph{British Journal of Psychology}, 75(4):451--468.

\bibitem[{Feng et~al.(2024)Feng, Htut, Qi, Xiao, Mager, Pappas, Halder, Li, Benajiba, and Roth}]{feng2024diverseagententropyquantifyingblackboxllm}
Yu~Feng, Phu~Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, and Dan Roth. 2024.
\newblock \href {http://arxiv.org/abs/2412.09572} {Diverseagententropy: Quantifying black-box llm uncertainty through diverse perspectives and multi-agent interaction}.
\newblock In \emph{arXiv}.

\bibitem[{Ferraz et~al.(2024)Ferraz, Mehta, Lin, Chang, Oraby, Liu, Subramanian, Chung, Bansal, and Peng}]{ferraz2024llmselfcorrectiondecrimdecompose}
Thomas~Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, and Nanyun Peng. 2024.
\newblock \href {https://aclanthology.org/2024.findings-emnlp.458/} {Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 7773--7812.

\bibitem[{Frick et~al.(2024)Frick, Li, Chen, Chiang, Angelopoulos, Jiao, Zhu, Gonzalez, and Stoica}]{frick2024evaluaterewardmodelsrlhf}
Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios~N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph~E. Gonzalez, and Ion Stoica. 2024.
\newblock \href {http://arxiv.org/abs/2410.14872} {How to evaluate reward models for rlhf}.
\newblock In \emph{arXiv}.

\bibitem[{Gandhi et~al.(2024)Gandhi, Lee, Grand, Liu, Cheng, Sharma, and Goodman}]{gandhi2024streams}
Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah~D. Goodman. 2024.
\newblock \href {http://arxiv.org/abs/2404.03683} {Stream of search (sos): Learning to search in language}.
\newblock In \emph{arXiv}.

\bibitem[{Gao et~al.(2024{\natexlab{a}})Gao, Cai, Xu, Wang, Zheng, Lin, Lu, Liu, Zhou, Xiao, Hu, Liu, and Chang}]{gao2024llmcriticshelpcatch}
Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce~Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2406.14024} {Llm critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback}.
\newblock In \emph{arXiv}.

\bibitem[{Gao et~al.(2025{\natexlab{a}})Gao, Song, Yang, Cai, Miao, Ma, Quan, Chen, Dong, Xu, Tang, Wang, Zan, Zhang, Li, Sha, Zhang, Ren, Liu, and Chang}]{gao2025omnimath}
Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Chenghao Ma, Shanghaoran Quan, Liang Chen, Qingxiu Dong, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Ge~Zhang, Lei Li, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2025{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=yaqPf0KAlN} {Omni-{MATH}: A universal olympiad level mathematic benchmark for large language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Gao et~al.(2025{\natexlab{b}})Gao, Li, Liu, Xie, Zhao, and Xu}]{gao2025principled}
Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, and Zhiqiang Xu. 2025{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2502.09650} {Principled data selection for alignment: The hidden risks of difficult examples}.
\newblock \emph{arXiv preprint arXiv:2502.09650}.

\bibitem[{Gao et~al.(2024{\natexlab{b}})Gao, Niu, He, Xu, Liu, Liu, Hu, and Wen}]{gao2024interpretable}
Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2410.01707} {Interpretable contrastive monte carlo tree search reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Geiping et~al.(2025)Geiping, McLeish, Jain, Kirchenbauer, Singh, Bartoldson, Kailkhura, Bhatele, and Goldstein}]{geiping2025scalingtesttimecomputelatent}
Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian~R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025.
\newblock \href {http://arxiv.org/abs/2502.05171} {Scaling up test-time compute with latent reasoning: A recurrent depth approach}.
\newblock In \emph{arXiv}.

\bibitem[{Glazer et~al.(2024)Glazer, Erdil, Besiroglu, Chicharro, Chen, Gunning, Olsson, Denain, Ho, de~Oliveira~Santos, Järviniemi, Barnett, Sandler, Vrzala, Sevilla, Ren, Pratt, Levine, Barkley, Stewart, Grechuk, Grechuk, Enugandla, and Wildon}]{glazer2024frontiermath}
Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline~Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de~Oliveira~Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav~Varma Enugandla, and Mark Wildon. 2024.
\newblock \href {http://arxiv.org/abs/2411.04872} {Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai}.

\bibitem[{Goertzel(2014)}]{Goertzel2014148}
Ben Goertzel. 2014.
\newblock Artificial general intelligence: Concept, state of the art, and future prospects.
\newblock \emph{Journal of Artificial General Intelligence}, pages 1--48.

\bibitem[{Google(2024)}]{geminithinking}
Google. 2024.
\newblock \href {https://cloud.google.com/vertex-ai/generative-ai/docs/thinking-mode} {Gemini 2.0 flash thinking}.

\bibitem[{Google(2025)}]{aime25}
Google. 2025.
\newblock \href {https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions?srsltid=AfmBOoqMRu7nCAWAghgecAPTxAuvFXG_dCw_lClA52zkIAtNCYHJ0XSZ} {Aime problems and solutions}.

\bibitem[{Gou et~al.(2024)Gou, Shao, Gong, yelong shen, Yang, Duan, and Chen}]{gou2024critic}
Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.
\newblock \href {https://openreview.net/forum?id=Sx038qxjek} {{CRITIC}: Large language models can self-correct with tool-interactive critiquing}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{GSEE(2025)}]{kaoyan}
GSEE. 2025.
\newblock Chinese graduate school entrance examinations.

\bibitem[{Guan et~al.(2025)Guan, Zhang, Liu, Shang, Sun, Zhu, Yang, and Yang}]{guan2025rstarmath}
Xinyu Guan, Li~Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi~Zhu, Fan Yang, and Mao Yang. 2025.
\newblock \href {http://arxiv.org/abs/2501.04519} {rstar-math: Small llms can master math reasoning with self-evolved deep thinking}.
\newblock In \emph{arXiv}.

\bibitem[{Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot, Ferret, and Blondel}]{guo2024direct}
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. 2024.
\newblock \href {http://arxiv.org/abs/2402.04792} {Direct language model alignment from online ai feedback}.

\bibitem[{Han et~al.(2025)Han, Wang, Fang, Zhao, Ma, and Chen}]{han2025tokenbudgetawarellmreasoning}
Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2025.
\newblock \href {http://arxiv.org/abs/2412.18547} {Token-budget-aware llm reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Hao et~al.(2024)Hao, Sukhbaatar, Su, Li, Hu, Weston, and Tian}]{hao2024training}
Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024.
\newblock \href {https://arxiv.org/abs/2412.06769} {Training large language models to reason in a continuous latent space}.
\newblock \emph{arXiv preprint arXiv:2412.06769}.

\bibitem[{He et~al.(2024{\natexlab{a}})He, Luo, Bai, Hu, Thai, Shen, Hu, Han, Huang, Zhang, Liu, Qi, Liu, and Sun}]{he2024olympiadbench}
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu~Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024{\natexlab{a}}.
\newblock {O}lympiad{B}ench: A challenging benchmark for promoting {AGI} with olympiad-level bilingual multimodal scientific problems.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, pages 3828--3850.

\bibitem[{He et~al.(2025)He, Zou, Li, Chen, Xing, and Ma}]{he2025enhancingllmreasoningmultipath}
Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, and Huimin Ma. 2025.
\newblock \href {http://arxiv.org/abs/2501.00430} {Enhancing llm reasoning with multi-path collaborative reactive and reflection agents}.
\newblock In \emph{arXiv}.

\bibitem[{He et~al.(2024{\natexlab{b}})He, Yao, Ma, Yu, Dai, Zhang, Lan, and Yu}]{he2024webvoyagerbuildingendtoendweb}
Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2401.13919} {Webvoyager: Building an end-to-end web agent with large multimodal models}.
\newblock In \emph{arXiv}.

\bibitem[{He et~al.(2024{\natexlab{c}})He, Li, Liu, Tan, Wang, Huang, Bu, Guo, Hu, Zheng, Lin, Liu, Sun, Lin, Zheng, Zhu, Su, and Zheng}]{he2024chinesesimpleqachinesefactuality}
Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, and Bo~Zheng. 2024{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2411.07140} {Chinese simpleqa: A chinese factuality evaluation for large language models}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.
\newblock \href {https://openreview.net/forum?id=7Bywt2mQsCe} {Measuring mathematical problem solving with the {MATH} dataset}.
\newblock In \emph{Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre}]{hoffmann2022trainingcomputeoptimallargelanguage}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre. 2022.
\newblock \href {http://arxiv.org/abs/2203.15556} {Training compute-optimal large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Hong et~al.(2024)Hong, Pang, and Zhang}]{10460413}
Ruixin Hong, Xinyu Pang, and Changshui Zhang. 2024.
\newblock Advances in reasoning by prompting large language models: A survey.
\newblock \emph{Cybernetics and Intelligence}, pages 1--15.

\bibitem[{Hooper et~al.(2025)Hooper, Kim, Moon, Dilmen, Maheswaran, Lee, Mahoney, Shao, Keutzer, and Gholami}]{hooper2025etsefficienttreesearch}
Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael~W. Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. 2025.
\newblock \href {http://arxiv.org/abs/2502.13575} {Ets: Efficient tree search for inference-time scaling}.
\newblock In \emph{arXiv}.

\bibitem[{Hosseini et~al.(2024)Hosseini, Yuan, Malkin, Courville, Sordoni, and Agarwal}]{hosseini2024vstartrainingverifiersselftaught}
Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024.
\newblock \href {https://openreview.net/forum?id=stmqBSW2dV} {V-star: Training verifiers for self-taught reasoners}.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[{Hou et~al.(2025)Hou, Lv, Lu, Zhang, Li, Yao, Li, Tang, and Dong}]{hou2025advancing}
Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025.
\newblock \href {http://arxiv.org/abs/2501.11651} {Advancing language model reasoning through reinforcement learning and inference scaling}.

\bibitem[{Hu et~al.(2025{\natexlab{a}})Hu, Liu, and Wei}]{hu2025reinforce++}
Jian Hu, Jason~Klein Liu, and Shen Wei. 2025{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2501.03262} {Reinforce++: A simple and efficient approach for aligning large language models}.
\newblock \emph{arXiv preprint arXiv:2501.03262}.

\bibitem[{Hu et~al.(2024)Hu, Wu, Zhu, Xianyu, Wang, Zhang, and Cao}]{hu2024openrlhf}
Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu~Cao. 2024.
\newblock \href {https://arxiv.org/abs/2405.11143} {Openrlhf: An easy-to-use, scalable and high-performance rlhf framework}.
\newblock \emph{arXiv preprint arXiv:2405.11143}.

\bibitem[{Hu et~al.(2025{\natexlab{b}})Hu, Zhang, Han, Jiang, and Xiangyu~Zhang}]{OpenReasonerZero2025}
Jingcheng Hu, Yinmin Zhang, Qi~Han, Daxin Jiang, and Heung-Yeung~Shum Xiangyu~Zhang. 2025{\natexlab{b}}.
\newblock Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model.
\newblock \url{https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero}.

\bibitem[{Huang et~al.(2025{\natexlab{a}})Huang, Wang, Yang, Zhao, Li, Lin, Zhang, Rajmohan, and Zhang}]{huang2025lean}
Chenghua Huang, Lu~Wang, Fangkai Yang, Pu~Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi~Zhang. 2025{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2502.16944} {Lean and mean: Decoupled value policy optimization with global value guidance}.
\newblock \emph{arXiv preprint arXiv:2502.16944}.

\bibitem[{Huang et~al.(2025{\natexlab{b}})Huang, Huang, Leng, Liu, and Huang}]{huang2025efficienttesttimescalingselfcalibration}
Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2503.00031} {Efficient test-time scaling via self-calibration}.
\newblock In \emph{arXiv}.

\bibitem[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, jiayi lei, Fu, Sun, and He}]{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, jiayi lei, Yao Fu, Maosong Sun, and Junxian He. 2023.
\newblock \href {https://openreview.net/forum?id=fOrm2rGX2r} {C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models}.
\newblock In \emph{Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Huang et~al.(2024{\natexlab{a}})Huang, Wang, Xia, Li, Zou, Xu, Fan, Ye, Chern, Ye, Zhang, Yang, Wu, Wang, Sun, Xiao, Li, Zhou, Chern, Qin, Ma, Su, Liu, Zheng, Zhang, Lin, Qiao, and Liu}]{huang2024olympicarena}
Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu~Qiao, and Pengfei Liu. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=ayF8bEKYQy} {Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent {AI}}.
\newblock In \emph{Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Huang et~al.(2024{\natexlab{b}})Huang, Zou, Li, Liu, Zheng, Chern, Xia, Qin, Yuan, and Liu}]{GAIR-o1p2}
Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2411.16489} {O1 replication journey -- part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?}
\newblock In \emph{arXiv}.

\bibitem[{HuggingFace(2025)}]{openr1}
HuggingFace. 2025.
\newblock \href {https://github.com/huggingface/open-r1} {Open r1: A fully open reproduction of deepseek-r1}.

\bibitem[{Irvine et~al.(2023)Irvine, Boubert, Raina, Liusie, Zhu, Mudupalli, Korshuk, Liu, Cremer, Assassi, Beauchamp, Lu, Rialan, and Beauchamp}]{irvine2023rewarding}
Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. 2023.
\newblock \href {http://arxiv.org/abs/2303.06135} {Rewarding chatbots for real-world engagement with millions of users}.
\newblock In \emph{arXiv}.

\bibitem[{Jain et~al.(2025)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar-Lezama, Sen, and Stoica}]{jain2025livecodebench}
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2025.
\newblock \href {https://openreview.net/forum?id=chfJJYC3iL} {Livecodebench: Holistic and contamination free evaluation of large language models for code}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ji et~al.(2024)Ji, Liu, Dai, Yang, Zheng, Wu, Dun, Gu, and Yan}]{ji2024enhancing}
Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, and Lin Yan. 2024.
\newblock Enhancing multi-step reasoning abilities of language models through direct q-function optimization.
\newblock \emph{arXiv preprint arXiv:2410.09302}.

\bibitem[{Ji et~al.(2025)Ji, Li, Ye, Wu, Xu, Mo, and Zhang}]{ji2025test}
Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, and Min Zhang. 2025.
\newblock \href {https://arxiv.org/abs/2501.02497} {Test-time computing: from system-1 thinking to system-2 thinking}.
\newblock \emph{arXiv preprint arXiv:2501.02497}.

\bibitem[{Jiang et~al.(2024{\natexlab{a}})Jiang, Li, Zhang, Huang, Lin, and Chen}]{jiang2024tigerscorebuildingexplainablemetric}
Dongfu Jiang, Yishan Li, Ge~Zhang, Wenhao Huang, Bill~Yuchen Lin, and Wenhu Chen. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2310.00752} {Tigerscore: Towards building explainable metric for all text generation tasks}.
\newblock In \emph{arXiv}.

\bibitem[{Jiang et~al.(2023)Jiang, Ren, and Lin}]{jiang2023llm}
Dongfu Jiang, Xiang Ren, and Bill~Yuchen Lin. 2023.
\newblock \href {https://aclanthology.org/2023.acl-long.792/} {{LLM}-blender: Ensembling large language models with pairwise ranking and generative fusion}.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, pages 14165--14178.

\bibitem[{Jiang et~al.(2024{\natexlab{b}})Jiang, Wang, Zeng, Zhong, Li, Mi, Shang, Jiang, Liu, and Wang}]{jiang2024followbenchmultilevelfinegrainedconstraints}
Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2310.20410} {Followbench: A multi-level fine-grained constraints following benchmark for large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Jimenez et~al.(2024)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan}]{jimenez2024swebench}
Carlos~E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik~R Narasimhan. 2024.
\newblock \href {https://openreview.net/forum?id=VTF8yNQM66} {{SWE}-bench: Can language models resolve real-world github issues?}
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Jin et~al.(2020)Jin, Pan, Oufattole, Weng, Fang, and Szolovits}]{jin2020diseasedoespatienthave}
Di~Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020.
\newblock \href {http://arxiv.org/abs/2009.13081} {What disease does this patient have? a large-scale open domain question answering dataset from medical exams}.
\newblock In \emph{arXiv}.

\bibitem[{Jin et~al.(2025)Jin, Yeom, Bae, and Kim}]{jin2025wellthinkingenhancingllm}
Hyunbin Jin, Je~Won Yeom, Seunghyun Bae, and Taesup Kim. 2025.
\newblock \href {http://arxiv.org/abs/2503.10167} {"well, keep thinking": Enhancing llm reasoning with adaptive injection decoding}.
\newblock In \emph{arXiv}.

\bibitem[{Jin et~al.(2024)Jin, Yu, Shu, Zhao, Hua, Meng, Zhang, and Du}]{jin2024impact}
Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. 2024.
\newblock \href {https://aclanthology.org/2024.findings-acl.108/} {The impact of reasoning step length on large language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics ACL 2024}, pages 1830--1842.

\bibitem[{Kahneman(2011)}]{kahneman2011thinking}
D.~Kahneman. 2011.
\newblock \emph{Thinking, Fast and Slow}.
\newblock Farrar, Straus and Giroux.

\bibitem[{Kahneman(2003)}]{daniel2003maps}
Daniel Kahneman. 2003.
\newblock Maps of bounded rationality: Psychology for behavioral economics.
\newblock \emph{The American Economic Review}, 93(5):1449--1475.

\bibitem[{Kang et~al.(2024)Kang, Li, Chen, Kazemi, Sun, Chen, Li, He, He, Wen, Hao, and Yao}]{kang2024mindstarenhancingmathreasoning}
Jikun Kang, Xin~Zhe Li, Xi~Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu~He, Quan He, Feng Wen, Jianye Hao, and Jun Yao. 2024.
\newblock \href {http://arxiv.org/abs/2405.16265} {Mindstar: Enhancing math reasoning in pre-trained llms at inference time}.
\newblock In \emph{arXiv}.

\bibitem[{Kang et~al.(2025)Kang, Zhao, and Song}]{kang2025scalablebestofnselectionlarge}
Zhewei Kang, Xuandong Zhao, and Dawn Song. 2025.
\newblock \href {http://arxiv.org/abs/2502.18581} {Scalable best-of-n selection for large language models via self-certainty}.
\newblock In \emph{arXiv}.

\bibitem[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scalinglawsneurallanguage}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
\newblock \href {http://arxiv.org/abs/2001.08361} {Scaling laws for neural language models}.
\newblock In \emph{arXiv}.

\bibitem[{Kazemnejad et~al.(2024)Kazemnejad, Aghajohari, Portelance, Sordoni, Reddy, Courville, and Roux}]{kazemnejad2024vineppo}
Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas~Le Roux. 2024.
\newblock \href {https://arxiv.org/abs/2410.01679} {Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment}.
\newblock \emph{arXiv preprint arXiv:2410.01679}.

\bibitem[{Kim et~al.(2025)Kim, Wu, Lee, Yue, Lee, Moon, Gashteovski, Lawrence, Hockenmaier, Neubig, and Welleck}]{kim2025scalingevaluationtimecomputereasoning}
Seungone Kim, Ian Wu, Jinu Lee, Xiang Yue, Seongyun Lee, Mingyeong Moon, Kiril Gashteovski, Carolin Lawrence, Julia Hockenmaier, Graham Neubig, and Sean Welleck. 2025.
\newblock \href {http://arxiv.org/abs/2503.19877} {Scaling evaluation-time compute with reasoning models as process evaluators}.
\newblock In \emph{arXiv}.

\bibitem[{Kimi(2025)}]{kimi-k1.5}
Kimi. 2025.
\newblock \href {http://arxiv.org/abs/2501.12599} {Kimi k1.5: Scaling reinforcement learning with llms}.
\newblock In \emph{arXiv}.

\bibitem[{Kong et~al.(2025)Kong, Zhao, Xu, Pang, Wang, Honig, Si, Li, Xie, Xie, and Wu}]{kong2025scalablelanguagemodelsposterior}
Deqian Kong, Minglu Zhao, Dehong Xu, Bo~Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, and Ying~Nian Wu. 2025.
\newblock \href {http://arxiv.org/abs/2502.01567} {Scalable language models with posterior inference of latent thought vectors}.
\newblock In \emph{arXiv}.

\bibitem[{Krishna et~al.(2025)Krishna, Krishna, Mohananey, Schwarcz, Stambler, Upadhyay, and Faruqui}]{krishna2025factfetchreasonunified}
Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2025.
\newblock \href {http://arxiv.org/abs/2409.12941} {Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation}.

\bibitem[{Lambert et~al.(2025)Lambert, Morrison, Pyatkin, Huang, Ivison, Brahman, Miranda, Liu, Dziri, Lyu, Gu, Malik, Graf, Hwang, Yang, Bras, Tafjord, Wilhelm, Soldaini, Smith, Wang, Dasigi, and Hajishirzi}]{lambert2025tulu3pushingfrontiers}
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James~V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena~D. Hwang, Jiangjiang Yang, Ronan~Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah~A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025.
\newblock \href {http://arxiv.org/abs/2411.15124} {Tulu 3: Pushing frontiers in open language model post-training}.
\newblock In \emph{arXiv}.

\bibitem[{Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi, Smith, and Hajishirzi}]{lambert2024rewardbenchevaluatingrewardmodels}
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah~A. Smith, and Hannaneh Hajishirzi. 2024.
\newblock \href {http://arxiv.org/abs/2403.13787} {Rewardbench: Evaluating reward models for language modeling}.
\newblock In \emph{arXiv}.

\bibitem[{Lau et~al.(2024)Lau, Hu, Liu, Chen, Ng, and Low}]{lau2024dipperdiversitypromptsproducing}
Gregory Kang~Ruey Lau, Wenyang Hu, Diwen Liu, Jizhuo Chen, See-Kiong Ng, and Bryan Kian~Hsiang Low. 2024.
\newblock \href {http://arxiv.org/abs/2412.15238} {Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks}.
\newblock In \emph{arXiv}.

\bibitem[{Lee et~al.(2025)Lee, Fischer, Wu, Marwood, Baluja, Schuurmans, and Chen}]{lee2025evolvingdeeperllmthinking}
Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025.
\newblock \href {http://arxiv.org/abs/2501.09891} {Evolving deeper llm thinking}.
\newblock In \emph{arXiv}.

\bibitem[{Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, Wu, Neyshabur, Gur-Ari, and Misra}]{lewkowycz2022solving}
Aitor Lewkowycz, Anders~Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay~Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022.
\newblock \href {https://openreview.net/forum?id=IFXTZERXdM7} {Solving quantitative reasoning problems with language models}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Li et~al.(2025{\natexlab{a}})Li, Wang, Gu, Chang, and Peng}]{li2025metalmultiagentframeworkchart}
Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, and Nanyun Peng. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2502.17651} {{METAL}: A multi-agent framework for chart generation with test-time scaling}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2025{\natexlab{b}})Li, Xue, Zhang, Yang, Zhang, Wang, Yu, Hui, Lin, and Liu}]{li2025startselftaughtreasonertools}
Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2503.04625} {{START}: Self-taught reasoner with tools}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2025{\natexlab{c}})Li, Xu, and Guo}]{li2025reasoningaslogicunitsscalingtesttimereasoning}
Cheryl Li, Tianyuan Xu, and Yiwen Guo. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2502.07803} {Reasoning-as-logic-units: Scaling test-time reasoning in large language models through logic unit alignment}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2025{\natexlab{d}})Li, Cao, Cao, Li, Tan, Keutzer, Xing, Gonzalez, and Stoica}]{li2025stesttimescaling}
Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph~E. Gonzalez, and Ion Stoica. 2025{\natexlab{d}}.
\newblock \href {http://arxiv.org/abs/2502.14382} {S*: Test time scaling for code generation}.

\bibitem[{Li et~al.(2025{\natexlab{e}})Li, Cao, Griggs, Liu, Mo, Tang, Hegde, Hakhamaneshi, Patil, Zaharia, Gonzalez, and Stoica}]{li2025llmseasilylearnreason}
Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir~G. Patil, Matei Zaharia, Joseph~E. Gonzalez, and Ion Stoica. 2025{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2502.07374} {Llms can easily learn to reason from demonstrations structure, not content, is what matters!}
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2024)Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin}]{li2024cmmlumeasuringmassivemultitask}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2024.
\newblock \href {http://arxiv.org/abs/2306.09212} {Cmmlu: Measuring massive multitask language understanding in chinese}.
\newblock In \emph{arXiv}.

\bibitem[{LI et~al.(2024)LI, Beeching, Tunstall, Lipkin, Soletskyi, Huang, Rasul, Yu, Jiang, Shen, Qin, Dong, Zhou, Fleureau, Lample, and Polu}]{numina_math_datasets}
Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi~Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li~Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024.
\newblock Numinamath.
\newblock \url{[https://github.com/project-numina/aimo-progress-prize](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)}.

\bibitem[{Li et~al.(2024{\natexlab{a}})Li, Liu, Deng, Joty, Chen, and Kan}]{li2024dnaevalenhancinglargelanguage}
Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy~F. Chen, and Min-Yen Kan. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2405.15329} {Dna-eval: Enhancing large language model evaluation through decomposition and aggregation}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2024{\natexlab{b}})Li, Chiang, Frick, Dunlap, Wu, Zhu, Gonzalez, and Stoica}]{li2024crowdsourceddatahighqualitybenchmarks}
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph~E. Gonzalez, and Ion Stoica. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2406.11939} {From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2025{\natexlab{f}})Li, Wang, Fu, Cui, Yang, and Cheng}]{li2025draftsanswersunlockingllm}
Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, and Yu~Cheng. 2025{\natexlab{f}}.
\newblock \href {http://arxiv.org/abs/2501.11877} {From drafts to answers: Unlocking llm potential via aggregation fine-tuning}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2025{\natexlab{g}})Li, Lyu, and Wang}]{li2025learningreasonfeedbacktesttime}
Yanyang Li, Michael Lyu, and Liwei Wang. 2025{\natexlab{g}}.
\newblock \href {http://arxiv.org/abs/2502.15771} {Learning to reason from feedback at test-time}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Lin, Zhang, Fu, Chen, Lou, and Chen}]{li2023making}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023{\natexlab{a}}.
\newblock \href {https://aclanthology.org/2023.acl-long.291/} {Making language models better reasoners with step-aware verifier}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5315--5333.

\bibitem[{Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, Hubert, Choy, de~Masson~d’Autume, Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Sutherland~Robson, Kohli, de~Freitas, Kavukcuoglu, and Vinyals}]{Li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, Thomas Hubert, Peter Choy, Cyprien de~Masson~d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel~J. Mankowitz, Esme Sutherland~Robson, Pushmeet Kohli, Nando de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, pages 1092--1097.

\bibitem[{Li et~al.(2024{\natexlab{c}})Li, Liu, Zhou, and Ma}]{li2024chainthoughtempowerstransformers}
Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. 2024{\natexlab{c}}.
\newblock \href {https://openreview.net/forum?id=3EWTEy9MTM} {Chain of thought empowers transformers to solve inherently serial problems}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Li et~al.(2025{\natexlab{h}})Li, Zhang, Zhang, Zhang, Liu, Yao, Xu, Zheng, Wang, Chen et~al.}]{li2025system}
Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et~al. 2025{\natexlab{h}}.
\newblock \href {https://arxiv.org/abs/2502.17419} {From system 1 to system 2: A survey of reasoning large language models}.
\newblock \emph{arXiv preprint arXiv:2502.17419}.

\bibitem[{Li et~al.(2025{\natexlab{i}})Li, Zhang, Wang, Xu, Zhang, Fei, Ji, Bai, Pan, Zhang, and Liu}]{li2025cmmath}
Zhongzhi Li, Ming-Liang Zhang, Pei-Jie Wang, Jian Xu, Rui-Song Zhang, Yin Fei, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Jiaxin Zhang, and Cheng-Lin Liu. 2025{\natexlab{i}}.
\newblock {CMM}a{TH}: A {C}hinese multi-modal math skill evaluation benchmark for foundation models.
\newblock In \emph{International Conference on Computational Linguistics}, pages 2690--2726.

\bibitem[{Li et~al.(2025{\natexlab{j}})Li, Feng, Cai, Zhang, Liu, Liang, Chen, Wang, and Zhao}]{li2025llmsgeneratebetteranswer}
Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, and Tuo Zhao. 2025{\natexlab{j}}.
\newblock \href {http://arxiv.org/abs/2503.04104} {Llms can generate a better answer by aggregating their own responses}.
\newblock In \emph{arXiv}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Xu, Zhang, Lin, Yu, Sun, and Luo}]{li2023remax}
Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2310.10505} {Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models}.
\newblock \emph{arXiv preprint arXiv:2310.10505}.

\bibitem[{Liang et~al.(2024)Liang, He, Jiao, Wang, Wang, Wang, Yang, Shi, and Tu}]{liang2024encouraging}
Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024.
\newblock \href {https://aclanthology.org/2024.emnlp-main.992/} {Encouraging divergent thinking in large language models through multi-agent debate}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, pages 17889--17904.

\bibitem[{Lifshitz et~al.(2025)Lifshitz, McIlraith, and Du}]{lifshitz2025multiagent}
Shalev Lifshitz, Sheila~A. McIlraith, and Yilun Du. 2025.
\newblock \href {https://openreview.net/forum?id=H22e93wnMe} {Multi-agent verification: Scaling test-time compute with goal verifiers}.
\newblock In \emph{Workshop on Reasoning and Planning for Large Language Models}.

\bibitem[{Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe}]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.
\newblock \href {https://openreview.net/forum?id=v8L0pN6EOi} {Let's verify step by step}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Lin et~al.(2025)Lin, Xu, Li, Hao, Zhang, and Cai}]{lin2025leveragingconstrainedmontecarlo}
Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, and Ruichu Cai. 2025.
\newblock \href {http://arxiv.org/abs/2502.11169} {Leveraging constrained monte carlo tree search to generate reliable long chain-of-thought for mathematical reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Lin et~al.(2024)Lin, Liang, Xu, Wang, Luo, Shi, Li, Yang, and Tu}]{lin2024critical}
Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. 2024.
\newblock \href {https://arxiv.org/abs/2411.19943} {Critical tokens matter: Token-level contrastive estimation enhence llm's reasoning capability}.
\newblock \emph{arXiv preprint arXiv:2411.19943}.

\bibitem[{Ling et~al.(2023)Ling, Fang, Li, Huang, Lee, Memisevic, and Su}]{ling2023deductive}
Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2023/hash/72393bd47a35f5b3bee4c609e7bba733-Abstract-Conference.html} {Deductive verification of chain-of-thought reasoning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, pages 36407--36433.

\bibitem[{Liu et~al.(2024{\natexlab{a}})Liu, Zhang, Ibrahimzada, and Jabbarvand}]{liu2024codemindframeworkchallengelarge}
Changshu Liu, Shizhuo~Dylan Zhang, Ali~Reza Ibrahimzada, and Reyhaneh Jabbarvand. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2402.09664} {Codemind: A framework to challenge large language models for code reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and Lee}]{liu2023visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023{\natexlab{a}}.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36:34892--34916.

\bibitem[{Liu et~al.(2024{\natexlab{b}})Liu, Wang, Liu, Zeng, Yan, Sun, Liu, and Zhou}]{liu2024improvingmultistepreasoningabilities}
Jiacai Liu, Chaojie Wang, Chris~Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, and Yahui Zhou. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2412.18279} {Improving multi-step reasoning abilities of large language models with direct advantage policy optimization}.
\newblock In \emph{arXiv}.

\bibitem[{Liu et~al.(2025{\natexlab{a}})Liu, Gao, Zhao, Zhang, Li, Qi, Ouyang, and Zhou}]{liu2025can}
Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. 2025{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2502.06703} {Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling}.
\newblock \emph{arXiv preprint arXiv:2502.06703}.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Guo, Yang, Hu, Zhang, Qiu, and Zhang}]{liu2023plan}
Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2023{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2023.emnlp-main.169/} {Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Liu et~al.(2023{\natexlab{c}})Liu, Iter, Xu, Wang, Xu, and Zhu}]{liu2023gevalnlgevaluationusing}
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2303.16634} {G-eval: Nlg evaluation using gpt-4 with better human alignment}.
\newblock In \emph{arXiv}.

\bibitem[{Liu et~al.(2024{\natexlab{c}})Liu, Yao, Min, Cao, Hou, and Li}]{liu2024rmbenchbenchmarkingrewardmodels}
Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2410.16184} {Rm-bench: Benchmarking reward models of language models with subtlety and style}.
\newblock In \emph{arXiv}.

\bibitem[{Liu et~al.(2025{\natexlab{b}})Liu, Yao, Min, Cao, Hou, and Li}]{liu2025pairjudgermperformbestofn}
Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2501.13007} {Pairjudge rm: Perform best-of-n sampling with knockout tournament}.
\newblock In \emph{arXiv}.

\bibitem[{Liu et~al.(2024{\natexlab{d}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2024mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2024{\natexlab{d}}.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock In \emph{European Conference on Computer Vision}, pages 216--233.

\bibitem[{Liu et~al.(2025{\natexlab{c}})Liu, Wang, Xu, Ma, Ruan, Li, Liu, and Wu}]{liu2025inferencetimescalinggeneralistreward}
Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu~Wu. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2504.02495} {Inference-time scaling for generalist reward modeling}.
\newblock In \emph{arXiv}.

\bibitem[{Lu et~al.(2024)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{lu2024mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024.
\newblock \href {http://arxiv.org/abs/2310.02255} {Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts}.
\newblock In \emph{arXiv}.

\bibitem[{Luo et~al.(2025)Luo, Tan, Wong, Shi, Tang, Roongta, Cai, Luo, Zhang, Li, Popa, and Stoica}]{deepscaler2025}
Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William~Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li~Erran Li, Raluca~Ada Popa, and Ion Stoica. 2025.
\newblock Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl.
\newblock \url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}.
\newblock Notion Blog.

\bibitem[{Luong et~al.(2024)Luong, Zhang, Jie, Sun, Jin, and Li}]{luong2024reftreasoningreinforcedfinetuning}
Trung~Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024.
\newblock \href {http://arxiv.org/abs/2401.08967} {Reft: Reasoning with reinforced fine-tuning}.
\newblock In \emph{arXiv}.

\bibitem[{Ma et~al.(2025{\natexlab{a}})Ma, Zhao, Zhang, He, and Kong}]{ma2025nonmyopic}
Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. 2025{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=OoNazl6T7D} {Non-myopic generation of language models for reasoning and planning}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ma et~al.(2025{\natexlab{b}})Ma, Tong, Jia, Hu, Su, Zhang, Yang, Li, Jaakkola, Jia, and Xie}]{ma2025inferencetimescalingdiffusionmodels}
Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2501.09732} {Inference-time scaling for diffusion models beyond scaling denoising steps}.
\newblock In \emph{arXiv}.

\bibitem[{Ma et~al.(2025{\natexlab{c}})Ma, Chen, Liu, Tian, Liu, Liu, and Luo}]{ma2025steplevelrewardmodelsrewarding}
Yiran Ma, Zui Chen, Tianqiao Liu, Mi~Tian, Zhuo Liu, Zitao Liu, and Weiqi Luo. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2412.15904} {What are step-level reward models rewarding? counterintuitive findings from mcts-boosted mathematical reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck, Yazdanbakhsh, and Clark}]{madaan2023selfrefine}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa~Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023.
\newblock \href {https://openreview.net/forum?id=S37hOerQLB} {Self-refine: Iterative refinement with self-feedback}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Mahmud et~al.(2025)Mahmud, Duan, Pasareanu, and Yang}]{mahmud2025enhancingllmcodegeneration}
Tarek Mahmud, Bin Duan, Corina Pasareanu, and Guowei Yang. 2025.
\newblock \href {http://arxiv.org/abs/2503.15838} {Enhancing llm code generation with ensembles: A similarity-based selection approach}.
\newblock In \emph{arXiv}.

\bibitem[{Marjanović et~al.(2025)Marjanović, Patel, Adlakha, Aghajohari, BehnamGhader, Bhatia, Khandelwal, Kraft, Krojer, Lù, Meade, Shin, Kazemnejad, Kamath, Mosbach, Stańczak, and Reddy}]{marjanović2025deepseekr1thoughtologyletsthink}
Sara~Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing~Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, and Siva Reddy. 2025.
\newblock \href {http://arxiv.org/abs/2504.07128} {Deepseek-r1 thoughtology: Let's <think> about llm reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{McAleese et~al.(2024)McAleese, Pokorny, Uribe, Nitishinskaya, Trebacz, and Leike}]{mcaleese2024llmcriticshelpcatch}
Nat McAleese, Rai~Michael Pokorny, Juan Felipe~Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024.
\newblock \href {http://arxiv.org/abs/2407.00215} {Llm critics help catch llm bugs}.
\newblock In \emph{arXiv}.

\bibitem[{Meng et~al.(2024)Meng, Xia, and Chen}]{meng2024simpo}
Yu~Meng, Mengzhou Xia, and Danqi Chen. 2024.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:124198--124235.

\bibitem[{Misaki et~al.(2025)Misaki, Inoue, Imajuku, Kuroki, Nakamura, and Akiba}]{misaki2025widerdeeperscalingllm}
Kou Misaki, Yuichi Inoue, Yuki Imajuku, So~Kuroki, Taishi Nakamura, and Takuya Akiba. 2025.
\newblock \href {http://arxiv.org/abs/2503.04412} {Wider or deeper? scaling llm inference-time compute with adaptive branching tree search}.
\newblock In \emph{arXiv}.

\bibitem[{Muennighoff et~al.(2025)Muennighoff, Yang, Shi, Li, Fei-Fei, Hajishirzi, Zettlemoyer, Liang, Candès, and Hashimoto}]{muennighoff2025s1}
Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang~Lisa Li, Li~Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025.
\newblock \href {http://arxiv.org/abs/2501.19393} {{s1}: Simple test-time scaling}.
\newblock In \emph{arXiv}.

\bibitem[{Munkhbat et~al.(2025)Munkhbat, Ho, Kim, Yang, Kim, and Yun}]{munkhbat2025selftrainingelicitsconcisereasoning}
Tergel Munkhbat, Namgyu Ho, Seo~Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025.
\newblock \href {http://arxiv.org/abs/2502.20122} {Self-training elicits concise reasoning in large language models}.
\newblock In \emph{arXiv}.

\bibitem[{NCEE(2025)}]{gaokao}
NCEE. 2025.
\newblock China's national college entrance examination.

\bibitem[{Nguyen et~al.(2024)Nguyen, Mekala, Dong, and Shang}]{nguyen2024consistent}
Alex Nguyen, Dheeraj Mekala, Chengyu Dong, and Jingbo Shang. 2024.
\newblock \href {http://arxiv.org/abs/2407.05778} {When is the consistent prediction likely to be a correct prediction?}
\newblock In \emph{arXiv}.

\bibitem[{Ni et~al.(2024)Ni, Allamanis, Cohan, Deng, Shi, Sutton, and Yin}]{ni2024nextteachinglargelanguage}
Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2024.
\newblock \href {http://arxiv.org/abs/2404.14662} {Next: Teaching large language models to reason about code execution}.
\newblock In \emph{arXiv}.

\bibitem[{Ni et~al.(2023)Ni, Iyer, Radev, Stoyanov, tau Yih, Wang, and Lin}]{ni2023leverlearningverifylanguagetocode}
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen tau Yih, Sida~I. Wang, and Xi~Victoria Lin. 2023.
\newblock \href {http://arxiv.org/abs/2302.08468} {Lever: Learning to verify language-to-code generation with execution}.
\newblock In \emph{arXiv}.

\bibitem[{Nori et~al.(2024)Nori, Usuyama, King, McKinney, Fernandes, Zhang, and Horvitz}]{nori2024medprompt}
Harsha Nori, Naoto Usuyama, Nicholas King, Scott~Mayer McKinney, Xavier Fernandes, Sheng Zhang, and Eric Horvitz. 2024.
\newblock From medprompt to o1: Exploration of run-time strategies for medical challenge problems and beyond.
\newblock \emph{arXiv preprint arXiv:2411.03590}.

\bibitem[{NovaSky(2025)}]{skyt12025}
NovaSky. 2025.
\newblock Sky-t1: Train your own o1 preview model within \$450.
\newblock https://novasky-ai.github.io/posts/sky-t1.
\newblock Accessed: 2025-01-09.

\bibitem[{Ong et~al.(2025)Ong, Almahairi, Wu, Chiang, Wu, Gonzalez, Kadous, and Stoica}]{ong2025routellm}
Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph~E. Gonzalez, M~Waleed Kadous, and Ion Stoica. 2025.
\newblock \href {https://openreview.net/forum?id=8sSqNntaMr} {Route{LLM}: Learning to route {LLM}s from preference data}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{OpenAI(2024{\natexlab{a}})}]{openai-gpt4}
OpenAI. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.
\newblock In \emph{arXiv}.

\bibitem[{OpenAI(2024{\natexlab{b}})}]{openai-o1}
OpenAI. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2412.16720} {Openai o1 system card}.
\newblock In \emph{arXiv}.

\bibitem[{OpenAI(2025)}]{openai-o3}
OpenAI. 2025.
\newblock \href {https://cdn.openai.com/o3-mini-system-card-feb10.pdf} {Openai o3-mini system card}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Gray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {https://openreview.net/forum?id=TG8KACxEON} {Training language models to follow instructions with human feedback}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 27730--27744.

\bibitem[{Pan et~al.(2025{\natexlab{a}})Pan, Deng, and Huang}]{pan2025coatchainofassociatedthoughtsframeworkenhancing}
Jianfeng Pan, Senyou Deng, and Shaomang Huang. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2502.02390} {Coat: Chain-of-associated-thoughts framework for enhancing large language models reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Pan et~al.(2025{\natexlab{b}})Pan, Zhang, Wang, Yuan, Peng, and Suhr}]{tinyzero}
Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025{\natexlab{b}}.
\newblock Tinyzero.
\newblock https://github.com/Jiayi-Pan/TinyZero.
\newblock Accessed: 2025-01-24.

\bibitem[{Pareja et~al.(2024)Pareja, Nayak, Wang, Killamsetty, Sudalairaj, Zhao, Han, Bhandwaldar, Xu, Xu, Han, Inglis, and Srivastava}]{pareja2024unveilingsecretrecipeguide}
Aldo Pareja, Nikhil~Shivakumar Nayak, Hao Wang, Krishnateja Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, and Akash Srivastava. 2024.
\newblock \href {http://arxiv.org/abs/2412.13337} {Unveiling the secret recipe: A guide for supervised fine-tuning small llms}.
\newblock In \emph{arXiv}.

\bibitem[{Parmar et~al.(2025)Parmar, Liu, Goyal, Chen, Le, Mishra, Mobahi, Gu, Wang, Nakhost, Baral, Lee, Pfister, and Palangi}]{parmar2025plangenmultiagentframeworkgenerating}
Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, Chitta Baral, Chen-Yu Lee, Tomas Pfister, and Hamid Palangi. 2025.
\newblock \href {http://arxiv.org/abs/2502.16111} {Plangen: A multi-agent framework for generating planning and reasoning trajectories for complex problem solving}.
\newblock In \emph{arXiv}.

\bibitem[{Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, and Gao}]{peng2023checkfactstryagain}
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu~Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023.
\newblock \href {http://arxiv.org/abs/2302.12813} {Check your facts and try again: Improving large language models with external knowledge and automated feedback}.
\newblock In \emph{arXiv}.

\bibitem[{Pfau et~al.(2024)Pfau, Merrill, and Bowman}]{pfau2024lets}
Jacob Pfau, William Merrill, and Samuel~R. Bowman. 2024.
\newblock \href {https://openreview.net/forum?id=NikbrdtYvG} {Let{\textquoteright}s think dot by dot: Hidden computation in transformer language models}.
\newblock In \emph{Conference on Language Modeling}.

\bibitem[{Phan et~al.(2025)Phan, Gatti, Han, Li, Hu, Zhang, Shi, Choi, Agrawal, Chopra et~al.}]{phan2025humanity}
Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et~al. 2025.
\newblock Humanity's last exam.
\newblock \emph{arXiv preprint arXiv:2501.14249}.

\bibitem[{Prasad et~al.(2024)Prasad, Koller, Hartmann, Clark, Sabharwal, Bansal, and Khot}]{prasad2024adaptasneededdecompositionplanning}
Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2024.
\newblock \href {https://aclanthology.org/2024.findings-naacl.264/} {Adapt: As-needed decomposition and planning with language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 4226--4252.

\bibitem[{Puri et~al.(2025)Puri, Sudalairaj, Xu, Xu, and Srivastava}]{puri2025probabilisticinferenceapproachinferencetime}
Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, and Akash Srivastava. 2025.
\newblock \href {http://arxiv.org/abs/2502.01618} {A probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods}.
\newblock In \emph{arXiv}.

\bibitem[{Qin et~al.(2024)Qin, Li, Zou, Liu, Xia, Huang, Ye, Yuan, Liu, Li, and Liu}]{GAIR-o1p1}
Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024.
\newblock \href {http://arxiv.org/abs/2410.18982} {O1 replication journey: A strategic progress report -- part 1}.
\newblock In \emph{arXiv}.

\bibitem[{Qiu et~al.(2024)Qiu, Lu, Zeng, Guo, Geng, Wang, Huang, Wu, and Wang}]{qiu2024treebonenhancinginferencetimealignment}
Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024.
\newblock Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling.
\newblock \emph{arXiv preprint arXiv:2410.16033}.

\bibitem[{Qwen(2024)}]{qwq-32b-preview}
Qwen. 2024.
\newblock \href {https://qwenlm.github.io/blog/qwq-32b-preview/} {Qwq: Reflect deeply on the boundaries of the unknown}.

\bibitem[{Ranaldi et~al.(2025)Ranaldi, Valentino, Polonsky, and Freitas}]{ranaldi2025improvingchainofthoughtreasoningquasisymbolic}
Leonardo Ranaldi, Marco Valentino, Alexander Polonsky, and Andrè Freitas. 2025.
\newblock \href {http://arxiv.org/abs/2502.12616} {Improving chain-of-thought reasoning via quasi-symbolic abstractions}.
\newblock In \emph{arXiv}.

\bibitem[{Rein et~al.(2024)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{rein2024gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman. 2024.
\newblock \href {https://openreview.net/forum?id=Ti67584b98} {{GPQA}: A graduate-level google-proof q\&a benchmark}.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[{Renze(2024)}]{renze2024effectsamplingtemperatureproblem}
Matthew Renze. 2024.
\newblock \href {https://aclanthology.org/2024.findings-emnlp.432/} {The effect of sampling temperature on problem solving in large language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 7346--7356.

\bibitem[{Rohrbach et~al.(2018)Rohrbach, Hendricks, Burns, Darrell, and Saenko}]{rohrbach2018object}
Anna Rohrbach, Lisa~Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.
\newblock Object hallucination in image captioning.
\newblock \emph{arXiv preprint arXiv:1809.02156}.

\bibitem[{Saad-Falcon et~al.(2024)Saad-Falcon, Lafuente, Natarajan, Maru, Todorov, Guha, Buchanan, Chen, Guha, Ré, and Mirhoseini}]{saadfalcon2024archonarchitecturesearchframework}
Jon Saad-Falcon, Adrian~Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E.~Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, and Azalia Mirhoseini. 2024.
\newblock \href {http://arxiv.org/abs/2409.15254} {Archon: An architecture search framework for inference-time techniques}.
\newblock In \emph{arXiv}.

\bibitem[{Saha et~al.(2025)Saha, Li, Ghazvininejad, Weston, and Wang}]{saha2025learningplanreason}
Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. 2025.
\newblock \href {http://arxiv.org/abs/2501.18099} {Learning to plan \& reason for evaluation with thinking-llm-as-a-judge}.
\newblock In \emph{arXiv}.

\bibitem[{Salemi and Zamani(2024)}]{salemi2024searchenginemachinesunified}
Alireza Salemi and Hamed Zamani. 2024.
\newblock \href {http://arxiv.org/abs/2405.00175} {Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Saunshi et~al.(2025)Saunshi, Dikkala, Li, Kumar, and Reddi}]{saunshi2025reasoninglatentthoughtspower}
Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank~J. Reddi. 2025.
\newblock \href {http://arxiv.org/abs/2502.17416} {Reasoning with latent thoughts: On the power of looped transformers}.
\newblock In \emph{arXiv}.

\bibitem[{Schaul(2024)}]{schaul2024boundlesssocraticlearninglanguage}
Tom Schaul. 2024.
\newblock \href {https://openreview.net/forum?id=6U4pQf3tlL} {Boundless socratic learning with language games}.
\newblock In \emph{Language Gamification-NeurIPS 2024 Workshop}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximalpolicyoptimizationalgorithms}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.
\newblock \href {http://arxiv.org/abs/1707.06347} {Proximal policy optimization algorithms}.

\bibitem[{Sel et~al.(2024)Sel, Tawaha, Khattar, Jia, and Jin}]{sel2024algorithm}
Bilgehan Sel, Ahmad Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. 2024.
\newblock Algorithm of thoughts: Enhancing exploration of ideas in large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 44136--44189. PMLR.

\bibitem[{Sessa et~al.(2024)Sessa, Dadashi, Hussenot, Ferret, Vieillard, Ramé, Shariari, Perrin, Friesen, Cideron, Girgin, Stanczyk, Michi, Sinopalnikov, Ramos, Héliou, Severyn, Hoffman, Momchev, and Bachem}]{sessa2024bondaligningllmsbestofn}
Pier~Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, and Olivier Bachem. 2024.
\newblock \href {http://arxiv.org/abs/2407.14622} {Bond: Aligning llms with best-of-n distillation}.
\newblock In \emph{arXiv}.

\bibitem[{Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu et~al.}]{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK~Li, Y~Wu, et~al. 2024.
\newblock \href {https://arxiv.org/abs/2402.03300} {Deepseekmath: Pushing the limits of mathematical reasoning in open language models}.
\newblock \emph{arXiv preprint arXiv:2402.03300}.

\bibitem[{Shen et~al.(2025{\natexlab{a}})Shen, Liu, Wu, Zhu, Yang, Xin, Yue, and Yan}]{shen2025exploringdatascalingtrends}
Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu~Yue, and Lin Yan. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2503.22230} {Exploring data scaling trends and effects in reinforcement learning from human feedback}.
\newblock In \emph{arXiv}.

\bibitem[{Shen et~al.(2025{\natexlab{b}})Shen, Wang, Shi, Wang, Zhao, and Gu}]{shen2025efficientreasoninghiddenthinking}
Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu~Zhao, and Jiuxiang Gu. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2501.19201} {Efficient reasoning with hidden thinking}.
\newblock In \emph{arXiv}.

\bibitem[{Shen et~al.(2025{\natexlab{c}})Shen, Yan, Zhang, Hu, Du, and He}]{shen2025codicompressingchainofthoughtcontinuous}
Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2502.21074} {Codi: Compressing chain-of-thought into continuous space via self-distillation}.
\newblock In \emph{arXiv}.

\bibitem[{Shi et~al.(2024)Shi, Tang, Narasimhan, and Yao}]{shi2024can}
Ben Shi, Michael Tang, Karthik~R Narasimhan, and Shunyu Yao. 2024.
\newblock \href {https://openreview.net/forum?id=kGa4fMtP9l} {Can language models solve olympiad programming?}
\newblock In \emph{Conference on Language Modeling}.

\bibitem[{Singh et~al.(2024)Singh, Co-Reyes, Agarwal, Anand, Patil, Garcia, Liu, Harrison, Lee, Xu, Parisi, Kumar, Alemi, Rizkowsky, Nova, Adlam, Bohnet, Elsayed, Sedghi, Mordatch, Simpson, Gur, Snoek, Pennington, Hron, Kenealy, Swersky, Mahajan, Culp, Xiao, Bileschi, Constant, Novak, Liu, Warkentin, Bansal, Dyer, Neyshabur, Sohl-Dickstein, and Fiedel}]{singh2024beyond}
Avi Singh, John~D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter~J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron~T Parisi, Abhishek Kumar, Alexander~A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin~Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura~A Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024.
\newblock \href {https://openreview.net/forum?id=lNAyUngGFK} {Beyond human data: Scaling self-training for problem-solving with language models}.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Singh et~al.(2025)Singh, Chakraborty, and Nambi}]{singh2025selfevolvedpreferenceoptimizationenhancing}
Joykirat Singh, Tanmoy Chakraborty, and Akshay Nambi. 2025.
\newblock \href {http://arxiv.org/abs/2503.04813} {Self-evolved preference optimization for enhancing mathematical reasoning in small language models}.
\newblock In \emph{arXiv}.

\bibitem[{Skalse et~al.(2025)Skalse, Howe, Krasheninnikov, and Krueger}]{skalse2025definingcharacterizingrewardhacking}
Joar Skalse, Nikolaus H.~R. Howe, Dmitrii Krasheninnikov, and David Krueger. 2025.
\newblock \href {http://arxiv.org/abs/2209.13085} {Defining and characterizing reward hacking}.
\newblock In \emph{arXiv}.

\bibitem[{Snell et~al.(2024)Snell, Lee, Xu, and Kumar}]{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024.
\newblock \href {https://arxiv.org/abs/2408.03314} {Scaling llm test-time compute optimally can be more effective than scaling model parameters}.
\newblock \emph{arXiv preprint arXiv:2408.03314}.

\bibitem[{Song et~al.(2024)Song, Wang, Li, and Lin}]{song2024good}
Yifan Song, Guoyin Wang, Sujian Li, and Bill~Yuchen Lin. 2024.
\newblock \href {http://arxiv.org/abs/2407.10457} {The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism}.
\newblock In \emph{arXiv}.

\bibitem[{Stanovich and West(2000)}]{Stanovich_West_2000}
Keith~E. Stanovich and Richard~F. West. 2000.
\newblock Advancing the rationality debate.
\newblock \emph{Behavioral and Brain Sciences}, page 701–717.

\bibitem[{Sui et~al.(2025)Sui, He, Cao, Han, and Hooi}]{sui2025metareasonerdynamicguidanceoptimized}
Yuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. 2025.
\newblock \href {http://arxiv.org/abs/2502.19918} {Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models}.

\bibitem[{Sun et~al.(2024)Sun, Han, Zhao, Ma, Shen, Chen, Chen, and Yu}]{sun2024scieval}
Liangtai Sun, Yang Han, Zihan Zhao, Da~Ma, Zhennan Shen, Baocai Chen, Lu~Chen, and Kai Yu. 2024.
\newblock Scieval: a multi-level large language model evaluation benchmark for scientific research.
\newblock In \emph{AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence}.

\bibitem[{Sun et~al.(2025)Sun, Guo, Lin, Chen, Qi, Tang, and Wen}]{sun2025uncertain}
Zexu Sun, Yiju Guo, Yankai Lin, Xu~Chen, Qi~Qi, Xing Tang, and Ji-Rong Wen. 2025.
\newblock \href {https://openreview.net/forum?id=iamWnRpMuQ} {Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback}.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}.

\bibitem[{Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan}]{NEURIPS2023_0764db11}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 2511--2565.

\bibitem[{Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour}]{sutton1999policy}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html} {Policy gradient methods for reinforcement learning with function approximation}.
\newblock \emph{Advances in neural information processing systems}, 12.

\bibitem[{Tan et~al.(2025)Tan, Zhuang, Montgomery, Tang, Cuadron, Wang, Popa, and Stoica}]{tan2025judgebenchbenchmarkevaluatingllmbased}
Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William~Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca~Ada Popa, and Ion Stoica. 2025.
\newblock \href {http://arxiv.org/abs/2410.12784} {Judgebench: A benchmark for evaluating llm-based judges}.
\newblock In \emph{arXiv}.

\bibitem[{Taubenfeld et~al.(2025)Taubenfeld, Sheffer, Ofek, Feder, Goldstein, Gekhman, and Yona}]{taubenfeld2025confidenceimprovesselfconsistencyllms}
Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. 2025.
\newblock \href {http://arxiv.org/abs/2502.06233} {Confidence improves self-consistency in llms}.
\newblock In \emph{arXiv}.

\bibitem[{Teng et~al.(2025)Teng, Yu, Shi, Zhang, Wu, and Luo}]{teng2025atom}
Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. 2025.
\newblock Atom of thoughts for markov llm test-time scaling.
\newblock \emph{arXiv preprint arXiv:2502.12018}.

\bibitem[{Tian et~al.(2024)Tian, Peng, Song, Jin, Yu, Han, Mi, and Yu}]{tian2024toward}
Ye~Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Lei Han, Haitao Mi, and Dong Yu. 2024.
\newblock \href {https://openreview.net/forum?id=tPdJ2qHkOB} {Toward self-improvement of {LLM}s via imagination, searching, and criticizing}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Tian et~al.(2025)Tian, Yan, Yang, Zhao, Chen, Wang, Luo, Ma, and Song}]{tian2025codehaluinvestigatingcodehallucinations}
Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, and Dawn Song. 2025.
\newblock \href {http://arxiv.org/abs/2405.00253} {Codehalu: Investigating code hallucinations in llms via execution-based verification}.
\newblock In \emph{arXiv}.

\bibitem[{Tong et~al.(2024)Tong, Brown, Wu, Woo, IYER, Akula, Yang, Yang, Middepogu, Wang et~al.}]{tong2024cambrian}
Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam~Vedagiri IYER, Sai~Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et~al. 2024.
\newblock Cambrian-1: A fully open, vision-centric exploration of multimodal llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:87310--87356.

\bibitem[{Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins}]{uesato2022solvingmathwordproblems}
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022.
\newblock \href {http://arxiv.org/abs/2211.14275} {Solving math word problems with process- and outcome-based feedback}.
\newblock In \emph{arXiv}.

\bibitem[{Vladika and Matthes(2024)}]{vladika2024improvinghealthquestionanswering}
Juraj Vladika and Florian Matthes. 2024.
\newblock \href {https://aclanthology.org/2024.findings-naacl.295/} {Improving health question answering with reliable and time-aware evidence retrieval}.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 4752--4763.

\bibitem[{Wan et~al.(2025)Wan, Chen, Stengel-Eskin, and Bansal}]{wan2025mammrefinerecipeimprovingfaithfulness}
David Wan, Justin Chih-Yao Chen, Elias Stengel-Eskin, and Mohit Bansal. 2025.
\newblock \href {http://arxiv.org/abs/2503.15272} {Mamm-refine: A recipe for improving faithfulness in generation with multi-agent collaboration}.
\newblock In \emph{arXiv}.

\bibitem[{Wan et~al.(2024)Wan, Feng, Wen, McAleer, Wen, Zhang, and Wang}]{wan2024alphazero}
Ziyu Wan, Xidong Feng, Muning Wen, Stephen~Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2024.
\newblock \href {https://openreview.net/forum?id=C4OpREezgj} {Alphazero-like tree-search can guide large language model decoding and training}.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Cassano, Wu, Bai, Song, Nath, Han, Hendryx, Yue, and Zhang}]{wang2024planningnaturallanguageimproves}
Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2409.03733} {Planning in natural language improves llm search for code generation}.
\newblock In \emph{arXiv}.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Hao, Dong, Zhang, Bao, Yang, and Wu}]{wang2024offline}
Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi~Wu. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2412.16145} {Offline reinforcement learning for llm multi-step reasoning}.
\newblock \emph{arXiv preprint arXiv:2412.16145}.

\bibitem[{Wang et~al.(2024{\natexlab{c}})Wang, Fang, Wan, Wen, Zhu, Liu, Gong, Song, Chen, Ni et~al.}]{wang2024openr}
Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel~M Ni, et~al. 2024{\natexlab{c}}.
\newblock \href {https://arxiv.org/abs/2410.09671} {Openr: An open source framework for advanced reasoning with large language models}.
\newblock \emph{arXiv preprint arXiv:2410.09671}.

\bibitem[{Wang et~al.(2025{\natexlab{a}})Wang, WANG, Athiwaratkun, Zhang, and Zou}]{wang2025mixtureofagents}
Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce~Zhang, and James Zou. 2025{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=h0ZfDIrj7T} {Mixture-of-agents enhances large language model capabilities}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2024{\natexlab{d}})Wang, Pan, Shi, Lu, Ren, Zhou, Zhan, and Li}]{wang2024measuring}
Ke~Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. 2024{\natexlab{d}}.
\newblock \href {https://openreview.net/forum?id=QWTCcxMpPA} {Measuring multimodal mathematical reasoning with {MATH}-vision dataset}.
\newblock In \emph{Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Wang et~al.(2024{\natexlab{e}})Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui}]{wang-etal-2024-math}
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu~Wu, and Zhifang Sui. 2024{\natexlab{e}}.
\newblock Math-shepherd: Verify and reinforce {LLM}s step-by-step without human annotations.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, pages 9426--9439.

\bibitem[{Wang et~al.(2025{\natexlab{b}})Wang, Pan, Li, Zhang, Jia, Diao, Pi, Hu, and Zhang}]{wang2025malotmultiagentleanbasedlong}
Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, and Tong Zhang. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2503.03205} {Ma-lot: Multi-agent lean-based long chain-of-thought reasoning enhances formal theorem proving}.
\newblock In \emph{arXiv}.

\bibitem[{Wang et~al.(2022)Wang, Jansen, C{\^o}t{\'e}, and Ammanabrolu}]{wang2022sciworld}
Ruoyao Wang, Peter Jansen, Marc-Alexandre C{\^o}t{\'e}, and Prithviraj Ammanabrolu. 2022.
\newblock \href {https://aclanthology.org/2022.emnlp-main.775/} {Scienceworld: Is your agent smarter than a 5th grader?}
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11279--11298.

\bibitem[{Wang et~al.(2024{\natexlab{f}})Wang, Chen, Han, and Bai}]{wang2024cpl}
Tianlong Wang, Junzhe Chen, Xueting Han, and Jing Bai. 2024{\natexlab{f}}.
\newblock \href {https://arxiv.org/abs/2409.08642} {Cpl: Critical plan step learning boosts llm generalization in reasoning tasks}.
\newblock \emph{arXiv preprint arXiv:2409.08642}.

\bibitem[{Wang et~al.(2025{\natexlab{c}})Wang, Feng, Li, Yuan, Zhang, Tan, Pan, Hu, and Li}]{wang2025makepennycountdifficultyadaptive}
Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2408.13457} {Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Wang et~al.(2024{\natexlab{g}})Wang, Caccia, Ostapenko, Yuan, Wang, and Sordoni}]{wang2024guiding}
Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William~Yang Wang, and Alessandro Sordoni. 2024{\natexlab{g}}.
\newblock \href {https://openreview.net/forum?id=wi9IffRhVM} {Guiding language model reasoning with planning tokens}.
\newblock In \emph{Conference on Language Modeling}.

\bibitem[{Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023.
\newblock \href {https://openreview.net/forum?id=1PL1NIMMrw} {Self-consistency improves chain of thought reasoning in language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2024{\natexlab{h}})Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen}]{wang2024mmlupro}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024{\natexlab{h}}.
\newblock \href {https://openreview.net/forum?id=y10DM6R2r3} {{MMLU}-pro: A more robust and challenging multi-task language understanding benchmark}.
\newblock In \emph{Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Wang et~al.(2025{\natexlab{d}})Wang, Yue, and Chen}]{wang2025critiquefinetuninglearningcritique}
Yubo Wang, Xiang Yue, and Wenhu Chen. 2025{\natexlab{d}}.
\newblock \href {http://arxiv.org/abs/2501.17703} {Critique fine-tuning: Learning to critique is more effective than learning to imitate}.
\newblock In \emph{arXiv}.

\bibitem[{Wang et~al.(2025{\natexlab{e}})Wang, Liu, Xu, Liang, Chen, He, Song, Yu, Li, Zhang, Wang, Tu, Mi, and Yu}]{wang2025thoughtsplaceunderthinkingo1like}
Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2501.18585} {Thoughts are all over the place: On the underthinking of o1-like llms}.
\newblock In \emph{arXiv}.

\bibitem[{Wang et~al.(2025{\natexlab{f}})Wang, Moriyama, Wang, Gangopadhyay, and Takamatsu}]{wang2025talkstructurallyacthierarchically}
Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, and Shingo Takamatsu. 2025{\natexlab{f}}.
\newblock \href {http://arxiv.org/abs/2502.11098} {Talk structurally, act hierarchically: A collaborative framework for llm multi-agent systems}.
\newblock In \emph{arXiv}.

\bibitem[{Weber et~al.(2024)Weber, Fu, Anthony, Oren, Adams, Alexandrov, Lyu, Nguyen, Yao, Adams, Athiwaratkun, Chalamala, Chen, Ryabinin, Dao, Liang, Ré, Rish, and Zhang}]{weber2024redpajamaopendatasettraining}
Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce~Zhang. 2024.
\newblock \href {http://arxiv.org/abs/2411.12372} {Redpajama: an open dataset for training large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Wei et~al.(2024{\natexlab{a}})Wei, Karina, Chung, Jiao, Papay, Glaese, Schulman, and Fedus}]{wei2024measuringshortformfactualitylarge}
Jason Wei, Nguyen Karina, Hyung~Won Chung, Yunxin~Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2411.04368} {Measuring short-form factuality in large language models}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al. 2022.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html} {Chain-of-thought prompting elicits reasoning in large language models}.
\newblock \emph{Advances in neural information processing systems}, 35:24824--24837.

\bibitem[{Wei et~al.(2024{\natexlab{b}})Wei, Yang, Song, Lu, Hu, Huang, Tran, Peng, Liu, Huang, Du, and Le}]{wei2024longformfactualitylargelanguage}
Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da~Huang, Cosmo Du, and Quoc~V. Le. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2403.18802} {Long-form factuality in large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Welleck et~al.(2024)Welleck, Bertsch, Finlayson, Schoelkopf, Xie, Neubig, Kulikov, and Harchaoui}]{welleck2024decoding}
Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024.
\newblock From decoding to meta-generation: Inference-time algorithms for large language models.
\newblock \emph{arXiv preprint arXiv:2406.16838}.

\bibitem[{Wen et~al.(2025)Wen, Cai, Xiao, He, An, Duan, Du, Liu, Tang, Lv et~al.}]{wen2025lightxi}
Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi~An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et~al. 2025.
\newblock \href {https://arxiv.org/abs/2503.10460} {Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond}.
\newblock \emph{arXiv preprint arXiv:2503.10460}.

\bibitem[{Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen}]{wettig2024quratingselectinghighqualitydata}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024.
\newblock \href {http://arxiv.org/abs/2402.09739} {Qurating: Selecting high-quality data for training language models}.
\newblock In \emph{arXiv}.

\bibitem[{Wu et~al.(2024{\natexlab{a}})Wu, Peng, Du, Zheng, Liu, Wu, Ma, Li, Yang, Zhou, Lin, Zhao, Zhang, Huang, Zhang, Lin, and Liu}]{wu2024comparativestudyreasoningpatterns}
Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge~Zhang, Chenghua Lin, and J.~H. Liu. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2410.13639} {A comparative study on reasoning patterns of openai's o1 model}.
\newblock In \emph{arXiv}.

\bibitem[{Wu et~al.(2024{\natexlab{b}})Wu, Lan, Yuan, Jiao, Weston, and Sukhbaatar}]{wu2024thinkingllmsgeneralinstruction}
Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2410.10630} {Thinking llms: General instruction following with thought generation}.
\newblock In \emph{arXiv}.

\bibitem[{Wu et~al.(2024{\natexlab{c}})Wu, Sun, Li, Welleck, and Yang}]{wu2024inference}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024{\natexlab{c}}.
\newblock Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models.
\newblock \emph{arXiv preprint arXiv:2408.00724}.

\bibitem[{Wu et~al.(2024{\natexlab{d}})Wu, Sun, Li, Welleck, and Yang}]{wu2024scaling}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024{\natexlab{d}}.
\newblock \href {https://openreview.net/forum?id=j7DZWSc8qu} {Scaling inference computation: Compute-optimal inference for problem-solving with language models}.
\newblock In \emph{Workshop on Mathematical Reasoning and AI at NeurIPS'24}.

\bibitem[{Wu et~al.(2025{\natexlab{a}})Wu, Sun, Li, Welleck, and Yang}]{wu2025inferencescalinglawsempirical}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2025{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=VNckp7JEHn} {Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving}.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}.

\bibitem[{Wu et~al.(2025{\natexlab{b}})Wu, Wang, Du, Jegelka, and Wang}]{wu2025lessunderstandingchainofthoughtlength}
Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2502.07266} {When more is less: Understanding chain-of-thought length in llms}.
\newblock In \emph{arXiv}.

\bibitem[{Wu and Ito(2025)}]{wu2025hiddenstrengthdisagreementunraveling}
Zengqing Wu and Takayuki Ito. 2025.
\newblock \href {http://arxiv.org/abs/2502.16565} {The hidden strength of disagreement: Unraveling the consensus-diversity tradeoff in adaptive multi-agent systems}.
\newblock In \emph{arXiv}.

\bibitem[{X-R1Team(2025)}]{xr12025}
X-R1Team. 2025.
\newblock X-r1.
\newblock \url{https://github.com/dhcode-cpp/X-R1}.
\newblock Github.

\bibitem[{xAI(2025)}]{xai-gork3}
xAI. 2025.
\newblock \href {https://x.ai/news/grok-3} {Grok 3 beta - the age of reasoning agents}.

\bibitem[{Xiang et~al.(2024)Xiang, Liu, Jiang, Nie, Huang, Fan, Li, Huang, Zeng, Han, Hong, Xu, and Liang}]{xiang2024atomthinkslowthinkingframework}
Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan Liang. 2024.
\newblock \href {http://arxiv.org/abs/2411.11930} {Atomthink: A slow thinking framework for multimodal mathematical reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Xiang et~al.(2025)Xiang, Snell, Gandhi, Albalak, Singh, Blagden, Phung, Rafailov, Lile, Mahan, Castricato, Franken, Haber, and Finn}]{xiang20252reasoningllmslearning}
Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, and Chelsea Finn. 2025.
\newblock \href {http://arxiv.org/abs/2501.04682} {Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought}.

\bibitem[{Xie et~al.(2024)Xie, Zhang, Chen, Zhu, Lou, Tian, Xiao, and Su}]{xie2024travelplannerbenchmarkrealworldplanning}
Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu~Su. 2024.
\newblock Travelplanner: A benchmark for real-world planning with language agents.
\newblock In \emph{International Conference on Machine Learning}, pages 54590--54613. PMLR.

\bibitem[{Xie et~al.(2025)Xie, Gao, Ren, Luo, Hong, Dai, Zhou, Qiu, Wu, and Luo}]{xie2025logic}
Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025.
\newblock \href {https://arxiv.org/abs/2502.14768} {Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning}.
\newblock \emph{arXiv preprint arXiv:2502.14768}.

\bibitem[{Xie et~al.(2023)Xie, Kawaguchi, Zhao, Zhao, Kan, He, and Xie}]{xie2023selfevaluation}
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu~Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023.
\newblock \href {https://openreview.net/forum?id=Bw82hwg5Q3} {Self-evaluation guided beam search for reasoning}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Xu et~al.(2025{\natexlab{a}})Xu, Wu, Wang, Li, Zheng, Chen, Hu, Kang, Ji, Zhang, Guo, Yang, Zhang, and Zhang}]{xu2025redstardoesscalinglongcot}
Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da~Zheng, Boyuan Chen, Yi~Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2501.11284} {Redstar: Does scaling long-cot data unlock better slow-reasoning systems?}
\newblock In \emph{arXiv}.

\bibitem[{Xu et~al.(2025{\natexlab{b}})Xu, Xie, Zhao, and He}]{xu2025chaindraftthinkingfaster}
Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2502.18600} {Chain of draft: Thinking faster by writing less}.
\newblock In \emph{arXiv}.

\bibitem[{Xu et~al.(2023)Xu, Wang, Pan, Song, Freitag, Wang, and Li}]{xu2023instructscore}
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William~Yang Wang, and Lei Li. 2023.
\newblock \href {https://openreview.net/forum?id=eaUi1mcvrM} {{INSTRUCTSCORE}: Towards explainable text generation evaluation with automatic feedback}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Xu et~al.(2025{\natexlab{c}})Xu, Guo, Zeng, and Miao}]{xu2025softcotsoftchainofthoughtefficient}
Yige Xu, Xu~Guo, Zhiwei Zeng, and Chunyan Miao. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2502.12134} {Softcot: Soft chain-of-thought for efficient reasoning with llms}.
\newblock In \emph{arXiv}.

\bibitem[{Yan et~al.(2024)Yan, Mao, Ji, Zhang, Patil, Stoica, and Gonzalez}]{berkeley-function-calling-leaderboard}
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir~G. Patil, Ion Stoica, and Joseph~E. Gonzalez. 2024.
\newblock Berkeley function calling leaderboard.
\newblock \url{https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html}.

\bibitem[{Yang et~al.(2025{\natexlab{a}})Yang, Yu, Cui, and Wang}]{yang2025reasonflux}
Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2502.06772} {Reasonflux: Hierarchical llm reasoning via scaling thought templates}.
\newblock In \emph{arXiv}.

\bibitem[{Yang et~al.(2025{\natexlab{b}})Yang, Ma, Lin, and Wei}]{yang2025towards}
Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025{\natexlab{b}}.
\newblock Towards thinking-optimal scaling of test-time compute for llm reasoning.
\newblock \emph{arXiv preprint arXiv:2502.18080}.

\bibitem[{Yang et~al.(2024)Yang, Liu, Yu, Keung, Li, Liu, Hong, Ma, Jin, and Li}]{yang2024exploringunleashingpowerlarge}
Zhen Yang, Fang Liu, Zhongxing Yu, Jacky~Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge~Li. 2024.
\newblock \href {http://arxiv.org/abs/2404.14646} {Exploring and unleashing the power of large language models in automated code translation}.
\newblock In \emph{arXiv}.

\bibitem[{Yao et~al.(2023{\natexlab{a}})Yao, Chen, Yang, and Narasimhan}]{yao2023webshop}
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2207.01206} {Webshop: Towards scalable real-world web interaction with grounded language agents}.
\newblock In \emph{arXiv}.

\bibitem[{Yao et~al.(2024)Yao, Shinn, Razavi, and Narasimhan}]{yao2024taubenchbenchmarktoolagentuserinteraction}
Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024.
\newblock \href {http://arxiv.org/abs/2406.12045} {$\tau$-bench: A benchmark for tool-agent-user interaction in real-world domains}.
\newblock In \emph{arXiv}.

\bibitem[{Yao et~al.(2023{\natexlab{b}})Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan}]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan Cao, and Karthik~R Narasimhan. 2023{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=5Xc1ecxO1h} {Tree of thoughts: Deliberate problem solving with large language models}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Yao et~al.(2023{\natexlab{c}})Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao}]{yao2023react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik~R Narasimhan, and Yuan Cao. 2023{\natexlab{c}}.
\newblock \href {https://openreview.net/forum?id=WE_vluYUL-X} {React: Synergizing reasoning and acting in language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ye et~al.(2025)Ye, Huang, Xiao, Chern, Xia, and Liu}]{ye2025limoreasoning}
Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025.
\newblock \href {http://arxiv.org/abs/2502.03387} {{LIMO}: Less is more for reasoning}.
\newblock In \emph{arXiv}.

\bibitem[{Ye et~al.(2024)Ye, Agarwal, Liu, Joshi, Velury, Le, Tan, and Liu}]{ye2024evolvingalignmentasymmetricselfplay}
Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc~V. Le, Qijun Tan, and Yuan Liu. 2024.
\newblock \href {http://arxiv.org/abs/2411.00062} {Evolving alignment via asymmetric self-play}.
\newblock In \emph{arXiv}.

\bibitem[{Yeo et~al.(2025)Yeo, Tong, Niu, Neubig, and Yue}]{yeo2025demystifying}
Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025.
\newblock \href {http://arxiv.org/abs/2502.03373} {Demystifying long chain-of-thought reasoning in llms}.
\newblock In \emph{arXiv}.

\bibitem[{Yi et~al.(2025)Yi, Li, Hu, Zhang, Zhang, and Liu}]{yi2025sppd}
Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di~Zhang, and Yong Liu. 2025.
\newblock \href {https://arxiv.org/abs/2502.13516} {Sppd: Self-training with process preference learning using dynamic value margin}.
\newblock \emph{arXiv preprint arXiv:2502.13516}.

\bibitem[{Yu et~al.(2024{\natexlab{a}})Yu, Peng, Tian, Song, Mi, and Yu}]{yu2024siamselfimprovingcodeassistedmathematical}
Dian Yu, Baolin Peng, Ye~Tian, Linfeng Song, Haitao Mi, and Dong Yu. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2408.15565} {Siam: Self-improving code-assisted mathematical reasoning of large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Yu et~al.(2024{\natexlab{b}})Yu, Gao, and Wang}]{yu2024ovm}
Fei Yu, Anningzhe Gao, and Benyou Wang. 2024{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2024.findings-naacl.55/} {Ovm, outcome-supervised value models for planning in mathematical reasoning}.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 858--875.

\bibitem[{Yu et~al.(2024{\natexlab{c}})Yu, Xu, Weston, and Kulikov}]{yu2024distilling21}
Ping Yu, Jing Xu, Jason~E Weston, and Ilia Kulikov. 2024{\natexlab{c}}.
\newblock \href {https://openreview.net/forum?id=WUoC4BpJBC} {Distilling system 2 into system 1}.
\newblock In \emph{The First Workshop on System-2 Reasoning at Scale, NeurIPS'24}.

\bibitem[{Yu et~al.(2025)Yu, Zhang, Zhu, Yuan, Zuo, Yue, Fan, Liu, Liu, Liu et~al.}]{yu2025dapo}
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu~Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et~al. 2025.
\newblock \href {https://arxiv.org/abs/2503.14476} {Dapo: An open-source llm reinforcement learning system at scale}.
\newblock \emph{arXiv preprint arXiv:2503.14476}.

\bibitem[{Yu et~al.(2024{\natexlab{d}})Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang}]{yu2024mm}
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024{\natexlab{d}}.
\newblock Mm-vet: Evaluating large multimodal models for integrated capabilities.
\newblock In \emph{International Conference on Machine Learning}, pages 57730--57754. PMLR.

\bibitem[{Yuan et~al.(2025)Yuan, Yue, Zhu, Fan, and Yan}]{yuan2025s}
Yufeng Yuan, Yu~Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. 2025.
\newblock \href {https://arxiv.org/abs/2503.01491} {What's behind ppo's collapse in long-cot? value optimization holds the secret}.
\newblock \emph{arXiv preprint arXiv:2503.01491}.

\bibitem[{Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Lu, Tan, Zhou, and Zhou}]{yuan2023scaling}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023.
\newblock \href {http://arxiv.org/abs/2308.01825} {Scaling relationship on learning mathematical reasoning with large language models}.

\bibitem[{Yue et~al.(2024)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, Wei, Yu, Yuan, Sun, Yin, Zheng, Yang, Liu, Huang, Sun, Su, and Chen}]{yue2024mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen. 2024.
\newblock \href {http://arxiv.org/abs/2311.16502} {Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi}.
\newblock In \emph{arXiv}.

\bibitem[{Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman}]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:15476--15488.

\bibitem[{Zeng et~al.(2025{\natexlab{a}})Zeng, Huang, Liu, Liu, He, Ma, and He}]{zeng2025simplerlzoo}
Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2503.18892} {Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild}.
\newblock \emph{arXiv preprint arXiv:2503.18892}.

\bibitem[{Zeng et~al.(2025{\natexlab{b}})Zeng, Huang, Liu, He, Liu, Ma, and He}]{zeng2025simplerl}
Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025{\natexlab{b}}.
\newblock 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient.
\newblock \url{https://hkust-nlp.notion.site/simplerl-reason}.
\newblock Notion Blog.

\bibitem[{Zeng et~al.(2025{\natexlab{c}})Zeng, Ding, Wang, Liu, Ning, Hou, Huang, Qin, and Liu}]{zeng2025itoolboostingtooluse}
Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu~Ning, Yutai Hou, Xu~Huang, Bing Qin, and Ting Liu. 2025{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2501.09766} {itool: Boosting tool use of large language models via iterative reinforced fine-tuning}.
\newblock In \emph{arXiv}.

\bibitem[{Zeng et~al.(2025{\natexlab{d}})Zeng, Cheng, Yin, Zhou, and Qiu}]{zeng2025revisiting}
Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. 2025{\natexlab{d}}.
\newblock Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities?
\newblock \emph{arXiv preprint arXiv:2502.12215}.

\bibitem[{Zeng et~al.(2024)Zeng, Liu, Wan, Li, Chen, Dai, Yao, Xu, Qi, Zhao, Shen, Lu, Tan, Chen, Zhang, Shi, Wang, Guo, and Jia}]{zeng2024mrben}
Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. 2024.
\newblock \href {https://openreview.net/forum?id=GN2qbxZlni} {{MR}-ben: A meta-reasoning benchmark for evaluating system-2 thinking in {LLM}s}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Zhai et~al.(2024)Zhai, Bai, Lin, Pan, Tong, Zhou, Suhr, Xie, LeCun, Ma, and Levine}]{zhai2024finetuninglargevisionlanguagemodels}
Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi~Ma, and Sergey Levine. 2024.
\newblock \href {http://arxiv.org/abs/2405.10292} {Fine-tuning large vision-language models as decision-making agents via reinforcement learning}.
\newblock In \emph{arXiv}.

\bibitem[{Zhang et~al.(2024{\natexlab{a}})Zhang, Zhoubian, Hu, Yue, Dong, and Tang}]{zhang2024rest}
Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=8rcFOqEud5} {Re{ST}-{MCTS}*: {LLM} self-training via process reward guided tree search}.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2025{\natexlab{a}})Zhang, Zhu, Sun, Luo, Qiao, Du, Zheng, Chen, and Zhang}]{zhang2025lightthinkerthinkingstepbystepcompression}
Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da~Zheng, Huajun Chen, and Ningyu Zhang. 2025{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2502.15589} {Lightthinker: Thinking step-by-step compression}.
\newblock In \emph{arXiv}.

\bibitem[{Zhang et~al.(2025{\natexlab{b}})Zhang, Li, Li, Dong, and Jin}]{zhang2025focused}
Kechi Zhang, Ge~Li, Jia Li, Yihong Dong, and Zhi Jin. 2025{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2502.11475} {Focused-dpo: Enhancing code generation through focused preference optimization on error-prone points}.
\newblock \emph{arXiv preprint arXiv:2502.11475}.

\bibitem[{Zhang et~al.(2025{\natexlab{c}})Zhang, Yao, Lai, Huang, Fang, Tao, Song, and Liu}]{zhang2025reasoning}
Kongcheng Zhang, Qi~Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, and Shunyu Liu. 2025{\natexlab{c}}.
\newblock Reasoning with reinforced functional token tuning.
\newblock \emph{arXiv preprint arXiv:2502.13389}.

\bibitem[{Zhang et~al.(2025{\natexlab{d}})Zhang, Hosseini, Bansal, Kazemi, Kumar, and Agarwal}]{zhang2025generativeverifiersrewardmodeling}
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2025{\natexlab{d}}.
\newblock \href {http://arxiv.org/abs/2408.15240} {Generative verifiers: Reward modeling as next-token prediction}.
\newblock In \emph{arXiv}.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Yin, and Liu}]{zhang2023multimodalneuralgeometricsolver}
Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. 2023{\natexlab{a}}.
\newblock \href {https://www.ijcai.org/proceedings/2023/0376.pdf} {A multi-modal neural geometric solver with textual clauses parsed from diagram}.
\newblock In \emph{Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence}, pages 3374--3382.

\bibitem[{Zhang et~al.(2025{\natexlab{e}})Zhang, Wang, Jiang, Li, Wu, Wang, Jiang, Shang, Tang, Lyu, and Ma}]{zhang2025crowd}
Qiyuan Zhang, Yufei Wang, Yuxin Jiang, Liangyou Li, Chuhan Wu, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, and Chen Ma. 2025{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2502.12501} {Crowd comparative reasoning: Unlocking comprehensive evaluations for llm-as-a-judge}.
\newblock In \emph{arXiv}.

\bibitem[{Zhang et~al.(2025{\natexlab{f}})Zhang, Wang, YU, Jiang, Wu, Li, Wang, Jiang, Shang, Tang, Lyu, and Ma}]{zhang2025reviseval}
Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, and Chen Ma. 2025{\natexlab{f}}.
\newblock \href {https://openreview.net/forum?id=1tBvzOYTLF} {Reviseval: Improving {LLM}-as-a-judge via response-adapted references}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Shen, Ding, Tenenbaum, and Gan}]{zhang2023planning}
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua~B. Tenenbaum, and Chuang Gan. 2023{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=Lr8cOOtYbfL} {Planning with large language models for code generation}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2024{\natexlab{b}})Zhang, Du, Pang, Liu, Gao, and Lin}]{zhang2024chain}
Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=2cczgOfMP4} {Chain of preference optimization: Improving chain-of-thought reasoning in {LLM}s}.
\newblock In \emph{Conference on Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2024{\natexlab{c}})Zhang, Chen, Zhou, Wang, Si, Wang, Lu, and Qin}]{zhang2024wrongofthoughtintegratedreasoningframework}
Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, and Libo Qin. 2024{\natexlab{c}}.
\newblock \href {https://aclanthology.org/2024.findings-emnlp.388/} {Wrong-of-thought: An integrated reasoning framework with multi-perspective verification and wrong information}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 6644--6653.

\bibitem[{Zhang et~al.(2024{\natexlab{d}})Zhang, Khalifa, Logeswaran, Kim, Lee, Lee, and Wang}]{zhang2024smalllanguagemodelsneed}
Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu~Wang. 2024{\natexlab{d}}.
\newblock \href {https://aclanthology.org/2024.findings-acl.924/} {Small language models need strong verifiers to self-correct reasoning}.
\newblock In \emph{ACL (Findings)}.

\bibitem[{Zhang et~al.(2024{\natexlab{e}})Zhang, Wu, Yang, Shu, Xiao, Kong, and Sang}]{zhang2024o1coder}
Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024{\natexlab{e}}.
\newblock \href {http://arxiv.org/abs/2412.00154} {o1-coder: an o1 replication for coding}.
\newblock In \emph{arXiv}.

\bibitem[{Zhao et~al.(2024)Zhao, Yin, Zeng, Wang, Shi, Lyu, Wang, Luo, and Zhang}]{zhao2024marcoo1openreasoningmodels}
Yu~Zhao, Huifeng Yin, Bo~Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024.
\newblock \href {http://arxiv.org/abs/2411.14405} {Marco-o1: Towards open reasoning models for open-ended solutions}.
\newblock In \emph{arXiv}.

\bibitem[{Zheng et~al.(2023{\natexlab{a}})Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica}]{NEURIPS2023_91f18a12}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph~E Gonzalez, and Ion Stoica. 2023{\natexlab{a}}.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, pages 46595--46623.

\bibitem[{Zheng et~al.(2023{\natexlab{b}})Zheng, Dou, Gao, Hua, Shen, Wang, Liu, Jin, Liu, Zhou et~al.}]{zheng2023secrets}
Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et~al. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2307.04964} {Secrets of rlhf in large language models part i: Ppo}.
\newblock \emph{arXiv preprint arXiv:2307.04964}.

\bibitem[{Zhong et~al.(2024)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2024agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2024.
\newblock {AGIE}val: A human-centric benchmark for evaluating foundation models.
\newblock In \emph{Findings of North American Chapter of the Association for Computational Linguistics}, pages 2299--2314.

\bibitem[{Zhou et~al.(2023{\natexlab{a}})Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le et~al.}]{zhou2023leasttomostpromptingenablescomplex}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V Le, et~al. 2023{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=WZH7099tgfM} {Least-to-most prompting enables complex reasoning in large language models}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{Zhou et~al.(2025)Zhou, Zheng, Wang, Xi, Dou, Bao, Shen, Xiong, Fan, Mou, Zheng, Gui, Zhang, and Huang}]{zhou2025rmbcomprehensivelybenchmarkingreward}
Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2025.
\newblock \href {http://arxiv.org/abs/2410.09893} {Rmb: Comprehensively benchmarking reward models in llm alignment}.
\newblock In \emph{arXiv}.

\bibitem[{Zhou et~al.(2023{\natexlab{b}})Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{zhou2023instructionfollowing}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2311.07911} {Instruction-following evaluation for large language models}.
\newblock In \emph{arXiv}.

\bibitem[{Zhou et~al.(2023{\natexlab{c}})Zhou, Xu, Zhu, Zhou, Lo, Sridhar, Cheng, Bisk, Fried, Alon et~al.}]{zhou2023webarena}
Shuyan Zhou, Frank~F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et~al. 2023{\natexlab{c}}.
\newblock \href {https://webarena.dev} {Webarena: A realistic web environment for building autonomous agents}.
\newblock \emph{arXiv preprint arXiv:2307.13854}.

\end{thebibliography}
