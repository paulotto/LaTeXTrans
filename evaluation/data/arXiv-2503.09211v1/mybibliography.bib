@inproceedings{
goyal2024think,
title={Think before you speak: Training Language Models With Pause Tokens},
author={Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ph04CRkPdC}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@inproceedings{
yao2023tree,
title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=5Xc1ecxO1h}
}

@misc{zelikman2024quietstar,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
huang2024large,
title={Large Language Models Cannot Self-Correct Reasoning Yet},
author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=IkmD3fKBPQ}
}

@misc{tyen2024llms,
      title={LLMs cannot find reasoning errors, but can correct them!}, 
      author={Gladys Tyen and Hassan Mansoor and Victor CÄƒrbune and Peter Chen and Tony Mak},
      year={2024},
      eprint={2311.08516},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{
turpin2023language,
title={Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
author={Miles Turpin and Julian Michael and Ethan Perez and Samuel R. Bowman},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=bzs4uPLXvi}
}

@inproceedings{
madaan2023selfrefine,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}

@misc{pfau2024lets,
      title={Let's Think Dot by Dot: Hidden Computation in Transformer Language Models}, 
      author={Jacob Pfau and William Merrill and Samuel R. Bowman},
      year={2024},
      eprint={2404.15758},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024language,
      title={Do language models plan ahead for future tokens?}, 
      author={Wilson Wu and John X. Morris and Lionel Levine},
      year={2024},
      eprint={2404.00859},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hong2024orpo,
      title={ORPO: Monolithic Preference Optimization without Reference Model}, 
      author={Jiwoo Hong and Noah Lee and James Thorne},
      year={2024},
      eprint={2403.07691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}