\begin{table}[t!]
	\caption{Using two models requiring roughly the same GPU memory, we observe that the memory breakdown and the impact of PEFT methods application are very different. For each model, we show the evolution of the GPU memory ($\times10^3$ MiB) required for performing one training step for OPT-1B3 \citep{zhang_22} with a batch size of 1 and a sequence length of \numprint{128} and \textsc{Bert}-base \citep{devlin_19} with a batch size of \numprint{256}, a sequence length of \numprint{128}. Fwd (w/o grad) corresponds to the execution of the forward pass, while disabling gradient computation.}
	\label{table: Distribution of the GPU memory.}
\setlength{\tabcolsep}{2pt}
    \begin{tabularx}{0.48\textwidth}{lYY|YY} \toprule
         & \multicolumn{2}{c}{}  & \multicolumn{2}{c}{w/ LoRA}\\
         & \textsc{Bert} & OPT  & \textsc{Bert} & OPT \\
        \midrule
Cuda Context & \numprint{0.8} & \numprint{0.8}  & \numprint{0.8} & \numprint{0.8}\\
+ Model weights & \numprint{1.3} &\textbf{\numprint{5.8}} & \numprint{1.3} &\textbf{\numprint{5.8}}\\
+ Fwd (w/o grad) & \numprint{2.9} &\numprint{6.1}&  \numprint{2.9} &\numprint{6.1}\\
+ Fwd (w/ grad) & \textbf{\numprint{24.8}} & \numprint{6.3}&\textbf{\numprint{20.6}}&\numprint{6.3} \\
+ Bwd & \numprint{25.2} & \textbf{\numprint{11.3}}&\numprint{21.0}& \numprint{6.3}\\
+ Optimizer step & \numprint{25.2} & \textbf{\numprint{21.4}}&\numprint{21.0}& \numprint{6.3}\\
\bottomrule
    \end{tabularx}
    \vspace{-0.75em}
\end{table}