\begin{table*}[t!]
    \centering
    \small
\caption{GPU memory required to fine-tune Llama2-7B~\citep{touvron_23} using \method with a varying selection ratio, as well as QLoRA and LoRA. 
    	Since we could not perform full fine-tuning on our hardware, we estimate the full fine-tuning memory based on the memory reported for \method,  \method + LoRA, and LoRA.
    	See \Cref{sec:exp:large:gpu_memory} and \Cref{fig:llm-memory} for details of the experiment.}
    \label{tab:gpu_mem_usage}

	\setlength{\tabcolsep}{1pt}    
    \begin{tabularx}{1.0\textwidth}{YYYYYYY}
    \toprule
    Selection Ratio & \method (Ours) + QLoRA & QLoRA & \method (Ours) + LoRA & LoRA & \method (Ours) & Full Fine-Tuning \\
    \midrule
    12.5\% & 11.7 GiB & 51.9 GiB & 44.6 GiB & 80.4 GiB & 64.0 GiB & 91.4 GiB \\
    25.0\% & 17.2 GiB & 51.9 GiB & 48.5 GiB & 80.4 GiB & 65.0 GiB & 91.4 GiB \\
    37.5\% & 22.0 GiB & 51.9 GiB & 53.7 GiB & 80.4 GiB & 66.3 GiB & 91.4 GiB \\
    50.0\% & 27.4 GiB & 51.9 GiB & 58.3 GiB & 80.4 GiB & 70.2 GiB & 91.4 GiB \\
    62.5\% & 32.7 GiB & 51.9 GiB & 63.0 GiB & 80.4 GiB & 74.6 GiB & 91.4 GiB \\
    75.0\% & 38.8 GiB & 51.9 GiB & 68.1 GiB & 80.4 GiB & 79.5 GiB & 91.4 GiB \\
    87.5\% & 43.7 GiB & 51.9 GiB & 73.4 GiB & 80.4 GiB & 83.8 GiB & 91.4 GiB \\
    99.9\% & 49.0 GiB & 51.9 GiB & 77.7 GiB & 80.4 GiB & 88.7 GiB & 91.4 GiB \\
\bottomrule
    \end{tabularx}
\end{table*}

