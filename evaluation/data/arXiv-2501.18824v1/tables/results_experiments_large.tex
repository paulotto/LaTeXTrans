\begin{table*}[t]
    \centering
\caption{Few-shot evaluation on question-answering benchmarks including: AI2 Reasoning Challenge (25-shot) \citep{clark_18}, MMLU (5-shot) \citep{hendrycks_21}, HellaSwag (10-shot) \citep{zellers19}, TruthfulQA (0-shot) \citep{lin_22}, and WinoGrande (0-shot) \citep{DBLP:conf/aaai/SakaguchiBBC20}. We use the evaluation scripts and prompt formatting from the "Language Model Evaluation Harness" \citep{eval-harness}. We report the average accuracy on five MMLU ethics tasks and WinoGrande, the normed accuracy on ARC and HellaSwag, and the MC2 score on TruthfulQA. 
   	We indicate in \textbf{bold} the best result for each task.
   	We report the results with the raw Llama2-7B model \citep{touvron_23} and the Llama2-7B fine-tuned on the Platypus curated instruction dataset \citep{lee_23} using LoRA \citep{hu_22}, QLoRA \citep{dettmers_23} and the proposed \method. When fine-tuning with \method, we select 30\% of the tokens for the gradient computation.}
    \label{tab:llm-perf}

	\setlength{\tabcolsep}{1pt}    


  


\resizebox{.98\textwidth}{!}{

\begin{tabularx}{\textwidth}{lYYYYYY}
	\toprule
	\makecell[c]{{Method}} & {MMLU} & {ARC} & \makecell[c]{{Hella}\\{Swag}} & \makecell[c]{{Truthful}\\{QA}} & \makecell[c]{{Wino}\\{Grande}} & {Avg. $\uparrow$}\\
	\midrule
	Llama 7B & 64.44 & 52.39 & \textbf{78.97} & 38.97 & 68.90 & 60.73\\\midrule
	Llama 7B w/ LoRA & \textbf{65.89} & 55.38 & 78.76 & 42.64 & 68.35 & 62.20\\
	\rowcolor{lightcyan}
	Llama 7B w/ LoRA+\method (Ours) & 65.42 & 54.01 & 78.82 & \textbf{43.78} & 68.35 & 62.08\\\midrule
	Llama 7B w/ QLoRA & 65.08 & \textbf{56.06} & 78.60 & 43.64 & 69.38 & \textbf{62.55}\\
	\rowcolor{lightcyan}
	Llama 7B w/ QLoRA+\method (Ours) & 65.78 & 53.92 & 78.74 & 41.91 & 69.38 & 61.95\\\midrule
	\rowcolor{lightcyan}
	Llama 7B w/ \method (Ours) & 63.06 & 53.07 & 77.90 & 42.18 & \textbf{69.93} & 61.23\\
	\bottomrule
\end{tabularx}

} 

\end{table*}