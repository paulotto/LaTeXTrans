







\begin{table}[t]
 	\centering
\caption{
   Few-shot evaluation results and peak memory usage (GiB) as Llama2-7B is fine-tuned on instruction datasets 
   with (a) \method, (b) \method + LoRA and (c) \method + QLoRA, varying the selection ratio of input tokens. Best results in \textbf{bold}.
 }
 	\label{tab:ablation:tokenratio}
 	\setlength{\tabcolsep}{0.3em}

        \begin{subtable}[c]{0.5\textwidth}
 		\centering
 		\caption{\method{}}
 		\label{tab:ablation:tokenratio:sft}
 		\resizebox{.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
 			\toprule
 			\makecell[c]{\textbf{Selection}\\\textbf{Ratio}} & \makecell[c]{\textbf{Peak}\\\textbf{Mem.}} & \textbf{MMLU} & \textbf{ARC} & \makecell[c]{\textbf{Hella}\\\textbf{Swag}} & \makecell[c]{\textbf{Truthful}\\\textbf{QA}} & \makecell[c]{\textbf{Wino}\\\textbf{Grande}} & \makecell[c]{\textbf{Avg.}\\\textbf{Perf.}}\\
 			\midrule
 			10\% & \textbf{64.40} & 61.56 & 51.71 & 78.35 & 41.88 & 70.01 & 60.70\\
 			20\% & 65.08 & \textbf{65.01} & 52.65 & \textbf{78.37} & 42.02 & 69.46 & \textbf{61.50}\\
 			30\% & 65.94 & 63.06 & \textbf{53.07} & 77.90 & \textbf{42.18} & 69.93 & 61.23\\
 			40\% & 68.42 & 63.78 & 52.90 & 77.90 & 41.45 & \textbf{70.32} & 61.27\\
 			50\% & 74.32 & 62.98 & 52.73 & 78.32 & 42.11 & 69.38 & 61.10\\
 			\bottomrule
 		\end{tabular}
 		}\end{subtable}
 	
 	\vspace{0.5em}
	
 	\begin{subtable}[c]{0.5\textwidth}
 		\centering
 		\caption{\method{} + LoRA}
 		\label{tab:ablation:tokenratio:lora+sft}
 		\resizebox{.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
 			\toprule
 			\makecell[c]{\textbf{Selection}\\\textbf{Ratio}} & \makecell[c]{\textbf{Peak}\\\textbf{Mem.}} & \textbf{MMLU} & \textbf{ARC} & \makecell[c]{\textbf{Hella}\\\textbf{Swag}} & \makecell[c]{\textbf{Truthful}\\\textbf{QA}} & \makecell[c]{\textbf{Wino}\\\textbf{Grande}} & \makecell[c]{\textbf{Avg.}\\\textbf{Perf.}}\\
 			\midrule
 			10\% & \textbf{45.47} & 64.17 & \textbf{54.44} & 78.68 & 38.77 & \textbf{69.61} & 61.13\\
 			20\% & 48.21 & 65.41 & 54.35 & \textbf{79.01} & 42.21 & 69.38 & 62.07\\
 			30\% & 52.77 & 65.42 & 54.01 & 78.82 & \textbf{43.78} & 68.35 & 62.08\\
 			40\% & 56.31 & 64.35 & 52.65 & 78.69 & 41.05 & 68.90 & 61.13\\
 			50\% & 64.34 & \textbf{65.87} & 54.01 & 78.68 & 42.46 & 69.38 & \textbf{62.08}\\
 			\bottomrule
 		\end{tabular}
 		}\end{subtable}
 	
 	\vspace{0.5em}
 	
 	\begin{subtable}[c]{0.5\textwidth}
 		\centering
 		\caption{\method{} + QLoRA}
 		\label{tab:ablation:tokenratio:qlora+sft}
 		\resizebox{.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
			\toprule
			\makecell[c]{\textbf{Selection}\\\textbf{Ratio}} & \makecell[c]{\textbf{Peak}\\\textbf{Mem.}} & \textbf{MMLU} & \textbf{ARC} & \makecell[c]{\textbf{Hella}\\\textbf{Swag}} & \makecell[c]{\textbf{Truthful}\\\textbf{QA}} & \makecell[c]{\textbf{Wino}\\\textbf{Grande}} & \makecell[c]{\textbf{Avg.}\\\textbf{Perf.}}\\
			\midrule
			10\% & \textbf{11.47} & 63.54 & 54.18 & 78.58 & 39.79 & 68.98 & 61.02\\
			20\% & 15.68 & 64.05 & 53.92 & \textbf{78.81} & 40.33 & \textbf{69.85} & 61.39\\
			30\% & 19.71 & \textbf{65.78} & 53.92 & 78.74 & 41.91 & 69.38 & \textbf{61.95}\\
			40\% & 24.11 & 64.85 & \textbf{54.35} & 78.70 & \textbf{41.98} & 69.14 & 61.80\\
			50\% & 31.06 & 65.29 & 53.75 & 78.70 & 40.63 & 69.06 & 61.49\\
			\bottomrule
		\end{tabular}
 		}\end{subtable}
\end{table}


 
%
