\section{\method}  \label{sec:method}

Previous studies analyzing the structure of the sparsity of activations and gradients~\citep{DBLP:conf/icml/KurtzKGMCGLMSA20,DBLP:conf/icml/LiuWDZY0S0TRC23, DBLP:conf/acl/DaiDHSCW22}
suggest that some neurons and activations could have a predominant importance, while some others may have smaller contributions to the loss and output computation.
Inspired by these works, we hypothesize that for many downstream tasks, 
not all tokens in the sequence would need to be involved in the fine-tuning---more specifically, backpropagation---of transformer models.
Instead, we conjecture that, when restricted to backpropagating through a subset of tokens, 
transformers could be further optimized for the downstream task 
by enabling the additional learning and adjustments, which need to happen during the fine-tuning for the given task, 
to be done in a more compact way, i.e., by incorporating the additional knowledge more succinctly with respect to the selected subset of tokens.




Figure~\ref{fig:method} illustrates \method, aiming at reducing the memory needed to store the intermediate activations used for gradient computation. Given an input sequence $X$, a transformer associates each token from the input sequence to an embedding and computes a corresponding sequence of hidden states $h$ through multiple~layer~applications.
For each input sequence, we select $k$ random positions.\footnote{We select the positions using a uniform distribution. However, we always include the \texttt{[CLS]} token---a special symbol prepended as the beginning of every input sentence.} We organize each layer's input in two groups, 
one with the $k$ selected input positions,~$h_{\mathcal{G}}$, and the other with the remaining un-selected positions, $h_{\bar{\mathcal{G}}}$, such that $h = [ h_{\mathcal{G}}, h_{\bar{\mathcal{G}}}]$, with $[\quad]$ denoting the concatenation operator and $\bigm|\mathcal{G}\bigm|=k$. The re-ordering does not impact the computation as the position is directly encoded in the hidden states. 
With this token selection scheme, the classification objective $\mathcal{L}_{\text{CLS}}$ and the language modeling objective $\mathcal{L}_{\text{LM}}$ used by \method are as follows.


\paragraph{Classification Task.} 
The goal is to assign the right class or label $y$ for the given sequence.
Given the hidden states from the transformer layers,
we use the average of the hidden states from the $k$ selected positions of the last layer as input for an MLP,
which outputs a probability distribution over the classes of the task, as given by Eq.~\ref{eq: pooler}. 
During the evaluation, we use the average from all hidden states of the last layer as input for the MLP.
\begin{equation}
\begin{split}
    \pi &= \text{MLP}\left(\frac{1}{k}\sum_{i \in \mathcal{G}}h_{i}\right)\\
    p(y | X) &= \text{softmax}(\pi) \\
    \mathcal{L}_{\text{CLS}} &= -\log p(y | X)
\end{split}
  \label{eq: pooler}
\end{equation}

\paragraph{Language Modeling Task.} 
The goal is to learn the probability distribution of a token, given all preceding tokens.
We train the language model  by applying the traditional cross-entropy loss to the set of $k$ randomly selected positions as given by Eq.~\ref{equ:ref_loss} below, with $W_{\text{lm}}$ denoting the head projecting the hidden state back into the vocabulary dimension.

\begin{equation}
\begin{split}
p(x_i | x_{<i}) &= \text{softmax}(h_{i} W_{\text{lm}} ) \\
\mathcal{L}_{\text{LM}} &= -\sum_{i \in \mathcal{G}} \log P(x_i | x_{<i})
\end{split}
\label{equ:ref_loss}
\end{equation}
The key element of our method is that we disable the gradient computation for the un-selected tokens in $\bar{\mathcal{G}}$. 
Thus, only the $k$ selected tokens in $\mathcal{G}$ contribute to the gradient computation during the backward~pass. We detail the method in the case of dense layers and attention mechanism in \Cref{method:dense} and \Cref{method:att}, respectively.

\subsection{\method for Dense and Normalization Layers}
\label{method:dense}

We consider a dense layer $a = \sigma (z) = \sigma (hW + b)$ with weight $W$, bias $b$, nonlinear function~$\sigma$, input $h$, pre-activation~$z$, and output~$a$.
Eq.~\ref{eq: gradient of weights} computes the gradient with respect to $W$ and $b$ when backpropagating a loss~$\mathcal{L}$ through the layer:
\begin{align}\label{eq: gradient of weights}
\begin{split}
 \frac{\partial \mathcal{L}}{dW} &= \frac{\partial \mathcal{L}}{\partial a}\frac{\partial a}{\partial z}\frac{\partial z}{\partial W} =
    \frac{\partial \mathcal{L}}{\partial a} \sigma' h \\
\frac{\partial \mathcal{L}}{db} &= \frac{\partial \mathcal{L}}{\partial a}\frac{\partial a}{\partial z}\frac{\partial z}{\partial b} = \frac{\partial \mathcal{L}}{\partial a} \sigma'
\end{split}
\end{align}If we backpropagate the error only through the selected tokens in $\mathcal{G}$, and disable the gradient computation for the unselected positions in $\bar{\mathcal{G}}$, we have:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial a} = \left[ \frac{\partial \mathcal{L}}{\partial a_{\mathcal{G}}}, \frac{\partial \mathcal{L}}{\partial a_{\bar{\mathcal{G}}}} \right] = \left[ \frac{\partial \mathcal{L}}{\partial a_{\mathcal{G}}}, 0 \right]
\end{align}

Plugging that into Eq.~\ref{eq: gradient of weights}, we have:
\begin{align} \label{eq: gradient of weights final}
    \frac{\partial \mathcal{L}}{dW} = \left[ \frac{\partial \mathcal{L}}{\partial a_{\mathcal{G}}} \sigma' h_{\mathcal{G}}, 0 \right];
    \hspace{0.5em} \frac{\partial \mathcal{L}}{db} = \left[ \frac{\partial \mathcal{L}}{\partial a_{\mathcal{G}}} \sigma', 0 \right]
\end{align}
Given Eq.~\ref{eq: gradient of weights final}, we only need to cache $h_{\mathcal{G}}$ for applying the chain rule, instead of the full activation~$h$. 

Regarding implementation, we use Algorithm~\ref{alg:tokentune} which explicitly splits the hidden states into two groups where $h_{\mathcal{G}}$ corresponds to the tokens selected to be fine-tuned and $h_{\bar{\mathcal{G}}}$ corresponds to the un-selected tokens. As shown in Eq.~\ref{eq:dense:v} and Eq.~\ref{eq:dense:no-grad}, the forward pass is identical to standard fine-tuning except that we disable the gradient computation for the positions for $h_{\bar{\mathcal{G}}}$ in Eq.~\ref{eq:dense:no-grad} with the context "\texttt{torch.no\_grad()}" in PyTorch. 
\begin{align}
h_{\mathcal{G}} &= h_{\mathcal{G}}W + b \label{eq:dense:v}\\
h_{\bar{\mathcal{G}}} &= h_{\bar{\mathcal{G}}}W + b\label{eq:dense:no-grad}
\end{align}
where $W$ denotes the weights $W_1$ and $W_2$ for the feed-forward layers. We apply the same methodology for normalization layers.


\input{tables/results_experiments_medium}

\subsection{\method for Attention Layers}
\label{method:att}

For attention layers, we compute the attention as:
\begin{gather}
\left[Q_{\mathcal{G}}, K_{\mathcal{G}}, V_{\mathcal{G}}\right] = h_{\mathcal{G}}W_{\left[Q,K,V\right]}  + b_{\left[Q,K,V\right]}
\label{transformer:v}\\
\left[Q_{\bar{\mathcal{G}}}, K_{\bar{\mathcal{G}}}, V_{\bar{\mathcal{G}}}\right] = h_{\bar{\mathcal{G}}} W_{\left[Q,K,V\right]}  + b_{\left[Q,K,V\right]}
\label{transformer:v-no-grad}\\
h_{\mathcal{G}} = \softmax \left( \nicefrac{Q_{\mathcal{G}} \left[K_{\bar{\mathcal{G}}}, K_{\mathcal{G}}\right]^{\top}}{\sqrt{d}}  \right) \left[V_{\bar{\mathcal{G}}}, V_{\mathcal{G}}\right] \label{transformer:att-grad}\\
h_{\bar{\mathcal{G}}} = \softmax \left( \nicefrac{Q_{\bar{\mathcal{G}}} \left[K_{\bar{\mathcal{G}}}, K_{\mathcal{G}}\right]^{\top}}{\sqrt{d}}  \right) \left[V_{\bar{\mathcal{G}}}, V_{\mathcal{G}}\right] \label{transformer:att-no-grad}
\end{gather}
where  $W_{\left[Q,K,V\right]} \in \mathbb{R}^{d\times 3d}$ denotes the concatenated weights for the queries, keys, and values.
For the computation of un-selected positions in Eq.~\ref{transformer:v-no-grad} and Eq.~\ref{transformer:att-no-grad}, we again disable the gradient computation in PyTorch.
Algorithm~\ref{alg:tokentune} illustrates the steps for the forward pass of a transformer model with the proposed \method algorithm described in \Cref{method:att,method:dense}.


\begin{algorithm}[t]
	\setstretch{1.1}  \setlength{\lineskip}{3pt}
\small
\caption{\method (We omit layer normalization, skip connections, non-linear functions, and multi-head attention for simplicity)}
	\label{alg:tokentune}
\SetAlgoVlined  \SetKwBlock{WithNoGrad}{with \text{\normalfont{torch.no\_grad():}}}{end}
	\KwIn{input sequence $X$
	}
	\KwOut{$h_{\mathcal{G}}, h_{\bar{\mathcal{G}}}$
	}
\BlankLine
	Compute input token embeddings $h$ \\
	Re-organize input tokens into two groups ($h_{\mathcal{G}}$ and $h_{\bar{\mathcal{G}}}$)
	\BlankLine
	
\For{{layer} $ \textbf{\textup{in}}~\textup{transformers' layers} $}{
		\tcp{Compute the attention layer}
$ \left[Q_{\mathcal{G}}, K_{\mathcal{G}}, V_{\mathcal{G}}\right] = h_{\mathcal{G}}W_{\left[Q,K,V\right]}  + b_{\left[Q,K,V\right]}$  \\
$ h_{\mathcal{G}}  = \softmax \left( \frac{Q_{\mathcal{G}} \left[K_{\bar{\mathcal{G}}}, K_{\mathcal{G}}\right]^{\top}}{\sqrt{d}}  \right) \left[V_{\bar{\mathcal{G}}}, V_{\mathcal{G}}\right] $
		
		\BlankLine
		\WithNoGrad{
$ \left[Q_{\bar{\mathcal{G}}}, K_{\bar{\mathcal{G}}}, V_{\bar{\mathcal{G}}}\right] = h_{\bar{\mathcal{G}}} W_{\left[Q,K,V\right]}  + b_{\left[Q,K,V\right]} $ \\
$ h_{\bar{\mathcal{G}}} = \softmax \left( \frac{Q_{\bar{\mathcal{G}}} \left[K_{\bar{\mathcal{G}}}, K_{\mathcal{G}}\right]^{\top}}{\sqrt{d}}  \right) \left[V_{\bar{\mathcal{G}}}, V_{\mathcal{G}}\right] $
		}
		
		\BlankLine
		\tcp{Compute the feed-forward layer}
            $h_{\mathcal{G}} = h_{\mathcal{G}}W_1 + b_1$ \\
            $h_{\mathcal{G}} = h_{\mathcal{G}}W_2 + b_2$ \\
            \WithNoGrad{
                $h_{\bar{\mathcal{G}}} = h_{\bar{\mathcal{G}}}W_1 + b_1$ \\
                $h_{\bar{\mathcal{G}}} = h_{\bar{\mathcal{G}}}W_2 + b_2$
            }
	}
	Re-organize input tokens into the original order
\end{algorithm}
