\vspace{-0.5em}
\section{Application to Large-Size Decoders}  \label{sec:exp:large}
\vspace{-0.5em}

We also seek to evaluate our method on larger size pre-trained language models (LLMs). 





\subsection{Instruction Tuning and Few-Shot Evaluation} 

LLMs are typically further fine-tuned on curated datasets to tailor them to specific domains and enhance their capacity to follow instructions \citep{wang_23, alpaca, mukherjee_23}. In this section, we employ instruction tuning on these datasets to fine-tune the LLMs and then assess the performance of the resulting models using few-shot benchmarks.

\paragraph{Instruction Tuning.} 
We fine-tune the \mbox{Llama2-7B} model~\citep{touvron_23}
via instruction tuning with the Open-Platypus\footnote{\url{https://huggingface.co/datasets/garage-bAInd/Open-Platypus}}~\citep{lee_23} dataset.
Note that, while Open-Platypus consists of 11 open-source datasets, 
we exclude two of them\footnote{\texttt{leetcode-solutions-python-testgen-gpt4} and \texttt{airoboros-gpt4-1.4.1}} that include outputs from GPT~\cite{DBLP:journals/corr/abs-2303-08774}, and instead use the other nine datasets for fine-tuning. 


\paragraph{Hyper-Parameter Settings.} We conduct all experiments in this section on Nvidia H100 GPU. Following \citet{lee_23}, we fine-tune the model for one epoch, and use a learning rate of $4e^{-4}$ for LoRA~\citep{hu_22} and QLoRA~\citep{dettmers_23}, and $4e^{-5}$ otherwise. We use a batch size of 1 with 32 gradient accumulation steps. We apply the adapters on the feed-forward modules from each layer, following the method described in \citet{he_22}. We prompt the model without step-wise reasoning using the Alpaca~\citep{alpaca} prompt template detailed in \Cref{app:instruct}.



\paragraph{Few-Shot Evaluation.} 
Then, we evaluate our method against other memory-efficient fine-tuning approaches by assessing its performance on several few-shot benchmarks, such as MMLU \citep{hendrycks_21}, ARC easy and challenge \citep{clark_18}, HellaSwag \citep{zellers19}, TruthfulQA \citep{lin_22}, and WinoGrande \citep{DBLP:conf/aaai/SakaguchiBBC20}. We utilize the evaluation scripts provided by the "Language Model Evaluation Harness" \citep{eval-harness}. During the evaluation process, the model outputs the probability associated with closed-form problems defined by the context, question, and multiple potential answers. We select the answer choice with the text associated with the highest probability. 

Table~\ref{tab:llm-perf} reports the accuracy of the model output against the ground truth answer. 
Our method achieves competitive performance gains that are comparable to the performance improvements obtained by other memory efficient fine-tuning approaches. 
We are able to improve the evaluation accuracy upon the base LLama2-7B model, increasing the average accuracy from 60.7 to 61.2. We observe the most significant improvements for TruthfulQA (+3.2) and WinoGrande (+1.0) tasks. We also combine \method with LoRA and QLoRA, further improving the evaluation accuracy compared to the use of \method alone.


	


\input{tables/results_ablation}









\subsection{Ratio of Tuned Input Positions}

As done for medium-size encoders in \Cref{sec:medium:ratio}, we then evaluate the impact of the ratio of tuned input positions on the few-shot accuracy. 
We measure the few-shot accuracy of Llama2-7B models fine-tuned using \method with varying ratio of tuned input positions. 
\Cref{tab:ablation:tokenratio} shows few-shot evaluation accuracy of Llama2-7B when the ratio of fine-tuned positions ranges from 10\% to 50\% .


Contrary to what we observed in \Cref{sec:medium:ratio}, we do not necessarily observe a strong correlation between the few-shot accuracy and the ratio of tuned positions. 
In fact, we obtain the best performances most often when 20\%--30\% of input positions are fine-tuned. 
It is important to observe that the average sequence length in these experiments far exceeds the one from the experiments on the GLUE benchmark. This suggests that tuning a relatively small number of positions may be sufficient to successfully fine-tune the model on specific datasets. 

\begin{figure}[t]
\centering
     \includegraphics[width=1.0\linewidth]{figures/mem_llama_2.png}
\caption{GPU memory required to fine-tune Llama2-7B~\citep{touvron_23}. We measure the memory by fine-tuning the model on artificially generated data with a given sequence length and batch size. We set the batch size to 1 and the sequence length to \numprint{2048}. We show the memory usage when combining  \method with LoRA and QLoRA and plot the evolution of the memory required to fine-tune the model on a H100 GPU with a number of trained positions ranging between \numprint{256} and \numprint{2046} (we leave at least 2 positions not tuned). 
    Since we could not perform full fine-tuning on our hardware, we estimate the full fine-tuning memory based on the memory reported for \method and LoRA.
    Specific memory usage values can be found in \Cref{tab:gpu_mem_usage}.}
    \label{fig:llm-memory}
\end{figure}

\subsection{GPU Memory Impact}
\label{sec:exp:large:gpu_memory}

As in \Cref{sec:medium:mem}, we analyze the impact of our method on the GPU memory required to fine-tune large language models. \Cref{fig:llm-memory} and \Cref{tab:ablation:tokenratio} report the GPU memory usage for fine-tuning Llama2-7B as the number of trained input tokens changes.
Given an input sequence of length \numprint{2048}, \Cref{fig:llm-memory} shows that our model reduces the memory usage by up to 28\%, from 89 GiB to 64 GiB when reducing the number of trained positions from \numprint{2046} to \numprint{256}. 

The advantage of the proposed method is that it can be combined with other memory saving methods. 
We measure the peak memory required to fine-tune LLama2-7B when combining \method with LoRA or QLoRA. 
Since these approaches target different parts of the memory footprint,
we observe cumulative savings when they are used together. 
When combining LoRA with \method, the peak memory ranges between 78 GiB to 45 GiB depending on the number of tuned positions. 
Similarly, when combining QLoRA with \method, the peak memory decreases from 49 GiB to 12 GiB as a smaller selection ratio is used.

Overall, \Cref{fig:llm-memory} and \Cref{tab:ablation:tokenratio} show that 
the performance of TokenTune is not very sensitive to the choice of token selection ratio, 
while the memory cost is significantly reduced with a smaller token selection ratio.
Based on these results, 
our recommendation is to use 20\%--30\% as the default token selection ratio, and 
test if further improvements in performance and memory usage can be obtained for the given task, with a smaller selection ratio.
