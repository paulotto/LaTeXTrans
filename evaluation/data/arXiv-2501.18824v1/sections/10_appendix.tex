


\appendix



























\section{Instruction Template}\label{app:instruct}
Regarding the instruction tuning of large LLMs, we prompt the model without step-wise reasoning using the Alpaca~\citep{alpaca} prompt template presented below.

\vspace{0.5em}
\noindent
\fbox{\ttfamily \begin{minipage}{1.0\columnwidth}
{``Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

\#\#\# Instruction:
\{instruction\}

\#\#\# Input:
\{input\}

\#\#\# Response:\\
''}
\end{minipage}}
\section{Software}\label{sec:app:software}
Here we provide the details of the software used for the implementation of \method 
as well as the fine-tuning and evaluation of \method and baselines.
Our implementation of \method builds upon the HuggingFace Transformers library\footnote{\url{https://github.com/huggingface/transformers}} (v4.33.1).
For LoRA~\citep{hu_22}, we used the HuggingFace PEFT library\footnote{\url{https://github.com/huggingface/peft}} (v.0.5.0).
Datasets used for fine-tuning were obtained from the HuggingFace Datasets library\footnote{\url{https://github.com/huggingface/datasets}} (v2.18.0).
We used Open-Platypus\footnote{\url{https://huggingface.co/datasets/garage-bAInd/Open-Platypus}} for fine-tuning.
For the evaluation with the Llama2 model in \Cref{sec:exp:large},
we used the lm-evaluation-harness framework\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} (v.0.4.2).
We used the PyTorch framework\footnote{\url{https://github.com/pytorch/pytorch}} (v.2.0.1). Results from \Cref{table:glue} are scored by the evaluation server.\footnote{\url{https://gluebenchmark.com/leaderboard}} As in \citet{devlin_19}, we discard results for the WNLI~task.\footnote{See (12) from \url{https://gluebenchmark.com/faq}}

\section{License}\label{sec:app:license}
The majority of \method is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Transformers is licensed under the Apache 2.0 license.
The license of other libraries used for this paper is as follows.
The PEFT and Datasets libraries from HuggingFace are under the Apache-2.0 license.
The lm-evaluation-harness framework is under the MIT license.
PyTorch is under the modified BSD-3 license.
Open-Platypus used for fine-tuning consists of multiple datasets; their license information can be found at \url{https://huggingface.co/datasets/garage-bAInd/Open-Platypus}.

\section{Training and Evaluation Data} \label{data}

\textsc{Bert} model has been pre-trained on \numprint{3300}M words.
Regarding the instruction tuning experiments, we tuned the \mbox{Llama2-7B} on \numprint{21221} samples from the Open-Platypus~\citep{lee_23} dataset. Note that, while Open-Platypus consists of 11 open-source datasets, 
we exclude two of them\footnote{\texttt{leetcode-solutions-python-testgen-gpt4} and \texttt{airoboros-gpt4-1.4.1}} that include outputs from GPT~\cite{DBLP:journals/corr/abs-2303-08774}, and instead use the other nine datasets for fine-tuning. Llama2-7B has been pre-trained on 2T tokens and fine-tuned on \numprint{100000} samples.\footnote{\url{https://llama.meta.com/llama2/}}



\section{Memory Breakdown}\label{sec:app:memorycomplexity:bkd}


Parameter-Efficient Fine-Tuning (PEFT) approaches aim at reducing the compute and storage requirements to fine-tune LLMs by only updating a small subset of the model parameters. 
As a result, we do not need to store any corresponding gradients and optimizer states for the frozen parameters. 
When parameters, gradients, and optimizer states represent the majority of the GPU memory usage,
these PEFT methods can effectively reduce the memory cost.
However, when most GPU memory is used to store intermediate activations, 
which are required for gradient computation during the backward pass, 
these PEFT methods cannot effectively cut down the memory cost.



\input{tables/app_gpu_memory_usage}

Table~\ref{table: Distribution of the GPU memory.} presents the GPU memory required to perform one training step with \textsc{Bert}-base \citep{devlin_19} and OPT \citep{zhang_22} on a consumer hardware GPU. We calibrate the example such that the memory requirement is roughly the same for both models. In this configuration we can only fit a single example for OPT, while we can use a batch size of \numprint{256} for \textsc{Bert}. We observe that the memory breakdown is very different between the two configurations. The required memory drastically increases during the forward pass for \textsc{Bert} and during the backward pass for OPT. When comparing the execution of forward pass with and without enabling gradient computation in PyTorch, we estimate that the memory cost to store intermediate activations represents around 22 Gb for \textsc{Bert} and less than 1 Gb for OPT. On the contrary, we estimate that computing and storing the parameter gradients increase the memory requirement by less than 1 Gb for \textsc{Bert} and around 5 Gb for OPT. When applying LoRA \citep{hu_22}, a PEFT method, we observe that the memory drastically decreases for OPT, while having a less significant impact on \textsc{Bert}.
These examples demonstrate that an effective memory reduction across different usage scenarios
can be achieved by combining a suite of memory-efficient fine-tuning methods that can complement each other by reducing different parts of the memory footprint simultaneously.



\input{tables/gpu_memory_distribution}


































\section{MRPC and STS-B Descriptive Statistics}\label{app:stats_desc}

\input{tables/exp_glue_length_distribution}

\Cref{table:table_stats} describes the relation between the absolute and relative number of frozen input positions. The statistics include distribution of the sentence length for the two subtasks (MRPC and STS-B) used to produce \Cref{fig:graphs} (right). We also report in \Cref{table:table_stats_2} the relative proportion of fine-tuned tokens averaged over MRPC and STS-B tasks, as the absolute number of fine-tuned tokens changes, along with the corresponding average performance, which is reported in \Cref{fig:graphs}  (right).

\vspace{0.45em}
\section{GPU Memory Usage}\label{app:gpu_mem_usage}
\vspace{0.05em}

\Cref{tab:gpu_mem_usage} shows the GPU memory usage required to fine-tune Llama2-7B~\citep{touvron_23} using the proposed \method with a varying selection ratio, as well as QLoRA and LoRA.
\Cref{fig:llm-memory} also visualizes the same results. See \Cref{sec:exp:large:gpu_memory} and \Cref{fig:llm-memory} for further details of the experiment.
