\pdfoutput=1


\documentclass[11pt,dvipsnames]{article}

\usepackage[final]{acl}

\input{math_commands}


\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{xcolor}         

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{numprint}
\usepackage{multirow}
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}
\usepackage{color, colortbl}
\definecolor{lightgrey}{gray}{0.9}
\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
\definecolor{lightcyan}{rgb}{0.88, 1.0, 1.0}
\usepackage{enumitem}
\usepackage{xspace} 
\newcommand{\method}{\textsc{TokenTune}\xspace}
\usepackage{stmaryrd}
\usepackage{cleveref}
\usepackage{numprint}
\usepackage{makecell}
\usepackage{titlesec}
\titlespacing{\paragraph}{0pt}{2ex}{0.1cm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{setspace}









\newcommand\blfootnote[1]{\begingroup
    \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\newcommand{\githubURL}{\url{https://github.com/facebookresearch/tokentune}}




\title{Memory-Efficient Fine-Tuning of Transformers via Token Selection}





\author{
\textbf{Antoine Simoulin\textsuperscript{\textbf{*}}},
\textbf{Namyong Park\textsuperscript{\textbf{*}}},
\textbf{Xiaoyi Liu},
\textbf{Grey Yang}
\\
Meta AI
\\
\small{
	\texttt{\{antoinesimoulin,namyongp,xiaoyiliu,glyang\}@meta.com}
}
}





\begin{document}
\maketitle


\input{sections/0_abstract}

\input{sections/1_introduction}



\input{sections/3_relatedwork}

\input{sections/4_methods}

\input{sections/5_experiments_medium}

\input{sections/6_experiments_large}



\input{sections/8_conclusion}

\clearpage
\input{sections/9_limitations}






{
\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Aghajanyan et~al.(2021)Aghajanyan, Gupta, and
  Zettlemoyer}]{DBLP:conf/acl/AghajanyanGZ20}
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.ACL-LONG.568} {Intrinsic
  dimensionality explains the effectiveness of language model fine-tuning}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers),
  Virtual Event, August 1-6, 2021}, pages 7319--7328. Association for
  Computational Linguistics.

\bibitem[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, Showk, Elhage,
  Hatfield{-}Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson,
  Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan}]{bai_22}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
  Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer~El Showk, Nelson Elhage,
  Zac Hatfield{-}Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
  Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom~B.
  Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared
  Kaplan. 2022.
\newblock \href {https://doi.org/10.48550/arXiv.2204.05862} {Training a helpful
  and harmless assistant with reinforcement learning from human feedback}.
\newblock \emph{CoRR}, abs/2204.05862.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{DBLP:conf/nips/BrownMRSKDNSSAA20}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Chen et~al.(2016)Chen, Xu, Zhang, and
  Guestrin}]{DBLP:journals/corr/ChenXZG16}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
\newblock \href {https://arxiv.org/abs/1604.06174} {Training deep nets with
  sublinear memory cost}.
\newblock \emph{CoRR}, abs/1604.06174.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord}]{clark_18}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord. 2018.
\newblock \href {https://arxiv.org/abs/1803.05457} {Think you have solved
  question answering? try arc, the {AI2} reasoning challenge}.
\newblock \emph{CoRR}, abs/1803.05457.

\bibitem[{Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and
  Wei}]{DBLP:conf/acl/DaiDHSCW22}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022.
\newblock \href {https://doi.org/10.18653/V1/2022.ACL-LONG.581} {Knowledge
  neurons in pretrained transformers}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 8493--8502. Association for Computational
  Linguistics.

\bibitem[{Das et~al.(2023)Das, Zhang, Shi, Yin, and
  Zhang}]{DBLP:conf/emnlp/DasZS0Z23}
Sarkar Snigdha~Sarathi Das, Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang.
  2023.
\newblock \href {https://doi.org/10.18653/V1/2023.EMNLP-MAIN.433} {Unified
  low-resource sequence labeling by sample-aware dynamic sparse finetuning}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages 6998--7010. Association for Computational Linguistics.

\bibitem[{Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer}]{dettmers_22}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022.
\newblock \href
  {http://papers.nips.cc/paper\_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html}
  {{GPT3}.int8(): 8-bit matrix multiplication for transformers at scale}.
\newblock In \emph{Advances in Neural Information Processing Systems 35: Annual
  Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
  Orleans, LA, USA, November 28 - December 9, 2022}.

\bibitem[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer}]{dettmers_23}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
\newblock \href
  {http://papers.nips.cc/paper\_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html}
  {{QLoRA}: Efficient finetuning of quantized llms}.
\newblock In \emph{Advances in Neural Information Processing Systems 36: Annual
  Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New
  Orleans, LA, USA, December 10 - 16, 2023}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{devlin_19}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/n19-1423} {{BERT:} pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pages 4171--4186. Association for Computational
  Linguistics.

\bibitem[{Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby}]{DBLP:conf/iclr/DosovitskiyB0WZ21}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.
\newblock \href {https://openreview.net/forum?id=YicbFdNTTy} {An image is worth
  16x16 words: Transformers for image recognition at scale}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[{Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur}]{DBLP:conf/iclr/ForetKMN21}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021.
\newblock \href {https://openreview.net/forum?id=6Tm1mposlrM} {Sharpness-aware
  minimization for efficiently improving generalization}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[{Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou. 2021.
\newblock \href {https://doi.org/10.5281/zenodo.5371628} {A framework for
  few-shot language model evaluation}.

\bibitem[{Gheini et~al.(2021)Gheini, Ren, and May}]{DBLP:conf/emnlp/Gheini0M21}
Mozhdeh Gheini, Xiang Ren, and Jonathan May. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.EMNLP-MAIN.132}
  {Cross-attention is all you need: Adapting pretrained transformers for
  machine translation}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana,
  Dominican Republic, 7-11 November, 2021}, pages 1754--1765. Association for
  Computational Linguistics.

\bibitem[{Gouk et~al.(2021)Gouk, Hospedales, and
  Pontil}]{DBLP:conf/iclr/GoukHP21}
Henry Gouk, Timothy~M. Hospedales, and Massimiliano Pontil. 2021.
\newblock \href {https://openreview.net/forum?id=IFqrg1p5Bc} {Distance-based
  regularisation of deep networks for fine-tuning}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[{Gruslys et~al.(2016)Gruslys, Munos, Danihelka, Lanctot, and
  Graves}]{DBLP:conf/nips/GruslysMDLG16}
Audrunas Gruslys, R{\'{e}}mi Munos, Ivo Danihelka, Marc Lanctot, and Alex
  Graves. 2016.
\newblock \href
  {https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html}
  {Memory-efficient backpropagation through time}.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 4125--4133.

\bibitem[{Guo et~al.(2021)Guo, Rush, and Kim}]{DBLP:conf/acl/GuoRK20}
Demi Guo, Alexander~M. Rush, and Yoon Kim. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.ACL-LONG.378}
  {Parameter-efficient transfer learning with diff pruning}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers),
  Virtual Event, August 1-6, 2021}, pages 4884--4896. Association for
  Computational Linguistics.

\bibitem[{Han et~al.(2024)Han, Gao, Liu, Zhang, and
  Zhang}]{DBLP:journals/corr/abs-2403-14608}
Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai~Qian Zhang. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2403.14608}
  {Parameter-efficient fine-tuning for large models: {A} comprehensive survey}.
\newblock \emph{CoRR}, abs/2403.14608.

\bibitem[{He et~al.(2022{\natexlab{a}})He, Zhou, Ma, Berg{-}Kirkpatrick, and
  Neubig}]{DBLP:conf/iclr/HeZMBN22}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg{-}Kirkpatrick, and Graham
  Neubig. 2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=0RDcd5Axok} {Towards a unified
  view of parameter-efficient transfer learning}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{He et~al.(2022{\natexlab{b}})He, Zhou, Ma, Berg{-}Kirkpatrick, and
  Neubig}]{he_22}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg{-}Kirkpatrick, and Graham
  Neubig. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=0RDcd5Axok} {Towards a unified
  view of parameter-efficient transfer learning}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{hendrycks_21}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt. 2021.
\newblock \href {https://openreview.net/forum?id=d7KBjmI3GmQ} {Measuring
  massive multitask language understanding}.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[{Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  de~Laroussilhe, Gesmundo, Attariyan, and
  Gelly}]{DBLP:conf/icml/HoulsbyGJMLGAG19}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
\newblock \href {http://proceedings.mlr.press/v97/houlsby19a.html}
  {Parameter-efficient transfer learning for {NLP}}.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  2790--2799. {PMLR}.

\bibitem[{Howard and Ruder(2018)}]{ruder_18}
Jeremy Howard and Sebastian Ruder. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1031} {Universal language
  model fine-tuning for text classification}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20,
  2018, Volume 1: Long Papers}, pages 328--339. Association for Computational
  Linguistics.

\bibitem[{Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and
  Chen}]{hu_22}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li,
  Shean Wang, Lu~Wang, and Weizhu Chen. 2022.
\newblock \href {https://openreview.net/forum?id=nZeVKeeFYf9} {{LoRA}: Low-rank
  adaptation of large language models}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Jin et~al.(2023)Jin, Zhang, and Zong}]{DBLP:conf/emnlp/JinZZ23}
Feihu Jin, Jiajun Zhang, and Chengqing Zong. 2023.
\newblock \href {https://doi.org/10.18653/V1/2023.EMNLP-MAIN.22}
  {Parameter-efficient tuning for large language model without calculating its
  gradients}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages 321--330. Association for Computational Linguistics.

\bibitem[{Kurtz et~al.(2020)Kurtz, Kopinsky, Gelashvili, Matveev, Carr, Goin,
  Leiserson, Moore, Shavit, and Alistarh}]{DBLP:conf/icml/KurtzKGMCGLMSA20}
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr,
  Michael Goin, William~M. Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh.
  2020.
\newblock \href {http://proceedings.mlr.press/v119/kurtz20a.html} {Inducing and
  exploiting activation sparsity for fast inference on deep neural networks}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 5533--5543. {PMLR}.

\bibitem[{Lawton et~al.(2023)Lawton, Kumar, Thattai, Galstyan, and
  Steeg}]{DBLP:conf/acl/LawtonKTGS23}
Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, and Greg~Ver Steeg.
  2023.
\newblock \href {https://doi.org/10.18653/V1/2023.FINDINGS-ACL.539} {Neural
  architecture search for parameter-efficient fine-tuning of large pre-trained
  language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 8506--8515. Association
  for Computational Linguistics.

\bibitem[{Lee et~al.(2023)Lee, Hunter, and Ruiz}]{lee_23}
Ariel~N. Lee, Cole~J. Hunter, and Nataniel Ruiz. 2023.
\newblock \href {https://doi.org/10.48550/arXiv.2308.07317} {Platypus: Quick,
  cheap, and powerful refinement of llms}.
\newblock In \emph{NeurIPS 2023 Workshop on Instruction Tuning and Instruction
  Following}.

\bibitem[{Lee et~al.(2019)Lee, Tang, and Lin}]{lee_19}
Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019.
\newblock \href {https://arxiv.org/abs/1911.03090} {What would elsa do?
  freezing layers during transformer fine-tuning}.
\newblock \emph{CoRR}, abs/1911.03090.

\bibitem[{Li and Zhang(2021)}]{DBLP:conf/nips/LiZ21}
Dongyue Li and Hongyang~R. Zhang. 2021.
\newblock \href
  {https://proceedings.neurips.cc/paper/2021/hash/e4a93f0332b2519177ed55741ea4e5e7-Abstract.html}
  {Improved regularization and robustness for fine-tuning in neural networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
  December 6-14, 2021, virtual}, pages 27249--27262.

\bibitem[{Li and Liang(2021)}]{DBLP:conf/acl/LiL20}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.ACL-LONG.353} {Prefix-tuning:
  Optimizing continuous prompts for generation}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers),
  Virtual Event, August 1-6, 2021}, pages 4582--4597. Association for
  Computational Linguistics.

\bibitem[{Li et~al.(2022)Li, Tram{\`{e}}r, Liang, and
  Hashimoto}]{DBLP:conf/iclr/LiTLH22}
Xuechen Li, Florian Tram{\`{e}}r, Percy Liang, and Tatsunori Hashimoto. 2022.
\newblock \href {https://openreview.net/forum?id=bVuP3ltATMz} {Large language
  models can be strong differentially private learners}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Liao et~al.(2023)Liao, Tan, and Monz}]{DBLP:conf/nips/LiaoTM23}
Baohao Liao, Shaomu Tan, and Christof Monz. 2023.
\newblock \href
  {http://papers.nips.cc/paper\_files/paper/2023/hash/3151e460c41ba67dc55412861184ef35-Abstract-Conference.html}
  {Make pre-trained model reversible: From parameter to memory efficient
  fine-tuning}.
\newblock In \emph{Advances in Neural Information Processing Systems 36: Annual
  Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New
  Orleans, LA, USA, December 10 - 16, 2023}.

\bibitem[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin_22}
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.229} {Truthfulqa:
  Measuring how models mimic human falsehoods}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 3214--3252. Association for Computational
  Linguistics.

\bibitem[{Lin et~al.(2024)Lin, Ma, Chu, Jin, Yang, Wang, and
  Mei}]{DBLP:journals/corr/abs-2404-09610}
Yang Lin, Xinyu Ma, Xu~Chu, Yujie Jin, Zhibang Yang, Yasha Wang, and Hong Mei.
  2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2404.09610} {{LoRA} dropout as
  a sparsity regularizer for overfitting control}.
\newblock \emph{CoRR}, abs/2404.09610.

\bibitem[{Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{liu_21}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie
  Tang. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-short.8} {{P}-tuning:
  Prompt tuning can be comparable to fine-tuning across scales and tasks}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 61--68,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2024)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi,
  Krishnamoorthi, and Chandra}]{liu_23}
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
  Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2024.
\newblock \href {https://aclanthology.org/2024.findings-acl.26} {{LLM-QAT:}
  data-free quantization aware training for large language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics,
  {ACL} 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024}, pages
  467--484. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2023)Liu, Wang, Dao, Zhou, Yuan, Song, Shrivastava, Zhang,
  Tian, R{\'{e}}, and Chen}]{DBLP:conf/icml/LiuWDZY0S0TRC23}
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
  Shrivastava, Ce~Zhang, Yuandong Tian, Christopher R{\'{e}}, and Beidi Chen.
  2023.
\newblock \href {https://proceedings.mlr.press/v202/liu23am.html} {Deja vu:
  Contextual sparsity for efficient llms at inference time}.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2023,
  23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of
  Machine Learning Research}, pages 22137--22176. {PMLR}.

\bibitem[{Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and
  Arora}]{DBLP:conf/nips/MalladiGNDL0A23}
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason~D. Lee, Danqi
  Chen, and Sanjeev Arora. 2023.
\newblock \href
  {http://papers.nips.cc/paper\_files/paper/2023/hash/a627810151be4d13f907ac898ff7e948-Abstract-Conference.html}
  {Fine-tuning language models with just forward passes}.
\newblock In \emph{Advances in Neural Information Processing Systems 36: Annual
  Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New
  Orleans, LA, USA, December 10 - 16, 2023}.

\bibitem[{Mao et~al.(2022)Mao, Mathias, Hou, Almahairi, Ma, Han, Yih, and
  Khabsa}]{DBLP:conf/acl/MaoMHAM0YK22}
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han,
  Scott Yih, and Madian Khabsa. 2022.
\newblock \href {https://doi.org/10.18653/V1/2022.ACL-LONG.433} {{UniPELT}: {A}
  unified framework for parameter-efficient language model tuning}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 6253--6264. Association for Computational
  Linguistics.

\bibitem[{Mishra et~al.(2022)Mishra, Khashabi, Baral, and
  Hajishirzi}]{DBLP:conf/acl/MishraKBH22}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
\newblock \href {https://doi.org/10.18653/V1/2022.ACL-LONG.244} {Cross-task
  generalization via natural language crowdsourcing instructions}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 3470--3487. Association for Computational
  Linguistics.

\bibitem[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,
  and Awadallah}]{mukherjee_23}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid
  Palangi, and Ahmed~Hassan Awadallah. 2023.
\newblock \href {https://doi.org/10.48550/arXiv.2306.02707} {Orca: Progressive
  learning from complex explanation traces of {GPT-4}}.
\newblock \emph{CoRR}, abs/2306.02707.

\bibitem[{OpenAI(2023)}]{DBLP:journals/corr/abs-2303-08774}
OpenAI. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2303.08774} {{GPT-4} technical
  report}.
\newblock \emph{CoRR}, abs/2303.08774.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{ouyang_22}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~F. Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href
  {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html}
  {Training language models to follow instructions with human feedback}.
\newblock In \emph{NeurIPS}.

\bibitem[{Pfeiffer et~al.(2021)Pfeiffer, Kamath, R{\"{u}}ckl{\'{e}}, Cho, and
  Gurevych}]{DBLP:conf/eacl/PfeifferKRCG21}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"{u}}ckl{\'{e}}, Kyunghyun Cho,
  and Iryna Gurevych. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.EACL-MAIN.39} {Adapterfusion:
  Non-destructive task composition for transfer learning}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume, {EACL} 2021,
  Online, April 19 - 23, 2021}, pages 487--503. Association for Computational
  Linguistics.

\bibitem[{Pfeiffer et~al.(2023)Pfeiffer, Ruder, Vulic, and Ponti}]{pfeiffer_23}
Jonas Pfeiffer, Sebastian Ruder, Ivan Vulic, and Edoardo~M. Ponti. 2023.
\newblock \href {https://openreview.net/forum?id=z9EkXfvxta} {Modular deep
  learning}.
\newblock \emph{Trans. Mach. Learn. Res.}, 2023.

\bibitem[{Qin and Eisner(2021)}]{qin_21}
Guanghui Qin and Jason Eisner. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.410} {Learning how
  to ask: Querying lms with mixtures of soft prompts}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2021, Online, June 6-11, 2021}, pages 5203--5212.
  Association for Computational Linguistics.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel_20}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock \href {http://jmlr.org/papers/v21/20-074.html} {Exploring the limits
  of transfer learning with a unified text-to-text transformer}.
\newblock \emph{J. Mach. Learn. Res.}, 21:140:1--140:67.

\bibitem[{Sakaguchi et~al.(2020)Sakaguchi, Bras, Bhagavatula, and
  Choi}]{DBLP:conf/aaai/SakaguchiBBC20}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.
\newblock \href {https://doi.org/10.1609/AAAI.V34I05.6399} {Winogrande: An
  adversarial winograd schema challenge at scale}.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 8732--8740. {AAAI} Press.

\bibitem[{Schick and Sch{\"{u}}tze(2021)}]{DBLP:conf/naacl/SchickS21}
Timo Schick and Hinrich Sch{\"{u}}tze. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.185} {It's not
  just size that matters: Small language models are also few-shot learners}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2021, Online, June 6-11, 2021}, pages 2339--2352.
  Association for Computational Linguistics.

\bibitem[{Shazeer and Stern(2018)}]{DBLP:conf/icml/ShazeerS18}
Noam Shazeer and Mitchell Stern. 2018.
\newblock \href {http://proceedings.mlr.press/v80/shazeer18a.html} {Adafactor:
  Adaptive learning rates with sublinear memory cost}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages
  4603--4611. {PMLR}.

\bibitem[{Simoulin et~al.(2023)Simoulin, Park, Liu, and
  Yang}]{simoulin2023memoryefficient}
Antoine Simoulin, Namyong Park, Xiaoyi Liu, and Grey Yang. 2023.
\newblock \href {https://openreview.net/forum?id=zaNbLceVwm} {Memory-efficient
  selective fine-tuning}.
\newblock In \emph{Workshop on Efficient Systems for Foundation Models @
  ICML2023}.

\bibitem[{Spall(1992)}]{Spall1992MultivariateSA}
James~C. Spall. 1992.
\newblock \href {https://api.semanticscholar.org/CorpusID:122365276}
  {Multivariate stochastic approximation using a simultaneous perturbation
  gradient approximation}.
\newblock \emph{IEEE Transactions on Automatic Control}, 37:332--341.

\bibitem[{Sung et~al.(2022)Sung, Cho, and Bansal}]{sung2022lst}
Yi{-}Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.
\newblock \href {https://openreview.net/forum?id=isPnnaTZaP5} {{LST}: Ladder
  side-tuning for parameter and memory efficient transfer learning}.
\newblock In \emph{NeurIPS}.

\bibitem[{Sung et~al.(2021)Sung, Nair, and Raffel}]{DBLP:conf/nips/SungNR21}
Yi{-}Lin Sung, Varun Nair, and Colin Raffel. 2021.
\newblock \href
  {https://proceedings.neurips.cc/paper/2021/hash/cb2653f548f8709598e8b5156738cc51-Abstract.html}
  {Training neural networks with fixed sparse masks}.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
  December 6-14, 2021, virtual}, pages 24193--24205.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,
  Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer,
  Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal,
  Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev,
  Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra,
  Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith,
  Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan,
  Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron_23}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
  Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
  2023.
\newblock \href {https://doi.org/10.48550/arXiv.2307.09288} {Llama 2: Open
  foundation and fine-tuned chat models}.
\newblock \emph{CoRR}, abs/2307.09288.

\bibitem[{Valipour et~al.(2023)Valipour, Rezagholizadeh, Kobyzev, and
  Ghodsi}]{DBLP:conf/eacl/ValipourRKG23}
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2023.
\newblock \href {https://doi.org/10.18653/V1/2023.EACL-MAIN.239} {{DyLoRA}:
  Parameter-efficient tuning of pre-trained models using dynamic search-free
  low-rank adaptation}.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter
  of the Association for Computational Linguistics, {EACL} 2023, Dubrovnik,
  Croatia, May 2-6, 2023}, pages 3266--3279. Association for Computational
  Linguistics.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang_18}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman. 2018.
\newblock \href {https://doi.org/10.18653/v1/W18-5446} {{GLUE}: A multi-task
  benchmark and analysis platform for natural language understanding}.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355,
  Brussels, Belgium. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi}]{wang_23}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.754} {Self-instruct:
  Aligning language models with self-generated instructions}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto,
  Canada, July 9-14, 2023}, pages 13484--13508. Association for Computational
  Linguistics.

\bibitem[{Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{wei_22}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2022.
\newblock \href {https://openreview.net/forum?id=gEZrGCozdqR} {Finetuned
  language models are zero-shot learners}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Xu et~al.(2023)Xu, Xie, Qin, Tao, and
  Wang}]{DBLP:journals/corr/abs-2312-12148}
Lingling Xu, Haoran Xie, Si{-}Zhao~Joe Qin, Xiaohui Tao, and Fu~Lee Wang. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2312.12148}
  {Parameter-efficient fine-tuning methods for pretrained language models: {A}
  critical review and assessment}.
\newblock \emph{CoRR}, abs/2312.12148.

\bibitem[{Yang et~al.(2024)Yang, Robeyns, Wang, and
  Aitchison}]{DBLP:conf/iclr/YangRWA24}
Adam~X. Yang, Maxime Robeyns, Xi~Wang, and Laurence Aitchison. 2024.
\newblock \href {https://openreview.net/forum?id=FJiUyzOF1m} {Bayesian low-rank
  adaptation for large language models}.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}.
  OpenReview.net.

\bibitem[{Zaken et~al.(2022)Zaken, Goldberg, and Ravfogel}]{zaken_22}
Elad~Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-short.1} {Bitfit: Simple
  parameter-efficient fine-tuning for transformer-based masked
  language-models}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 1--9. Association for Computational
  Linguistics.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi}]{zellers19}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock \href {https://doi.org/10.18653/v1/p19-1472} {Hellaswag: Can a
  machine really finish your sentence?}
\newblock In \emph{Proceedings of the 57th Conference of the Association for
  Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2,
  2019, Volume 1: Long Papers}, pages 4791--4800. Association for Computational
  Linguistics.

\bibitem[{Zhang et~al.(2020)Zhang, Sax, Zamir, Guibas, and Malik}]{zhang_20}
Jeffrey~O. Zhang, Alexander Sax, Amir Zamir, Leonidas~J. Guibas, and Jitendra
  Malik. 2020.
\newblock \href {https://doi.org/10.1007/978-3-030-58580-8\_41} {Side-tuning:
  {A} baseline for network adaptation via additive side networks}.
\newblock In \emph{Computer Vision - {ECCV} 2020 - 16th European Conference,
  Glasgow, UK, August 23-28, 2020, Proceedings, Part {III}}, volume 12348 of
  \emph{Lecture Notes in Computer Science}, pages 698--714. Springer.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Zhang, Shi, Chu, and
  Li}]{DBLP:journals/corr/abs-2308-03303}
Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo~Li.
  2023{\natexlab{a}}.
\newblock \href {https://doi.org/10.48550/ARXIV.2308.03303} {{LoRA-FA}:
  Memory-efficient low-rank adaptation for large language models fine-tuning}.
\newblock \emph{CoRR}, abs/2308.03303.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Bukharin, He, Cheng,
  Chen, and Zhao}]{DBLP:conf/iclr/ZhangCBH0CZ23}
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu
  Chen, and Tuo Zhao. 2023{\natexlab{b}}.
\newblock \href {https://openreview.net/pdf?id=lq62uWRJjiY} {Adaptive budget
  allocation for parameter-efficient fine-tuning}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura,
  Sridhar, Wang, and Zettlemoyer}]{zhang_22}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona~T. Diab, Xian Li, Xi~Victoria Lin, Todor
  Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh
  Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022{\natexlab{a}}.
\newblock \href {https://doi.org/10.48550/arXiv.2205.01068} {{OPT:} open
  pre-trained transformer language models}.
\newblock \emph{CoRR}, abs/2205.01068.

\bibitem[{Zhang et~al.(2022{\natexlab{b}})Zhang, Zhou, and
  Liu}]{DBLP:journals/corr/abs-2206-04673}
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. 2022{\natexlab{b}}.
\newblock \href {https://doi.org/10.48550/ARXIV.2206.04673} {Neural prompt
  search}.
\newblock \emph{CoRR}, abs/2206.04673.

\bibitem[{Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and
  Tian}]{zhao2024galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and
  Yuandong Tian. 2024.
\newblock \href {https://openreview.net/forum?id=hYHsrKDiX7} {{GaLore}:
  Memory-efficient {LLM} training by gradient low-rank projection}.
\newblock In \emph{Forty-first International Conference on Machine Learning,
  {ICML} 2024}.

\bibitem[{Zhou et~al.(2024)Zhou, Wan, Vulic, and
  Korhonen}]{DBLP:journals/tacl/ZhouWVK24}
Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. 2024.
\newblock \href {https://doi.org/10.1162/TACL\_A\_00662} {{AutoPEFT}: Automatic
  configuration search for parameter-efficient fine-tuning}.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 12:525--542.

\bibitem[{Zhu et~al.(2021)Zhu, Feng, Zhao, Wang, and
  Li}]{DBLP:conf/emnlp/ZhuFZWL21}
Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, and Lei Li. 2021.
\newblock \href {https://doi.org/10.18653/V1/2021.FINDINGS-EMNLP.240}
  {Counter-interference adapter for multilingual machine translation}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
  2021}, pages 2812--2823. Association for Computational Linguistics.

\end{thebibliography}
 
}
\clearpage

\input{sections/10_appendix}

\end{document}
