

\section{Method}


\textbf{Overview.}\ \ In this section, we describe the components of TRODO, which employs an adversarial attack (here we use PGD \cite{pgd}) to increase the ID-Score of OOD samples to shift them towards the training data distribution. We then measure the magnitude of the difference in ID-Scores between OOD samples and their perturbed counterparts. We denote this as the ID-Score difference ($\Delta \text{ID-Score}$) and use it as a signature to scan for trojans. This signature is more discriminative between clean and trojaned classifiers when near-OOD samples are used (See Figure \ref{fig:nearood} for some samples). Unlike many existing trojan scanners, which fail in setups lacking training data, TRODO can successfully conduct scans owing to its robust and universal signature. Further details are provided in subsequent sections. The pseudocode of our scanning algorithm is provided in \ref{app:alg}. 

\subsection{Design and Definition of TRODO's Signature}
 \textbf{OOD Set Crafting.} 
To obtain a set of OOD samples, we propose two scenarios. In the first scenario, a portion of the clean training data is available for the given classifier. Here, the OOD set is obtained by applying transformations known to compromise the semantic integrity of an image. Although the results of these transformations deviate from the ID characteristics, these transformed samples visually resemble ID ones. We utilize these as proxies for near-OOD samples. To ensure that the transformations significantly alter the sample characteristics and shift them far enough from the training data distribution, we define a set of hard transformations \(\mathcal{T} = \{T_i\}_{i=1}^{k}\), with each \(T_i\) representing a specific type of hard augmentation. For each ID sample \(x\), a random permutation of \(\mathcal{T}\) is selected \(\{T_{j_1},T_{j_2},\ldots,T_{j_k}\}\), and the transformations are sequentially applied, resulting in \(T_{j_k}(\ldots T_{j_1}(x))\). This method generates a diverse set of OOD samples, particularly valuable in environments with limited access to training data. Each transformed training sample \(x\) becomes a crafted OOD sample \(x'\), with the transformation process denoted by \(G(\cdot)\), i.e., \(x' = G(x)\). We set $k=3$ as a rule of thumb. For more details on these hard transformations, refer to Appendix Section \ref{sec:nearood}.
In the second scenario, where no training data is available, we employ a smaller dataset as the OOD set. Specifically, we utilize Tiny ImageNet \cite{imagenet} for this purpose. Considering that many training datasets (e.g., CIFAR-10 \cite{cifar}) share concepts with our OOD set, we apply \(G(\cdot)\) on Tiny ImageNet samples before using them as the OOD set, ensuring that they do not reflect the training distribution characteristics. In scenarios where a small portion of clean training data is available, we call our method \textbf{TRODO}, and when there is no access to training data, it is referred to as \textbf{TRODO-Zero}.


\textbf{Adversarial Attack on ID-Score.}\ \ In this section, we formulate an adversarial attack on OOD samples to shift them toward the ID region. First, we define the maximum softmax probability (MSP) as the ID-Score, which is an indicator of the classifier's confidence in recognizing an input sample belonging to the ID. Noteworthy that it has been shown that MSP is a simple yet effective metric to be used as ID-Score \cite{msp}. The adversarial perturbation aims to find a shortcut path to increase the ID-Score, effectively shifting the OOD sample toward the blind spots of the trojaned classifier. This process results in a significant increase in the ID-Score, highlighting the introduced signature. Formally, the PGD attack to the ID-Score for a sample $x$ corresponding to a classifier $f$ can be formulated as:
{\small
\begin{equation}
J(f(x)) = \text{ID-Score}_{f}(x), \quad
    x^{0*} = x, \quad 
    x^{t+1*} = \Pi_{x+\mathcal{S}} (x^{t*} + \alpha \cdot \text{sign}\left(\nabla_x J(f(x^{t*}))) \right), \quad
    x^{*} = x^{N*}
    \label{eq:pgd_attack}
\end{equation}}


where the noise is projected on the $\ell_{2}$ norm ball $\mathcal{S}$ with radius $\epsilon$ around $x$ in each step: $ \|x^{t*} - x\|_2 \leq \epsilon$.

To define our signature, we assume a set of OOD samples denoted as $D_{\text{OOD}} = \{ x_i^{\text{OOD}} \}$ is available. For a given classifier $f$, we define our signature $S(f, D_{\text{OOD}})$ as:

{\small
\begin{equation}
    S_i(f, D_{\text{OOD}}) = 
    \text{ID-Score}_{f}(x_i^{\text{OOD*}}) - \text{ID-Score}_{f}(x_i^{\text{OOD}}), \ \ \ 
    S(f, D_{\text{OOD}})=\frac{\sum_{i=1}^{|D_{\text{OOD}}|} S_i(f, D_{\text{OOD}})}{|D_{\text{OOD}}|}
    \label{eq:signal}
\end{equation}}

where $x_i^{\text{OOD*}}$ is obtained by adding adversarial perturbation to $x_i^{\text{OOD}}$ via a PGD attack mentioned in above equation \ref{eq:pgd_attack}.

A higher value of $S(f, D_{\text{OOD}})$ indicates that $f$ is trojaned with higher probability. To detect whether a classifier $f$ is trojaned, we utilize a validation set and a thresholding mechanism, which is well described in the next part. 

\subsection{Validation Data Utilization in TRODO}

\textbf{Leveraging Validation Set for Trojan Scanning.} \ \
In this study, we assume access to a benign validation set denoted as $D_v$ (e.g., Tiny ImageNet), which is realistic given the abundance of available datasets in real-world scenarios. We craft an OOD set $D_{\text{OOD}}$ by applying the mentioned strategy, i.e., $D_{\text{OOD}} = G(D_v)$. Note that we apply harsh augmentations to ensure that the OOD dataset does not belong to ID (in case the validation dataset's distribution resembles training data distribution). These datasets are used for computing $\epsilon$ for our Projected Gradient Descent (PGD) attack as mentioned in the above equations. Moreover, leveraging them, we propose a threshold mechanism to determine whether an input classifier is trojaned, using the signature $S(f, D_{\text{OOD}})$.


Initially, we note that the ID-Score of an OOD sample \(x\) resembles a uniform distribution \(\mathcal{U}(K)\), and the \(\text{ID-Score}_f(x_{\text{OOD}})\) is approximately equal to \(\frac{1}{k}\), where \(k\) denotes the number of classes in the training data. We propose that an effective \(\epsilon\) should shift OOD samples toward ID regions. We consider 0.5 as a hyperparameter, denoted by \(\gamma\), which we refer to as the boundary confidence level. As a result, we propose computing \(\epsilon\) by finding the minimum perturbation that can increase the ID-Score (i.e., MSP) from \(\frac{1}{k}\) to 0.5 for the crafted OOD set \(D_{\text{OOD}}\), corresponding to a surrogate classifier \(g\) as a clean trained model. Specifically, we use the method proposed in DeepFool \cite{deepfool} to find the minimum perturbation that can satisfy the mentioned constraint:
{\small
\begin{equation}
 \quad \epsilon = \arg \min_{\delta} \|\delta\|_{2} \quad \text{subject to } \quad \frac{ \sum_{x\in D_{\text{OOD}}}{\text{ID-Score}_{g}(x + \delta)}}{|D_{\text{OOD}}|} \ge \gamma.
\end{equation}
}
\textbf{Threshold Computing.} \ \
Once the signature value \( S(f, D_{\text{OOD}}) \) has been computed for the given classifier \( f \), it is critical to determine whether \( f \) has been compromised by a trojan, using a threshold-based strategy. This process is achieved by employing a statistical test on a set of scores computed for a surrogate classifier \( g \). Specifically, given the surrogate classifier \( g \) and the OOD set \( D_{\text{OOD}} \), we generate a set of baseline scores denoted as \(\{ S_i(g, D_{\text{OOD}})\}_{i=1}^{N} \). These scores represent the signature values assigned by a clean classifier \( g \). For the input classifier \( f \), we calculate its signature using the formula described in Equation \ref{eq:signal}. When the model is trojaned, its corresponding signature will be an outlier to the distribution of \( S_i(g, D_{\text{OOD}}) \).
We estimate this null distribution with a Normal distribution to find a threshold $\tau$ satisfying \(Prob(\underset{i=1,\ldots,N}{max} -log(1 - S_i(g, D_{\text{OOD}})) \leq \tau) > 0.95 \). Solving for $\tau$, gives the following threshold: $\tau = \Phi^{-1}_{\text{}}(\sqrt[N]{0.95})$, where $\Phi$ is the CDF of our estimated truncated normal distribution and we set $N=50$. We refer to $\tau$ as 
\textbf{scanning threshold}.

% {\small
% \begin{equation*}
%  \tau = \Phi^{-1}_{\text{}}(\sqrt[N]{0.95}),
% \end{equation*}
% }
% \kian{I suggest putting a pseudocode (1 or 2 algorithm boxes) to explain the method. right now the reviewer needs to read two pages to understand the trojan detection algorithm and it is confusing.} 
% \ali{We are going to do this, but it won't fit in main pages. We will provide it in Appendix.}
% \textbf{Shifting OOD Samples as a Signature}



% By applying a common attack such as PGD on OOD samples to adversarially increase their ID-Scores, we shift these samples toward the training data distribution. 




% \textbf{Threshold} \ \ % Definition of the threshold for detecting backdoored models
% \bd{
% In this study, we define the detection threshold \(\theta\) as \(\theta = \mu - z_{0.95} \times \sigma\), where \(\mu\) and \(\sigma\) represent the mean and standard deviation of the scores \(S\) from models trained under standard conditions. The value \(z_{0.95}\) corresponds to the critical z-value for a significance level of \(\alpha = 0.05\), typically associated with the 95\% confidence interval under the null hypothesis ($H_0$) that the model is not backdoored. A score \(S\) that results in a p-value less than \(\alpha\) suggests rejection of $H_0$, indicating anomalous behavior consistent with a backdoor.
% }




% In this section, we describe the use of the DeepFool attack to find an effective perturbation that leads to an error for a specific objective function. DeepFool is an iterative attack designed to find the minimal perturbation required to change the classification of a given sample.

% Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a classifier, where $f(x)$ denotes the output of the classifier for input $x$, and let $L$ be the objective function. The goal of DeepFool is to find the smallest perturbation $\delta$ such that the perturbed input $x + \delta$ is misclassified.

% The DeepFool algorithm proceeds as follows:

% 1. Initialize the perturbed sample as $x_0 = x$.
% 2. For $t = 0, 1, 2, \ldots$ until convergence, compute the perturbation $\delta_t$:
%     \begin{equation}
%     \delta_t = - \frac{f(x_t)}{\|\nabla f(x_t)\|_2^2} \nabla f(x_t),
%     \end{equation}
%     where $\nabla f(x_t)$ is the gradient of the classifier at $x_t$.
% 3. Update the perturbed sample:
%     \begin{equation}
%     x_{t+1} = x_t + \delta_t.
%     \end{equation}
% 4. Check if $f(x_{t+1}) \neq f(x)$. If so, stop the iteration and set $\delta = \sum_{i=0}^{t} \delta_i$.

% The resulting perturbation $\delta$ is the minimal perturbation required to change the classification of $x$ under the objective function $L$. This can be formally expressed as:
% \begin{equation}
% \delta = \arg \min_{\delta'} \|\delta'\|_2 \quad \text{subject to} \quad f(x + \delta') \neq f(x).
% \end{equation}

% Using the DeepFool attack, we effectively identify perturbations that exploit the vulnerabilities of the classifier with respect to the objective function $L$, leading to a misclassification.














% ****
% for computing $\epsilon$ for adverserial attack on OOD samples, we first train a surrgate classifier $g$ on   $D_{v}$ to obtain a  clean well-trained classifer. 


% specificly for computing $\epsilon$ we train a surrgate classifier $g$ on   $D_{v}$ to obtain a  clean well-trained classifer. 

% \begin{equation}
% \delta = \arg \min_{\delta'} \|\delta'\|_{2} \quad \text{subject to} \quad f(x + \delta') \neq f(x).
% \end{equation}



% it worth noting the logits of an OOD sample \(x\) resemble a uniform distribution \(\mathcal{U}_K\) and \({\text{ID-Score}}_{f}(x_{\text{OOD}}) \approx \frac{1}{k}\). To formulate attack on OOD sample   \(x_{\text{OOD}}\) to shifted to in-distribution, we aim to increase \({\text{ID-Score}}_{f}(x_{\text{OOD}})\) up to \(\frac{1}{2}\). 


% This corresponds to the classifier model \(f\)  recognize \(x_{\text{OOD}}\) with significant confidence as ID.   


% noteworthy there have many other metric as OOD score which are mostly based on last layers features and logits including KNN distance, mahalanobis distance, ODIN. in this study we considered ${\text{MSP}}_{f}(x)$ as our default posthoc method for detecting OOD data ability of classifier, however other scoring strategy have been explored.  \ali{nigga what? ablation miarim?}(see section X).




% This shortcut path is created by directing the optimizer to craft a perturbation that acts as a proxy for triggers, even when they are not explicitly available. As a result, a PGD attack with the same budget leads to a significant increase in the ID-Score for perturbed OOD samples in a trojan classifier compared to clean ones. 


% We denote a perturbed version of the OOD sample \(x^{\text{OOD}}\) as \(x^{\text{OOD*}}_f\). Then we define our signature for a classifier \(f\) as: 

% $\quad \quad \quad \quad \quad \quad  \quad \quad S(f,x^{\text{OOD}}) = \text{ID-Score}_{f}(x^{\text{OOD*}}) - \text{ID-Score}_{f}(x^{\text{OOD}})$  

% \frac{\sum_{i=1}^{|D_{\text{OOD}}|} (\text{ID-Score}_{f}(x_i^{\text{OOD*}}) - \text{ID-Score}_{f}(x_i^{\text{OOD}}))}{|D_{\text{OOD}}|}

% in this study we assume that there is an access to a benign validation set denoted as $D_{v}$ (i.e. tiny Imagenet), which holds in real world scenario where there are tons of availble datasets. we craft a OOD set $D_{\text{OOD}}$  by applying mentioned strategy  i.e. $D_{\text{OOD}}=G(D_{\text{v}})$ 
% we use theses dataset for computing $\epsilon$ of our PGD attack mentioned in above equtions. morever leveraging them we propse a threshold mechanism to determine wether an input classfier is trojaned, using its signiture $S(f, D_{\text{OOD}}) $. 


% at first we note that the logits of an OOD sample \(x\) resemble a uniform distribution \(\mathcal{U}_K\) and \({\text{ID-Score}}_{f}(x_{\text{OOD}}) \approx \frac{1}{k}\) where $k$ denotes the number of classes of training data. we propose an effective $\epsilon$ should shift OOD samples toward ID regions significantly. as a result we propose computing $\epsilon$ by founding which value of that could increase ID-Score  from $\frac{1}{2}$ to $0.5$  for OOD set  $D_{\text{OOD}}$ corresponding to surrogate classifier $g$. specificly we use Deepfool attack to find the minimum purtubation that can satisfy the mentioned constraint:
% \begin{equation}
% \text{For} \ x \ in \ D_{\text{OOD}}:  \ \delta = \arg \min_{\delta'} \|\delta'\|_{2} \quad \text{subject to  } \quad \text{ID-Score}_{g}(x) \approx 0.5. 
% \end{equation}

% after computing those purtubation for each OOD sample we use 


% To formulate attack on OOD sample   \(x_{\text{OOD}}\) to shifted to in-distribution, we aim to increase \({\text{ID-Score}}_{f}(x_{\text{OOD}})\) up to \(\frac{1}{2}\). 
% give a signiture value has been achived for input classifier $f$ , we need to determine wether it has been trojaned base  threshold strategy.  we over come this by using statistical test on set of computed scores for $g$. specificly given surrogate classifier $g$ and OOD set $D_{\text{OOD}} $ we creat a set of scores denoted as $S(g, D_{\text{OOD}})$ corresponding to signitures value assigned by a clean classiferi $g$. for an input classifier $f$ we compute its signture using \ref{eq:signal} then given $S(g, D_{\text{OOD}})$ we determined if the $f$ is trojaned by applying statstical test between them.

% In this section, we formulate an adversarial attack on OOD samples to shift them towards the ID region. First, we define the maximum softmax probability (MSP) as the ID-Score, which is an indicator of the classifier's confidence in recognizing an input sample as belonging to the ID. The crafted perturbation aims to find a shortcut path to increase the ID-Score, effectively shifting the OOD sample towards the blind spots of the trojaned classifier. This process results in a significant increase in the ID-Score, highlighting the introduced signature.   Formally PGD attack to ID-Score for a  sample $x$ corresponsinf to a classifier $f$ could be formaluated as:

% $  x_{0}^{*} = x, \quad \quad x_{t+1}^{*} = x_{t}^{*} + \alpha \cdot \text{sign}\left(\nabla_x  J(f(x_{t}^{*})) \right),  \quad \quad  x^{*}=x_{N}^{*},  \quad \quad J(f(x)) = -\text{ID-Score}_{f}(x)$.
% where  the noise is projected within the \(\ell_{2}\) norm in each step: $\|x - x_0\|_2 \leq \epsilon$. 

% to define our signiture we assume a set as source of OOD sample denoted as $D_{\text{OOD}}=\{ x_i^{\text{OOD}} \}$ is availble, where for   considering $f$ 

% we will define our signiture   $S(f,x^{\text{OOD}})$ as:

% $\quad \quad \quad \quad  \quad \quad S(f,D_{\text{OOD}}) = \frac{\sum_{i=1}^{|D_{\text{OOD}}|}  ( \text{ID-Score}_{f}(x_i^{\text{OOD*}}) - \text{ID-Score}_{f}(x_i^{\text{OOD}}))}{|D_{\text{OOD}}|}$  where $x_i^{\text{OOD*}}$ have obtained by adding purtutabtion to $x_i^{\text{OOD}}$ by a  PGD attack.


%  which higher value for that indicate that $f$ is backdoored with higher probability. as we need a thresholding mechanism for detecting whether an input classifier $f$ is backdoored or not we utilize a validation set and a classifier as 

% . 

% $  x_{0}^{*} = x, \quad \quad x_{t+1}^{*} = x_{t}^{*} + \alpha \cdot \text{sign}\left(\nabla_x  J(f(x_{t}^{*})) \right),  \quad \quad  x^{*}=x_{N}^{*},  \quad \quad J(f(x)) = -\text{ID-Score}_{f}(x)$


  
% we use a valition set as source of OOD data and if training samples are areplace them with near OOD data


% to create OOD set, In scenarios where  a limited number of trained samples are available, we transform them with hard augmentation to generate related OOD samples. otherwise we use which leads to a more discriminative signature. Otherwise, we use a validation set as the OOD set. 


% 


% scanning for \textbf{\textit{TRO}}janed Models via \textbf{\textit{D}}etection of Adversarial Shifts in Near \textbf{\textit{O}}OD Samples (TRODO) where in

 % $ x^*_{\text{OOD},0} = x_{\text{OOD}},  \quad \quad  \quad  x^*_{\text{OOD}, t+1} = x^*_{\text{OOD}, t} + \alpha \cdot \text{sign}(\nabla_{x} J(f(x^*_{\text{OOD}, t}))), \quad \quad  x^*_{\text{OOD}} = x^*_{\text{OOD}, N}$

% denoted as ${D^{OOD^*}_{f}}$ in this study. using a validation set for creating a pool of $\{D^{OOD_i^*}_{f}\}$ corresponding to a  clean classifier f and set of OOD samples $\{OOD_i\}$, we compute  ${D^{OOD^*}_{g}}$ for a input classifier $g$ and with comparing $\{D^{OOD_i^*}_{f}\}$ determine wether $g$ is scanned.


% , we utilize the distance of shifted perturbed OOD samples as a signature to indicate whether a classifier has been Trojaned. Specifically, this signature is more pronounced when OOD samples are near the in-distribution boundary. We utilize this signature to scan an input model to determine whether it is backdoored, specifically for a given classifier \( f \) and samples \( \{x^i_{\text{ID}}\} \) that belong to the training set \( D \). We craft near OOD samples by applying \( G(.) \), reaching \( \{x^i_{\text{OOD}}\} = G(\{x^i_{\text{ID}}\}) \). Due to the strategy mentioned in Section X, by adding perturbation to OOD samples \( \{{x^{i*}}_{\text{OOD}}\} \), we create adversarial OOD samples. The feature space distance between them, i.e., \( J(f(x^*_{\text{OOD}}) \| f(x_{\text{OOD}})) \), is used as a signature. Here, \( J(\cdot \| \cdot) \) is the Jensen-Shannon divergence, a symmetric version of the Kullback-Leibler divergence, commonly used to compute the distance between two distributions.  
% $$\text{KL}(P \parallel Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right), J(P \| Q) = \text{KL}(P \parallel Q) + \text{KL}(Q \parallel P) $$

% Formally we consider the $J(f(x^*_{\text{OOD}}) \| f(x_{\text{OOD}})$  as an indicator which higher value for that indicate that $f$ is backdoored with higher probability. as we need a thresholding mechanism for detecting whether an input classifier $f$ is backdoored or not we utilize a validation set and a classifier as 



%  \textbf{validation set as OOD source}

% \textbf{validation set for backdoor model scanning}
% we consider a dataset as validation set were we train a clean classifier on that then utilize our proposed signature on that to exploit scanning score for a clean model. specifically we use tinyimagenet and train a classifier $f$  on that then by proposed strategy in section X we find scores correspond to a clean model.

% To obtain a set of OOD samples, we propose two scenarios. In the first scenario, there is access to a portion of the training data for the input test classifier. In this scenario, the OOD set can be generated by applying transformations known to be harmful for preserving the semantic integrity of an image, such as cutout. These transformations produce samples which, while not part of the ID, retain a degree of visual similarity to ID samples. In this study, we utilize such samples as a proxy for Near OOD samples. To ensure that these transformations meaningfully alter the characteristics of the samples, distancing them from the training data distribution, we define a set of hard transformations \(\mathcal{T} = \{T_i\}_{i=1}^{k}\), where each \(T_i\) represents a specific hard augmentation. For each ID sample \(x\), a random permutation of \(\mathcal{T}\) is selected, and the transformations are applied in a randomized sequence to the sample, producing \(T_{i_k}(\ldots T_{i_1}(x))\). This process is designed to craft a diverse set of OOD samples, especially useful in settings where access to the training set is limited. Each transformed training sample \(x\) results in a crafted OOD sample \(x'\), where the transformation process is denoted as \(G(\cdot)\), i.e., \(x' = G(x)\). regarding which hard transformation seleted please see appendix X. in other scanrio that there is no access to a training data we utilze a small dataset as OOD set. specificly we utilze tiny imagenet as OOD set for this setup. as may training data (e.g. CIFAR10) share some concept with our OOD set in secanrio we apply $G(.)$ on TinyImagnet before utilizing as OOD set, to ensure that they do not blong to training distribution.) we coin our method in supervised scenorio ad TRODO and unsupervised scenario as UNTRODO.




% This method systematically ensures that \(X'\) significantly diverges from the ID, validating their use as effective OOD samples. 


 


% In this section we explain compononets of TRODO. which using an algorithm attack such as PGD aims  increasing MSP score of OOD samples to shift toward training data distribution. in next step we measure magnituted of shifting distnace of purturbed OOD samples  compare to it clean counterpart denoted as in feature distance as a signiture. in scenario that evenv a limited of trained samples are availble we transform them with hard augmention as related OOD samples leads to more discriminative signutre. other wise we utilize a validation set as OOD set. more details have been provided in following.


% For an in-distribution sample \(x_{\text{ID}}\) and its adversarially shifted version toward in-distribution, denoted as \(x^*_{\text{OOD}}\), the feature space distance between them, i.e.,  \(J (x^*_{\text{OOD}}\| x_{\text{OOD}})\), could be used as a signature. Here, \(J (. \| .)\) is the Jensen-Shannon divergence, a symmetric version of the Kullback–Leibler divergence, commonly used to compute the distance between two distributions.

% We aim to adversarially move crafted near-OOD samples toward the in-distribution while shifting ID samples toward out-of-distribution using the PGD algorithm, by adding small perturbations at each step. Specifically, we target the MSP values of ID and OOD samples, aiming to decrease the MSP value for ID samples and increase it for OOD samples.

% Given an ID sample \(x\), we craft its OOD counterpart \(G(x)\) by applying the specified augmentations. We hypothesize that the vector between \(x\) and \(G(x)\) represents a transition from out-of-distribution to in-distribution. We further hypothesize that this vector in backdoored classifiers is more susceptible to change compared to clean models. This is primarily based on the intuition that backdoored models, through overfitting to triggered samples during training, show different generalization behavior. 

% By targeting MSP values, we aim to use adversarial strategies to manipulate the perceived distribution of samples, thereby testing the resilience of classifiers against such model manipulations.



% \textbf{shifting OOD samples as a signiture} \ \ motivated by mentioned insights in theoritical section we utilize shifted OOD samples distance as a signature of classifier is tronjaned. specificly this signiture is more highlighted  when OOD samples are near to in-distribution boundry. conditioning 
  
  
%   specificly for a $x_{\text{ID}}$ and its shifted version to in-distribution denoted as   $x^*_{\text{OOD}}$ the distance between them in feature space i.e. $J (x^*_{\text{OOD}} \| x_{\text{OOD}}) $ could be use as a signiture where $J (. \|.) $ is Jensen-Shannon divergence, which is a symmetric version of Kullback–Leibler divergence a common metric for computing distance between two distribution. 

  
% we aim to adverserially move crafted near OOD samples toward in-distribution  meanwhile shift ID samples twoard out-of-distribtion by PGD by  adding small purtubation with PGD algorithm in each step.  to do this we consider MSP of ID and OOD samples as target for shifting data, specficly we adverserially aim to decrease MSP value for ID samples and increase that for OOD samples:

 

 
% given an ID sample $x$, we craft its OOD-counter part $G(x)$  by applying mentioned augmentions. we hypothesize that the vector between $(x,G(x))$ represents a vector from out of the distribution to in-distribution. we hypothesis that this vector in backdoored classifiers are more vulnerable to change compare to clean models. this is mainly because based on the intuition that backdoored models generaliztion through overfitting to triggered samples during training cause. 
% we aim to adverserially move crafted near OOD samples toward in-distribution  meanwhile shift ID samples twoard out-of-distribtion by PGD by  adding small purtubation with PGD algorithm in each step.  to do this we consider MSP of ID and OOD samples as target for shifting data, specficly we adverserially aim to decrease MSP value for ID samples and increase that for OOD samples:

  

    



% ***********

%  $x$

%  $ \quad \quad  \quad \quad  \quad \quad x_{0}^{*} = x, \quad \quad x_{t+1}^{*} = x_{t}^{*} + \alpha \cdot \text{sign}\left(\nabla_x J\left( x_{t}^{*}, y\right)\right),  \quad \quad  x^{*}=x_{N}^{*}$







% Classifiers such as $f$ trained on a dataset like $D$ can be utilized as OOD detectors due to their learned features from $D$ samples. specificly another dataset with seperate semantics presented in $D$ denoted as $D'$ is assumed as source of OOD samples. for instance a classifer trained on $Cifar10$ is use to detect $Cifar100$ as OOD samples. the $D$ and $D'$ are refered as ID and OOD set as well. the instructions to practically do OOD detecin this is using the output distribution of a classifier over classes of $D$ for a given input, offering insights into the model's prediction confidence. specifily  It is observed that classifiers tend to be more confident about ID samples compared to OOD samples. The Maximum Softmax Probability (MSP) has been proposed as an indicator of this confidence.  


% Consider a well-trained classifier \(f_{\theta}\) on an ID set with \(k\) classes. The classifier's logits from its penultimate layer for a given input sample \(x\) are represented by \(z = f_{\theta}(x)\), where \(z\) is a vector of unnormalized prediction scores \([z_1, z_2, \dots, z_k]\) for each class. Applying the softmax function results in:
% \begin{equation}
% \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}, 
% \end{equation}
% for each class \(i\), where \(z_i\) is the logit corresponding to the \(i\)-th class. This function outputs a probability distribution \(p\) over the classes, where:
% $p_i = \text{softmax}(z_i) $. The ${\text{MSP}}_{f}(x)$ for the input sample \(x\) is determined by:
% \begin{equation}
% \text{MSP}(x) = \max(p) = \max_{i \in \{1, \dots, k\}} p_i
% \end{equation}












% The previous section provides an intuition about how classifiers can be fooled with an input sample \(x \in D\). Here, we aim to formulate adversarial attacks on OOD samples such as \(x \in D'\). Specifically, we target the MSP score for OOD samples, where an increase in \({\text{MSP}}_{f}(x)\) implies that the confidence of \(f\) regarding the classification of \(x\) as belonging to in-distribution increases. Intuitively, this type of attack shifts an OOD sample \(x\) into the in-distribution category by adding adversarial perturbations.

% Previous studies have shown that even classifiers adversarially trained on \(D\) (the in-distribution set) are vulnerable to adversarial perturbations on OOD samples, mistakenly classifying them as ID.


%  where previous section provides intution about how classfiers fool on input sample $x \in D$. here we aim to formulate adverserial attack on OOD samples such as $x \in D'$. to formulate this we target MSP score for OOD samples, where increasing ${\text{MSP}}_{f}(x)$  means that the  $f$ confidence regarding that $x$ belongs to in-distribution increases intutatively this kind of attack shift OOD sample $x$ in to in-distribution by adding adverserial purtubation.  previous studies have been show that even classfiers adversrially trained on $D$ (i.e. in-distribution set) are vulnerable to adverserial purtubation on OOD samples and consider them as ID. 



%  $$L(x,y,F)=:\mathds{1}[y=0].L_{\mathrm{CE}}\left(F\left(x\right), \mathcal{U}_K\right)-\mathds{1}[y=1].\sum_{i=1}^K F_i\left(x\right) \log F_i\left(x\right)$$ 

 

% we aim to adverserially move crafted near OOD samples toward in-distribution  meanwhile shift ID samples twoard out-of-distribtion by PGD by  adding small purtubation with PGD algorithm in each step.  to do this we consider MSP of ID and OOD samples as target for shifting data, specficly we adverserially aim to decrease MSP value for ID samples and increase that for OOD samples:

 

 
% given an ID sample $x$, we craft its OOD-counter part $G(x)$  by applying mentioned augmentions. we hypothesize that the vector between $(x,G(x))$ represents a vector from out of the distribution to in-distribution. we hypothesis that this vector in backdoored classifiers are more vulnerable to change compare to clean models. this is mainly because based on the intuition that backdoored models generaliztion through overfitting to triggered samples during training cause. 
% we aim to adverserially move crafted near OOD samples toward in-distribution  meanwhile shift ID samples twoard out-of-distribtion by PGD by  adding small purtubation with PGD algorithm in each step.  to do this we consider MSP of ID and OOD samples as target for shifting data, specficly we adverserially aim to decrease MSP value for ID samples and increase that for OOD samples:

% specificly for a ed

%  $$L(x,y,F)=:\mathds{1}[y=0].L_{\mathrm{CE}}\left(F\left(x\right), \mathcal{U}_K\right)-\mathds{1}[y=1].\sum_{i=1}^K F_i\left(x\right) \log F_i\left(x\right)$$ 


%  $$x_{0}^*=x, \quad \quad   x_{adv}^{(t+1)} = x_{adv}^{(t)} + \alpha \cdot \text{sign}\left(\nabla_x J\left(F\left(x_{adv}^{(t)}\right), y\right)\right)$$

%    $$x_{0}^*=x, \quad \quad   x_{adv}^{(t+1)} = x_{adv}^{(t)} + \alpha \cdot \text{sign}\left(\nabla_x J\left(F\left(x_{adv}^{(t)}\right), y\right)\right)$$
%  where  $\mathcal{U}_K$
%  is a uniform distribution.

 

 

% with crafting $X=X_{ID} \bigcup X_{OOD}, \quad  Y=(1\bigcup 0)$ and applying mentioned adverserial attack, targeting MSP for ID and OOD samples we reach purturbed version of them denoted as 

%  $X=X_{ID}^{adv} \bigcup X_{OOD}^{adv}$ 
%  we show observe that lack of robustness of backdoor model leads to its 



% $$v_1=X_{ID}^{adv}-X_{ID}  \quad v_2=X_{OOD}^{adv}-X_{OOD} $$




% $$v_1=X_{OOD} -X_{ID}  \quad v_2=X_{OOD}^{adv}-X_{ID}^{adv} $$

% \begin{equation}
%     J (v_1 \| v_2)=D_{\mathrm{KL}}(v_1 \| v_2)+D_{\mathrm{KL}}(v_2 \| v_1)
% \end{equation}



% we use  $J (v_1 \| v_2)$ as a measure of robustness of trageted classifier where larger value indicates the classifier is backdoored with higher probablity and lower valu indicates the classifier is clean and considered as a clean model  



% in many case there is no access to training samples  of a dataset in such cases as a result crafting OOD samples would be a challange. for those cases we propose utilizing a validation set as OOD and adapt metnioned eqution above for those secnario, to solely apply for OOD samples and consider change of distribution of OOD samples before and after adverserial attack as signiture.

% which means 

% $$v_1=X_{ID}^{adv}-X_{ID}  \quad v_2=X_{OOD}^{adv}-X_{OOD} $$

% \begin{equation}
%    D_{\mathrm{KL}}(v_2 \| v_1)
% \end{equation}



 
 
% post-hoc methods utilzes the logits or features extracted by  $f_{\theta}$ to distingushes OOD data from ID data. an   detector method by assigning OOD score to samples. ideally an OOD detectors assigned scores tottally distingushes two in- and out-of-distribution data.   


%  The 


% Considering ID samples include  we assume that 0.01 of trainig set samples 






 
% \textbf{Minimum Step to Shift ID samples.}
% It has been shown that DNNs are vulnerable to adverserial attacks.


%  \textbf{Adverserial Attack on  a classifiers }
%  It has been shown that DNNs are vulnerable to adverserial attack on various task, which mostly explored on classification task, where adverserial purtubation during inference added to input data to make classifier mispredict its label. in other word a purturbation such as $\delta$ is added to sample $x$ with label $y$ to fool model to output $\hat{y}$ for adverserial input sample $x^{*}$  where $x^{*}=x+\delta$. specificly PGD is a terative adverserial attack to craft adverserial attack where the noise should be projected in $\ell_{2}$:

%   $$x_{0}^*=x, \quad \quad   x_{t+1}^{*} = x_{t}^{*} + \alpha \cdot \text{sign}\left(\nabla_x J\left( x_{t}^{*}, y\right)\right)$$

% where   $ J\left( x_{t}^{*}, y\right) $  denotes the for targetting objective function where for classifyers is cross entropy.   

% previous studies has been shown that  standard trained classfiers  are vulnerable to   adverserial attack even waek attacks such as FGSM, and differnet kinds of defence mechanism have been proposed to tackle this, most effective adverserial training on $D$.


 
% =D_{\mathrm{KL}}(v_1 \| v_2)+D_{\mathrm{KL}}(v_2 \| v_1)

 

% The previous section provides an intuition on how a classifier could  misclassify a input sample x with adding adverserial purtutbation and targeting cross-entropy loss function. in this section we formulate adverserial attack to OOD samples to  move it toward the in-distribution. 
% Specifically, by targeting an increase in the MSP score for OOD samples,  the confidence of \(f\) in considering \(x\) as belonging to the in-distribution increases. at first step logits of an OOD sample $x$ is uniform like  $\mathcal{U}_K$ and \({\text{MSP}}_{f}(x_{\text{OOD}}) \approx \frac{1}{k}\). for ensuring  $x_{\text{OOD}}$ has been shifted to in-distribution we   increase \({\text{MSP}}_{f}(x_{\text{OOD}})\) up to $\frac{1}{2}$ which corresponds to that classifier model $f$ with significant confidence consider $x_{\text{OOD}})$ as in-distribution. in seek of 



% can be deceived by an input sample \(x \in D\)  .


% Specifically, by targeting an increase in the MSP score for OOD samples, these samples gradually move toward the in-distribution boundary.

% An increase in \({\text{MSP}}_{f}(x)\) suggests that the confidence of \(f\) in classifying \(x\) as belonging to the in-distribution increases.

% at first 


% We consider \({\text{MSP}}_{f}(x) = 0.5\) as a threshold for determining if a sample \(x\) is in-distribution.

% Intuitively, this type of attack shifts an OOD sample \(x\) into the in-distribution category by adding adversarial perturbations.

% Previous studies have demonstrated that even classifiers adversarially trained on \(D\) (the in-distribution set) are vulnerable to adversarial perturbations on OOD samples, mistakenly detecting them as ID.




% \textbf{ a Classifier as OOD data detector}

% it has been show that a classifier trained on training set (ID samples) could be utilize as a OOD sample detector owe to its learned features from ID samples. to identify data samples that deviate from the distribution the classifier was trained on it has proposed to utilize distribution provided by classifer in the final lyer  over classes for a given input. This distribution can give insights into how confident the model is about its predictions. it has been shown that a clissifier is more confident about ID samples compare to OOD samples, and one proposed maximum softmax probablity as an indicator of this confidnece.
% given that a classifier $f_{\theta}$ is well trained on a ID set with $k$-classes, post-hoc shows the classifeirs logits and it pneumlates layer would be a reprsentitve as a data belongs to ID data. specificly maximum soft max probablity value for a input sample $x$



%  specificly Maximum Softmax Probability (MSP) is used in the context of out-of-distribution (OOD) sample detection in  models, particularly classifiers. It leverages the softmax output probabilities to determine the confidence level of the predictions, aiding in distinguishing between in-distribution and out-of-distribution samples.

 
% consider   outputs logits \( z = f(x) \) for a given input sample \( x \). Here, \( z \) is a vector of unnormalized prediction scores \( z = [z_1, z_2, \dots, z_K] \) for each of the \( K \) classes.  by applying soft max function we will have:

%  \begin{equation}
% \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
% \end{equation}
% for each class \( i \), where \( z_i \) is the logit corresponding to the \( i \)-th class. This function outputs a probability distribution \( p \) over the classes, where:
% \begin{equation}
% p_i = \text{softmax}(z_i)
% \end{equation}

%  The MSP for the input sample \( x \) is determined by:\begin{equation}
% \text{MSP}(x) = \max(p) = \max_{i \in \{1, \dots, K\}} p_i
% \end{equation}
% This represents the highest probability assigned to any class by the model, reflecting the confidence level of the model's prediction. 


% \textbf{Near OOD sample Generation.} In the field of Out-Of-Distribution (OOD) detection, it is generally presumed that the training dataset \(\mathcal{D}_{\text{train}}\) solely comprises In-Distribution (ID) samples. If we have access to a subset of this dataset, denoted as \(X\), we can generate pseudo-OOD samples, \(X'\), by applying hard transformations that are known to be harmful to semantic preserving, such as cutout. These transformations craft samples that, although not belonging to the ID, retain a degree of visual similarity to ID samples, considering them as Near OOD samples, which means play as place holder for boundry of distribution of training set.






% To ensure that these transformations substantially alter the characteristics of the samples away from the ID, we create a set of hard transformation  \(\textit{T} = \{T_i\}_{i=1}^{k}\) where each \(T_i\) is a specific hard augmentation,  and For each ID sample, we randomly select a subset of \(T\)   and  These transformations are applied in a randomized sequence to the sample, producing \(T_{i_k}(\ldots T_{i_1}(x))\), we do this to craft diverse set of OOD samples, specificly for many setting where our access to training set is limited (0.01 percent of training set). for each ID sample $x$ we denote crafted OOD sample as $x'$ where we denote the applying transformation proccess as $G(.)$ , i.e. $x'=G(x)$

%   This method allows us to systematically ensure that \(X'\) diverges significantly from the ID, validating their use as OOD samples.


% OOD detection litrture assumes that training set  $\mathcal{D}_{train}$ just includes ID samples. considering we have access to a seubset of training set denoted as $X$   we craft psudo-OOD samples $X'$ by applying hard transformations that have shown to be harmful for preseriving semantic such as cutout. this strategy leads to crafting samples that do not belong  to in-distribution, however  share some appearence relevence to ID samples, leadning to considering them as Near OOD samples as mentioned  by . to ensure that by applying hard  augmention  a sample considerably diverges from ID and could be coniseder as a OOD sample, an random subset of hard transformation apply on that. including set of $\textit{T}=\{T_i\}_{i=1}^{k}$ where each $T_{i}$ represents an hard augmetnion, we apply a non empty subset of $T$ and denote that as $G$.  
