\section{Related Work}
% \textbf{Backdoor Attacks.}\ \ Injecting pre-defined triggers into the training data is the most common approach to implement backdoor attacks. BadNet \cite{badnets} is the first backdoor attack against DNN models, which involves modifying a clean image by inserting a small, predetermined pattern at a fixed location, thus replacing the original pixels. Blended \cite{blended} aimed to enhance the invisibility of the trigger pattern by seamlessly blending it into the clean image through alpha blending. SIG \cite{sig} utilized a sinusoidal waveform signal as the trigger pattern. To achieve better stealthiness, many attacks with invisible and dynamic triggers have been proposed. Input-aware \cite{inputaware} proposed a training-controllable attack method that simultaneously learned the model parameters and a trigger generator to produce a unique trigger pattern for each clean test sample. For more details regarding other attacks including BPP \cite{bpp}, SSBA \cite{ssba}, WaNet \cite{wanet}, and \cite{color} read Appendix Section \ref{app:backdoor-attacks}.


\textbf{Trojan Scanning.}\ \ Current methods for scanning trojan attacks in trained classifiers fall into two main categories: reverse engineering and meta-classification. Reverse engineering methods, such as NC \cite{NC}, ABS \cite{ABS}, TABOR \cite{TABOR}, PTRED \cite{PTRED}, and DeepInspect \cite{DeepInspect}, identify trojaned models by applying and optimizing a trigger pattern to inputs, causing them to predict the trojan label. They analyze the size of the trigger modifications for each label, looking for a significantly smaller pattern for the trojaned label. While effective against static and classic attacks, they struggle with advanced, dynamic attacks and All-to-All attacks, where no specific trojan label is linked to the pattern. UMD \cite{umd} attempts to detect X2X attacks but is limited to specific types and single trigger patterns. FreeEagle \cite{freeeagle} optimizes intermediate representations for each class and scan for a class with particularly high posteriors, if any. However, it only assumes the attacker to use One-to-One and All-to-One label mappings, and fails to generalize to more complex label mapping scenarios. Meta-classification detector methods like ULP\cite{ULP} and MNTD \cite{MNTD} train a binary meta-classifier on numerous clean and trojaned shadow classifiers to learn distinguishing features. These methods perform well on known attacks but fail to generalize to new backdoor attacks and require extensive computational resources to train shadow models \cite{xiang2024cbd}. Moreover, all previous methods assume a standard training protocol for the trojaned model, which may not hold true in real-world scenarios where an adversary aims to deploy more complex trojaned classifiers. By implementing adversarial training on poisoned training data, the effectiveness of previous methods, which rely on exploiting known signatures, may be compromised, as observed by \cite{odyssey,zhang2021cassandra}. 

% Maybe should be added later

% Many other recent methods have been proposed as well \cite{hou2024ibd, qu4821388input, zhu2024neuralsanitizer, guan2024ufid, cheng2024lotus, lyu2024task, zhu2024seer, murad2024advancing}.

\textbf{ID-Score and OOD Detection Task.}\ \  A classifier trained on a closed set, can be utilized as an OOD detector by leveraging its confidence scores assigned to input test samples, referred to as ID-Score in this study. Here, the closed set is the training set used for the classification task, and the samples within this set are called ID samples. Various strategies have been proposed to compute ID-Scores from a classifier, among which the MSP has proven to be an effective and general scoring strategy compared to others \cite{liang2017enhancing,kong2021opengan,fort2021exploring,hendrycks2016baseline,ruff2021unifying,salehi2021unified}. The classifier assigns higher ID-Scores to samples that belong to the ID set and lower scores to OOD samples. In this study, we have adopted MSP as our ID-Score based on its demonstrated efficacy in OOD detection literature \cite{msp} and its constrained range between $(0.0, 1.0)$, unlike other ID-Score methods such as KNN distance \cite{sun2022out}, which do not have defined upper and lower bounds. We consistently employ MSP in our methodology, hypothesizing that an MSP value of 0.5 (we call this value boundary confidence level and denote it as $\gamma$) signifies regions near the classifier's decision boundary. Notably, our study includes a comprehensive ablation study of this hyperparameter, detailed in Table \ref{tab:bcl}.






% Current techniques for determining if a trained classifier has been compromised by a backdoor attack are primarily divided into two main groups. Detectors based on reverse engineering and detectors based on Meta-classification. Reverse engineering approaches \cite{NC, ABS, TABOR, PTRED, DeepInspect} specifically identify trojaned models by applying a short-cut modification (i.e. the trigger pattern) to inputs causing them to be predicted as the trojan label. subsequently, these methods calculates such modification through optimization for each label and analyzes if there is a short-cut which is much smaller in size than the modifications of other labels. These approaches while effective on static and classic attacks, fall short in detecting advanced and dynamic attacks. These methods are unable to detect All-to-All attacks where the pattern itself does not lead to a specific trojan label, as the shortcut no longer exists in such cases. However, while UMD \cite{umd} attempts to detect any X2X attack, it only considers limited types of attacks and those with only one trigger pattern.  
%  Meta-classification detectors \cite{ULP, MNTD}  train a binary meta classifier on a large number of clean and trojaned shadow classifiers for learning the distinguish features between two set. Although these methods are effective on the seen attacks during training, they can not generalize to unseen backdoor attack. Additionally, training the shadow networks is computationally intensive. Moreover, all these methods assume a standard training protocol for the trojaned model, which may not hold true in real-world scenarios where an adversary aims to deploy more complex trojaned classifiers. By implementing adversarial training on poisoned training data, the effectiveness of previous methods, which rely on exploiting known signatures, may be compromised, as observed by \cite{odyssey,zhang2021cassandra}. For more details on baselines, refer to Appendix Section\ref{app:baselines}.
 
%  Additionally, these approaches presume standard training of the trojaned model and may not perform well under adversarial training conditions. As adversarial training alters the decision boundaries, makes distribution shifting difficult due to increased robustness.

 
%  Odyssey \cite{odyssey,zhang2021cassandra} also acknowledges this limitation, failing to handle adversarially robust training effectively due to the changed decision boundaries. For more details on baselines, refer to Appendix Section\ref{app:baselines}.
  
 % most scanning methods struggle to modify source class samples to be predicted as another class. 

% The OOD detection task refers to identifying samples that diverge from ID. practically previous works formulate the OOD detection task by considering two datasets $D$ and $D'$ respectively as the source of ID and OOD data samples (e.g. CIfar10 vs. Cifar100 task). During training,  dataset $D$, with labels are available, is used. In most previous works \cite{kong2021opengan,fort2021exploring,hendrycks2016baseline}, the detector $f$ is trained on the task of classifying ID classes. An OOD detector operates by assigning an OOD score to an input sample, where a higher score corresponds to OOD samples. The main challenge in OOD detection is finding an ideal score function that can assign scores to separate ID and OOD samples effectively. Existing score functions for an input test sample $x$ operate on the logits of $f(x)$, hypothesizing that OOD sample logits differ from ID ones. Typically, OOD samples' logits are more uniform because they are unseen by $f$ \cite{ruff2021unifying,salehi2021unified}.


\textbf{Adversarial Risk.}\ \ Adversarial risk refers to the vulnerability of machine learning models to adversarial examples \cite{pmlr-v80-uesato18a, pmlr-v89-suggala19a}. Previous work has established bounds on this metric via function transformation \cite{khim2019adversarial}, PAC-Bayesian \cite{pmlr-v238-mustafa24a}, sparsity-based compression \cite{balda2019adversarial}, optimal transport and couplings \cite{pmlr-v119-pydi20a}, or in terms of input dimension \cite{simon2019first}. This metric has been studied in the context of OOD generalization as well \cite{zou2024adversarial, fort2022adversarial, augustin2020adversarial}. High lower bounds of the metric have also been proved under some conditions such as benign overfitting for linear and two-layered networks \cite{hao2024surprising}.

For an extended related work, see Appendix Section \ref{ext_rel}.

 
% WaNet is  sample-specific
% backdoor attack that  uese the image warping technique as triggers



% BppAttack crafted poisoned images by exploiting color quantization, leveraging imperceptible image distortions as triggers.



% Generally, backdoor attacks are based on the assumption that an adversary can only modify a small fraction of the training data, and the model is then trained  poisoned training dataset by a regular training methods 




%     Gu et al. \cite{gu2017badnets} were the first to demonstrate a backdoor attack on deep neural network (DNN) models, using pixel modifications as a trigger that, while noticeable to humans, initiated a new era in stealthier attack methodologies. Advances in this area have led to two main strategies for hiding these triggers. Invisible triggers, which are subtle changes hard to detect by eye by minimizing the differences in pixels between altered and original images\cite{li2020invisible, chen2017targeted, zhong2020backdoor}, including techniques that maintain similarity in the model's internal representations of both\cite{doan2021backdoor, zhao2022defeat}; and natural triggers, which subtly alter the image's style to appear normal, employing methods like mimicking natural reflections \cite{liu2020reflection}, using social media-style filters\cite{liu2019abs}, applying generative adversarial networks\cite{cheng2021deep}, or performing image transformations\cite{nguyen2021wanet}. A common factor between all of these attacks is focusing on saving clean classification accuracy while achieving a high attack success rate (ASR), which measures the percentage of predicting attack target labels when adding a trigger to an image from another class, but in this work, we focus on out-of-distribution detection of backdoor attacks, and therefore we use other metrics explained in  
