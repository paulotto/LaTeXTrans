\section{Theoretical Analysis }
\label{sec:theory}
{\small
In this section, we provide theoretical insights that underline the susceptibility of trojaned models to adversarial perturbations, particularly in near-OOD regions. 

\textbf{Notation.} In this section, L1 and L2 norms are denoted by $|.|$ and $\|.\|$ respectively. $Y = \Omega(X)$ is equivalent to $Y \geq cX$ for all $X \geq X_0$ where $c, X_0 \in \mathbb{R}^{+}$ are some constants. For vectors $x = (x_i)_{i=1}^{d}$, $\gamma = (\gamma_i)_{i=1}^{d}$, and function $h$, we define: $x^\gamma = x_1^{\gamma_1}\dots x_d^{\gamma_d}$, $\nabla_x^{\gamma} h = \frac{\partial^{|\gamma|}h}{\partial_{x_1}^{\gamma_1}\dots\partial_{x_d}^{\gamma_d}}$, $\nabla_x h = [\frac{\partial h}{\partial_{x_1}}, \dots, \frac{\partial h}{\partial_{x_d}}]^\top$, and $\gamma! = \gamma_1!\dots \gamma_d!$.

We aim to show that a neural network is more sensitive to adversarial perturbations when it receives a backdoor attack, especially in near-OOD data. Let $h(w,x): \mathbb{R}^{d_w} \times \mathbb{R}^{d_x} \to \mathbb{R}$ be a black-box function (e.g., loss or output of a neural network) with learnable parameters $w$ and input $x$.\\ \textbf{Adversarial\ risk} of $h$ in radius $\alpha$ under a distribution $\mathcal{P}$ is defined as follows:
\begin{equation*}
    \mathcal{R}^{\mathcal{P}}_{\alpha}(h,w) := \mathbb{E}_{x \sim \mathcal{P}} \left[  \sup_{\|\delta\| \leq \alpha} h(w,x+\delta) - h(w,x) \right] \approx \alpha \mathbb{E}_{x \sim \mathcal{P}} \| \nabla_x h(w,x) \|.
\end{equation*}

The approximation converges as $\alpha \rightarrow 0$, thus we use the last term in our analysis similar to \cite{simon2019first, hao2024surprising}.

% The following theorem shows that by shifting the distribution $\mathcal{P}$ to $\mathcal{P} + d$, the adversarial risk will increase linearly in terms of $\| d \|$. 

% The following theorem shows that by shifting the $k$-th moment of $\mathcal{P}$ with $d$, the adversarial risk will increase linearly in terms of $\| d \|$. 

We formulate a near-OOD around $\mathcal{P}$ by shifting only the moments of an order $k$. Formally, for any $k \in \mathbb{N}$ and $s \in \mathbb{R}$, we define $\mathcal{P}^{k}_{+s}$ by $\mathbb{E}_{x \sim \mathcal{P}^{k}_{+s}} \left[ x^v \right] = \mathbb{E}_{x \sim \mathcal{P}} \left[ x^v \right] + s$ for any $v \in \mathbb{N}_0^{d_x}$ with $|v|=k$ , and $\mathbb{E}_{x \sim \mathcal{P}^{k}_{+s}} \left[ x^u \right] = \mathbb{E}_{x \sim \mathcal{P}} \left[ x^u \right] $ for any $u \in \mathbb{N}_0^{d_x}$ with $|u| \neq k$. The following theorem shows that the adversarial risk under $\mathcal{P}^{k}_{+s}$ will increase linearly in terms of $|s|$. The proof is given in Appendix Section \ref{proof_th_1}.

\begin{theorem}
\label{th_1}
(Adversarial risk in near-OOD) 
\[
\mathcal{R}^{\mathcal{P}^{k}_{+s}}_{\alpha}(h,w) \geq
\alpha |s| \max_{x} \| \nabla_x \sum_{|\gamma|=k} \frac{\nabla_x^{\gamma} h(w,x)}{\gamma!} \| -  \alpha \| \mathbb{E}_{x \sim \mathcal{P}} \nabla_x h(w,x) \|.
\]

\end{theorem}


\begin{remark}

Theorem \ref{th_1} is applicable when $\nabla_{x_i}^{k+1} h \neq 0$ which is usually true if $h$ contains non-linear exponential activation functions (e.g., softmax, sigmoid, tanh, ELU, and SELU) being infinitely many times differentiable, or if it contains polynomial activation functions with total degree greater than $k+1$. Under this assumption, if we consider $h(w,.)$ as a fixed model trained on a fixed distribution $\mathcal{P}$, then the only variable in the lower bound will be $|s|$ hence we conclude $\mathcal{R}^{\mathcal{P}^{k}_{+s}}_{\alpha}(h,w) = \Omega(|s|)$.

\end{remark}

We now study how the adversarial risk will increase under a backdoor attack. Let $\mathcal{D} = \{(x_i, y_i) = w^{\star \top} x_i) : 1 \leq i \leq n \} $ with $x_i \overset{iid}{\sim} \mathcal{P}$  be the clean training set, $\mathcal{D}^\prime = \{(x^\prime_i + t, y_c) : 1 \leq i \leq m \}$ with $x^\prime_i \overset{iid}{\sim} \mathcal{P}$ be the poisoned training set, $t \in \mathbb{R}^{d_x}$ be the trigger, and $y_c$ be the target class of the attack. We consider $\hat{w}$ as the optimal solution of the least square optimization on the data $\mathcal{D} \cup \mathcal{D}^\prime$:
\begin{equation}
\label{eq_w}
    \hat{w} = \argmin_w \left( \sum_{i=1}^n(h(w,x_i) - y_i)^2 + \sum_{i=1}^m(h(w,(x'_i + t)) - y_c)^2 \right)
\end{equation}
We focus on linear and two-layer networks defined as follows:
$$
h_1(w,x) = w^\top x
, \quad
h_2(w,x) = \frac{1}{\sqrt{ld_x}} \sum_{j=1}^l u_j \text{ReLU}(\theta_j^T x),
$$
where in the latter $w = [\theta_j^\top, u_j]_{j=1}^l \in \mathbb{R}^{l(d_x+1)}$ represents the vectorized parameters of the network, with each pair $[\theta_j^\top, u_j] \in \mathbb{R}^{d_x+1}$, and $\text{ReLU}(z) = \max\{0, z\}$ is the activation function. We approximate $h_2(w,x)$ using the neural tangent kernel (NTK) \cite{jacot2018neural} method with first-order Taylor expansion around an initial point $w_0$:
\[
\Tilde{h_2}(w,x) = h_2(w_0,x) + \nabla_w h_2(w_0,x)^T (w - w_0).
\]
We use the same gradient descent training process as in \cite{hao2024surprising}. The following theorem shows that as the ratio of triggered samples, i.e., $\frac{m}{n}$, or the norm of the trigger $t$ increases, then the adversarial risk will also increase linearly. The proof is given in Appendix Section \ref{proof_th_2}.

\begin{theorem}
\label{th_2}
(Adversarial risk after backdoor attack) for $h \in \{h_1, \Tilde{h_2}\}$, if $\hat{w}$ is learned through the Equation \ref{eq_w} on a fixed  training distribution $\mathcal{P}$, we have:
\[
\lim_{n\to\infty} \mathcal{R}^{\mathcal{P}}_{\alpha}(h,\hat{w}) = \Omega \left(\frac{m}{n} \| t \| \right).
\]
\end{theorem}
}
% $
% \quad
% \lim_{n\to\infty} \mathcal{R}^{\mathcal{P}}_{\alpha}(h,\hat{w}) = \Omega \left(\frac{m}{n} \| t \| \right).
% $

% \begin{remark}

% Theorem \ref{th_2} shows that as the ratio of triggered samples, i.e., $\frac{m}{n}$, or the norm of the trigger $t$ increases, then the adversarial risk will also increase linearly.

% \end{remark}

% In this section, we aim to show that a neural network is more sensitive to adversarial perturbations when it receives a backdoor attack. We study one-layer and two-layer networks with the least square loss function. We also show that the gap increases on OOD data on two-layer networks. Let $\mathcal{D} = \{(x_i, y_i) | \ y_i = w^{\star \top} x_i : 1\leq i \leq n\}$ with $x_i \overset{iid}{\sim} p_X$ be the clean training set, $\mathcal{D}^\prime = \{(x^\prime_i + t, y_c) : 1 \leq i \leq m \}$ with $x^\prime_i \overset{iid}{\sim} p_X$ be the poisoned training set, $t$ be the trigger, and $y_c$ be the target class of the attack.
% Let $f(w;x)$ be a neural network with parameters $w$ and input $x$. We define unsupervised adversarial risk as a metric of risk to adversarial perturbations in a fixed radius.

% \begin{definition} For a given neural network $f(w,x)$ we define the \textbf{unsupervised adversarial risk} as follows:

% \begin{equation}
% \label{eq:unadvrisk}
% \mathcal{R}^{\text{adv}}_{\alpha}(w) := \mathbb{E}_{x} \left[ \sup_{\|\delta\|_2 \leq \alpha} f(w,x+\delta) - f(w,x) \right].    
% \end{equation}

% \end{definition}

% We consider two trained models on data $\mathcal{D}$ (clean training data) and $\mathcal{D} \cup \mathcal{D}^\prime$ (poisoned training data) to obtain two learned parameters $\hat{w}$ and $\hat{w}^\prime$ respectively. Our goal is to find a lower bound for the gap $\mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}) - \mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}^\prime)$. 

% \textbf{Linear neural network}

% \begin{condition}
% \label{con_1}
% We make the following assumptions on data for the linear model:

% \begin{enumerate}

%   \item $E[y_i \mid x_i] = x_i^T \theta$

%   \item $\lim_{n\to\infty} m = \infty,\ \lim_{n\to\infty} \frac{m}{n} = 0$

% \end{enumerate}

% \end{condition}

% \begin{theorem} For a given linear classifier $f(w,x) = w^\top x$, if the Condition \ref{con_1} is satisfied then we have:


% \begin{equation}
% \lim_{n\to\infty} \| \mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}) - \mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}^\prime) \| = \infty.
% \end{equation}
% \end{theorem}

% The proof is given in Appendix Section \ref{app:theroy_linear}

% \textbf{Two-layer neural network}
% We consider a two-layer classifier $f(w,x)$ is given defined as 

% \[
% f(w,x) = \frac{1}{\sqrt{kp}} \sum_{j=1}^k u_j h(\theta_j^T x),
% \]

% where $w = [\theta_j^T, u_j]_{j=1}^k \in \mathbb{R}^{k(p+1)}$ represents the vectorized parameters of the network, with each pair $[\theta_j, u_j] \in \mathbb{R}^{p+1}$, and $h(z) = \max\{0, z\}$ is the ReLU activation function.

% Using the neural tangent kernel (NTK) \cite{ntk} regime, we can approximate $f(w,x)$ using its first order Taylor expansion around the initial point $w_0$:
% \[
% f_{NTK}(w,x) = f(w_0,x) + \nabla_w f(w_0,x)^T (w - w_0),
% \]
% where $\nabla_w f(w_0,x)$ is the gradient of the network function with respect to the weights at $w_0$. 

% We define $F = [f(w_0, x_1), \ldots, f(w_0, x_n)] \in \mathbb{R}^n$, $\nabla F = [\nabla_w f(w_0, x_1), \ldots, \nabla_w f(w_0, x_n)] \in \mathbb{R}^{k(p+1) \times n}$, $G = [f(w_0, x'_1 + t), \ldots, f(w_0, x'_m + t)] \in \mathbb{R}^m$, $\nabla G = [\nabla_w f(w_0, x'_1 + t), \ldots, \nabla_w f(w_0, x'_m + t)] \in \mathbb{R}^{k(p+1) \times m}$, $y = [y_1, \ldots, y_n]^T \in \mathbb{R}^n$, and $y' = [y_c, \ldots, y_c]^T \in \mathbb{R}^m$.


% \begin{lemma}Training the two-layer classifier $f(w,x)$ on data $\mathcal{D}$ with the least square error using gradient descent and the learning rate $\eta < 1 / \lambda_{max}(\nabla F \nabla F^T)$, converges to the following solution:
% $$
% \hat{w} = w_0 + \nabla F (\nabla F^T \nabla F)^{-1} (y - F^T).
% $$
  
% \end{lemma}

% \begin{proof}
% The proof is given in Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation.
% \end{proof}
% \

% \begin{theorem}
% Assume the two-layer classifier $f(w,x)$ as given above is trained using gradient descent on $\mathcal{D}$ and $\mathcal{D} \cup \mathcal{D}^\prime$ with learning rates $\eta_1 < 1 / \lambda_{max}(\nabla F \nabla F^T)$ and $\eta_2 < 1 / \lambda_{max}([\nabla F, \nabla G] [\nabla F, \nabla G]^T)$. Then we have:

% $$
% \| \mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}) - \mathcal{R}^{\text{adv}}_{\alpha}(\hat{w}^\prime) \| = \Omega(\| t \|).
% $$
% \end{theorem}

% The proof is given in Appendix Section \ref{app:theroy_two_layer}.

% Let $\mathcal{D} = \{(x_i, y_i) = w^{\star \top} x_i) : 1 \leq i \leq n \},$ with $x_i \overset{iid}{\sim} p_X$  be the clean training set, and $\mathcal{D}^\prime = \{(x^\prime_i + t, y_c) : 1 \leq i \leq m \}$ be the poisoned training set  with $x^\prime_i \overset{iid}{\sim} p_X$, $t$ be the trigger, and $y_c$ be the target class of the attack.
% Let $f(x) = \hat{w}^\top x$ be the classifier. 

% Adopting the mean-squared loss, clean training involves a least squares optimization:

% \begin{equation}
% \min_{{w}} \sum_{i=1}^n(w^\top x_i - w^{\star\top} x_i)^2,
% \end{equation}
% resulting in
% $\hat{w} = w^\star$ if $X = [x_1 | ... | x_n]$ is full-rank. 

% If trained on both the poisoned and clean data, i.e. $D^{final} = D \cup D^{\prime}$, and taking the gradient of the loss, one would get:

% \begin{equation}
%     \sum_{i=1}^{n} x_i x_i^\top (w - w^\star) + \sum_{i=1}^{m}  (x^\prime_i + t) ((x^\prime_i + t)^\top w - y_c) = 0.
% \end{equation}
% Let $A := \sum_i x_i x_i^\top = X X^\top$, and $A^\prime := \sum_i x^\prime_i x^{\prime\top}_i = X^\prime X^{\prime\top}$. Assuming that $\mathbb{E}(x^\prime_i) = 0$, i.e. the samples are centered around their mean, $\sum_i x_i \approx 0$. Then, the last equation reduces to:
% \begin{equation}
%     Aw - Aw^\star + (A^\prime + m tt^\top)w - m y_c t = 0.
% \end{equation}
% Therefore, we get:

% \begin{equation}
%     w = (A + A^\prime + m tt^\top)^{-1} A w^\star + m y_c (A + A^\prime + m tt^\top)^{-1} t.
% \end{equation}
% To simplify the analysis, let's assume that $m$ is large, and by the concentration of measure,  $A^\prime \approx m/n A$. From the result by Ken Miller:
% \begin{multline}
%      B := ((m + n)/n A + m tt^\top)^{-1} = \frac{n}{m+n} A^{-1} \\ - \frac{1}{1 +  \tr(m n/(m+n) tt^\top A^{-1})} m n^2/(m+n)^2 A^{-1} tt^\top A^{-1}.   
% \end{multline}

% Let's further assume that $w^{\star \top} t = 0$, to account for the fact that the trigger $t$ does not change the ``meaning'' of the data.
% These together simplify $\hat{w}$ to:

% \begin{equation}
% \hat{w} = n/(m+n) w^\star + m y_c B t.   
% \end{equation}

% Now, let's consider an adversarial attack on a test sample $x$ in this linear model:

% \begin{equation}
%     x_{adv} = x + \alpha\hat{w}.
% \end{equation} This leads to the estimated target of the adversarial samples in the healthy classifier:

% \begin{equation}
%     y_{adv} = w^{\star\top} x + \alpha \| w^{\star} \|^2.
% \end{equation} 

% However, for the backdoored model:

% \begin{equation}
%     y_{adv} = \hat{w}^{\top} x + \alpha \| \hat{w} \|^2.
% \end{equation} 

% But note that:

% \begin{equation}
%     \| \hat{w} \|^2 = n^2/(m + n)^2 \| w^\star \|^2 + \| m y_c B t \|^2 + 2n/(m+n) y_c w^{\star\top} B t
% \end{equation}

% We make two assumptions to make this expression simpler. First, note that for a hypothetical data $x_i = y_i w^\star/\| w^\star\|$,  which does not include the trigger, $\hat{w}^\top x_i \approx y_i = n/(n+m) y_i + m y_c y_i/\| w^\star \| w^{\star\top} B t$. Therefore, if $m \ll n$, $w^{\star\top} B t \approx 0$. 
% Also, if $m \ll n$, then 
% \begin{equation}
%     \| \hat{w} \|^2 = \| w^\star \|^2 + k^2.
% \end{equation}
% Therefore, if $\hat{w}^\top x \approx w^{\star\top} x$, the estimated targets for the adversarial input is seperated by a margin of $k^2 := \| m y_c B t \|^2$.


