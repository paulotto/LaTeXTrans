\section{Experiments}
\label{sec:experiments}
\input{Tables/main_results}

\input{Tables/trojai}
We evaluated our proposed method across a diverse range of benchmarks and compared its performance with various existing scanning methods. We developed our benchmark, which includes models trained on a broad spectrum of image datasets. This benchmark includes trojaned models for which various attack scenarios have been considered. The results of these experiments are provided in Table \ref{table:main}. Furthermore, we present an evaluation of TrojAI in Table \ref{tab:eval2} as a challenging benchmark.

% We incorporated benchmarks from TrojAI \cite{trojai} and Odysseus \cite{odyssey} to cover a wide range of scenarios.




\textbf{Baselines.} \ \ In our evaluation, TRODO and TRODO-Zero are assessed alongside previous SOTA scanning methods including Neural Cleanse (NC) \cite{NC}, ABS \cite{ABS}, PT-RED \cite{PTRED}, TABOR \cite{TABOR}, K-Arm \cite{kARM}, MM-BD \cite{MMBD}, and UMD \cite{umd}. Performance details are in Table \ref{table:main}, with further information in Appendix Section \ref{app:baselines} and \ref{sec:base_eval_bench}.


\textbf{Implementation Details.} \ \
\label{implementation_details} As stated earlier, we used Tiny ImageNet as our validation set to tune our hyperparameters $\epsilon$ and $\tau$ (scanning threshold); details are provided in Table \ref{table:epta}. We used PGD-10 as the adversarial attack. Our experiments on our method and other baselines were conducted on a single RTX 3090 GPU.


\textbf{Our Designed Benchmark.} \ \
We developed a benchmark to model real-world scanning scenarios, including various datasets, classifiers, trojan attacks, and label mappings. This benchmark covers both standard and adversarial training methods, ensuring a comprehensive evaluation of scanning methods. Our benchmark includes image datasets from CIFAR10, CIFAR100 \cite{cifar}, GTSRB \cite{gtsrb}, PubFig \cite{pubfig}, and MNIST, with two label mappings: All to One and All to All. It incorporates eight trojan attacks: BadNet \cite{badnets}, Input-aware \cite{inputaware}, BPP \cite{bpp}, SIG \cite{sig}, WaNet \cite{wanet}, Color \cite{color}, SSBA \cite{ssba} and Blended \cite{blended}. Each combination of a dataset and label mapping has 320 models: 20 trojaned models per attack and 160 clean models (check Appendix Section \ref{appendix:ModelsDatasetCreationDetails} for more details). Both standard and adversarial training were employed. We considered various architectures, including ResNet18 \cite{resnet}, PreActResNet18 \cite{preact}, and ViT-B/16 \cite{vit}. While previous works focused on CNN-based architectures, our experiments are more general. Table \ref{table:main} presents the evaluation of ResNet18; evaluations of other architectures are in Appendix Section \ref{app:more_results}, with more details on our benchmark creation in Appendix Section \ref{prop_bench}.

\textbf{TrojAI Benchmark.} \ \ The TrojAI \cite{trojai} benchmark, developed by IARPA, addresses backdoor detection challenges and includes test, hold-out, and training sets with nearly half of the models being trojaned. These models may have various backdoor triggers, such as pixel patterns and filters, activated under specific conditions. More details are in Appendix Section \ref{troj+Od_bench}.

\textbf{Analysis of the Results.} \ \ As the results indicate, presented in Tables \ref{table:main} and \ref{tab:eval2}, TRODO surpasses previous scanning methods by a large margin in terms of accuracy and time. Specifically, TRODO achieves superior performance with an \textbf{11.4\%} improvement in scenarios where trojan classifiers have been trained in a standard (non-adversarial) setting and a \textbf{24.8\%} improvement in scenarios where trojan classifiers have been adversarially trained. Our method demonstrates superior performance in both All-to-One and All-to-All scenarios, highlighting the generality of our proposed method. Notably, TRODO-Zero, which operates without access to any training samples, preserves significant performance compared to other methods, with only a minor drop in performance compared to TRODO. The same trend holds on TrojAI, a well-known and challenging benchmark. Regarding scanning time, as shown in Table \ref{tab:eval2}, TRODO demonstrates high computational efficiency, achieving competitive accuracy with significantly lower scanning time compared to other methods. This is mainly due to the simple yet effective signature it uses to scan for trojans. Further experimental results, including error bars, qualitative visualizations, and the limitations of our work, can be found in the Appendix Section \ref{app:more_results}.


\textbf{Adaptive Attack.} \ \ 
In our analysis of Adaptive Attacks on TRODO, we define two strong approaches aimed at circumventing the model’s defense mechanism. The first adaptive strategy trains a classifier with a custom loss function designed to equalize the confidence level (ID-Score) for both in-distribution (ID) and out-of-distribution (OOD) samples. This loss function, defined as
{ \small
\[
L_{\text{adaptive1}} = \mathbb{E}_{(x,y) \sim D_{\text{in}}} \left[ -\log f_y(x) \right] - \lambda_1 \mathbb{E}_{(z,y) \sim D_{\text{out}}} \left[ H(U; f(z)) \right] + \lambda_2 \mathbb{E}_{(x,y) \sim D_{\text{in}}} \left[ H(U; f(x)) \right]
\]

}

where \( x, y \) are data samples and their labels, \( f_y(x) \) denotes the \( y \)-th output of the classifier, \( U \) is the uniform distribution over classes, and \( H \) is the cross-entropy. The first term is the classification term (cross-entropy), while the other terms force the classifier to decrease MSP (ID-Score) for ID samples while increasing it for OOD samples. Setting $\lambda_1 = \lambda_2 = 0.5$, inspired by \cite{hendrycks2019oe}, balances the importance of the first term. By this loss function, we hope the ID-Score for both OOD and ID samples will be altered, though the classifier's decisions remain fixed.

Additionally, we introduce a second loss function targeting TRODO’s detection signature by reducing the ID-Score gap between benign and perturbed OOD samples, making it challenging for TRODO to distinguish trojaned classifiers from clean ones. This second loss function is defined as
{\small
\[
L_{\text{adaptive2}} = \mathbb{E}_{(x,y) \sim D_{\text{in}}} \left[ -\log f_y(x) \right] - \lambda_3 \mathbb{E}_{(z,y) \sim D_{\text{out}}} \left[ H(f(x); f(x^*)) \right]
\]
}

where \( x^* \) denotes the adversarially perturbed sample. Although these attacks attempt to subvert our defense, TRODO’s use of random transformations in creating OOD samples provides resilience, as these transformations hinder the model’s ability to learn patterns that could be exploited by an adaptive adversary.


\input{Tables/adaptive}

 % \textbf{Results} \ Due to the poor performance of meta-classifier-based methods on earlier rounds, as stated in \cite{kARM}, we only evaluate methods that do not require a training set of models. Hence, we use the training sets of the rounds as the evaluation sets, inspired by \cite{kARM, odyssey}.


% We report results in terms of Accuracy in Table \ref{table:main}. Note that these results are for Resnet18 \cite{resnet} architecture. Additional results for PreActRenset18 \cite{preact} and ViTb-16 \cite{vit} are provided in Appendix Section \ref{app:more_results}.
% \textbf{Attack Scenarios} \ \ \bd{
% We constructed a comprehensive set comprising 320 models per dataset for each architecture. This collection included 160 clean models and 160 Trojaned models, 20 for each of the 8 Trojan attacks. 
% Our Trojan attacks included BadNet \cite{badnets}; Input-aware \cite{inputaware}; BPP \cite{bpp}; SIG \cite{sig}; WaNet \cite{wanet}; Color \cite{color}; SSBA \cite{ssba}; and Blended \cite{blended}. For detailed descriptions of each attack, please refer to Appendix Section\ref{app:backdoor-attacks}. All models were trained using the Backdoor Bench \cite{wu2022backdoorbench}, which facilitates consistent and reproducible backdoor insertion tailored to each specific attack type. For the Color attack \cite{color}, however, we utilized the original repository provided by the authors to train the models. Details for training are provided in Appendix Section\ref{app:training-procedures}.}