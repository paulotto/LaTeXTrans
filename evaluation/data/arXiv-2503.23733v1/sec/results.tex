\section{Results}
\label{sec:results}
% \input{floats/qwen-table}
% \input{floats/llava2cog}
% \input{floats/qwen0.05}
\input{floats/emb_em}
\input{floats/qwen-0.1}
\input{floats/reverse}

As described in Section~\ref{models}, the main results on distinct vision-language benchmarks are conducted with two representative MLLM pairs. Specifically, Table~\ref{tab:llava2qwen} shows the results of merging LLaVA-OneVision-7B's parameters into Qwen2-VL-7B's parameters and architecture, and Table~\ref{tab:cogvlm} shows the results of merging LLaVA-v1.5-7B's parameters into CogVLM-chat-7B's parameters and architecture. Results of other model pairs and larger models can be found in Appendix~\ref{appendix:additional}.

\textbf{\ours  addresses the challenges of merging for heterogeneous MLLMs and outperforms strong baselines.} As demonstrated in Table~\ref{tab:llava2qwen} and Table~\ref{tab:cogvlm}, our proposed \ours model merging method achieves the highest cumulative performance scores across both MLLM pairs, indicating its effectiveness in merging heterogeneous MLLMs. Ranks among the top two performs in 8 out of 9 metrics in Table~\ref{tab:llava2qwen} and 7 metrics in Table~\ref{tab:cogvlm}, demonstrating its consistent ability to adaptively improve performance across most tasks. 
Moreover, our method stands out as the only approach where the merged model significantly outperforms both pre-merged models, achieving an average gain of +3.36~(total gain of +26.84) over Qwen2-VL and +3.90 over CogVLM across 8 tasks.
Given that most baseline methods employ supervised search techniques that incorporate additional information, our unsupervised search approach demonstrates exceptional performance on vision-language benchmarks, as detailed illustrated in Appendix~\ref{appendix:unsupervised}. 
%结合数字，列出来特殊性，有信息量的。
%比较两个表

\textbf{The proposed unsupervised hyper-parameter selection method is able to select a near-optimal $\alpha$.} We evaluate the performance across different coefficient values $\alpha$, comparing the results obtained through our unsupervised hyper-parameter selection method against those achieved with the optimal $\alpha$ chosen by the actual best results, which serves as the theoretical upper bound. As shown in Table \ref{tab:qwen0.1}, our method consistently performs remarkably close to this upper bound, with a maximum deviation of only 0.5 points. These results demonstrate the capability of our method to accurately identify near-optimal $\alpha$ values, achieving performance levels approaching the theoretical best.

% As shown in Table \ref{tab:qwen0.1}, we compare the performance of $\alpha$ chosen by our unsupervised hyper-parameter selection method, and the performance of the optimal $alpha$ chosen by the Oracle, serving as the upper bound of our method. In these benchmarks, our method is at most 0.5 points below the upper bound. The results indicates that our method can accurately select the near-optimal $\alpha$, obtaining performances close to the best possible choice. 

Note that on the OCRBench and TextVQA benchmarks, all model merging methods, including \ours, show a performance drop compared to the original base model.
We hypothesize that this is due to the large performance gap between the two original models on these benchmarks.
Even though, \ours still outperforms most of the baselines, showing the robustness of our method on various scenarios.

% : while original models with similar performance tends to benefit from model merging, merging a worse model to the base model on certain tasks may lead to performance drop. 
% TODO: 这段也有点分析，先不写上

% \input{floats/qwen0.02}
%好像有点不连贯。 先说总体每个任务都不错，为什么。 然后baseline中metagpt表现还可以，因为他选了综合最优的alpha。然后是qwen达到了最优的效果，证明方法的优越性。最后是limit证明可口扩展性。
% Our primary objective is to adaptively integrate two MLLMs into a single MLLM with better performance on given tasks without labeled data. Unlike all baseline methods, which are task-agnostic and may underperform in scenarios with substantial discrepancies between candidate MLLMs, our approach tailors a specific MLLM to each task. Despite this customization, the additional costs are minimal and also save storage. Mms-merging requires only 100 unlabeled samples for the search phase, and we store only the hyperparameter coefficient, rather than the entire model. 

% Overall, our mms-merging consistently achieved optimal performance across all evaluated tasks. Metagpt \cite{metagpt} performed suboptimal and stable, which computed the ideal coefficient value for linear interpolation through analytical reasoning but ignored difference among tasks. Other baselines were unable to search for the optimal hyperparameter value due to the absence of label data, so we assigned empirical values for them.


%  As shown in Table \ref{tab:llava2qwen}, when Llava-one-vision-Qwen \cite{llava-onevison} was merged with Qwen(base) \cite{qwen2-vl}, our mms-merging achieved State-of-the-Art performance in 7B MLLMs on certain tasks and demonstrated the strongest overall capabilities. This proves our effectiveness and potentiality.
 
%  The final row displays the results obtained from sampling only 100 samples (10\%-0.5\% of original datasets). It is evident that the performance remains consistent with the full dataset.
 
%  % Our results revealed that for the OCRBench and TextVQA datasets, none of the merging methods outperformed the original MLLM. This trend was also observed in most other model fusions, leading us to suppose that the original models may have already overfitted to these two popular datasets.

  
% Overall, our approach outperform the baseline and original MLLM. Besides, our method exhibits considerable robustness, stability and scalability. 




