\section{Related Work}
\label{sec:bk}

\subsection{Model Merging}

Recent researches on model merging techniques have contributed to building more capable models by providing an efficient approach to combine abilities on various tasks from different models that requires less data and compute.
Some studies focus on merging homogeneous models with identical architecture, while others focus on tackle the challenge on heterogeneous models which have different architectures.
In merging homogeneous models, Task Arithmetic \cite{task-arithmetic} proposes the concept of task vectors, which subtracts fine-tuned weights from pre-train weights to obtain task-related weight difference as the object of merging. Ties-Merging \cite{ties} and DARE \cite{dare} further improve the performance by mitigating parameter interference during the merging process through parameter pruning and conflict resolving. MetaGPT \cite{metagpt} scales the task vectors with task-agnostic coefficients in closed-form by seperating data term and scaling coefficients in the optimization objective. Although these methods improves the performance of the merged models, they cannot be directly applied on models with architecture difference.
% Learnable-MAM \cite{sundar2024multimodalattentionmergingimproved}
In fusing heterogeneous models, DAMC \cite{modelcompose} employs parameter decoupling and adaptive adjustment to enhance model merging strategies for fusing modalities on MLLMs with different modality encoders, but this work still focus on merging identical language model architecture. To consolidate LLMs with different architectures, FuseLLM \cite{fusellm} and FuseChat \cite{fusechat} applies token alignment and model fusion strategies with knowledge distillation before continue training the model, but they need labeled data and computation resources for continue training.
In fact, the majority of previous works on model merging requires labeled data for validation search or supervised training \cite{task-arithmetic, ties, dare, modelcompose, fusellm, fusechat}. In this work, we eliminate the need of labeled data by leveraging our unsupervised hyper-parameter selection method, and enable model merging strategies to be applied on heterogeneous MLLMs with architecture differences.

%[TODO: previous ones?]. 
% Task Arithmetic \cite{task-arithmetic} proposes the concept of task vectors, which subtracts fine-tuned weights from pre-train weights to obtain task-related weight difference as the object of merging. To further improve the performance of the merged model, TIES-Merging \cite{ties} addresses parameter interference by dropping parameters with low magnitude and resolving sign conflicts, and DARE \cite{dare} applies random dropping and rescale to get rid of redundant parameters. MetaGPT \cite{metagpt} scales the task vectors with task-agnostic coefficients in closed-form by seperating data term and scaling coefficients in the optimization objective.
% With these model merging techniques, Sundar et. al combines knowledge to improve model performance on speech recognition tasks, DAMC \cite{modelcompose} additionally employs parameter decoupling and adaptive adjustment strategies to integrate multiple modalities. To consolidate LLMs with different architectures, FuseLLM \cite{fusellm} and FuseChat \cite{fusechat} applies token alignment and model fusion strategies with knowledge distillation before continue training the model. Our work follows the previous path of model merging techniques, and enables model merging to be applied on heterogeneous MLLMs with a superior performance.

\subsection{Multimodal Large Language Models} \label{mllms}

As large language models demonstrate huge success in obtaining great abilities in general, recent researches on MLLMs have successfully appending multimodal processing and generation ability on LLMs, especially on the vision modality \cite{llava1.5, sharegpt4v, mplugowl2, cogvlm, qwen2-vl, llava-onevison}. However, these models often adapts unique modifications on language model architecture, resulting in a set of heterogeneous MLLMs, which prevent model merging methods to be applied on them. Specifically, there are two levels of architecture differences among MLLMs. 
First, two MLLMs may be designed from different pre-trained language model. For example, Qwen2-VL \cite{qwen2-vl} and LLaVA-OneVision-Qwen \cite{llava-onevison} are designed from Qwen2 \cite{qwen2}, while LLaVA \cite{llava1.5}, mPLUG-Owl2 \cite{mplugowl2}, CogVLM \cite{cogvlm} and ShareGPT4V \cite{sharegpt4v} are designed following the LLaMA \cite{llama} architecture. 
Second, two MLLMs developed from the same pre-trained language model can still be heterogeneous because they are designed with different modifications on the language model. For example, although CogVLM and mPLUG-Owl2 are both developed from LLaMA architecture, CogVLM adapts visual experts by duplicating query, key and value weights in attention head, while mPLUG-Owl2 is designed to duplicate key, value, and layer norm weights instead. 
The first level of differences is hard to merge, since model merging applies to parameters that are trained from the same pre-training weights \cite{task-arithmetic}. In this work, we tackle the second level of architecture differences via our proposed \ours method.


% LLaVA-v1.5 \cite{llava1.5} connects vision encoder and LLM with an MLP projection, and the end-to-end MLLM is trained on massive multimodal datasets. Following this design, mPLUG-Owl2 \cite{mplugowl2} introduces a modality-adaptive module for self-attention block to improve the modality collaboration between vision and language. CogVLM \cite{cogvlm} adds a trainable visual expert to the language model in each layer to enable deep visual-language feature alignment. Qwen2-VL \cite{qwen2-vl} enables the model to dynamically process images of varying resolutions into different numbers of visual tokens through the Naive Dynamic Resolution mechanism. ShareGPT4V \cite{sharegpt4v} and LLaVA-OneVision \cite{llava-onevison} also push the frontier of vision MLLMs with better training data and vision encoders. Although there are many open-source MLLMs with powerful language and vision capabilities, combining their abilities through model merging technique faces great challenges for their inherent heterogeneous property due to their differences in language model architecture and vision encoder choice. Our proposed model merging technique alleviates these challenges and produces MLLMs with performance improvements on various vision-language tasks.