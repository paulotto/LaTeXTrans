\section{Conclusion}
\label{sec:conclusion}

In this work, we propose a novel model merging method \ours to address the challenges in merging heterogeneous MLLMs. We first connect the parameters of different MLLMs through a mapping function, enabling merging operations. We then apply linear interpolation to the mapped model weights to adaptively optimize performance across tasks. To optimize the interpolation coefficient without labeled data, we introduce an unsupervised hyperparameter searching method based on our discovery in the parameter space: model performance can be estimated through the generation consistency. We demonstrate that 100 data samples are enough to search for near-optimal coefficients effectively.
Extensive experimental results show that \ours outperforms existing model merging methods for MLLMs and successfully addresses the challenges in merging heterogeneous MLLMs. We hope that our work mitigates the limitations of heterogeneous model merging methods and provides valuable insights for future research on unsupervised performance estimation and optimization.
% in parameter space through our hyperparameter selection method.

% tackle the challenges in model merging for heterogeneous MLLMs by proposing a novel model merging method named \ours. In \ours, MLLMs with different language model architectures are first connected with a mapping function, allowing merging operations to be applied. Then we apply linear interpolation on the mapped model weights to adaptively optimize the performance on each task. To search for the linear interpolation coefficient without the need of labeled data, we proposed a unsupervised hyper-parameter selection method, which is based on our discovery on the parameter space that the model performance can be estimated through the consistency of generated responses. We demonstrated that it only needs a small amount of responses to search for the near-optimal coefficient in an efficient way. Extensive results shows that \ours outperforms previous model merging methods on MLLMs, tackles the challenges in merging heterogeneous MLLMs. We hope our work can unlock the constraints on model merging methods for heterogeneous models, and provide insights to future research on unsupervised performance estimation on parameter space through our hyper-parameter selection method.