\clearpage
\appendix

% AH-135143

\section{Results on Additional Model Pairs}
\label{appendix:additional}

We conducted experiments on additional model pairs, summarized in Table~\ref{tab:avg6}, which highlights the cumulative performance gains across tasks for six different model pairs. The model pairs include: (1) merging LLaVA-OneVision \cite{llava-onevison} into Qwen2-VL \cite{qwen2-vl} (Table~\ref{tab:llava2qwen}), (2) merging LLaVA-v1.5 \cite{llava1.5} into CogVLM \cite{cogvlm} (Table~\ref{tab:cogvlm}), (3) merging mPLUG-Owl2 into LLaVA-v1.5, (4) merging LLaVA-v1.5 into mPLUG-Owl2 \cite{mplugowl2} (Table~\ref{tab:llava2mplug}), (5) merging CogVLM into mPLUG-Owl2 (Table~\ref{tab:cog2mplug}) and (6) merging mPLUG-Owl2 into CogVLM (Table~\ref{tab:mplug2cog}).
% \ours demonstrates strong effectiveness by achieving the highest overall performance gain and consistently ranking among the top two for improvements on all tasks. 
The performance gain for each task is computed as the difference between the performance of our method (or baselines) and the average performance of the two original models, with positive values indicating an improvement.
In Table~\ref{tab:avg6}, the SUM column presents the total performance gains across all tasks, where \ours outperforms all baselines, achieving +91.92 performance gain, and consistently ranks among the top two in performance gains across all benchmarks.
It is noteworthy that on GQA \cite{gqa} and VizWiz \cite{vizwiz} benchmarks in Table~\ref{tab:llava2mplug} and Table~\ref{tab:cog2mplug}, all model merging methods experience a performance drop. We attribute this decline to the significant performance gap between the original models on these benchmarks. In these scenarios, \ours demonstrates the smallest performance decrease among them.
In Table~\ref{tab:llava2mplug}, Table~\ref{tab:cog2mplug} and Table~\ref{tab:mplug2cog}, \ours obtains the second best result in the sum of all benchmarks, with a small gap compared to the best baseline. 

To investigate the effect of altering base models on performances, we analyze experiments on merging the same model pair with different base models. For the model pair of mPLUG-OWl2 and CogVLM, results in Table~\ref{tab:cog2mplug} use mPLUG-Owl2 as the base model, and results in Table~\ref{tab:mplug2cog} use CogVLM as the base model.
On benchmarks where the original models exhibit a significant performance gap, such as OCRBench \cite{ocrbench} and TextVQA \cite{textvqa}, model merging methods, including \ours, achieve only marginal performance improvements. In contrast, on benchmarks where the original models have comparable performance, \ours consistently enhances the base model’s performance (with the exception of GQA \cite{gqa} for the mPLUG-Owl2 architecture), irrespective of the choice of base model.
Notably, even when merging a weaker model into a stronger one for a specific task, \ours can sometimes boost the stronger model's performance. For instance, this effect is observed on SEEDBench \cite{seedbench}, OKVQA \cite{okvqa}, and GQA \cite{gqa} in Table~\ref{tab:mplug2cog}. These results highlight that our model merging technique can further optimize the performance of a strong model, even when another model demonstrates weaker performance on the same task.

Additionally, to demonstrate the effectiveness of our method on larger models, we conducted experiments on Cambrian and Yi-VL with 34B language model size. Table~\ref{tab:larger} shows that \textbf{\ours also merges the abilities in larger MLLMs effectively}.

\begin{table}[!ht]
    \centering\small
    \begin{tabular}{lll}
    \hline
        Model & $\mathrm{OCRBench}$ & $\mathrm{MME}$  \\ \hline
        Cambrian\footnotesize(base) & 58.70 & 72.50 \\ 
        Yi-VL & 29.70 & 73.65\\ 
        AVG & 44.20 & 73.08  \\        
        AdaMMS & \textbf{59.20} & \textbf{74.07}  \\ \hline
    \end{tabular}
    \caption{Results on merging Yi-VL into Cambrian.}
    \label{tab:larger}
\end{table}


\section{Implementation Details of \ours}
\label{appendix:impl}

The implementation details of \ours are as follows:

\noindent\textbf{Mapping} \  In this step, we identify parameters in the language models that account for additional weights. For CogVLM \cite{cogvlm}, all weights within the visual experts in the attention mechanism (including the QKV matrix and the FFN of the visual expert) are treated as additional weights. For mPLUG-Owl2 \cite{mplugowl2}, vision representation weights within the Modality-Adaptive Modules (such as the decoupled vision layer-norm and KV matrix) are considered additional weights. For different vision encoders, the vision encoder weights of the base model are retained as the final weights after merging, regardless of the vision encoder in the other model.

\noindent\textbf{Merging} \  During this step, we first merge the weights in the language model of the base model. If the weights are not classified as additional weights in the Mapping step, they are merged using linear interpolation or other baseline merging techniques. For weights categorized as additional weights, we check whether the other model has duplicated the same weights. Based on this, we (1) merge the weights if duplicates exist, or (2) retain the original weights in the base model if no duplicates are found.

\noindent\textbf{Searching} \  In the final step, we randomly select a subset of 100 test inputs to determine the optimal $\alpha$. For each $\alpha$ candidate, we generate model responses for the selected inputs. To select the best $\alpha$, we apply the Exact Match metric for the total difference score: for each input, if the merged model's response with a given $\alpha$ matches the response with adjacent $\alpha$ values, the difference score is 0; otherwise, it is 1. The total difference score is the sum of scores across all inputs in the subset. The $\alpha$ with the lowest total difference score is selected as the final choice. Note that the small subset of 100 inputs is randomly sampled using the method in LMMs-Eval framework \cite{lmms-eval}. We have repeated the sampling process to ensure that the randomness in sampling does not affect the performance of our method.

\section{Evaluation Details}
\label{appendix:eval}
% frameworks and prompts

We utilize LMMs-Eval \cite{lmms-eval} and VLMEvalKit \cite{vlmevalkit}, two open-source evaluation frameworks for MLLMs, to assess our models. Specifically, for evaluating MMMU \cite{mmmu}, MME \cite{mme}, SEEDBench \cite{seedbench}, OCRBench \cite{ocrbench}, and TextVQA \cite{textvqa} within the Qwen2-VL \cite{qwen2-vl} architecture, we use the VLMEvalKit framework, while LMMs-Eval is employed for the others. To ensure consistency with the reported results for LLaVA and mPLUG-Owl2 on OK-VQA \cite{okvqa}, we adapted the prompt template in the evaluation framework, as detailed in Table~\ref{tab:prompt}. Other prompt templates remains the same in the evaluation frameworks.

\section{Comparing Supervised and Unsupervised}
\label{appendix:unsupervised}

We compared \ours with baseline merging methods with supervised hyper-parameter selection. Due to the absence of separate test sets, we trained the supervised baseline on either a subset or the entirety of the evaluation set. \textbf{This implies that the supervised baseline was in a more favorable position compared to our method, as our method does not have access to the groudtruth labels.} Table~\ref{tab:supervised} shows that \ours still outperforms it, indicating the superiority of our unsupervised method.

% \section{Results on Merging Language Models}
% \label{appendix:llm}

% We conduct experiments on merging Qwen2.5-Math and Qwen2.5-Coder, which is evaluated on GSM8K and HumanEval benchmarks. Result in Table~\ref{tab:llm} shows that only \ours is still effective in merging language models. Baselines use the same hyper-parameters as the main experiments but perform poorly, highlighting the heightened sensitivity of LLM merging to hyper-parameter tuning.

\section{Intermediate Results in Searching}
\label{appendix:searching}
%diff和acc的趋势图

%alpha具体的表格
We present an example of the intermediate results during the selection of $\alpha$. As shown in Figure~\ref{tab:inter_alpha}, \ours effectively identifies a near-optimal $\alpha$, achieving performance close to the best possible outcome. Specifically, our unsupervised hyper-parameter selection method successfully chooses the optimal $\alpha$ candidate in half of the benchmarks and maintains a deviation of no more than 0.2 from the best $\alpha$ in the remaining cases.

Figure~\ref{fig:consistency_acc4} illustrates the relationship between model performance and generation consistency across MMMU, MME, SeedBench, and OCRBench when merging LLaVA-OneVision into Qwen2-VL. The observed trends validate our approach in the search step, where model performance is approximated using generation consistency without relying on labeled data. Notably, for these tasks, the $\alpha$ selected by our method corresponding to the highest generation consistency deviates from the $\alpha$ achieving the best performance by no more than 0.1, showing that our hyper-parameter selection method achieves near-optimal performance.

\begin{table}[h]
   
     \resizebox{0.478\textwidth}{!}{%
    \begin{tabular}{ccc}
    \hline
        Framework & Base Model & Prompt   \\ \hline
        \multirow{2}{*}{LMMs-Eval} & LLaVA &  \multirow{2}{*}{Answer the question using a single word or phrase.} \\
         & mPLUG-Owl2 &  \\ \hline
          
    \end{tabular}
    }
    \caption{Altered prompt for evaluation on OK-VQA.}
    \label{tab:prompt}
\end{table}

\section{Supplementary Proof}
\label{appendix:proof}

We provide the following proof as the theoretical justification for relationship between generation consistency correlates and model performance.

\noindent\textit{Proof.} Using the notation in Section~\ref{searching}, for an arbitrary task $t_i$, let $S_{t_i}(\alpha)$ be the ratio of correct answer at position $\alpha$, and $D_{t_i}(\alpha; \alpha^-)$ be the ratio of the difference in generated responses between position $\alpha$ and its adjacent candidate $\alpha^-$.
Since the difference in $S_{t_i}(\alpha)$ is only influenced  by the subset of generated responses where the correctness status changes (i.e., transitions between correct and incorrect), we have $|S_{t_i}(\alpha)-S_{t_i}(\alpha^-)| \leq D_{t_i}(\alpha; \alpha^-)$. For the same reason with $\alpha^+$, we can prove  $|S_{t_i}(\alpha)-S_{t_i}(\alpha^-)| + |S_{t_i}(\alpha)-S_{t_i}(\alpha^+)| \leq 2D_{t_i}(\alpha; \alpha^-, \alpha^+)$.
Therefore, a higher generation consistency with small $D_{t_i}(\alpha; \alpha^-, \alpha^+)$ implies a higher model performance $S_{t_i}(\alpha)$, due to its convexity.

\section{Experimental Results in Granularity for \texorpdfstring{$\alpha$}{alpha}}
\label{appendix:granularity}

Figure~\ref{fig:step_size} presents the result of \ours at different granularities of $\alpha$. The point in stars indicates the best $\alpha$ by our unsupervised parameter selection method. The result shows that these granualities in $\{0.02, 0.05, 0.10\}$ behave similarly in terms of the final performance, indicating the robustness of \ours. Therefore, in practice we choose a larger $\alpha$ so that we have fewer $\alpha$ candidates, which reduces the computation cost.

% \twocolumn[{%
%     \renewcommand\twocolumn[1][]{#1} % 临时打破双栏布局
\begin{table*}[t]
    \centering
    
     \resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllllll}
    \hline
        Model & $\mathrm{MMMU_{val}}$ & $\mathrm{MME_{sum}}$ & $\mathrm{SeedBench_{all}}$ & $\mathrm{OCRBench}$ & $\mathrm{TextVQA_{val}}$ & $\mathrm{OKVQA}$ & $\mathrm{GQA}$ & $\mathrm{VizWiz_{val}}$ & $\mathrm{Sum}$ & $\mathrm{Diff}$  \\ \hline
        AdaMMS & 34.90  & 69.09  & 64.12  & 55.70  & 76.90  & 61.11  & 60.12  & 37.27  & 459.21  & +31.23 \\ 
        Ties-Merging  & 34.00  & 57.29  & 38.97  & 55.00  & 59.73  & 40.31  & 51.97  & 24.36  & 361.63  & -66.35 \\ 
        Ties-Merging (supervised with 100 eval. samples) & 37.20  & 57.29  & 63.12  & 55.90  & 76.50  & 61.45  & 55.81  & 37.98  & 445.25  & +17.27 \\ 
        Ties-Merging (supervised with all eval. data) & 37.20  & 63.96  & 65.43  & 55.90  & 76.55  & 61.45  & 57.99  & 38.21  & 456.69  & +28.71 \\ 
        \hline
        
\end{tabular}
    }
    
    % \vspace{0.05cm}
    
    \captionof{table}{ \ours and Ties-Merging with \textit{supervised} hyper-parameter selection via validation set.}

    % \vspace{0.05cm}
    
    \label{tab:supervised}
    % \centering
    % \resizebox{\textwidth}{!}{
    % \begin{tabular}{lccccccccc}
    %     \hline
    %     Model        & Qwen2.5-Math(base) & Qwen2.5-Coder & AVG & Task Arithmetic & Ties-Merging & DARE-Linear & DARE-Ties & MetaGPT & AdaMMS \\ \hline
    %     GSM8K        & 84.23        & 69.45   & 76.84  & 40.94  & 0.00     & 0.00      & 0.00    & 54.81  & 80.21  \\ 
    %     HumanEval    & 32.93     & 80.49    & 56.71  & 0.00      & 0.00    & 0.00     & 0.00   & 4.88     & 40.85  \\ \hline
    % \end{tabular}}

    % \vspace{0.05cm}
    
    % \captionof{table}{ \footnotesize  Results on merging Qwen2.5-Coder with Qwen2.5-Math.}
    % \vspace{0.15cm}
    % \label{tab:llm}

% }]
\end{table*}

\begin{figure*}
    \centering
   
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth, bb=0 0 720 432]{figure/mme_values_plot_new.pdf}
        % \caption{图2的说明}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth, bb=0 0 720 432]{figure/ocr_values_plot_new.pdf}
        % \caption{caption}
    \end{subfigure}
    \caption{Results on linear interpolation at different granularities of $\alpha$ when merging LLaVA-OneVison-7B into Qwen2-VL-7B-7B. (Left: MME, Right: OCRBench)}
    \label{fig:step_size}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth, bb=0 0 1007 576]{figure/consistency_acc4.pdf}
    \caption{Generation consistency and model performance (score) for MME, MMMU, OCRBench and SeedBench when merging LLaVA-OneVision-7B into Qwen2-VL-7B. Generation consistency is calculated as the reciprocal of the sum of different responses from models with adjacent $\alpha$ candidates. The horizontal axis is the $\alpha$ of the linear interpolation.}
    \label{fig:consistency_acc4}
\end{figure*}

% \section{Qualitative Results}
% \label{appendix:example}

% Figure X shows the qualitative results of \ours with examples on Y benchmark.


% \input{floats/avg5_gain}
\input{floats/avg6_gain}
\input{floats/mplug2llava}
\input{floats/llava2mplugowi}
\input{floats/cog2mplug}
\input{floats/mplug2cog}
\input{floats/llava2qwen-alpha}
% \input{floats/llava2cog-alpha}