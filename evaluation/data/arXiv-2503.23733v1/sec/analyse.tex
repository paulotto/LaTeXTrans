\section{Analysis}
\label{sec:ana}
\begin{figure}
    \centering
    \includegraphics[width=0.4 \textwidth, bb=0 0 461 346]{figure/diff_acc_plot_new.pdf}
    \caption{Results on merging LLaVA-v1.5-7B into Qwen2-VL-7B. The $\alpha$ with the best perfo, bb=0 0 461 346rmance are the same as the $\alpha$ with the fewest response differences.}
    \label{fig:difference}
\end{figure}

%同构模型融合，以及searching方法的有效性。
\subsection{Different Factors for Calculating Generation Consistency}
We conducted analytical experiments on our generation consistency calculation methods, focusing on two key factors: the choice between using a 100-sample subset versus the complete dataset, and the selection of evaluation metrics. In the searching step of our method, we employed an exact match metric to calculate $\operatorname*{DiffCnt}$ in Algorithm~\ref{alg:composition}, which serves as our generation consistency indicator for model performance prediction. Given that exact match is a binary, rigorous evaluation metric, we explored an alternative, more flexible approach to measure generation consistency. Specifically, we computed the cosine similarity between sentence embeddings generated by all-MiniLM-L6-v2 \cite{sentence-bert}, which was used to calculate $\operatorname*{DiffCnt}$. The analysis results are presented in Table~\ref{tab:embedding}. Although embeddings theoretically offer more fine-grained semantic representations, our results demonstrate that the embedding-based metric performs comparably to the exact match metric. Furthermore, our experiments confirm that sampling 100 instances achieves results nearly equivalent to the complete dataset.


% In the searching step of \ours, we employed exact match metric to calculate $\operatorname*{DiffCnt}$ in Algorithm~\ref{alg:composition}, which is the the indicator of generation consistency that is used to predict the model performance.
% Since exact match is a binary, hard-judgment metric, we investigate an alternative, softer metric to capture generation consistency. Specifically, we compute the cosine similarity between sentence embeddings generated by all-MiniLM-L6-v2 \cite{sentence-bert}, which we use to calculate $\operatorname*{DiffCnt}$.
% % Given that the model often produces responses in sentence format, we incorporated embedding method to assess the consistency of responses, aiming to compare the effectiveness of these two evaluation approaches. 
% While embeddings theoretically offer a fine-grained semantic representation, the results depicted in Table \ref{tab:embedding} reveal that the embedding metric performs similarly with the exact match metric.
% % This disparity can be attributed to the fact that the consistency evaluated by the embedding metric does not correspond to alterations in the models' hidden states.
% % The initial assumption was that the responses from both models would identical, indicating similarity in their hidden states and loss, thereby illustrating a smoother model within the vicinity of that alpha value in the linear interpolation space.
% This indicates that our choice in the main experiments of the exact match metric is sufficient for demonstrating the generation consistency for performance estimation.


\subsection{Merging with Large Performance Gap}

As discussed in Section~\ref{sec:results}, all model merging methods experience performance drop after the merging on two benchmarks, OCRBench and TextVQA. It shows that merging a model with significant \textit{lower} performance into the base model will decrease the performance on the task. Conversely, in Table~\ref{tab:reverse}, the merging from Qwen2-VL-7B to LLaVA-OneVision-7B shows that merging a model with significant \textit{higher} performance into the base model will not necessarily improve the model performance. And in this case, \ours is the only model merging method that resists the performance drop after merging.
In general, we observed that original models with similar performance tends to benefit from model merging, while original models with large performance gap do not.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, bb=0 0 678 389]{figure/crop_diff-case.pdf}
    \caption{Model responses with the change of $\alpha$ in linear interpolation. Similar colors indicate similar responses.  }
    \label{fig:asymmetry}
\end{figure}

% The training data for different MLLMs varies, leading to performance differences across tasks. Model performance tends to be more stable when merging on tasks with smaller differences. Conversely, merging on tasks with large gap tends to unstable performance. Our mms-merging method exhibits strong stability, preventing models with poor performance from dragging down the overall results. As shown in Table \ref{tab:llava2qwen}  there is a 17\% gap between the two models for OCRBench \cite{ocrbench}. Only our mms-merging and MetaGPT \cite{metagpt} methods maintain the high performance of Qwen2\_VL \cite{Qwen2-VL-7B}.
%不同的MLLM训练数据不一样，这就导致了他们在任务上的表现有差异。在差异小的任务上进行融合时模型表现会比较稳定。而在差异大的任务进行融合则会表现动荡。 我们的方法具有很好的稳定性，可以避免表现差的模型拉低结果。可以看到表1中的OCRBENCH，两个模型相差17%，只有我们的mms-merging和metagpt保持住qwen2的好表现。
\subsection{Asymmetry in the Parameter Space of Heterogeneous Models} \label{sec:asymmetry_sec}
In Section~\ref{impl}, we discussed that merging with large $\alpha$ often results in collapsing language ability. To validate our choice of the subinterval $[0, 0.6]$ in determining candidates of $\alpha$, we demonstrate this phenomenon in Figure~\ref{fig:asymmetry}, which shows that the model generates consistently near the parameters of the base model with small $\alpha$, and collapses gradually with larger $\alpha$. We attribute the phenomenon to the asymmetry in the parameter space, as the two original models have unequal status that comes from the choice of base architecture.

\subsection{Selection of Granularity for \texorpdfstring{$\alpha$}{alpha}}
\label{sec:interval}
To validate our choice of the granularity in Section~\ref{impl}, we conducted additional experiments with various granularities of $\alpha$ candidates on n MME and OCRBench when merging LLaVA-OneVison-7B into Qwen2-VL-7B. As shown in Appendix~\ref{appendix:granularity}, the result shows that the difference of selected $\alpha$ and model performance do not change significantly with different granularities. This shows that our choice of granularity as 0.1 would result in comparable performance, with less computation cost.

% Generally speaking, there is a trade-off between the granularity of $\alpha$ and the results. The finer grained the alpha, the better the potential outcomes, but the higher the cost. In practice, we found that refining the granularity of $\alpha$  to 0.05 or 0.02 did not result in significant improvements. They are displayed in the Figure \ref{sec:interval}. Therefore selecting an interval of 0.1 for alpha in the main experiment was sufficient.





% \subsection{Homogeneous V.S. Heterogeneous Merging}
% % 我们的diff方法在同构模型中仍然有效。需要强调的是同构模型融合的所有结果增益都比较小，不如异构融合带来的增益。
% Both ShareGPT4V and LLaVA-1.5 include three integral components:(1)A vision encoder utilizing the CLIP-Large model.(2) A projector, two layer multi-layer perception (MLP),  to connect the vision and language modalities. (3) A LLM derived from LLaMA2.

% The merging method applied to these two homogeneous MLLMs is identical to that used on homogeneous LLMs. The results after merging remain stable, but the improvements are minimal. Therefore, we argue that the gains of merging heterogeneous MLLMs surpass homogeneous ones.
% \input{floats/llava2sharegpt}



% \begin{figure}
%   \centering 
%  \includegraphics[width=0.4\textwidth]{figure/mmmu_mme_ocr.pdf}
%   \caption{Results with alpha intervals of 0.1, 0.05, and 0.02 in linear interpolation.}
%   \label{intervals}
% \end{figure}



 % \input{floats/cog2mplug}
% \input{floats/llava2mplug}