
\begin{table}[ht]
    \centering
    % \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc}
        \toprule
        {Model} & $\mathrm{MMMU_{val}}$ &  $\mathrm{MME_{sum}}$ & $\mathrm{OCRBench}$   \\ 
        \midrule
        \(\alpha\)-0.00 & 50.11  & 81.44  & \textbf{86.00}  \\ 
       
        \(\alpha\)-0.10 & 50.56  & 81.46  & 85.50  \\ 
        
        \(\alpha\)-0.20 & 51.11  & 82.36  & 85.20  \\ 
       
    \(\alpha\)-0.30 & \textbf{51.22}  & \textbf{83.36}  & 84.40  \\ 
     
        \(\alpha\)-0.40 & 50.67  & 83.03  & 80.70  \\ 
     
        \(\alpha\)-0.50 & 50.00  & 81.37  & 76.40  \\ 
     
        \(\alpha\)-0.60 & 47.00  & 82.06  & 71.20 \\
          \midrule
    Oracle & 51.22\footnotesize(0.30) & 83.36\footnotesize(0.30) & 85.50\footnotesize(0.10) \\   
       \ours &   51.11\footnotesize(0.20)  & 83.36\footnotesize(0.30)  &  85.50\footnotesize(0.10) \\
     \bottomrule
    \end{tabular}%
    % }
    \caption{Results with $\alpha$ granularity of 0.1 when merging LLaVA-OneVision-7B into Qwen2-VL-7B. The values in parentheses indicate the selected $\alpha$. Oracle represents the best possible performance (upper bound) for each task, while AdaMMS shows the results achieved by our unsupervised selection method.}
    \label{tab:qwen0.1}
\end{table}