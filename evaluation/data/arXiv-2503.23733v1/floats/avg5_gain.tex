\begin{table*}[!ht]
    \centering
       \resizebox{\textwidth}{!}{%      
    \begin{tabular}{lcccccccccc}
    \toprule
        \textbf{Model} & $\mathrm{MMMU_{val}}$ &  $\mathrm{MME_{sum}}$ &  $\mathrm{SeedBench_{all}}$ & $\mathrm{OCRBench}$  &  $\mathrm{TextVQA_{val}}$  & $\mathrm{OKVQA}$ & $\mathrm{GQA}$  &  $\mathrm{VizWiz_{val}}$ & $\mathrm{SUM}$ & $\mathrm{Top2}$ \\ 
        \midrule
        
       Task Arithmetic  & \underline{12.21}  & \textbf{19.27}  & \textbf{13.06}  & 0.50  & 1.10  & \underline{14.29}  & \underline{-3.61}  & \underline{-20.60}  & \underline{36.28} &7  \\ 
       
         Ties-Merging  & -1.92  & -22.34  & -27.69  & 3.80  & -23.86  & -18.49  & -29.04  & -37.32  & -156.80 &0 \\ 
         DARE-Linear  & 7.15  & -4.61  & 9.14  & -10.30  & -14.49  & 5.55  & -13.30  & -33.81  & -54.61 &0 \\ 
         DARE-Ties  & -11.53  & -55.63  & -7.05  & -44.30  & -38.43  & -19.88  & -24.80  & -42.54  & -244.12 & 0 \\ 
         MetaGPT  & 1.14  & -5.81  & -5.51  & \textbf{17.40}  & \underline{5.16}  & -6.09  & -24.07  & -30.21  & -47.94 &2 \\ 
         \textbf{AdaMMS}  & \textbf{14.38 } & \underline{15.21}  & \underline{10.17}  & \underline{15.30}  & \textbf{22.52 } & \textbf{15.00}  & \textbf{-1.39}  & \textbf{-13.60}  & \textbf{77.64} &9 \\ \hline

    \end{tabular}
    }
    \caption{Results of the performance gain among all model pairs.}
    \label{tab:avg}
\end{table*}