\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
\centering
\resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figures/MIEB_backup.pdf}}
\caption{\textbf{Overview of MIEB task categories with examples.} See \autoref{tab:MIEB big tasks} for details about capabilities measured and other information.} 
\label{fig:mieb_tasks}
\end{figure*}

Image and text embeddings power a wide range of use cases, from search engines to recommendation systems~\citep{geng2015learning,pinterest,huang2020embedding}. However, evaluation protocols for image and multimodal embedding models vary widely, ranging from image-text retrieval, zero-shot classification \citep{radford2021learning,zhai2023sigmoid}, linear probing \citep{radford2021learning,oquab2024dinov2}, fine-tuning the models \cite{chen2020simclr,he2019moco}, and using MLLM performance as proxies \cite{tong2024cambrian}.  These divergent protocols reveal the lack of standardized criteria for assessing image representations.

We introduce the Massive Image Embedding Benchmark (MIEB) to provide a unified comprehensive evaluation protocol to spur the field's advancement toward universal image-text embedding models. We build on the standard for the evaluation of text embeddings, MTEB~\citep{muennighoff2023mteb}, extending its codebase and leaderboard for image and image-text embedding models. MIEB spans 130 tasks grouped into 8 task categories: Aligning with MTEB, we integrate \textbf{Clustering}, \textbf{Classification}, and \textbf{Retrieval}. Notably, we consider fine-grained aspects, such as \textit{interleaved retrieval}, \textit{multilingual retrieval}, \textit{instruction-aware retrieval}. We additionally include \textbf{Compositionality Evaluation} and \textbf{Vision Centric Question Answering}, respectively assessing nuanced information encoded in embeddings and their capabilities in solving vision-centric QA tasks. We focus on tasks that require strong \textit{visual understanding of texts}, for which we include \textbf{Visual STS}, the visual counterpart of semantic textual similarity in NLP, and \textbf{Document Understanding}, assessing the vision-only understanding of high-resolution documents with dense texts and complex layout, enabling evaluation that pushes forward the development of natural interleaved embeddings.

Our analysis across task categories shows that the performance of current image embedding models is fragmented, with no method dominating all task categories. We further study the predictability of the performance of visual encoders as part of Multimodal Large Language Models (MLLMs), via a large-scale correlation study. We find that the performance of vision encoders on MIEB strongly correlates with the performance of MLLMs that use the same vision encoder. For instance, the performance on our Visual STS tasks has over 99\% correlation with the performance of an MLLM leveraging the same vision encoder on tasks like OCRBench and TextVQA. This provides a practical way to select vision encoders for MLLMs based on MIEB results.