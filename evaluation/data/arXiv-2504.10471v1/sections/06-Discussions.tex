\section{Discussions}

\subsection{K-shot Linear Probing}
\label{subsec: k-shot}

We opt for k-shot linear probing instead of full-dataset linear probing as the default setting in MIEB (\autoref{sec:mieb}) to make the evaluation cheaper given the large size of the benchmark. In \autoref{fig: k-shot linear probe}, we ablate this design by training k-shot classifiers with k in \{8,16,32,64,128,256\}. We find that different values of k preserve the same model rank on both \textbf{fine-grained classification} (Birdsnap, Caltech101, CIFAR100, Country211, FGVCAircraft, Food101, Imagenet1k, OxfordFlowers, OxfordPets, RESISC45, StanfordCars, SUN397, UCF101) and \textbf{coarse-grained classification} (CIFAR10, DTD, EuroSAT, FER2013, GTSRB, MNIST, PatchCamelyon, STL10) tasks. As a result, we choose a modest 16-shot evaluation by default.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/correlation_small.pdf}
\caption{\textbf{Correlations between performance on generative MLLM benchmarks from \citet{tong2024cambrian} (y-axis) and our Visual STS (x-axis).} High correlation means that our Visual STS tasks can predict generative performance.}
\label{fig:correlation small}
\end{figure}

\subsection{On the predictability of MLLM performance}

MLLM evaluation has been proposed as a robust method to assess visual representations~\citep{tong2024cambrian}, where the performance of an MLLM provides information about the strength of its visual encoder. However, this evaluation paradigm is much more computationally intensive than benchmarking only the vision encoder, given the large sizes of MLLMs and the large hyperparameter search space (data size, LLM choice, instruction-tuning details, etc.). Thus, it remains impractical as a general benchmarking method.

We explore the opposite: Can MLLM performance be predicted from the vision encoder~\citep{yang2024law}? To do so, we calculate correlations between vision encoder performance on MIEB tasks and their MLLM counterparts across 16 benchmarks using results from \citet{tong2024cambrian}. \autoref{fig:correlation small} shows these correlations using our Visual STS protocol as an example~\citep{xiao2024pixel}. Given the common need for visual text interpretation in MLLM tasks, vision encodersâ€™ performance on Visual STS has a strong correlation with the performance of their MLLM counterparts. The pattern is most pronounced for the 4 OCR and Chart tasks in \cite{tong2024cambrian}, and least pronounced for CV-bench 3D, which relies little on visual text understanding. This highlights the utility of MIEB for selecting MLLM vision encoders.

\subsection{MIEB-lite: A lightweight Benchmark} 
\label{sec: MIEB-lite}

Computationally efficient benchmarks are more usable~\citep{enevoldsen2025mmtebmassivemultilingualtext}. While MIEB avoids training MLLMs, evaluating 130 tasks remains resource-intensive. While a more comprehensive coverage allows for more nuanced analysis, many tasks have high correlations (e.g., Visual STS in \autoref{fig:correlation small}). To enable lightweight evaluation, we build MIEB-lite by iteratively removing redundant tasks while preserving task category coverage and inter-task correlation.

We first compute pairwise task correlations using model performance, then iteratively remove tasks with average correlations above 0.5 (11 tasks) and 0.45 (32 tasks). Key patterns emerged: 1) Established tasks (e.g., CLIP benchmark linear probing~\citep{radford2021learning}) had high redundancy, possibly due to dataset exposure in pretraining; 2) Easy OCR tasks correlated unexpectedly with non-OCR tasks, though Visual STS and VIDORE remained distinct; 3) Novel tasks (e.g., ARO benchmark, M-BEIR protocols) had low correlations.

To capture nuanced task relationships, we cluster tasks via UMAP+HDBSCAN~\citep{mcinnes2018umap,mcinnes2017hdbscan} using correlation vectors, yielding 17 interpretable clusters (e.g., `fine-grained zero-shot', `language-centric', `easy OCR', `VQA', `low resolution tasks', etc). The outlier cluster (-1 label) spanned all categories, serving as a foundation for balanced selection.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Model Name}} &\textbf{$\#$ Params} &  \multicolumn{3}{c}{\textbf{Runtime (NVIDIA H100 GPU hours)}} \\
&\textbf{(M)} &  MIEB & MIEB-lite & Reduction $\%$ \\
\midrule
E5-V & 8360 & 264.0 & 46.4 & 82.4$\%$ $\downarrow$ \\
CLIP (base-patch32) & 151 & 16.6 & 4.5 & $72.9\%$ $\downarrow$ \\
\bottomrule
\end{tabular}}
\caption{\textbf{MIEB vs. MIEB-lite runtime comparison.}}
\label{tab: run time}
\end{table}

\textbf{MIEB-lite has 51 tasks} by combining the above two approaches and excluding large-scale tasks (e.g., EDIS and GLD-v2 take 60-80 GPU hours for 7B models). MIEB-lite reduces computation while maintaining category balance and diagnostic power: 1) \autoref{tab: run time} compares model runtime on MIEB and MIEB-lite showing a reduction of $82.4\%$ for E5-V, an 8B model. 2) We find that the overall average performance of 38 models on MIEB and MIEB-lite has a Spearman correlation of 0.992 and a Pearson correlation of 0.986. See Tables \ref{tab:datasets_Any2AnyRetrieval}, \ref{tab:datasets_ImageClassification}, and \ref{tab:datasets_ImageTextPairClassification} for all MIEB-lite tasks. 