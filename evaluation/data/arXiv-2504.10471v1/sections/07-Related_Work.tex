\section{Related Work}

\paragraph{Benchmarks} Prior efforts toward universal image embedding benchmarks focus on narrow scopes. The CLIP Benchmark~\citep{radford2021learning} evaluates semantic similarity via classification and retrieval, while UnED~\citep{ypsilantis2023towards} and M-BEIR~\citep{wei2023uniir} expand retrieval evaluation to multi-domain and mixed-modality settings. However, three critical gaps persist: \textbf{(1) Limited task diversity}: Existing benchmarks overlook tasks like multi-modal composition~\citep{yuksekgonul2023aro}, social media understanding~\citep{jin2024mm}, and multilingual evaluation~\citep{pmlr-v162-bugliarello22a}, restricting cross-domain insights. \textbf{(2) Neglect visual text tasks}: While understanding text in images is key to many MLLM use cases~\citep{faysse2024colpali}, benchmarks for OCR~\citep{liu2024ocrbench} and visual document retrieval remain sparse. \textbf{(3) Under-explored instruction tuning}: Though instruction-tuned embeddings show promise for generalization~\citep{lin2025mmembed,zhang2024gme}, their evaluation beyond retrieval is limited. MIEB addresses these gaps via unified protocols spanning 130 tasks, consolidating prior benchmarks into a holistic framework.

\paragraph{Protocol limitations} Prior work relies heavily on linear probing and retrieval~\citep{he2019moco,radford2021learning}, which struggle to assess generalization to complex tasks. While fine-tuning~\citep{chen2020simclr} adapts embeddings to specific tasks, it incurs high computational costs and risks overfitting. MIEB evaluates frozen embeddings through a broader suite of protocols including retrieval, linear probing, zero-shot classification, and novel additions like pair-wise classification and clustering, providing a more flexible and comprehensive assessment.