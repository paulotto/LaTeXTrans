\section{VAPO: Addressing the Challenges in Long-CoT RL}
% In this section, we present targeted methodologies to systematically address the three core challenges identified earlier. 

\subsection{Mitigating Value Model Bias over Long Sequences}
Building upon the analysis of value-model-based models presented in section \ref{challenge1}, we propose to use Value-Pretraining and decoupled-GAE to address the critical challenges in value model bias over long sequences. 
Both of these two techniques draw upon methodologies previously introduced in VC-PPO.

\textbf{Value-Pretraining} is proposed to mitigate the value initialization bias. Naively applying PPO to long-CoT tasks leads to failures such as collapsed output lengths and degraded performance. The reason is that the value model is initialized from the reward model while the reward model shares a mismatched objective with the value model. This phenomenon is first identified and addressed in VC-PPO \citep{vc-ppo}. In this paper, we follow the Value-Pretraining technique and the specific steps are outlined as follows:

\begin{enumerate}[leftmargin=*]
    \item Continuously generate responses by sampling from a fixed policy, for instance, $\pi_{\text{sft}}$, and update the value model with Monte-Carlo return.
    \item Train the value model until key training metrics, including value loss and explained variance \citep{explained_variance}, attain sufficiently low values.
    \item Save the value checkpoint and load this checkpoint for subsequent experiments. 
\end{enumerate}
% To enhance both exploration and exploitation in policy learning, we employ a higher clipping threshold for importance sampling and incorporate self-imitation learning to reinforce positive examples. Furthermore, in the final two subsections, we introduce token-level loss and prompt-level contrastive learning to improve learning efficiency.

\textbf{Decoupled-GAE} is proven effective in VC-PPO \citep{vc-ppo}. This technique decouples the advantage computation for the value and the policy. For value updates, it is recommended to compute the value-update target with $\lambda = 1.0$. This choice results in an unbiased gradient-descent optimization, effectively addressing the reward-decay issues in long CoT tasks.

However, for policy updates, using a smaller $\lambda$ is advisable to accelerate policy convergence under computational and time constraints. In VC-PPO, this is achieved by employing different coefficients in advantage computation: $\lambda_{\text{critic}} = 1.0$ and $\lambda_{\text{policy}} = 0.95$. In this paper, we adopt the core idea of decoupling GAE computation. 
% Moreover, we enhance this idea by introducing a length-adaptive $\lambda$ for the policy, which is illustrated in the following section.

\subsection{Managing Heterogeneous Sequence Lengths during Training}
To address the challenge of heterogeneous sequence lengths during training, we propose the \textbf{Length-Adaptive GAE}. This method dynamically adjusts the parameter in GAE according to the sequence length, enabling adaptive advantage estimation for sequences of varying lengths. Additionally, to enhance the training stability of mixed-length sequences, we replace the conventional sample-level policy gradient loss with a token-level policy gradient loss. The key technical details are elaborated as follows:

\textbf{Length-Adaptive GAE} is specifically proposed to to address the inconsistency in optimal $\lambda_{\text{policy}}$ values across sequences of varying lengths. In VC-PPO, $\lambda_{\text{policy}}$ is set to a constant value of $\lambda_{\text{policy}}=0.95$. However, when considering the GAE computation, for longer output sequences with lengths $l>100$, the coefficient of the TD-error corresponding to the reward is $0.95^{100}\approx0.006$, which is effectively zero. As a result, with a fixed $\lambda_{\text{policy}} = 0.95$, the GAE computation becomes dominated by potentially biased bootstrapping TD-errors. This approach may not be optimal for handling extremely long output sequences.

To address this shortcoming, we propose \textbf{Length-Adaptive GAE} for policy updates. Our method aims to ensure a more uniform distribution of TD-errors across both short and long sequences. We design the sum of the coefficients $\lambda_{\text{policy}}$ to be proportional to the output length $l$:
\begin{equation}
\sum_{t = 0}^{\infty}\lambda_{\text{policy}}^t\approx\frac{1}{1-\lambda_{\text{policy}}}=\alpha l,
\label{eq:variable_lam}
\end{equation}
where $\alpha$ is a hyper-parameter controlling the overall bias-variance trade-off. By solving Equation \ref{eq:variable_lam} for $\lambda_{\text{policy}}$, we derive a length-adaptive formula:
\begin{equation}
\lambda_{\text{policy}} = 1-\frac{1}{\alpha l}
\end{equation}

This length-adaptive approach to $\lambda_{\text{policy}}$ in GAE calculation allows for a more effective handling of sequences of varying lengths. 

\textbf{Token-Level Policy Gradient Loss}.
Following DAPO~\cite{dapo}, we have also modified the computation method of the policy gradient loss to adjust the loss weight allocation in long COT scenarios. Specifically, in previous implementations, the policy gradient loss was computed as follows:
\begin{align}
\mathcal{L}_{\text{PPO}}(\theta) =- \frac{1}{G} \sum_{i = 1}^G \frac{1}{|o_i|}\sum_{t = 1}^{|o_i|} \min \left(r_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1-\varepsilon, 1 + \varepsilon\right) \hat{A}_{i,t}\right),
\end{align}
where $G$ is the size of training batch, $o_i$ is the trajectory of the $i$th sample.
In this loss formulation, the losses of all tokens are first averaged at the sequence level before being further averaged at the batch level. This approach results in tokens from longer sequences contributing less to the final loss value. Consequently, if the model encounters critical issues in processing long sequences, a scenario that is prone to occur during the exploration phase of RL training, the insufficient suppression caused by their diminished weighting may lead to training instability or even collapse. To address this imbalance in token-level contribution to the final loss, we revise the loss function into the following form:
\begin{align}
\mathcal{L}_{\text{PPO}}(\theta) =- \frac{1}{\sum_{i = 1}^G |o_i|} \sum_{i = 1}^G \sum_{t = 1}^{|o_i|} \min \left(r_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1-\varepsilon, 1 + \varepsilon\right) \hat{A}_{i,t}\right),
\end{align}
where all tokens within a single training batch are assigned uniform weights, thereby enabling the problems posed by long sequences to be addressed with enhanced efficiency.

\subsection{Dealing with Sparsity of Reward Signal in Verifier-based Tasks}
As analyzed in Section \ref{challenge3}, enhancing the efficiency of exploration-exploitation tradeoff in RL training becomes critically challenging under scenarios with highly sparse reward signals. To address this key issue, we adopt three methods: Clip-Higher, Positive Example LM Loss and Group-Sampling. The technical details are elaborated as follows:

\textbf{Clip-Higher} is used to mitigate the entropy collapse issue encountered in PPO and GRPO training process, which is first proposed in DAPO \citep{dapo}. We decouple the lower and higher clipping range as $\varepsilon_\text{low}$ and $\varepsilon_\text{high}$
\begin{align}
\mathcal{L}_{\text{PPO}}(\theta) =- \frac{1}{\sum_{i = 1}^G |o_i|} \sum_{i = 1}^G \sum_{t = 1}^{|o_i|} \min \left(r_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1-\textcolor{red}{\varepsilon_\text{low}}, 1 + \textcolor{red}{\varepsilon_\text{high}}\right) \hat{A}_{i,t}\right),
\end{align}
We increase the value of $\varepsilon_\text{high}$ to leave more room for the increase of low-probability tokens. We opt to keep $\varepsilon_\text{low}$ relatively small, because increasing it will suppress the probability of these tokens to 0, resulting in the collapse of the sampling space.

\textbf{Positive Example LM Loss} is designed to enhance the utilization efficiency of positive samples during RL training process. In the context of RL for complex reasoning tasks, some tasks demonstrate remarkably low accuracy, with the majority of training samples yielding incorrect answers. Traditional policy optimization strategies that suppress the generation probability of erroneous samples suffer from inefficiency during RL training, as the trial-and-error mechanism incurs substantial computational costs. Given this challenge, it is critical to maximize the utility of correct answers when they are sampled by the policy model. To address this challenge, we adopt an imitation learning approach by incorporating an additional negative log-likelihood (NLL) loss for the correct outcomes sampled during RL training. The corresponding formula is as follows:
\begin{align}
\mathcal{L}_{\text{NLL}}(\theta) =- \frac{1}{\sum_{o_i \in \mathcal{T}}|o_i|} \sum_{o_i \in \mathcal{T}} \sum_{t = 1}^{|o_i|} \log\pi_{\theta}\left(a_t|s_t\right),
\end{align}
where $\mathcal{T}$ denotes the set of correct answers. The final NLL loss is combined with the policy gradient loss through a weighting coefficient $\mu$, which collectively serves as the objective for updating the policy model:
\begin{align}
\mathcal{L}(\theta) = \mathcal{L}_{\text{PPO}}(\theta) + \mu * \mathcal{L}_{\text{NLL}}(\theta).
\end{align}

\textbf{Group-Sampling} is used to sample discriminative positive and negative samples within the same prompt. 
Given a fixed computational budget, there exist two primary approaches to allocating computational resources. The first approach utilizes as many prompts as possible, with each prompt sampled only once. The second approach reduces the number of distinct prompts per batch and redirects computational resources toward repeated generations. We observed that the latter approach yields marginally better performance, attributed to the richer contrastive signals it introduces, which enhance the policy modelâ€™s learning capability.
% Furthermore, while this work has not yet explored it, implementing a strategy of sampling the same prompt multiple times in value-based RL algorithms could enable future investigations into data filtering and sample weighting, leveraging statistical insights derived from repeated samplings.
%\subsection{Hello World}