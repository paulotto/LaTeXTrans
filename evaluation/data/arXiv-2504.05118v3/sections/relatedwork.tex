

\section{Preliminaries}
This section presents the fundamental concepts and notations that serve as the basis for our proposed algorithm. We first explore the basic framework of representing language generation as a reinforcement learning task. Subsequently, we introduce Proximal Policy Optimization and Generalized Advantage Estimation.

\subsection{Modeling Language Generation as Token-Level MDP}
Reinforcement learning centers around the learning of a policy that maximizes the cumulative reward for an agent as it interacts with an environment. In this study, we cast language generation tasks within the framework of a Markov Decision Process (MDP) \citep{ouyang2022training}.

Let the prompt be denoted as $x$, and the response to this prompt as $y$. Both $x$ and $y$ can be decomposed into sequences of tokens. For example, the prompt $x$ can be expressed as $x=(x_0,\dots,x_m)$, where the tokens are drawn from a fixed discrete vocabulary $\mathcal{A}$.

We define the token-level MDP as the tuple $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathbb{P},R,d_0,\omega)$. Here is a detailed breakdown of each component:
\begin{itemize}[leftmargin=*]
    \item \textbf{State Space ($\mathcal{S}$)}: This space encompasses all possible states formed by the tokens generated up to a given time step. At time step $t$, the state $s_t$ is defined as $s_t=(x_0,\dots,x_m,y_0,\dots,y_t)$.
    \item \textbf{Action Space ($\mathcal{A}$)}: It corresponds to the fixed discrete vocabulary, from which tokens are selected during the generation process.
    \item \textbf{Dynamics ($\mathbb{P}$)}: These represent a deterministic transition model between tokens. Given a state $s_t=(x_0,\dots,x_m,y_0,\dots,y_t)$, an action $a = y_{t + 1}$, and the subsequent state $s_{t+1}=(x_0,\dots,x_m,y_0,\dots,y_t,y_{t+1})$, the probability $\mathbb{P}(s_{t+1}|s_t,a)=1$.
    \item \textbf{Termination Condition}: The language generation process concludes when the terminal action $\omega$, typically the end-of-sentence token, is executed.
    \item \textbf{Reward Function ($R(s,a)$)}: This function offers scalar feedback to evaluate the agent's performance after taking action $a$ in state $s$. In the context of Reinforcement Learning from Human Feedback (RLHF)~\cite{rlhf,rlhf-sw}, the reward function can be learned from human preferences or defined by a set of rules specific to the task.
    \item \textbf{Initial State Distribution ($d_0$)}: It is a probability distribution over prompts $x$. An initial state $s_0$ consists of the tokens within the prompt $x$.
\end{itemize}

\subsection{RLHF Learning Objective}
We formulate the optimization problem as a KL-regularized RL task. Our objective is to approximate the optimal KL-regularized policy, which is given by:
\begin{align}\label{eq:objective}
   \pi^* =  \arg\max_\pi \mathbb{E}_{\pi, s_0 \sim d_0} \left[ \sum_{t = 0}^H  \left(R(s_t, a_t)-\beta \text{KL} \big( \pi(\cdot | s_t) \| \pi_{\text{ref}}(\cdot | s_t) \big)\right) \right]
\end{align}
In this equation, $H$ represents the total number of decision steps, $s_0$ is a prompt sampled from the dataset, $R(s_t, a_t)$ is the token-level reward obtained from the reward function, $\beta$ is a coefficient that controls the strength of the KL-regularization, and $\pi_{\text{ref}}$ is the initialization policy.

In traditional RLHF and most tasks related to LLMs, the reward is sparse and is only assigned at the terminal action $\omega$, that is, the end-of-sentence token \texttt{<eos>}. 

\subsection{Proximal Policy Optimization}
PPO \citep{ppo} uses a clipped surrogate objective to update the policy. The key idea is to limit the change in the policy during each update step, preventing large policy updates that could lead to instability.

Let $\pi_{\theta}(a|s)$ be the policy parameterized by $\theta$, and $\pi_{\theta_{\text{old}}}(a|s)$ be the old policy from the previous iteration. The surrogate objective function for PPO is defined as:

\begin{equation}
\mathcal{L}^{CLIP}(\theta)=\hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t,\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
\end{equation}

where $r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the probability ratio, $\hat{A}_t$ is the estimated advantage at time step $t$, and $\epsilon$ is a hyperparameter that controls the clipping range.

Generalized Advantage Estimation \citep{schulman2015high} is a technique used to estimate the advantage function more accurately in PPO. It combines multiple-step bootstrapping to reduce the variance of the advantage estimates. For a trajectory of length $T$, the advantage estimate $\hat{A}_t$ at time step $t$ is computed as:

\begin{equation}
\hat{A}_t=\sum_{l = 0}^{T-t-1}(\gamma\lambda)^l\delta_{t + l}
\label{eq:gae_definition}
\end{equation}

where $\gamma$ is the discount factor, $\lambda\in[0, 1]$ is the GAE parameter, and $\delta_t=R(s_t, a_t)+\gamma V(s_{t + 1})-V(s_t)$ is the temporal-difference (TD) error. Here, $R(s_t, a_t)$ is the reward at time step $t$, and $V(s)$ is the value function. Since it is a common practice to use discount factor $\gamma = 1.0$ in RLHF, to simplify our notation, we omit $\gamma$ in later sections of this paper.
\iffalse
\subsection{valued-based RL algorithm}
Proximal Policy Optimization is a cornerstone RL algorithm that has revolutionized policy optimization through its simplicity, stability, and broad applicability. At its core, PPO addresses the inherent challenges of traditional policy gradient methods, such as high variance and unstable updates, by enforcing constrained policy updates within a trust region, thereby ensuring monotonic improvement in expected returns. PPO operates by optimizing the following objective function: \[ \mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right], \] where \(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\) denotes the policy ratio between the current and old policies, \(\hat{A}_t\) is the estimated advantage function at timestep \(t\), and \(\epsilon\) is a hyperparameter defining the clip range. This clip mechanism limits the policy update to prevent drastic changes in the action distribution, mitigating the risk of catastrophic performance degradation during training. A key innovation of PPO lies in its dual formulation: PPO-Clip (as shown above) and PPO-Penalty, with the former being more widely adopted due to its empirical robustness. Both variants leverage advantage estimation (e.g., generalized advantage estimation, GAE \cite{gae}) to reduce variance while maintaining a bias-variance tradeoff. By iteratively collecting trajectories from the current policy and performing multiple epochs of mini-batch updates, PPO strikes a balance between sample efficiency and training stability. Owing to its elegant design, PPO has achieved state-of-the-art results across diverse domains, including game playing, robotic control, and language model fine-tuning. However, as detailed in the subsequent sections, the algorithm faces distinct challenges when applied to long-form chain-of-thought (CoT) generation in large language models, particularly in scenarios requiring extended sequential reasoning and nuanced reward propagation.

As presented in the paper \cite{vc-ppo}, the root causes of PPO's failure in long CoT tasks are identified as value initialization bias and reward signal decay. The common practice of initializing the value model from the reward model leads to a significant bias in the value model before and during training. This bias causes inaccurate value prediction, especially in the early training stage, as the advantage estimation is affected. Additionally, in tasks with long sequences and rewards at the end, the Generalized Advantage Estimation (GAE) computation results in a decaying reward signal, which fails to effectively propagate the reward to preceding tokens.

To address these issues, the authors propose Value-Calibrated PPO (VC-PPO). In VC-PPO, value pre-training is employed to tackle the value initialization bias. By training the value model offline with responses generated by a fixed SFT policy, the value model can better estimate expected rewards, reducing the bias in the early training phase. Moreover, Decoupled-GAE is introduced to mitigate the value bias during training. This method decouples the GAE computation for the policy and the value, allowing the value function to use a larger $\lambda$ for more effective reward signal propagation along the long sequence, while the policy can maintain the original $\lambda$ to ensure convergence under time and computational constraints.

This research not only provides a practical solution to improve the performance of PPO in long CoT tasks but also reveals the differences in variance-bias preferences between value and policy models, which offers valuable insights for future research in RLHF.

\subsection{RL algorithm without value model}
Unlike PPO, GRPO eliminates the value function and estimates advantages in a group-relative fashion. For a given question-answer pair \((q, a)\), the behavior policy \(\pi_{\theta_{\text{old}}}\) samples a group of \(G\) responses \(\{ o_i \}_{i=1}^G\). The advantage of the \(i\)-th response is then calculated by normalizing the group-level rewards \(\{ R_i \}_{i=1}^G\):
\[
\hat{A}_{i,t} = \frac{r_i-\text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}.
\]

Similar to PPO, GRPO employs a clipped objective but additionally incorporates an explicit KL penalty term. The objective function is formulated as:
\[
\begin{aligned}
\mathcal{J}_\text{GRPO}(\theta) &= \mathbb{E}_{\substack{(q,a) \sim \mathcal{D}, \\ \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)}} \Bigg[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \Bigg( 
\min \Big( r_{i,t}(\theta) \hat{A}_{i,t},  
\ \text{clip} \Big( r_{i,t}(\theta), 1-\varepsilon, 1 + \varepsilon \Big) \hat{A}_{i,t} \Big) \\
&\quad-\beta D_{\text{KL}}(\pi_{\theta} \mid\mid \pi_{\text{ref}}) 
\Bigg) \Bigg],
\end{aligned}
\]
where \(r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}\).

DAPO enhances GRPO with four key improvements:  
\begin{enumerate}
\item  Decoupled Clipping (\(\varepsilon_{\text{low}}/\varepsilon_{\text{high}}\)) to boost low-probability token exploration, addressing entropy collapse.  
\item Dynamic Sampling to filter out all-correct/incorrect prompt groups, maintaining effective gradients and sample efficiency.  
\item  Token-Level Loss Calculation to prioritize contributions from longer reasoning sequences, avoiding the undervaluation of lengthy, high-quality responses in gradient updates.  
\item Overlong Reward Shaping to penalize excessively long outputs softly, reducing training noise.  
\end{enumerate}
\fi

\section{Challenges in Long-CoT RL for Reasoning Tasks}
Long-CoT tasks present unique challenges to RL training, especially for methods that employ a value model to reduce variance. In this section, we systematically analyze the technical issues arising from sequence length dynamics, value function instability, and reward sparsity.

\subsection{Value Model Bias over Long Sequences}
\label{challenge1}
As identified in VC-PPO \citep{vc-ppo}, initializing the value model with a reward model introduces significant initialization bias. This positive bias arises from an objective mismatch between the two models. The reward model is trained to score on the \texttt{<EOS>} token, incentivizing it to assign lower scores to earlier tokens due to their incomplete context. In contrast, the value model estimates the expected cumulative reward for all tokens preceding \texttt{<EOS>} under a given policy. During early training phases, given the backward computation of GAE, there will be a positive bias at every timestep $t$ that accumulates along the trajectory.

Another standard practice of using GAE with $\lambda=0.95$ might exacerbates this issue. The reward signal $R(s_T, \text{<EOS>})$ at the termination token propagates backward as $\lambda^{T-t} R(s_T, \text{<EOS>})$ to the $t$-th token. For long sequences where $T-t \gg 1$, this discounting reduces the effective reward signal to near zero. Consequently, value updates become almost entirely bootstrapped, relying on highly biased estimates that undermine the value model's role as a reliable variance-reduction baseline.

\subsection{Heterogeneous Sequence Lengths during Training}

In complex reasoning tasks where a long CoT is essential for arriving at the correct answer, models often generate responses with highly variable lengths. This variability requires algorithms to be robust enough to manage sequences that can range from very short to extremely long. As a result, the commonly-applied GAE method with a fixed $\lambda$ parameter encounters significant challenges.

Even when the value model is perfect, a static $\lambda$ may not effectively adapt to sequences of varying lengths. For short-length responses, the estimates obtained through GAE tend to suffer from high variance. This is because GAE represents a trade-off between bias and variance. In the case of short responses, the estimates are skewed towards the variance-dominated side. On the other hand, for long-length responses, GAE often leads to high bias due to bootstrapping. The recursive nature of GAE, which relies on future state values, accumulates errors over long sequences, exacerbating the bias issue. These limitations are deeply rooted in the exponentially-decaying nature of GAE's computational framework.

\subsection{Sparsity of Reward Signal in Verifier-based Tasks}
\label{challenge3}

Complex reasoning tasks frequently deploy a verifier as a reward model \citep{o1, deepseekai2025deepseekr1incentivizingreasoningcapability}. Unlike traditional language-model-based reward models that provide a dense signal, such as a continuous value ranging from -4 to 4, verifier-based reward models typically offer binary feedback, such as 0 and 1. The sparsity of the reward signal is further compounded by long CoT reasoning. As CoT significantly elongates output lengths, it not only increases computational time but also reduces the frequency of receiving non-zero rewards. In policy optimization, the sampled responses with correct answer could be extremely scarce and valuable.

This situation poses a distinct exploration-exploitation dilemma. On one hand, the model must maintain relatively high uncertainty. This enables it to sample a diverse range of responses, increasing the likelihood of generating the correct answer for a given prompt. On the other hand, algorithms need to effectively utilize the correctly sampled responses—obtained through painstaking exploration—to enhance learning efficiency. By failing to strike the right balance between exploration and exploitation, the model may either get stuck in suboptimal solutions due to excessive exploitation or waste computational resources on unproductive exploration.