@inproceedings{
    loshchilov2018decoupled,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@misc{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{grok,
  title={Grok 3 Beta — The Age of Reasoning Agents},
  author={XAI},
  year={2024},
  url = {https://x.ai/news/grok-3}
}

@misc{gemini-thinking,
  title={Gemini 2.0 Flash Thinking},
  author={Google DeepMind},
  year={2024},
  url = {https://deepmind.google/technologies/gemini/flash-thinking/}
}

@misc{qwq,
  title={QwQ-32B: Embracing the Power of Reinforcement Learning},
  author={Qwen},
  year={2024},
  url = {https://qwenlm.github.io/blog/qwq-32b/}
}

@article{k1.5,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@article{gpt4,
  title={{GPT4} technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{gandhi2025cognitive,
  title={Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs},
  author={Gandhi, Kanishk and Chakravarthy, Ayush and Singh, Anikait and Lile, Nathan and Goodman, Noah D},
  journal={arXiv preprint arXiv:2503.01307},
  year={2025}
}
@misc{claude35sonnet,
  title = {Claude 3.5 Sonnet},
  author = {Anthropic},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  year={2024}
}
@article{wang2025thoughts,
  title={Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs},
  author={Wang, Yue and Liu, Qiuzhi and Xu, Jiahao and Liang, Tian and Chen, Xingyu and He, Zhiwei and Song, Linfeng and Yu, Dian and Li, Juntao and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2501.18585},
  year={2025}
}
@article{cuadron2025danger,
  title={The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks},
  author={Cuadron, Alejandro and Li, Dacheng and Ma, Wenjie and Wang, Xingyao and Wang, Yichuan and Zhuang, Siyuan and Liu, Shu and Schroeder, Luis Gaspar and Xia, Tian and Mao, Huanzhi and others},
  journal={arXiv preprint arXiv:2502.08235},
  year={2025}
}
@inproceedings{NEURIPS2022_b1efde53,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{alphazero,
  author       = {David Silver and
                  Thomas Hubert and
                  Julian Schrittwieser and
                  Ioannis Antonoglou and
                  Matthew Lai and
                  Arthur Guez and
                  Marc Lanctot and
                  Laurent Sifre and
                  Dharshan Kumaran and
                  Thore Graepel and
                  Timothy P. Lillicrap and
                  Karen Simonyan and
                  Demis Hassabis},
  title        = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
                  Learning Algorithm},
  journal      = {CoRR},
  volume       = {abs/1712.01815},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.01815},
  eprinttype    = {arXiv},
  eprint       = {1712.01815},
  timestamp    = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@misc{kl,
  title={Approximating kl divergence},
  author={John Schulman},
  year={2020},
  url = {http://joschu.net/blog/kl-approx.html}
}

@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21314--21328},
  year={2022}
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{sheng2024hybridflow,
  title={Hybridflow: A flexible and efficient rlhf framework},
  author={Sheng, Guangming and Zhang, Chi and Ye, Zilingfeng and Wu, Xibin and Zhang, Wang and Zhang, Ru and Peng, Yanghua and Lin, Haibin and Wu, Chuan},
  journal={arXiv preprint arXiv:2409.19256},
  year={2024}
}

@misc{amodei2016concreteproblemsaisafety,
      title={Concrete Problems in AI Safety}, 
      author={Dario Amodei and Chris Olah and Jacob Steinhardt and Paul Christiano and John Schulman and Dan Mané},
      year={2016},
      eprint={1606.06565},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1606.06565}, 
}

@misc{everitt2017reinforcementlearningcorruptedreward,
      title={Reinforcement Learning with a Corrupted Reward Channel}, 
      author={Tom Everitt and Victoria Krakovna and Laurent Orseau and Marcus Hutter and Shane Legg},
      year={2017},
      eprint={1705.08417},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.08417}, 
}

@misc{everitt2021rewardtamperingproblemssolutions,
      title={Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective}, 
      author={Tom Everitt and Marcus Hutter and Ramana Kumar and Victoria Krakovna},
      year={2021},
      eprint={1908.04734},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1908.04734}, 
}

@misc{google2020specialgaming,
  title={Specification gaming: the flip side of AI ingenuity},
  author={Victoria Krakovna and Jonathan Uesato and Vladimir Mikulik and Matthew Rahtz and Tom Everitt and Ramana Kumar and Zac Kenton and Jan Leike and Shane Legg},
  year={2020},
  url = {https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/}
}

@misc{gao2022scalinglawsrewardmodel,
      title={Scaling Laws for Reward Model Overoptimization}, 
      author={Leo Gao and John Schulman and Jacob Hilton},
      year={2022},
      eprint={2210.10760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.10760}, 
}

@misc{langosco2023goalmisgeneralizationdeepreinforcement,
      title={Goal Misgeneralization in Deep Reinforcement Learning}, 
      author={Lauro Langosco and Jack Koch and Lee Sharkey and Jacob Pfau and Laurent Orseau and David Krueger},
      year={2023},
      eprint={2105.14111},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.14111}, 
}

@misc{pan2022effectsrewardmisspecificationmapping,
      title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models}, 
      author={Alexander Pan and Kush Bhatia and Jacob Steinhardt},
      year={2022},
      eprint={2201.03544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.03544}, 
}

@article{weng2024rewardhack,
  title   = "Reward Hacking in Reinforcement Learning.",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2024",
  month   = "Nov",
  url     = "https://lilianweng.github.io/posts/2024-11-28-reward-hacking/"
}


@misc{polu2020generativelanguagemodelingautomated,
      title={Generative Language Modeling for Automated Theorem Proving}, 
      author={Stanislas Polu and Ilya Sutskever},
      year={2020},
      eprint={2009.03393},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.03393}, 
}

@misc{gehring2025rlefgroundingcodellms,
      title={RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning}, 
      author={Jonas Gehring and Kunhao Zheng and Jade Copet and Vegard Mella and Quentin Carbonneaux and Taco Cohen and Gabriel Synnaeve},
      year={2025},
      eprint={2410.02089},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02089}, 
}

@misc{shinn2023reflexionlanguageagentsverbal,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2303.11366}, 
}

@misc{chen2023teachinglargelanguagemodels,
      title={Teaching Large Language Models to Self-Debug}, 
      author={Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
      year={2023},
      eprint={2304.05128},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.05128}, 
}

@misc{google2024alphageometry,
  title={AlphaGeometry: An Olympiad-level AI system for geometry},
  author={Trieu Trinh and Thang Luong},
  year={2024},
  url = {https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/}
}

@misc{google2024alphaproofandalphageometry,
  title={AI achieves silver-medal standard solving International Mathematical Olympiad problems},
  author={AlphaProof and AlphaGeometry Teams},
  year={2024},
  url = {https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/}
}

@misc{schulman2018highdimensionalcontinuouscontrolusing,
      title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
      author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
      year={2018},
      eprint={1506.02438},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1506.02438}, 
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{dsv3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{rendarl,
  title={An Empirical Study on Eliciting and Improving R1-like Reasoning Models},
  author={Chen, Zhipeng and Min, Yingqian and Zhang, Beichen and Chen, Jie and Jiang, Jinhao and Cheng, Daixuan and Zhao, Wayne Xin and Liu, Zheng and Miao, Xu and Lu, Yang and others},
  journal={arXiv preprint arXiv:2503.04548},
  year={2025}
}
@misc{OpenReasonerZero2025,
  title={Open-Reasoner-Zero: An Open Source Approach to Scaling Reinforcement Learning on the Base Model},
  author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang, Heung-Yeung Shum},
  year={2025},
  howpublished={\url{https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero}},
}

@article{hu2025reinforce++,
  title={REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}

@article{cui2025process,
  title={Process reinforcement through implicit rewards},
  author={Cui, Ganqu and Yuan, Lifan and Wang, Zefan and Wang, Hanbin and Li, Wendi and He, Bingxiang and Fan, Yuchen and Yu, Tianyu and Xu, Qixin and Chen, Weize and others},
  journal={arXiv preprint arXiv:2502.01456},
  year={2025}
}

@article{lee2024token,
  title={Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models},
  author={Lee, Jung Hyun and Yang, June Yong and Heo, Byeongho and Han, Dongyoon and Yoo, Kang Min},
  journal={arXiv preprint arXiv:2407.12863},
  year={2024}
}

@article{kazemnejad2024vineppo,
  title={Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment},
  author={Kazemnejad, Amirhossein and Aghajohari, Milad and Portelance, Eva and Sordoni, Alessandro and Reddy, Siva and Courville, Aaron and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2410.01679},
  year={2024}
}

@article{yuan2025s,
  title={What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret},
  author={Yuan, Yufeng and Yue, Yu and Zhu, Ruofei and Fan, Tiantian and Yan, Lin},
  journal={arXiv preprint arXiv:2503.01491},
  year={2025}
}



@misc{gae,
      title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
      author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
      year={2018},
      eprint={1506.02438},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1506.02438}, 
}



@article{li2024llava,
  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@misc{dapo,
      title={DAPO: An Open-Source LLM Reinforcement Learning System at Scale}, 
      author={Qiying Yu and Zheng Zhang and Ruofei Zhu and Yufeng Yuan and Xiaochen Zuo and Yu Yue and Tiantian Fan and Gaohong Liu and Lingjun Liu and Xin Liu and Haibin Lin and Zhiqi Lin and Bole Ma and Guangming Sheng and Yuxuan Tong and Chi Zhang and Mofan Zhang and Wang Zhang and Hang Zhu and Jinhua Zhu and Jiaze Chen and Jiangjie Chen and Chengyi Wang and Hongli Yu and Weinan Dai and Yuxuan Song and Xiangpeng Wei and Hao Zhou and Jingjing Liu and Wei-Ying Ma and Ya-Qin Zhang and Lin Yan and Mu Qiao and Yonghui Wu and Mingxuan Wang},
      year={2025},
      eprint={2503.14476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.14476}, 
}

@misc{vc-ppo,
      title={What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret}, 
      author={Yufeng Yuan and Yu Yue and Ruofei Zhu and Tiantian Fan and Lin Yan},
      year={2025},
      eprint={2503.01491},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.01491}, 
}

@article{explained_variance,
author = {Good, Ron and Fletcher, Harold J.},
title = {Reporting explained variance},
journal = {Journal of Research in Science Teaching},
volume = {18},
number = {1},
pages = {1-7},
doi = {https://doi.org/10.1002/tea.3660180102},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/tea.3660180102},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/tea.3660180102},
abstract = {Abstract The importance of reporting explained variance (sometimes referred to as magnitude of effects) in ANOVA designs is discussed in this paper. Explained variance is an estimate of the strength of the relationship between treatment (or other factors such as sex, grade level, etc.) and dependent variables of interest to the researcher(s). Three methods that can be used to obtain estimates of explained variance in ANOVA designs are described and applied to 16 studies that were reported in recent volumes of this journal. The results show that, while in most studies the treatment accounts for a relatively small proportion of the variance in dependent variable scores., in., some studies the magnitude of the treatment effect is respectable. The authors recommend that researchers in science education report explained variance in addition to the commonly reported tests of significance, since the latter are inadequate as the sole basis for making decisions about the practical importance of factors of interest to science education researchers.},
year = {1981}
}

@inproceedings{hendrycks2021measuring,
author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 title = {Measuring Mathematical Problem Solving With the MATH Dataset},
volume = {1},
 year = {2021}
}
@article{wilcox2022monte,
  title={Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations},
  author={Wilcox, Albert and Balakrishna, Ashwin and Dedieu, Jules and Benslimane, Wyame and Brown, Daniel and Goldberg, Ken},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={2254--2267},
  year={2022}
}
@article{pezeshkpour2024reasoning,
  title={Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions},
  author={Pezeshkpour, Pouya and Kandogan, Eser and Bhutani, Nikita and Rahman, Sajjadur and Mitchell, Tom and Hruschka, Estevam},
  journal={arXiv preprint arXiv:2402.01108},
  year={2024}
}
@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{zhang2024rest,
  title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},
  author={Zhang, Dan and Zhoubian, Sining and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{bowman2022measuring,
  title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={CoRR},
  year={2022}
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{saha2021optimal,
  title={Optimal algorithms for stochastic contextual preference bandits},
  author={Saha, Aadirupa},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30050--30062},
  year={2021}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{munosnash,
  title={Nash Learning from Human Feedback},
  author={Munos, Remi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Fiegel, C{\^o}me and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017},
  publisher={Journal of Machine Learning Research/Massachusetts Institute of Technology~…}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{wu2024self,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{zhang2024iterative,
  title={Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning},
  author={Zhang, Yuheng and Yu, Dian and Peng, Baolin and Song, Linfeng and Tian, Ye and Huo, Mingyue and Jiang, Nan and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2407.00617},
  year={2024}
}

@article{richemond2024offline,
  title={Offline Regularised Reinforcement Learning for Large Language Models Alignment},
  author={Richemond, Pierre Harvey and Tang, Yunhao and Guo, Daniel and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rafailov, Rafael and Pires, Bernardo Avila and Tarassov, Eugene and Spangher, Lucas and Ellsworth, Will and others},
  journal={arXiv preprint arXiv:2405.19107},
  year={2024}
}

@InProceedings{Martin18Learning,
  title = 	 {Learning by Playing Solving Sparse Reward Tasks from Scratch},
  author =       {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and van de Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4344--4353},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
}


@inproceedings{torne2023breadcrumbs,
  title={Breadcrumbs to the goal: goal-conditioned exploration from human-in-the-loop feedback},
  author={Torne, Marcel and Balsells, Max and Wang, Zihan and Desai, Samedh and Chen, Tao and Agrawal, Pulkit and Gupta, Abhishek},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={63222--63258},
  year={2023}
}

@article{park2024hiql,
  title={Hiql: Offline goal-conditioned rl with latent states as actions},
  author={Park, Seohong and Ghosh, Dibya and Eysenbach, Benjamin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@inproceedings{snelloffline,
  title={Offline RL for Natural Language Generation with Implicit Language Q Learning},
  author={Snell, Charlie Victor and Kostrikov, Ilya and Su, Yi and Yang, Sherry and Levine, Sergey},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{chen2024step,
  title={Step-level Value Preference Optimization for Mathematical Reasoning},
  author={Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai},
  journal={arXiv preprint arXiv:2406.10858},
  year={2024}
}

@article{lai2024step,
  title={Step-dpo: Step-wise preference optimization for long-chain reasoning of llms},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya},
  journal={arXiv preprint arXiv:2406.18629},
  year={2024}
}

@article{xie2024monte,
  title={Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},
  author={Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael},
  journal={arXiv preprint arXiv:2405.00451},
  year={2024}
}

@article{xiong2024building,
  title={Building Math Agents with Multi-Turn Iterative Preference Learning},
  author={Xiong, Wei and Shi, Chengshuai and Shen, Jiaming and Rosenberg, Aviv and Qin, Zhen and Calandriello, Daniele and Khalman, Misha and Joshi, Rishabh and Piot, Bilal and Saleh, Mohammad and others},
  journal={arXiv preprint arXiv:2409.02392},
  year={2024}
}

@article{shani2024multi,
  title={Multi-turn Reinforcement Learning from Preference Human Feedback},
  author={Shani, Lior and Rosenberg, Aviv and Cassel, Asaf and Lang, Oran and Calandriello, Daniele and Zipori, Avital and Noga, Hila and Keller, Orgad and Piot, Bilal and Szpektor, Idan and others},
  journal={arXiv preprint arXiv:2405.14655},
  year={2024}
}

@article{han2024psydial,
  title={PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models},
  author={Han, Ji-Eun and Koh, Jun-Seok and Seo, Hyeon-Tae and Chang, Du-Seong and Sohn, Kyung-Ah},
  journal={arXiv preprint arXiv:2404.00930},
  year={2024}
}

@article{alawwad2024enhancing,
  title={Enhancing textbook question answering task with large language models and retrieval augmented generation},
  author={Alawwad, Hessa Abdulrahman and Alhothali, Areej and Naseem, Usman and Alkhathlan, Ali and Jamal, Amani},
  journal={arXiv preprint arXiv:2402.05128},
  year={2024}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@inproceedings{
  chen2024teaching,
  title={Teaching Large Language Models to Self-Debug},
  author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wang2024opendevin,
  title={Opendevin: An open platform for ai software developers as generalist agents},
  author={Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},
  journal={arXiv preprint arXiv:2407.16741},
  year={2024}
}
@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{saha2021optimal,
  title={Optimal algorithms for stochastic contextual preference bandits},
  author={Saha, Aadirupa},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30050--30062},
  year={2021}
}

@article{gentile2022fast,
  title={Fast rates in pool-based batch active learning},
  author={Gentile, Claudio and Wang, Zhilei and Zhang, Tong},
  journal={arXiv preprint arXiv:2202.05448},
  year={2022}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@article{kirsch2021simple,
  title={A simple baseline for batch active learning with stochastic acquisition functions},
  author={Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin},
  journal={arXiv preprint arXiv:2106.12059},
  year={2021}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{wu2023new,
  title={A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions},
  author={Wu, Siwei and Shen, Xiangqing and Xia, Rui},
  journal={arXiv preprint arXiv:2310.03293},
  year={2023}
}

@inproceedings{knox2008tamer,
  title={Tamer: Training an agent manually via evaluative reinforcement},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={2008 7th IEEE international conference on development and learning},
  pages={292--297},
  year={2008},
  organization={IEEE}
}

@article{sheng2024hybridflow,
  title={Hybridflow: A flexible and efficient rlhf framework},
  author={Sheng, Guangming and Zhang, Chi and Ye, Zilingfeng and Wu, Xibin and Zhang, Wang and Zhang, Ru and Peng, Yanghua and Lin, Haibin and Wu, Chuan},
  journal={arXiv preprint arXiv:2409.19256},
  year={2024}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{kang2024mindstar,
  title={MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time},
  author={Kang, Jikun and Li, Xin Zhe and Chen, Xi and Kazemi, Amirreza and Chen, Boxing},
  journal={arXiv preprint arXiv:2405.16265},
  year={2024}
}

@inproceedings{miaoselfcheck,
  title={SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning},
  author={Miao, Ning and Teh, Yee Whye and Rainforth, Tom},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{
    Choshen2020On,
    title={On the Weaknesses of Reinforcement Learning for Neural Machine Translation},
    author={Leshem Choshen and Lior Fox and Zohar Aizenbud and Omri Abend},
    booktitle={International Conference on Learning Representations},
    year={2020}
}

@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Yu and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@inproceedings{yumetamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Jincheng, YU and Liu, Zhengying and Zhang, Yu and Kwok, James and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{cot_2022,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  timestamp    = {Tue, 12 Nov 2024 16:50:49 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{huang2024nimplementationdetailsrlhf,
      title={The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization}, 
      author={Shengyi Huang and Michael Noukhovitch and Arian Hosseini and Kashif Rasul and Weixun Wang and Lewis Tunstall},
      year={2024},
      eprint={2403.17031},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.17031}, 
}

@misc{zheng2023secretsrlhflargelanguage,
      title={Secrets of RLHF in Large Language Models Part I: PPO}, 
      author={Rui Zheng and Shihan Dou and Songyang Gao and Yuan Hua and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Yuhao Zhou and Limao Xiong and Lu Chen and Zhiheng Xi and Nuo Xu and Wenbin Lai and Minghao Zhu and Cheng Chang and Zhangyue Yin and Rongxiang Weng and Wensen Cheng and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
      year={2023},
      eprint={2307.04964},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.04964}, 
}

@inproceedings{conf/iros/TodorovET12,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  biburl = {https://www.bibsonomy.org/bibtex/22218cb5d1a40b6e6c2b6077fdff08fbf/dblp},
  booktitle = {IROS},
  ee = {https://www.wikidata.org/entity/Q56812558},
  interhash = {869ab290a52bafc47114756b66df3c25},
  intrahash = {2218cb5d1a40b6e6c2b6077fdff08fbf},
  isbn = {978-1-4673-1737-5},
  keywords = {dblp},
  pages = {5026-5033},
  publisher = {IEEE},
  timestamp = {2019-10-17T14:19:43.000+0200},
  title = {MuJoCo: A physics engine for model-based control.},
  url = {http://dblp.uni-trier.de/db/conf/iros/iros2012.html#TodorovET12},
  year = 2012
}

@Article{bellemare13arcade,
    author = {{Bellemare}, M.~G. and {Naddaf}, Y. and {Veness}, J. and {Bowling}, M.},
    title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
    journal = {Journal of Artificial Intelligence Research},
    year = "2013",
    month = "jun",
    volume = "47",
    pages = "253--279",
}

@misc{ahmadian2024basicsrevisitingreinforcestyle,
      title={Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs}, 
      author={Arash Ahmadian and Chris Cremer and Matthias Gallé and Marzieh Fadaee and Julia Kreutzer and Olivier Pietquin and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2402.14740},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.14740}, 
}
@misc{o3,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2025},
  url = {https://openai.com/index/openai-o3-mini/}
}

@inproceedings{rloo,
  author       = {Wouter Kool and
                  Herke van Hoof and
                  Max Welling},
  title        = {Buy 4 {REINFORCE} Samples, Get a Baseline for Free!},
  booktitle    = {Deep Reinforcement Learning Meets Structured Prediction, {ICLR} 2019
                  Workshop, New Orleans, Louisiana, United States, May 6, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=r1lgTGL5DE},
  timestamp    = {Fri, 17 Apr 2020 14:12:02 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KoolHW19a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{gpqa,
      title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark}, 
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}, 
}

@misc{qwen2.5,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}


author = {Good, Ron and Fletcher, Harold J.},
title = {Reporting explained variance},
journal = {Journal of Research in Science Teaching},
volume = {18},
number = {1},
pages = {1-7},
doi = {https://doi.org/10.1002/tea.3660180102},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/tea.3660180102},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/tea.3660180102},
abstract = {Abstract The importance of reporting explained variance (sometimes referred to as magnitude of effects) in ANOVA designs is discussed in this paper. Explained variance is an estimate of the strength of the relationship between treatment (or other factors such as sex, grade level, etc.) and dependent variables of interest to the researcher(s). Three methods that can be used to obtain estimates of explained variance in ANOVA designs are described and applied to 16 studies that were reported in recent volumes of this journal. The results show that, while in most studies the treatment accounts for a relatively small proportion of the variance in dependent variable scores., in., some studies the magnitude of the treatment effect is respectable. The authors recommend that researchers in science education report explained variance in addition to the commonly reported tests of significance, since the latter are inadequate as the sole basis for making decisions about the practical importance of factors of interest to science education researchers.},
year = {1981}
}

@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}

@misc{liu2025understandingr1zeroliketrainingcritical,
      title={Understanding R1-Zero-Like Training: A Critical Perspective}, 
      author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
      year={2025},
      eprint={2503.20783},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.20783}, 
}
@misc{hu2025openreasonerzeroopensourceapproach,
      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, 
      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},
      year={2025},
      eprint={2503.24290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.24290}, 
}

@InProceedings{SIL,
  title = 	 {Self-Imitation Learning},
  author =       {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3878--3887},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/oh18b/oh18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/oh18b.html},
  abstract = 	 {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent’s past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.}
}

@inproceedings{areal,
  author       = {Mei, Zhiyu and Fu, Wei and Li, Kaiwei and Wang, Guangju and Zhang, Huanchen and Wu, Yi},
  title        = {ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation},
  booktitle    = {Proceedings of the Eighth Conference on Machine Learning and Systems,
                  MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025},
  publisher    = {mlsys.org},
  year         = {2025},
}

@book{sutton1998rl,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}


@article{rlhf,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rlhf-sw,
  title={Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback},
  author={Shen, Wei and Liu, Guanlin and Wu, Zheng and Zhu, Ruofei and Yang, Qingping and Xin, Chao and Yue, Yu and Yan, Lin},
  journal={arXiv preprint arXiv:2503.22230},
  year={2025}
}