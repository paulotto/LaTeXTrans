@article{qi2023zero,
  title={Zero Bubble Pipeline Parallelism},
  author={Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
  journal={arXiv preprint arXiv:2401.10241},
  year={2023}
}

@article{jain2024livecodebench,
  title={LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{mqa,
  author       = {Noam Shazeer},
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal      = {CoRR},
  volume       = {abs/1911.02150},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.02150},
}

@article{wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

@article{tora,
  author       = {Zhibin Gou and
                  Zhihong Shao and
                  Yeyun Gong and
                  Yelong Shen and
                  Yujiu Yang and
                  Minlie Huang and
                  Nan Duan and
                  Weizhu Chen},
  title        = {ToRA: {A} Tool-Integrated Reasoning Agent for Mathematical Problem
                  Solving},
  journal      = {CoRR},
  volume       = {abs/2309.17452},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.17452},
  doi          = {10.48550/ARXIV.2309.17452},
  eprinttype    = {arXiv},
  eprint       = {2309.17452},
  timestamp    = {Tue, 17 Oct 2023 13:50:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-17452.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pot,
  author       = {Wenhu Chen and
                  Xueguang Ma and
                  Xinyi Wang and
                  William W. Cohen},
  title        = {Program of Thoughts Prompting: Disentangling Computation from Reasoning
                  for Numerical Reasoning Tasks},
  journal      = {CoRR},
  volume       = {abs/2211.12588},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.12588},
  doi          = {10.48550/ARXIV.2211.12588},
  eprinttype    = {arXiv},
  eprint       = {2211.12588},
  timestamp    = {Tue, 29 Nov 2022 17:41:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-12588.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pal,
  author       = {Luyu Gao and
                  Aman Madaan and
                  Shuyan Zhou and
                  Uri Alon and
                  Pengfei Liu and
                  Yiming Yang and
                  Jamie Callan and
                  Graham Neubig},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {{PAL:} Program-aided Language Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {10764--10799},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/gao23f.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/GaoMZ00YCN23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cot,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lila,
  author       = {Swaroop Mishra and
                  Matthew Finlayson and
                  Pan Lu and
                  Leonard Tang and
                  Sean Welleck and
                  Chitta Baral and
                  Tanmay Rajpurohit and
                  Oyvind Tafjord and
                  Ashish Sabharwal and
                  Peter Clark and
                  Ashwin Kalyan},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {{LILA:} {A} Unified Benchmark for Mathematical Reasoning},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {5807--5832},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.392},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.392},
  timestamp    = {Thu, 10 Aug 2023 12:35:34 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/MishraFLTWBRTSC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{MathInstruct,
  author       = {Xiang Yue and
                  Xingwei Qu and
                  Ge Zhang and
                  Yao Fu and
                  Wenhao Huang and
                  Huan Sun and
                  Yu Su and
                  Wenhu Chen},
  title        = {MAmmoTH: Building Math Generalist Models through Hybrid Instruction
                  Tuning},
  journal      = {CoRR},
  volume       = {abs/2309.05653},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.05653},
  doi          = {10.48550/ARXIV.2309.05653},
  eprinttype    = {arXiv},
  eprint       = {2309.05653},
  timestamp    = {Fri, 15 Sep 2023 12:26:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-05653.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{metamath,
  author       = {Longhui Yu and
                  Weisen Jiang and
                  Han Shi and
                  Jincheng Yu and
                  Zhengying Liu and
                  Yu Zhang and
                  James T. Kwok and
                  Zhenguo Li and
                  Adrian Weller and
                  Weiyang Liu},
  title        = {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2309.12284},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.12284},
  doi          = {10.48550/ARXIV.2309.12284},
  eprinttype    = {arXiv},
  eprint       = {2309.12284},
  timestamp    = {Mon, 25 Sep 2023 15:34:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-12284.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1145/2692916.2555258,
author = {Bauer, Michael and Treichler, Sean and Aiken, Alex},
title = {Singe: leveraging warp specialization for high performance on {GPUs}},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2692916.2555258},
doi = {10.1145/2692916.2555258},
abstract = {We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs. Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block. Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers. Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation. Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories. Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler. We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.},
journal = {SIGPLAN Not.},
month = feb,
pages = {119–130},
numpages = {12},
keywords = {DSL, GPU, warp specialization, warp-specializing compiler}
}

@inproceedings{warp-spec,
author = {Bauer, Michael and Treichler, Sean and Aiken, Alex},
title = {Singe: leveraging warp specialization for high performance on {GPUs}},
year = {2014},
isbn = {9781450326568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2555243.2555258},
doi = {10.1145/2555243.2555258},
abstract = {We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs. Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block. Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers. Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation. Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories. Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler. We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {119–130},
numpages = {12},
keywords = {DSL, GPU, warp specialization, warp-specializing compiler},
location = {Orlando, Florida, USA},
series = {PPoPP '14}
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{chatgpt,
    title = {Introducing {ChatGPT}} ,
    author = {OpenAI},
    url = {https://openai.com/blog/chatgpt},
    year = {2022}
}

@misc{haillm,
    title = {HAI-LLM: 高效且轻量的大模型训练工具} ,
    author = {High-flyer},
    institution = {High-flyer},
    url = {https://www.high-flyer.cn/en/blog/hai-llm},
    year = {2023}
}

@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{megatron2,
  title={Efficient large-scale language model training on {GPU} clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{megatron3,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  year={2023}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@misc{li2021ccpm,
      title={CCPM: A Chinese Classical Poetry Matching Dataset}, 
      author={Wenhao Li and Fanchao Qi and Maosong Sun and Xiaoyuan Yi and Jiarui Zhang},
      year={2021},
      eprint={2106.01979},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mihaylov2018suit,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{claude,
  title = {Introducing {Claude}},
  author = {Anthropic},
  institution = {Anthropic},
  url = {https://www.anthropic.com/index/introducing-claude},
  year={2023}
}

@misc{bard,
  title = {An important next step on our {AI} journey},
  author = {Google},
  url = {https://blog.google/technology/ai/bard-google-ai-search-updates/},
  year={2023}
}

@misc{gemini,
  title = {Introducing Gemini: our largest and most capable AI model},
  author = {Google},
  url = {https://blog.google/technology/ai/google-gemini-ai/},
  year={2023}
}

@misc{gemini1_5,
  title = {Our next-generation model: Gemini 1.5},
  author = {Google},
  url = {https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024},
  year={2024}
}

@misc{sun2019investigating,
      title={Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension}, 
      author={Kai Sun and Dian Yu and Dong Yu and Claire Cardie},
      year={2019},
      eprint={1904.09679},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{cui-etal-2019-span,
    title = "A Span-Extraction Dataset for {C}hinese Machine Reading Comprehension",
    author = "Cui, Yiming  and
      Liu, Ting  and
      Che, Wanxiang  and
      Xiao, Li  and
      Chen, Zhipeng  and
      Ma, Wentao  and
      Wang, Shijin  and
      Hu, Guoping",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1600",
    doi = "10.18653/v1/D19-1600",
    pages = "5883--5889",
    abstract = "Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several baseline systems as well as anonymous submissions for demonstrating the difficulties in this dataset. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research. Resources are available: \url{https://github.com/ymcui/cmrc2018}",
}

@misc{sakaguchi2019winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023cmath,
      title={CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?}, 
      author={Tianwen Wei and Jian Luan and Wei Liu and Shuang Dong and Bin Wang},
      year={2023},
      eprint={2306.16636},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{clue,
  author       = {Liang Xu and
                  Hai Hu and
                  Xuanwei Zhang and
                  Lu Li and
                  Chenjie Cao and
                  Yudong Li and
                  Yechen Xu and
                  Kai Sun and
                  Dian Yu and
                  Cong Yu and
                  Yin Tian and
                  Qianqian Dong and
                  Weitang Liu and
                  Bo Shi and
                  Yiming Cui and
                  Junyi Li and
                  Jun Zeng and
                  Rongzhao Wang and
                  Weijian Xie and
                  Yanting Li and
                  Yina Patterson and
                  Zuoyu Tian and
                  Yiwen Zhang and
                  He Zhou and
                  Shaoweihua Liu and
                  Zhe Zhao and
                  Qipeng Zhao and
                  Cong Yue and
                  Xinrui Zhang and
                  Zhengliang Yang and
                  Kyle Richardson and
                  Zhenzhong Lan},
  editor       = {Donia Scott and
                  N{\'{u}}ria Bel and
                  Chengqing Zong},
  title        = {{CLUE:} {A} Chinese Language Understanding Evaluation Benchmark},
  booktitle    = {Proceedings of the 28th International Conference on Computational
                  Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13,
                  2020},
  pages        = {4762--4772},
  publisher    = {International Committee on Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.coling-main.419},
  doi          = {10.18653/V1/2020.COLING-MAIN.419},
  timestamp    = {Tue, 12 Dec 2023 12:16:23 +0100},
  biburl       = {https://dblp.org/rec/conf/coling/XuHZLCLXSYYTDLS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{cmmlu,
  title={{CMMLU}: Measuring massive multitask language understanding in {Chinese}},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@inproceedings{chid,
  author       = {Chujie Zheng and
                  Minlie Huang and
                  Aixin Sun},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {ChID: {A} Large-scale Chinese IDiom Dataset for Cloze Test},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {778--787},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1075},
  doi          = {10.18653/V1/P19-1075},
  timestamp    = {Sun, 02 Oct 2022 15:53:46 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZhengHS19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{race,
  author       = {Guokun Lai and
                  Qizhe Xie and
                  Hanxiao Liu and
                  Yiming Yang and
                  Eduard H. Hovy},
  editor       = {Martha Palmer and
                  Rebecca Hwa and
                  Sebastian Riedel},
  title        = {{RACE:} Large-scale ReAding Comprehension Dataset From Examinations},
  booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
                  9-11, 2017},
  pages        = {785--794},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/d17-1082},
  doi          = {10.18653/V1/D17-1082},
  timestamp    = {Fri, 06 Aug 2021 00:40:22 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/LaiXLYH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{drop,
  author       = {Dheeru Dua and
                  Yizhong Wang and
                  Pradeep Dasigi and
                  Gabriel Stanovsky and
                  Sameer Singh and
                  Matt Gardner},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{DROP:} {A} Reading Comprehension Benchmark Requiring Discrete Reasoning
                  Over Paragraphs},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2368--2378},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1246},
  doi          = {10.18653/V1/N19-1246},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DuaWDSS019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ceval,
  title={{C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and others},
  journal={arXiv preprint arXiv:2305.08322},
  year={2023}
}

@article{llama,
  title={{LLaMA}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288}
}
@misc{QwQ,
  title={QwQ: Reflect Deeply on the Boundaries of the Unknown},
  author={Qwen},
  year={2024},
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/}
}
@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@misc{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}
@article{code_llama,
  title={Code {Llama}: Open foundation models for code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@article{pile,
  title={The {Pile}: An {800GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{agieval,
  author       = {Wanjun Zhong and
                  Ruixiang Cui and
                  Yiduo Guo and
                  Yaobo Liang and
                  Shuai Lu and
                  Yanlin Wang and
                  Amin Saied and
                  Weizhu Chen and
                  Nan Duan},
  title        = {{AGIEval}: {A} Human-Centric Benchmark for Evaluating Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.06364},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.06364},
  doi          = {10.48550/arXiv.2304.06364},
  eprinttype    = {arXiv},
  eprint       = {2304.06364},
  timestamp    = {Wed, 19 Apr 2023 12:42:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-06364.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{codex,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  volume       = {abs/2107.03374},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03374},
  eprinttype    = {arXiv},
  eprint       = {2107.03374},
}

@misc{gu2024cruxeval,
      title={CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution}, 
      author={Alex Gu and Baptiste Rozière and Hugh Leather and Armando Solar-Lezama and Gabriel Synnaeve and Sida I. Wang},
      year={2024},
      eprint={2401.03065},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lin2022truthfulqa,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hellaswag,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1472},
  doi          = {10.18653/v1/p19-1472},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/aaai.v34i05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{siqa,
  author       = {Maarten Sap and
                  Hannah Rashkin and
                  Derek Chen and
                  Ronan Le Bras and
                  Yejin Choi},
  title        = {{SocialIQA}: Commonsense Reasoning about Social Interactions},
  journal      = {CoRR},
  volume       = {abs/1904.09728},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09728},
  eprinttype    = {arXiv},
  eprint       = {1904.09728},
  timestamp    = {Sat, 29 Apr 2023 10:09:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09728.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gaokao-bench,
  author       = {Xiaotian Zhang and
                  Chunyang Li and
                  Yi Zong and
                  Zhengyu Ying and
                  Liang He and
                  Xipeng Qiu},
  title        = {Evaluating the Performance of Large Language Models on {GAOKAO} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2305.12474},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.12474},
  doi          = {10.48550/arXiv.2305.12474},
  eprinttype    = {arXiv},
  eprint       = {2305.12474},
  timestamp    = {Fri, 26 May 2023 11:29:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-12474.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mathshepherd,
  title={Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, RX and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y and Sui, Zhifang},
  journal={arXiv preprint arXiv:2312.08935},
  year={2023}
}
@article{deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Y and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
@article{minicpm,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{arc,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
                  Challenge},
  journal      = {CoRR},
  volume       = {abs/1803.05457},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.05457},
  eprinttype    = {arXiv},
  eprint       = {1803.05457},
  timestamp    = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-05457.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{alpaca2.0,
  title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{li2024crowdsourced,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}
@inproceedings{boolq,
  author       = {Christopher Clark and
                  Kenton Lee and
                  Ming{-}Wei Chang and
                  Tom Kwiatkowski and
                  Michael Collins and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2924--2936},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1300},
  doi          = {10.18653/v1/n19-1300},
  timestamp    = {Tue, 16 Aug 2022 23:04:27 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/ClarkLCK0T19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{IFeval,
  title={Instruction-Following Evaluation for Large Language Models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@misc{chatglm2,
    title={{ChatGLM2-6B}: An Open Bilingual Chat {LLM}},
    author={{ChatGLM2 Team}},
    url = {https://github.com/THUDM/ChatGLM2-6B},
    year={2023}
}

@inproceedings{glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Qwen},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@techreport{baichuan2,
    author = {Aiyuan Yang and Bin Xiao and Bingning Wang and Borong Zhang and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and Fei Deng and Feng Wang and Feng Liu and Guangwei Ai and Guosheng Dong and Haizhou Zhao and Hang Xu and Haoze Sun and Hongda Zhang and Hui Liu and Jiaming Ji and Jian Xie and Juntao Dai and Kun Fang and Lei Su and Liang Song and Lifeng Liu and Liyun Ru and Luyao Ma and Mang Wang and Mickel Liu and MingAn Lin and Nuolan Nie and Peidong Guo and Ruiyang Sun and Tao Zhang and Tianpeng Li and Tianyu Li and Wei Cheng and Weipeng Chen and Xiangrong Zeng and Xiaochuan Wang and Xiaoxi Chen and Xin Men and Xin Yu and Xuehai Pan and Yanjun Shen and Yiding Wang and Yiyu Li and Youxin Jiang and Yuchen Gao and Yupeng Zhang and Zenan Zhou and Zhiying Wu},
    title = {Baichuan 2: Open Large-scale Language Models},
    institution = {Baichuan Inc.},
    year = {2023},
    url = {https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf}
}

@inproceedings{commonsenseqa,
  author       = {Alon Talmor and
                  Jonathan Herzig and
                  Nicholas Lourie and
                  Jonathan Berant},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{CommonsenseQA}: {A} Question Answering Challenge Targeting Commonsense
                  Knowledge},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4149--4158},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1421},
  doi          = {10.18653/v1/n19-1421},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/TalmorHLB19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{pipedream,
  title={PipeDream: Fast and Efficient Pipeline Parallel DNN Training}, 
  author={Aaron Harlap and Deepak Narayanan and Amar Phanishayee and Vivek Seshadri and Nikhil Devanur and Greg Ganger and Phil Gibbons},
  year={2018},
  eprint={1806.03377},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  url={https://arxiv.org/abs/1806.03377}, 
}

@misc{zerobubble,
  title={Zero Bubble Pipeline Parallelism}, 
  author={Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
  year={2023},
  eprint={2401.10241},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  url={https://arxiv.org/abs/2401.10241}, 
}

@inproceedings{chimera, series={SC ’21},
   title={Chimera: efficiently training large-scale neural networks with bidirectional pipelines},
   url={http://dx.doi.org/10.1145/3458817.3476145},
   DOI={10.1145/3458817.3476145},
   booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
   publisher={ACM},
   author={Li, Shigang and Hoefler, Torsten},
   year={2021},
   month=nov, pages={1–14},
   collection={SC ’21} }


@article{adamW,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{roberta,
  title={{RoBERTa}: A robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{scalinglaw,
  author       = {Jared Kaplan and
                  Sam McCandlish and
                  Tom Henighan and
                  Tom B. Brown and
                  Benjamin Chess and
                  Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Jeffrey Wu and
                  Dario Amodei},
  title        = {Scaling Laws for Neural Language Models},
  journal      = {CoRR},
  volume       = {abs/2001.08361},
  year         = {2020},
  url          = {https://arxiv.org/abs/2001.08361},
  eprinttype    = {arXiv},
  eprint       = {2001.08361},
  timestamp    = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chinchilla,
  author       = {Jordan Hoffmann and
                  Sebastian Borgeaud and
                  Arthur Mensch and
                  Elena Buchatskaya and
                  Trevor Cai and
                  Eliza Rutherford and
                  Diego de Las Casas and
                  Lisa Anne Hendricks and
                  Johannes Welbl and
                  Aidan Clark and
                  Tom Hennigan and
                  Eric Noland and
                  Katie Millican and
                  George van den Driessche and
                  Bogdan Damoc and
                  Aurelia Guy and
                  Simon Osindero and
                  Karen Simonyan and
                  Erich Elsen and
                  Jack W. Rae and
                  Oriol Vinyals and
                  Laurent Sifre},
  title        = {Training Compute-Optimal Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2203.15556},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.15556},
  doi          = {10.48550/ARXIV.2203.15556},
  eprinttype    = {arXiv},
  eprint       = {2203.15556},
  timestamp    = {Mon, 04 Apr 2022 18:01:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-15556.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{naturalquestions,
  author       = {Tom Kwiatkowski and
                  Jennimaria Palomaki and
                  Olivia Redfield and
                  Michael Collins and
                  Ankur P. Parikh and
                  Chris Alberti and
                  Danielle Epstein and
                  Illia Polosukhin and
                  Jacob Devlin and
                  Kenton Lee and
                  Kristina Toutanova and
                  Llion Jones and
                  Matthew Kelcey and
                  Ming{-}Wei Chang and
                  Andrew M. Dai and
                  Jakob Uszkoreit and
                  Quoc Le and
                  Slav Petrov},
  title        = {Natural Questions: a Benchmark for Question Answering Research},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {7},
  pages        = {452--466},
  year         = {2019},
  url          = {https://doi.org/10.1162/tacl\_a\_00276},
  doi          = {10.1162/tacl\_a\_00276},
  timestamp    = {Tue, 16 Aug 2022 23:05:11 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/KwiatkowskiPRCP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{tsinghua_safety,
      title={Safety Assessment of Chinese Large Language Models}, 
      author={Hao Sun and Zhexin Zhang and Jiawen Deng and Jiale Cheng and Minlie Huang},
      journal={arXiv preprint arXiv:2304.10436},
      year={2023}
}

@article{align_bench,
  author       = {Xiao Liu and
                  Xuanyu Lei and
                  Shengyuan Wang and
                  Yue Huang and
                  Zhuoer Feng and
                  Bosi Wen and
                  Jiale Cheng and
                  Pei Ke and
                  Yifan Xu and
                  Weng Lam Tam and
                  Xiaohan Zhang and
                  Lichao Sun and
                  Hongning Wang and
                  Jing Zhang and
                  Minlie Huang and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {AlignBench: Benchmarking Chinese Alignment of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.18743},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.18743},
  doi          = {10.48550/ARXIV.2311.18743},
  eprinttype    = {arXiv},
  eprint       = {2311.18743},
  timestamp    = {Tue, 05 Dec 2023 14:40:42 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-18743.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{switch,
  author    = {William Fedus and
               Barret Zoph and
               Noam Shazeer},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal   = {CoRR},
  volume    = {abs/2101.03961},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.03961},
  archivePrefix = {arXiv},
  eprint    = {2101.03961},
}

@article{dpo,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
}

@article{gpt4,
  title={{GPT4} technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{gpt4o,
  title = {Hello {GPT-4o}},
  author = {OpenAI},
  url = {https://openai.com/index/hello-gpt-4o/},
  year={2024}
}

@misc{claude35sonnet,
  title = {Claude 3.5 Sonnet},
  author = {Anthropic},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  year={2024}
}

@article{yi,
  title={Yi: Open foundation models by 01.ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tulu2,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
}

@misc{mtbench,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
}

@article{donotanswer,
  author       = {Yuxia Wang and
                  Haonan Li and
                  Xudong Han and
                  Preslav Nakov and
                  Timothy Baldwin},
  title        = {Do-Not-Answer: {A} Dataset for Evaluating Safeguards in LLMs},
  journal      = {CoRR},
  volume       = {abs/2308.13387},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.13387},
  doi          = {10.48550/ARXIV.2308.13387},
  eprinttype    = {arXiv},
  eprint       = {2308.13387},
  timestamp    = {Fri, 01 Sep 2023 14:25:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-13387.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@misc{hftokenizers,
    title={{Tokenizers}: Fast State-of-the-Art Tokenizers optimized for Research and Production},
    author={{Huggingface Team}},
    url = {https://github.com/huggingface/tokenizers},
    year={2019}
}

@article{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}

@software{redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@article{longpre2023pretrainer,
  title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \& Toxicity},
  author={Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}


% Scaling 
@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}

@article{shallue2019measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={112},
  pages={1--49},
  year={2019}
}
@inproceedings{AIME2024,
  author={MAA},
  title = {American Invitational Mathematics Examination - AIME},
  booktitle = {American Invitational Mathematics Examination - AIME 2024},
  year = {2024},
  month = {February},
  url = {https://maa.org/math-competitions/american-invitational-mathematics-examination-aime}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% arch
@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{moe,
  author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1ckMDqlg},
}

@article{deepseekmoe,
  author       = {Damai Dai and
                  Chengqi Deng and
                  Chenggang Zhao and
                  R. X. Xu and
                  Huazuo Gao and
                  Deli Chen and
                  Jiashi Li and
                  Wangding Zeng and
                  Xingkai Yu and
                  Y. Wu and
                  Zhenda Xie and
                  Y. K. Li and
                  Panpan Huang and
                  Fuli Luo and
                  Chong Ruan and
                  Zhifang Sui and
                  Wenfeng Liang},
  title        = {DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.06066},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.06066},
}

@inproceedings{gshard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=qrwe7XHTmYb},
}

@article{dsvi,
  author       = {DeepSeek-AI},
  title        = {DeepSeek {LLM:} Scaling Open-Source Language Models with Longtermism},
  journal      = {CoRR},
  volume       = {abs/2401.02954},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.02954},
}

@inproceedings{bpr,
  author       = {Carlos Riquelme and
                  Joan Puigcerver and
                  Basil Mustafa and
                  Maxim Neumann and
                  Rodolphe Jenatton and
                  Andr{\'{e}} Susano Pinto and
                  Daniel Keysers and
                  Neil Houlsby},
  title        = {Scaling Vision with Sparse Mixture of Experts},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021},
  pages        = {8583--8595},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html},
}

@article{kv_quant,
  author       = {Coleman Hooper and
                  Sehoon Kim and
                  Hiva Mohammadzadeh and
                  Michael W. Mahoney and
                  Yakun Sophia Shao and
                  Kurt Keutzer and
                  Amir Gholami},
  title        = {KVQuant: Towards 10 Million Context Length {LLM} Inference with {KV}
                  Cache Quantization},
  journal      = {CoRR},
  volume       = {abs/2401.18079},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.18079},
}

@article{atom,
  author       = {Yilong Zhao and
                  Chien{-}Yu Lin and
                  Kan Zhu and
                  Zihao Ye and
                  Lequn Chen and
                  Size Zheng and
                  Luis Ceze and
                  Arvind Krishnamurthy and
                  Tianqi Chen and
                  Baris Kasikci},
  title        = {Atom: Low-bit Quantization for Efficient and Accurate {LLM} Serving},
  journal      = {CoRR},
  volume       = {abs/2310.19102},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.19102},
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{wang2023making,
  title={Making large language models better reasoners with alignment},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Song, Feifan and Lin, Binghuai and Cao, Yunbo and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2309.02144},
  year={2023}
}

@article{wang2023math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, RX and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y and Sui, Zhifang},
  journal={CoRR, abs/2312.08935},
  year={2023}
}

@misc{shoeybi2020megatronlm,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
  author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  year={2020},
  eprint={1909.08053},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{bai2024longbench2,
  title={{LongBench} v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks}, 
  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal={arXiv preprint arXiv:2412.15204},
  year={2024}
}

@article{frames,
  author       = {Satyapriya Krishna and
                  Kalpesh Krishna and
                  Anhad Mohananey and
                  Steven Schwarcz and
                  Adam Stambler and
                  Shyam Upadhyay and
                  Manaal Faruqui},
  title        = {Fact, Fetch, and Reason: {A} Unified Evaluation of Retrieval-Augmented
                  Generation},
  journal      = {CoRR},
  volume       = {abs/2409.12941},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.12941},
  doi          = {10.48550/ARXIV.2409.12941},
  eprinttype    = {arXiv},
  eprint       = {2409.12941},
  timestamp    = {Thu, 17 Oct 2024 12:28:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-12941.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@software{Lin_ZeroEval_A_Unified_2024,
    author = {Lin, Bill Yuchen},
    month = jul,
    title = {{ZeroEval: A Unified Framework for Evaluating Language Models}},
    url = {https://github.com/WildEval/ZeroEval},
    year = {2024}
}
@misc{llama3_1_405b,
  title={Llama 3.1 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md}
}

@misc{qwen2_5,
  title={Qwen2.5: A Party of Foundation Models},
  author={Qwen},
  year={2024},
  url = {https://qwenlm.github.io/blog/qwen2.5}
}
@misc{swe_verified,
  title={Introducing {SWE}-bench Verified
We’re releasing a human-validated subset of SWE-bench that more},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/introducing-swe-bench-verified/}
}
@article{gpqa,
  title={{GPQA}: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@misc{aider,
  title={Aider {LLM} Leaderboard},
  author={Aider-AI},
  year={2024},
  url = {https://aider.chat/docs/leaderboards/}
}


@misc{simpleqa,
  title={Introducing {SimpleQA}},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/introducing-simpleqa/}
}
@article{csimpleqa,
  title={Chinese simpleqa: A chinese factuality evaluation for large language models},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others},
  journal={arXiv preprint arXiv:2411.07140},
  year={2024}
}


@article{qwen2,
  author       = {Qwen},
  title        = {Qwen2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2407.10671},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.10671},
}

@misc{mixtral8x22b,
  title = {Cheaper, Better, Faster, Stronger: Continuing to push the frontier of AI and making it accessible to all},
  author = {Mistral},
  url = {https://mistral.ai/news/mixtral-8x22b},
  year={2024}
}

@misc{qwen1_5,
  title = {Introducing {Qwen1.5}},
  author = {Qwen},
  url = {https://qwenlm.github.io/blog/qwen1.5},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{dsvii,
  author       = {DeepSeek-AI},
  title        = {DeepSeek-V2: {A} Strong, Economical, and Efficient Mixture-of-Experts
                  Language Model},
  journal      = {CoRR},
  volume       = {abs/2405.04434},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.04434},
}

@article{noaux_tc,
  author       = {Lean Wang and
                  Huazuo Gao and
                  Chenggang Zhao and
                  Xu Sun and
                  Damai Dai},
  title        = {Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts},
  journal      = {CoRR},
  volume       = {abs/2408.15664},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.15664},
}

@article{dscodervi,
  author       = {Daya Guo and
                  Qihao Zhu and
                  Dejian Yang and
                  Zhenda Xie and
                  Kai Dong and
                  Wentao Zhang and
                  Guanting Chen and
                  Xiao Bi and
                  Y. Wu and
                  Y. K. Li and
                  Fuli Luo and
                  Yingfei Xiong and
                  Wenfeng Liang},
  title        = {DeepSeek-Coder: When the Large Language Model Meets Programming -
                  The Rise of Code Intelligence},
  journal      = {CoRR},
  volume       = {abs/2401.14196},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.14196},
}

@article{dscodervii,
  author       = {DeepSeek-AI},
  title        = {DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in
                  Code Intelligence},
  journal      = {CoRR},
  volume       = {abs/2406.11931},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.11931},
}

%%%% Low-bit related
@article{bf16train,
  title={A study of BFLOAT16 for deep learning training},
  author={Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal={arXiv preprint arXiv:1905.12322},
  year={2019}
}

@misc{transformerengine,
  author = {NVIDIA},
  title = {{TransformerEngine}},
  year = {2024},
  url = {https://github.com/NVIDIA/TransformerEngine},
  note = {Accessed: 2024-11-19}
}

@inproceedings{fp16train,
  title={Mixed precision training},
  author={Narang, Sharan and Diamos, Gregory and Elsen, Erich and Micikevicius, Paulius and Alben, Jonah and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  booktitle={Int. Conf. on Learning Representation},
  year={2017}
}

@article{fp8lm,
  title={{FP8-LM}: Training {FP8} large language models},
  author={Peng, Houwen and Wu, Kan and Wei, Yixuan and Zhao, Guoshuai and Yang, Yuxiang and Liu, Ze and Xiong, Yifan and Yang, Ziyue and Ni, Bolin and Hu, Jingcheng and others},
  journal={arXiv preprint arXiv:2310.18313},
  year={2023}
}

@article{scalefp8train,
  title={Scaling {FP8} training to trillion-token LLMs},
  author={Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2409.12517},
  year={2024}
}

@article{xi2024coat,
  title={COAT: Compressing Optimizer states and Activation for Memory-Efficient {FP8} Training},
  author={Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song},
  journal={arXiv preprint arXiv:2410.19313},
  year={2024}
}

@article{xi2024jetfire,
  title={Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization},
  author={Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and Teh, Kai Jun and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2403.12422},
  year={2024}
}


@article{llm.int8,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{switchback,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={10271--10298},
  year={2023}
}

@article{int4train,
  title={Training transformers with 4-bit integers},
  author={Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={49146--49168},
  year={2023}
}

@article{rouhani2023microscaling,
  title={Microscaling data formats for deep learning},
  author={Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and others},
  journal={arXiv preprint arXiv:2310.10537},
  year={2023}
}

@article{sunfp42020,
  title={Ultra-low precision 4-bit training of deep neural networks},
  author={Sun, Xiao and Wang, Naigang and Chen, Chia-Yu and Ni, Jiamin and Agrawal, Ankur and Cui, Xiaodong and Venkataramani, Swagath and El Maghraoui, Kaoutar and Srinivasan, Vijayalakshmi Viji and Gopalakrishnan, Kailash},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1796--1807},
  year={2020}
}

@article{bitnet,
  title={The era of 1-bit llms: All large language models are in 1.58 bits},
  author={Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  journal={arXiv preprint arXiv:2402.17764},
  year={2024}
}

@inproceedings{tpu,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@misc{h100,
  author = {NVIDIA},
  title = {{NVIDIA H100 tensor core GPU}},
  year = {2024},
  url = {https://www.nvidia.com/en-us/ data-center/h100/},
  note = {Accessed: 2024-11-19}
}

@article{fp8format,
  title={{FP8} formats for deep learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@article{hfp8,
  title={Hybrid 8-bit floating point ({HFP8}) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{8-bit-numerical,
  title={8-bit numerical formats for deep neural networks},
  author={Noune, Badreddine and Jones, Philip and Justus, Daniel and Masters, Dominic and Luschi, Carlo},
  journal={arXiv preprint arXiv:2206.02915},
  year={2022}
}

@article{hifp8format,
  title={Ascend {HiFloat8} Format for Deep Learning},
  author={Luo, Yuanyong and Zhang, Zhongxing and Wu, Richard and Liu, Hu and Jin, Ying and Zheng, Kai and Wang, Minmin and He, Zhanying and Hu, Guipeng and Chen, Luyao and others},
  journal={arXiv preprint arXiv:2409.16626},
  year={2024}
}

@article{microscaling,
  title={Microscaling data formats for deep learning},
  author={Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and others},
  journal={arXiv preprint arXiv:2310.10537},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{understandoutlier,
  title={Understanding and Minimising Outlier Features in Transformer Training},
  author={He, Bobby and Noci, Lorenzo and Paliotta, Daniele and Schlag, Imanol and Hofmann, Thomas},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
}

@article{massiveoutlier,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@software{Thakkar_CUTLASS_2023,
author = {Thakkar, Vijay and Ramani, Pradeep and Cecka, Cris and Shivam, Aniket and Lu, Honghao and Yan, Ethan and Kosaian, Jack and Hoemmen, Mark and Wu, Haicheng and Kerr, Andrew and Nicely, Matt and Merrill, Duane and Blasig, Dustyn and Qiao, Fengqi and Majcher, Piotr and Springer, Paul and Hohnerbach, Markus and Wang, Jin and Gupta, Manish},
license = {BSD-3-Clause},
month = jan,
title = {{CUTLASS}},
url = {https://github.com/NVIDIA/cutlass},
version = {3.0.0},
year = {2023}
}
%%%%

@article{livecodebench,
  author       = {Naman Jain and
                  King Han and
                  Alex Gu and
                  Wen{-}Ding Li and
                  Fanjia Yan and
                  Tianjun Zhang and
                  Sida Wang and
                  Armando Solar{-}Lezama and
                  Koushik Sen and
                  Ion Stoica},
  title        = {LiveCodeBench: Holistic and Contamination Free Evaluation of Large
                  Language Models for Code},
  journal      = {CoRR},
  volume       = {abs/2403.07974},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.07974},
}

@article{mmlu_redux,
  author       = {Aryo Pradipta Gema and
                  Joshua Ong Jun Leang and
                  Giwon Hong and
                  Alessio Devoto and
                  Alberto Carlo Maria Mancino and
                  Rohit Saxena and
                  Xuanli He and
                  Yu Zhao and
                  Xiaotang Du and
                  Mohammad Reza Ghasemi Madani and
                  Claire Barale and
                  Robert McHardy and
                  Joshua Harris and
                  Jean Kaddour and
                  Emile van Krieken and
                  Pasquale Minervini},
  title        = {Are We Done with MMLU?},
  journal      = {CoRR},
  volume       = {abs/2406.04127},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.04127},
}

@article{mmlu_pro,
  author       = {Yubo Wang and
                  Xueguang Ma and
                  Ge Zhang and
                  Yuansheng Ni and
                  Abhranil Chandra and
                  Shiguang Guo and
                  Weiming Ren and
                  Aaran Arulraj and
                  Xuan He and
                  Ziyan Jiang and
                  Tianle Li and
                  Max Ku and
                  Kai Wang and
                  Alex Zhuang and
                  Rongqi Fan and
                  Xiang Yue and
                  Wenhu Chen},
  title        = {MMLU-Pro: {A} More Robust and Challenging Multi-Task Language Understanding
                  Benchmark},
  journal      = {CoRR},
  volume       = {abs/2406.01574},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.01574},
}

@inproceedings{mgsm,
  author       = {Freda Shi and
                  Mirac Suzgun and
                  Markus Freitag and
                  Xuezhi Wang and
                  Suraj Srivats and
                  Soroush Vosoughi and
                  Hyung Won Chung and
                  Yi Tay and
                  Sebastian Ruder and
                  Denny Zhou and
                  Dipanjan Das and
                  Jason Wei},
  title        = {Language models are multilingual chain-of-thought reasoners},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=fR3wGCk-IXp},
}

@misc{mmmlu,
  title={Multilingual Massive Multitask Language Understanding (MMMLU)},
  author={OpenAI},
  year={2024},
  url = {https://huggingface.co/datasets/openai/MMMLU}
}

@inproceedings{meta_mtp,
  author       = {Fabian Gloeckle and
                  Badr Youbi Idrissi and
                  Baptiste Rozi{\`{e}}re and
                  David Lopez{-}Paz and
                  Gabriel Synnaeve},
  title        = {Better {\&} Faster Large Language Models via Multi-token Prediction},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=pEWAcejiU2},
}

@inproceedings{eagle,
  author       = {Yuhui Li and
                  Fangyun Wei and
                  Chao Zhang and
                  Hongyang Zhang},
  title        = {{EAGLE:} Speculative Sampling Requires Rethinking Feature Uncertainty},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=1NdN7eXyb4},
}

@inproceedings{speculative_xhm,
  author       = {Heming Xia and
                  Tao Ge and
                  Peiyi Wang and
                  Si{-}Qing Chen and
                  Furu Wei and
                  Zhifang Sui},
  title        = {Speculative Decoding: Exploiting Speculative Execution for Accelerating
                  Seq2seq Generation},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2023, Singapore, December 6-10, 2023},
  pages        = {3909--3925},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-emnlp.257},
}

@inproceedings{speculative_google,
  author       = {Yaniv Leviathan and
                  Matan Kalman and
                  Yossi Matias},
  title        = {Fast Inference from Transformers via Speculative Decoding},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {19274--19286},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/leviathan23a.html},
}

@article{Ding2024FewerTI,
  title={Fewer Truncations Improve Language Modeling},
  author={Hantian Ding and Zijian Wang and Giovanni Paolini and Varun Kumar and Anoop Deoras and Dan Roth and Stefano Soatto},
  journal={arXiv preprint arXiv:2404.10830},
  year={2024}
}

@article{shibata1999byte,
  title={Byte pair encoding: A text compression scheme that accelerates pattern matching},
  author={Shibata, Yusuxke and Kida, Takuya and Fukamachi, Shuichi and Takeda, Masayuki and Shinohara, Ayumi and Shinohara, Takeshi and Arikawa, Setsuo},
  year={1999},
  publisher={Technical Report DOI-TR-161, Department of Informatics, Kyushu University}
}

@misc{tokenboundary,
    title = {The Art of Prompt Design: Prompt Boundaries and Token Healing} ,
    author = {Scott Lundberg},
    url = {https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38},
    year = {2023}
}

@misc{nvidia_tensor_cores,
  author       = {NVIDIA},
  title        = {Blackwell Architecture},
  howpublished = {\url{https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/}},
  year         = {2024},
}

@article{lambert2024rewardbench,
  title={Rewardbench: Evaluating reward models for language modeling},
  author={Lambert, Nathan and Pyatkin, Valentina and Morrison, Jacob and Miranda, LJ and Lin, Bill Yuchen and Chandu, Khyathi and Dziri, Nouha and Kumar, Sachin and Zick, Tom and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2403.13787},
  year={2024}
}

@misc{nvidia_ibgda,
  author       = {NVIDIA},
  title        = {Improving Network Performance of {HPC} Systems Using {NVIDIA Magnum IO NVSHMEM} and {GPUDirect Async}},
  howpublished = {\url{https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async}},
  year         = {2022},
}


@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{nvsharp,
  title={Scalable hierarchical aggregation protocol ({SHArP}): A hardware architecture for efficient data reduction},
  author={Graham, Richard L and Bureddy, Devendar and Lui, Pak and Rosenstock, Hal and Shainer, Gilad and Bloch, Gil and Goldenerg, Dror and Dubman, Mike and Kotchubievsky, Sasha and Koushnir, Vladimir and others},
  booktitle={2016 First International Workshop on Communication Optimizations in HPC (COMHPC)},
  pages={1--10},
  year={2016},
  organization={IEEE}
}

@article{agentless,
  author    = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  title     = {Agentless: Demystifying LLM-based Software Engineering Agents},
  year      = {2024},
  journal   = {arXiv preprint},
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@Article{AlphaGeometryTrinh2024,
  author  = {Trinh, Trieu and Wu, Yuhuai and Le, Quoc and He, He and Luong, Thang},
  journal = {Nature},
  title   = {Solving Olympiad Geometry without Human Demonstrations},
  year    = {2024},
  doi     = {10.1038/s41586-023-06747-5}
}

@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}

@misc{feng2024alphazeroliketreesearchguidelarge,
      title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training}, 
      author={Xidong Feng and Ziyu Wan and Muning Wen and Stephen Marcus McAleer and Ying Wen and Weinan Zhang and Jun Wang},
      year={2024},
      eprint={2309.17179},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.17179}, 
}

@misc{xin2024deepseekproverv15harnessingproofassistant,
      title={DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search}, 
      author={Huajian Xin and Z. Z. Ren and Junxiao Song and Zhihong Shao and Wanjia Zhao and Haocheng Wang and Bo Liu and Liyue Zhang and Xuan Lu and Qiushi Du and Wenjun Gao and Qihao Zhu and Dejian Yang and Zhibin Gou and Z. F. Wu and Fuli Luo and Chong Ruan},
      year={2024},
      eprint={2408.08152},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08152}, 
}

@misc{gao2022scalinglawsrewardmodel,
      title={Scaling Laws for Reward Model Overoptimization}, 
      author={Leo Gao and John Schulman and Jacob Hilton},
      year={2022},
      eprint={2210.10760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.10760}, 
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}


@article{alphago,
  author       = {David Silver and
                  Julian Schrittwieser and
                  Karen Simonyan and
                  Ioannis Antonoglou and
                  Aja Huang and
                  Arthur Guez and
                  Thomas Hubert and
                  Lucas Baker and
                  Matthew Lai and
                  Adrian Bolton and
                  Yutian Chen and
                  Timothy P. Lillicrap and
                  Fan Hui and
                  Laurent Sifre and
                  George van den Driessche and
                  Thore Graepel and
                  Demis Hassabis},
  title        = {Mastering the game of Go without human knowledge},
  journal      = {Nat.},
  volume       = {550},
  number       = {7676},
  pages        = {354--359},
  year         = {2017},
  url          = {https://doi.org/10.1038/nature24270},
  doi          = {10.1038/NATURE24270},
  timestamp    = {Mon, 22 Jul 2024 08:26:52 +0200},
  biburl       = {https://dblp.org/rec/journals/nature/SilverSSAHGHBLB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{alphazero,
  author       = {David Silver and
                  Thomas Hubert and
                  Julian Schrittwieser and
                  Ioannis Antonoglou and
                  Matthew Lai and
                  Arthur Guez and
                  Marc Lanctot and
                  Laurent Sifre and
                  Dharshan Kumaran and
                  Thore Graepel and
                  Timothy P. Lillicrap and
                  Karen Simonyan and
                  Demis Hassabis},
  title        = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
                  Learning Algorithm},
  journal      = {CoRR},
  volume       = {abs/1712.01815},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.01815},
  eprinttype    = {arXiv},
  eprint       = {1712.01815},
  timestamp    = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}