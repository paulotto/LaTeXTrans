\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{llama3_1_405b}
AI@Meta.
\newblock Llama 3.1 model card, 2024.
\newblock URL \url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md}.

\bibitem[Anthropic(2024)]{claude35sonnet}
Anthropic.
\newblock Claude 3.5 sonnet, 2024.
\newblock URL \url{https://www.anthropic.com/news/claude-3-5-sonnet}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{codex}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger, M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder, M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P. Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert{-}Voss, W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji, S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra, E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer, P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and W.~Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{llama3}
A.~Dubey, A.~Jauhri, A.~Pandey, A.~Kadian, A.~Al-Dahle, A.~Letman, A.~Mathur, A.~Schelten, A.~Yang, A.~Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{alpaca2.0}
Y.~Dubois, B.~Galambosi, P.~Liang, and T.~B. Hashimoto.
\newblock Length-controlled alpacaeval: A simple way to debias automatic evaluators.
\newblock \emph{arXiv preprint arXiv:2404.04475}, 2024.

\bibitem[Feng et~al.(2024)Feng, Wan, Wen, McAleer, Wen, Zhang, and Wang]{feng2024alphazeroliketreesearchguidelarge}
X.~Feng, Z.~Wan, M.~Wen, S.~M. McAleer, Y.~Wen, W.~Zhang, and J.~Wang.
\newblock Alphazero-like tree-search can guide large language model decoding and training, 2024.
\newblock URL \url{https://arxiv.org/abs/2309.17179}.

\bibitem[Gao et~al.(2022)Gao, Schulman, and Hilton]{gao2022scalinglawsrewardmodel}
L.~Gao, J.~Schulman, and J.~Hilton.
\newblock Scaling laws for reward model overoptimization, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.10760}.

\bibitem[Gema et~al.(2024)Gema, Leang, Hong, Devoto, Mancino, Saxena, He, Zhao, Du, Madani, Barale, McHardy, Harris, Kaddour, van Krieken, and Minervini]{mmlu_redux}
A.~P. Gema, J.~O.~J. Leang, G.~Hong, A.~Devoto, A.~C.~M. Mancino, R.~Saxena, X.~He, Y.~Zhao, X.~Du, M.~R.~G. Madani, C.~Barale, R.~McHardy, J.~Harris, J.~Kaddour, E.~van Krieken, and P.~Minervini.
\newblock Are we done with mmlu?
\newblock \emph{CoRR}, abs/2406.04127, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.04127}.

\bibitem[Google(2024)]{gemini1_5}
Google.
\newblock Our next-generation model: Gemini 1.5, 2024.
\newblock URL \url{https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024}.

\bibitem[He et~al.(2024)He, Li, Liu, Tan, Wang, Huang, Bu, Guo, Hu, Zheng, et~al.]{csimpleqa}
Y.~He, S.~Li, J.~Liu, Y.~Tan, W.~Wang, H.~Huang, X.~Bu, H.~Guo, C.~Hu, B.~Zheng, et~al.
\newblock Chinese simpleqa: A chinese factuality evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2411.07140}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, et~al.]{ceval}
Y.~Huang, Y.~Bai, Z.~Zhu, J.~Zhang, J.~Zhang, T.~Su, J.~Liu, C.~Lv, Y.~Zhang, J.~Lei, et~al.
\newblock {C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}, 2023.

\bibitem[Jain et~al.(2024)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar{-}Lezama, Sen, and Stoica]{livecodebench}
N.~Jain, K.~Han, A.~Gu, W.~Li, F.~Yan, T.~Zhang, S.~Wang, A.~Solar{-}Lezama, K.~Sen, and I.~Stoica.
\newblock Livecodebench: Holistic and contamination free evaluation of large language models for code.
\newblock \emph{CoRR}, abs/2403.07974, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.07974}.

\bibitem[Krishna et~al.(2024)Krishna, Krishna, Mohananey, Schwarcz, Stambler, Upadhyay, and Faruqui]{frames}
S.~Krishna, K.~Krishna, A.~Mohananey, S.~Schwarcz, A.~Stambler, S.~Upadhyay, and M.~Faruqui.
\newblock Fact, fetch, and reason: {A} unified evaluation of retrieval-augmented generation.
\newblock \emph{CoRR}, abs/2409.12941, 2024.
\newblock \doi{10.48550/ARXIV.2409.12941}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.12941}.

\bibitem[Kumar et~al.(2024)Kumar, Zhuang, Agarwal, Su, Co-Reyes, Singh, Baumli, Iqbal, Bishop, Roelofs, et~al.]{kumar2024training}
A.~Kumar, V.~Zhuang, R.~Agarwal, Y.~Su, J.~D. Co-Reyes, A.~Singh, K.~Baumli, S.~Iqbal, C.~Bishop, R.~Roelofs, et~al.
\newblock Training language models to self-correct via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2409.12917}, 2024.

\bibitem[Li et~al.(2023)Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin]{cmmlu}
H.~Li, Y.~Zhang, F.~Koto, Y.~Yang, H.~Zhao, Y.~Gong, N.~Duan, and T.~Baldwin.
\newblock {CMMLU}: Measuring massive multitask language understanding in {Chinese}.
\newblock \emph{arXiv preprint arXiv:2306.09212}, 2023.

\bibitem[Li et~al.(2024)Li, Chiang, Frick, Dunlap, Wu, Zhu, Gonzalez, and Stoica]{li2024crowdsourced}
T.~Li, W.-L. Chiang, E.~Frick, L.~Dunlap, T.~Wu, B.~Zhu, J.~E. Gonzalez, and I.~Stoica.
\newblock From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.
\newblock \emph{arXiv preprint arXiv:2406.11939}, 2024.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
H.~Lightman, V.~Kosaraju, Y.~Burda, H.~Edwards, B.~Baker, T.~Lee, J.~Leike, J.~Schulman, I.~Sutskever, and K.~Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lin(2024)]{Lin_ZeroEval_A_Unified_2024}
B.~Y. Lin.
\newblock {ZeroEval: A Unified Framework for Evaluating Language Models}, July 2024.
\newblock URL \url{https://github.com/WildEval/ZeroEval}.

\bibitem[MAA(2024)]{AIME2024}
MAA.
\newblock American invitational mathematics examination - aime.
\newblock In \emph{American Invitational Mathematics Examination - AIME 2024}, February 2024.
\newblock URL \url{https://maa.org/math-competitions/american-invitational-mathematics-examination-aime}.

\bibitem[OpenAI(2024{\natexlab{a}})]{gpt4o}
OpenAI.
\newblock Hello {GPT-4o}, 2024{\natexlab{a}}.
\newblock URL \url{https://openai.com/index/hello-gpt-4o/}.

\bibitem[OpenAI(2024{\natexlab{b}})]{o1}
OpenAI.
\newblock Learning to reason with llms, 2024{\natexlab{b}}.
\newblock URL \url{https://openai.com/index/learning-to-reason-with-llms/}.

\bibitem[OpenAI(2024{\natexlab{c}})]{simpleqa}
OpenAI.
\newblock Introducing {SimpleQA}, 2024{\natexlab{c}}.
\newblock URL \url{https://openai.com/index/introducing-simpleqa/}.

\bibitem[OpenAI(2024{\natexlab{d}})]{swe_verified}
OpenAI.
\newblock Introducing {SWE}-bench verified weâ€™re releasing a human-validated subset of swe-bench that more, 2024{\natexlab{d}}.
\newblock URL \url{https://openai.com/index/introducing-swe-bench-verified/}.

\bibitem[Qwen(2024{\natexlab{a}})]{QwQ}
Qwen.
\newblock Qwq: Reflect deeply on the boundaries of the unknown, 2024{\natexlab{a}}.
\newblock URL \url{https://qwenlm.github.io/blog/qwq-32b-preview/}.

\bibitem[Qwen(2024{\natexlab{b}})]{qwen2_5}
Qwen.
\newblock Qwen2.5: A party of foundation models, 2024{\natexlab{b}}.
\newblock URL \url{https://qwenlm.github.io/blog/qwen2.5}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{gpqa}
D.~Rein, B.~L. Hou, A.~C. Stickland, J.~Petty, R.~Y. Pang, J.~Dirani, J.~Michael, and S.~R. Bowman.
\newblock {GPQA}: A graduate-level google-proof q\&a benchmark.
\newblock \emph{arXiv preprint arXiv:2311.12022}, 2023.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{deepseekmath}
Z.~Shao, P.~Wang, Q.~Zhu, R.~Xu, J.~Song, M.~Zhang, Y.~Li, Y.~Wu, and D.~Guo.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Silver et~al.(2017{\natexlab{a}})Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis]{alphazero}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez, M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, T.~P. Lillicrap, K.~Simonyan, and D.~Hassabis.
\newblock Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
\newblock \emph{CoRR}, abs/1712.01815, 2017{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1712.01815}.

\bibitem[Silver et~al.(2017{\natexlab{b}})Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den Driessche, Graepel, and Hassabis]{alphago}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez, T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~P. Lillicrap, F.~Hui, L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nat.}, 550\penalty0 (7676):\penalty0 354--359, 2017{\natexlab{b}}.
\newblock \doi{10.1038/NATURE24270}.
\newblock URL \url{https://doi.org/10.1038/nature24270}.

\bibitem[Snell et~al.(2024)Snell, Lee, Xu, and Kumar]{snell2024scalingllmtesttimecompute}
C.~Snell, J.~Lee, K.~Xu, and A.~Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.03314}.

\bibitem[Trinh et~al.(2024)Trinh, Wu, Le, He, and Luong]{AlphaGeometryTrinh2024}
T.~Trinh, Y.~Wu, Q.~Le, H.~He, and T.~Luong.
\newblock Solving olympiad geometry without human demonstrations.
\newblock \emph{Nature}, 2024.
\newblock \doi{10.1038/s41586-023-06747-5}.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solving}
J.~Uesato, N.~Kushman, R.~Kumar, F.~Song, N.~Siegel, L.~Wang, A.~Creswell, G.~Irving, and I.~Higgins.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Wang et~al.(2023)Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui]{mathshepherd}
P.~Wang, L.~Li, Z.~Shao, R.~Xu, D.~Dai, Y.~Li, D.~Chen, Y.~Wu, and Z.~Sui.
\newblock Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2312.08935}, 2023.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~Le, E.~Chi, S.~Narang, A.~Chowdhery, and D.~Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wang et~al.(2024)Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen]{mmlu_pro}
Y.~Wang, X.~Ma, G.~Zhang, Y.~Ni, A.~Chandra, S.~Guo, W.~Ren, A.~Arulraj, X.~He, Z.~Jiang, T.~Li, M.~Ku, K.~Wang, A.~Zhuang, R.~Fan, X.~Yue, and W.~Chen.
\newblock Mmlu-pro: {A} more robust and challenging multi-task language understanding benchmark.
\newblock \emph{CoRR}, abs/2406.01574, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.01574}.

\bibitem[Xia et~al.(2024)Xia, Deng, Dunn, and Zhang]{agentless}
C.~S. Xia, Y.~Deng, S.~Dunn, and L.~Zhang.
\newblock Agentless: Demystifying llm-based software engineering agents.
\newblock \emph{arXiv preprint}, 2024.

\bibitem[Xin et~al.(2024)Xin, Ren, Song, Shao, Zhao, Wang, Liu, Zhang, Lu, Du, Gao, Zhu, Yang, Gou, Wu, Luo, and Ruan]{xin2024deepseekproverv15harnessingproofassistant}
H.~Xin, Z.~Z. Ren, J.~Song, Z.~Shao, W.~Zhao, H.~Wang, B.~Liu, L.~Zhang, X.~Lu, Q.~Du, W.~Gao, Q.~Zhu, D.~Yang, Z.~Gou, Z.~F. Wu, F.~Luo, and C.~Ruan.
\newblock Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.08152}.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou]{IFeval}
J.~Zhou, T.~Lu, S.~Mishra, S.~Brahma, S.~Basu, Y.~Luan, D.~Zhou, and L.~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}, 2023.

\end{thebibliography}
