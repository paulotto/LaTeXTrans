
\documentclass{article} % For LaTeX2e
\usepackage[final]{colm2025_conference}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{authblk} % Load the authblk package

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}



\title{\centering Synthetic Data Generation \& Multi-Step RL \\ for Reasoning \& Tool Use}

% Define the author block
\author{
  % Authors with superscripts for notes/affiliations
  Anna Goldie\textsuperscript{* 1}\enskip
  Azalia Mirhoseini\textsuperscript{* 1}\enskip
  Hao Zhou\textsuperscript{2}\enskip
  Irene Cai\textsuperscript{2}\enskip
  Christopher D. Manning\textsuperscript{1} \\ % End of author line
   % Add a bit of vertical space
  % Affiliation definitions
  \textsuperscript{1} Department of Computer Science, Stanford University \\ % Affiliation 1
  \textsuperscript{2} Google DeepMind \\   % Affiliation 2
  \textsuperscript{*} Equal contribution \\
  % Notes and Emails
    % Note for the asterisk
  \texttt{\{agoldie,azalia\}@cs.stanford.edu} % Emails for first authors
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\Tau}{\mathrm{T}}


\ifcolmsubmission
\linenumbers
\fi


\begin{document}
\maketitle

\begin{abstract}
Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5\%, 12.3\%, 14.8\%, 11.1\%, and 15.3\% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9\%.


\end{abstract}
%This strategy is used for both filtering synthetic data and for RL finetuning, enabling better attribution across the steps of an episode. 
\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities in Natural Language Processing  \citep{geminiteam2024gemini15, anthropic2024claude3, openai2024gpt4technicalreport}. However, they often struggle to answer complex queries that require reasoning and tool use across multiple steps \citep{wu2024cofcastepwisecounterfactualmultihop}, such as multi-hop question-answering, mathematical problem-solving, coding, and other agentic tasks, \citep{yang2018hotpotqadatasetdiverseexplainable, trivedi2022musiquemultihopquestionssinglehop,wu2024cofcastepwisecounterfactualmultihop,gsm8k2021,jimenez2024swebenchlanguagemodelsresolve, ehrlich2025codemonkeysscalingtesttimecompute,li2022competition}.

Traditional reinforcement learning (RL) approaches, such as RL From Human Feedback (RLHF) \citep{christiano2017rlhf}, RL from AI Feedback (RLAIF) \citep{bai2022constitutionalaiharmlessnessai}, and RL from Execution Feedback (RLEF) \citep{gehring2025rlefgroundingcodellms}, have focused on single-step optimization, leaving the challenge of multi-step tasks largely unaddressed. Many real-world problems require a sequence of interrelated actions; for example, when answering a challenging question, a model must determine not just what information to seek, but when to stop searching and synthesize its findings. Multi-step reasoning creates a compounding challenge, as incorrect intermediate steps often lead to incorrect final results, making it critical to maintain accuracy across the entire chain of actions or learn to effectively recover from such errors. 

To address this challenge, we present Step-Wise Reinforcement Learning (SWiRL), an offline multi-step optimization technique. We consider a setting where the model has access to a tool, such as a search engine or calculator, and can run a sequence of tool use calls as needed to answer the question. Our goal is to teach the model how to decompose complex problems into a sequence of more manageable subtasks, when to call the tool, how to formulate a call to the tool, when to use the results of these queries to answer the question, and how to effectively synthesize its findings. In particular, we propose a two stage approach, in which we first generate multi-step synthetic data and then learn from these data using a step-wise reinforcement learning method. This approach has the key practical advantage that we can quickly generate large volumes of multi-step training data via parallel calls to avoid throttling the training process with slow tool use execution. In addition, this offline process enables greater reproducibility due to having a fixed dataset.

% Beyond enabling parallelization, training in an offline fashion can be beneficial as it breaks the feedback loop during training, and enables greater reproducibility via a fixed training dataset that can shared.

To generate multi-step synthetic training data, we provide an open-source LLM (Gemma 2 \citep{gemma2024gemma2improvingopen}) with access to a relevant tool (e.g., a search engine or calculator). We iteratively prompt the model to generate multi-step trajectories; at each step, the model is free to generate a chain of thought, and may either call a tool or produce a final answer, which we refer to as the model's action. If the model generates a tool use call, its query is automatically extracted from the overall response and executed in the environment, and the result is presented to the model in the next step. 
The trajectory ends when the model generates an answer to the original question, which it indicates using special markers. We convert each trajectory with $k$ actions into $k$ subtrajectories, containing the context from the beginning of the trajectory up to that action. We then use a step-wise reinforcement learning approach to optimize over this dataset, employing a generative reward model that evaluates each action in the context of its subtrajectory. 

This granular approach enables us to apply direct feedback after each step of the trajectory, and to do so in a manner that is contextually aware. Unlike prior RL finetuning approaches used in frontier open-source models like DeepSeek-R1 \citep{deepseekr1} and Llama-3 \citep{grattafiori2024llama3herdmodels}, we do not solely optimize for final performance, and use no golden labels; however, by optimizing for the reasonableness of each step given prior steps, SWiRL does in fact improve final performance. 

% To compile a large-scale collection of synthetic trajectories, we augmented a language model (Gemma 2 2b, 9b, or 27b in our experiments) with a tool (e.g. a search engine or calculator). Specifically, we iteratively prompted the model to generate multi-step trajectories. At each step, the model may either call a tool or produce a final answer, and is always free to generate chains of thought (which it typically does). If the model generates a tool use call, it is parsed from the overall response, executed in the environment, and the result is presented to the model in the next step. See Appendix \ref{sec:appendix} for the prompt which contains a question, the results of prior tool use calls, and explicit instructions regarding multi-step tool utilization.

In addition to evaluating SWiRL on challenging multi-hop question-answering and mathematical problem-solving tasks, we also study the generalization properties of this methodology. This is of key interest because there is an explosion of agentic applications for language models, and methods that generalize across datasets and tasks will be easier, cheaper and faster to adapt to new environments.
%or "diverse agentic applications of LLMs"
% [Chris: I t hink join these paragraphs!]
We also measure the effectiveness of different synthetic data filtering strategies, study SWiRL's ability to generalize across datasets and tasks, measure the impact of model size and dataset size, and explore the mechanism driving these performance improvements.

Our contributions are as follows: 

\begin{itemize}
    \item We propose Step-Wise Reinforcement Learning (SWiRL), an approach to synthetic data generation and offline RL that advances multi-step reasoning and tool use. 
    \item  We demonstrate generalization across datasets. For example, training SWiRL on HotPotQA not only improves performance on the dataset itself, but also yields superior performance on other multi-hop question-answering datasets, e.g., 21.5\% on GSM8K \citep{gsm8k2021}, 15.3\% on BeerQA \citep{qi2021beerqa}, 11.1\% on MuSiQue \citep{trivedi2022musiquemultihopquestionssinglehop} and 14.8\% on CofCA \citep{wu2024cofcastepwisecounterfactualmultihop}.
    \item We also show transfer across disparate tasks, namely mathematical reasoning to question-answering and vice versa. Training only on multi-hop HotPotQA question-answering improves performance on GSM8K \citep{gsm8k2021} (a math dataset) by 16.9\%, and training on GSM8K improves performance on HotPotQA (multi-hop question-answering) by 9.2\%.
    \item We analyze the impact of synthetic data filtering strategies in a multi-step reasoning and tool use setting, and demonstrate that models learn best from datasets which have been filtered step-wise to ensure high-quality reasoning traces, but which are not filtered by outcome (correct final answer).
    \item We explore the impact of training dataset size and model size on SWiRL, observing that significant gains can be achieved even with just 1000 trajectories and that smaller models (Gemma-2-2b and 9b) can benefit from in-domain SWiRL, but do not display the same generalization as their larger counterpart, Gemma-2-27b.
    \item We demonstrate that SWiRL effectively improves the average process reward, even when evaluated on out-of-distribution tasks, suggesting that the downstream performance gains are driven by improved multi-step reasoning.
\end{itemize}
 

\section{Methodology}

Our methodology, Step-Wise Reinforcement Learning (SWiRL), consists of two stages. In the first stage, we generate and filter synthetic data. In the second stage, we use a step-wise reinforcement learning approach to optimize a generative base model on the synthetic trajectories. SWiRL does not require golden labels or human annotations, and instead relies entirely on model-based judgments for data generation, filtering, and RL optimization. The overall flow of our methodology is depicted in Figure \ref{fig:synthetic} (Stage 1) and Figure \ref{fig:process-rl} (Stage 2).

\subsection{Multi-Step Data Collection}\label{sec:synthetic-data-collection}

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{syntheticdata.png}
    \caption{In SWiRL Stage 1, we generate and filter multi-step synthetic trajectories. At each step, the model is free to generate a chain of thought, call a tool such as a search engine or calculator, and/or produce an answer to original question. Process-filtered data corresponds to trajectories in which every step is judged to be reasonable by a model judge (Gemini 1.5 Pro Thinking). Outcome-filtered data corresponds to trajectories with a final answer that matches the golden label.}
    \label{fig:synthetic}
\end{figure*}

In Stage 1 (see Figure \ref{fig:synthetic}), we generate synthetic trajectories consisting of multiple steps of reasoning and tool use, which we use as training data for the step-wise RL methodology described in the next section. To compile a large-scale collection of synthetic trajectories, we augment a language model with a tool (e.g., a search engine or calculator), and iteratively prompt the model to generate multi-step trajectories. At each step, the model is asked to choose whether to call a tool or produce a final answer, and is always free to generate chains of thought (which it typically does). If the model generates a tool use call, it is parsed from the overall response, executed in the environment, and the result is presented to the model in the next step. See Appendix \ref{sec:appendix} for the prompt, which contains a question, explicit instructions regarding multi-step tool utilization, and the results of prior tool use calls.

For each multi-step synthetic trajectory, we define the following annotations. The trajectory itself is denoted by \( \tau = (s_1, a_1, \dots, s_K, a_K) \). The first state $s_1$ is the original prompt. Each following state $s_i$ contains the entire context so far, containing state $s_{i-1}$, action $a_{i-1}$, and the environment (tool call) response to $a_{i-1}$. Each action $a_i$ is the model response, given state $s_i$. The last action, $a_K$, is the model's answer to the original prompt.

In this work, we compiled a dataset of 50,000 synthetic trajectories seeded by 10,000 multi-step questions from the HotPotQA training set \citep{yang2018hotpotqadatasetdiverseexplainable} (i.e., 5 trajectories per question), and a mathematical reasoning dataset of 37,500 synthetic trajectories seeded by the 7,500 questions in the GSM8K training set \citep{gsm8k2021}. Note that, for HotPotQA, we filtered out ``Easy'' questions, which can typically be answered with a single search query. To prevent synthetic trajectories from being excessively long, we set a maximum step count of 5 for HotPotQA questions, and 10 for GSM8K questions (which typically require 2-8 steps to solve).

Having compiled these datasets, we consider four different filtering strategies and measure their impact on performance (Figure \ref{fig:synthetic}): (1) No filtering; (2) Process filtering, where we retain trajectories in which each step was deemed reasonable given all previous steps. Concretely, a model (Gemini 1.5 Pro Thinking, in our case) is prompted to render a binary judgment as to whether action $a_i$ is reasonable given the context $s_{i}$. See Appendix \ref{sec:appendix} for our prompt. No golden labels are used; (3) Outcome filtering, where we select trajectories based solely on whether the final response, $a_K$, matches the golden answer; and (4) Process and outcome filtering, in which we take the intersection of both filtering approaches and retain only trajectories that exhibit both step-wise soundness and correct final outcomes.

Recent approaches to synthetic data distillation, such as Deep-Seek R1 \citep{deepseekr1}, have demonstrated that synthetic data filtered for correct outcomes can lead to good performance with single-step RL and supervised finetuning (SFT). In this work, we sought to explore whether this pattern would hold in a multi-step, tool use setting, and to explore the impact of both outcome and process filters. Like these prior work, we observed that filtering multi-step trajectories for correctness was effective for SFT, and in fact critical for good performance. However, we found that SWiRL, unlike SFT, can learn even from trajectories that end in incorrect final answers. In fact, we achieve our best results by including process-filtered data, regardless of the correctness of the outcome.

\subsection{Step-Wise Reinforcement Learning Methodology}

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=1.0\textwidth]{stepwise-rl.pdf}
    \caption{In SWiRL Stage 2, we perform step-wise RL to train on the synthetic multi-step trajectories from Stage 1. Each step contains an action, which corresponds to a tool call or the final response. The model is free to generate chains of thought during each step. The environment responses are captured in the prior steps of the synthetic trajectories, which were generated offline. Granular feedback is provided by a generative reward model, which is used to perform RL optimization directly on each action, given the prior context.}
    \label{fig:process-rl}
\end{figure*}

As shown in Figure \ref{fig:process-rl}, we propose a RL approach capable of learning effectively from the synthetic multi-step trajectories generated in Stage 1. At each step, a base model is optimized to predict either the next intermediate step or the final response based on preceding context. At each step $i$, the model has access to the full contextual history, including the original prompt, all previous model-generated steps and any applicable environment response corresponding to those steps.

Thus, our objective function is the expected sum of stepwise rewards:

$$J(\theta) = E_{s\sim\Tau,~a\sim \pi_\theta(s)} \left[R(a|s) \right]$$


Here, $\pi_\theta$ is the base model parametrized by $\theta$, which is finetuned via SWiRL (Note that we also use $\pi_\theta$ to generate synthetic data.) $\Tau$ denotes the set of all states in the synthetic multi-step trajectories, i.e. each incremental state $s$ within each trajectory $\tau$. The reward signal $R(a|s)$ is derived from a generative reward model, specifically Gemini 1.5 Pro in our experiments, which assesses the quality of the generated response $a$ given the context $s$. No golden labels are used.

We optimize the above expected reward using the same policy gradient algorithm as used in Gemma 2 for optimizing the human feedback reward \citep{gemma2024gemmaopenmodelsbased, gemma2024gemma2improvingopen}. Our granular, step-by-step finetuning paradigm enables the model to learn both local decision-making (next-step prediction) and global trajectory optimization (final response generation) while being guided by immediate feedback on the soundness of each prediction.

\subsection{Step-Wise Inference-time Evaluation}
\label{sec:swirl-inference}

As shown in Figure \ref{fig:swirl-inference}, at inference time, we iteratively prompt the model to either call a tool or produce a final answer. If the model generates a search query (indicated by $<$search\_query$>$ $<$/search\_query$>$ tags), we parse out that query, embed it with a Gecko model, perform a nearest neighbor lookup in the corresponding vector database, and inject the retrieved article into the model's context window. If the model generates a calculator tool call (indicated by $<$math\_exp$>$ $<$/math\_exp$>$ tags), we parse out the mathematical expression, execute it with a SymPy interpreter, and inject the calculated results into the context window. This process terminates when the model either produces an answer (signaled by producing $<$answer$>$ $<$/answer$>$ tags) or reaches the maximum number of queries (5 for question-answering datasets, and 10 for mathematical reasoning datasets). See Appendix \ref{sec:example-trajectories} for example trajectories.

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=1.0\textwidth]{swirl-inference.png}
    \caption{At inference time, we iteratively prompt the model to call the tool as many times as necessary (up to a limit) before answering the original user question.}
    \label{fig:swirl-inference}
\end{figure*}


\section{Related Work}

\noindent\textbf{Reinforcement Learning for LLM Finetuning.} One prominent approach, Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022traininglanguagemodelsfollow,christiano2017rlhf}, consists of training a reward model on human preference labels at the response level, followed by RL optimization using Proximal Policy Optimization (PPO) \citep{schulman2017proximalpolicyoptimizationalgorithms}. Building upon this framework, Reinforcement Learning with AI Feedback (RLAIF) \citep{bai2022constitutionalaiharmlessnessai} has emerged as a scalable alternative that leverages AI models to generate feedback based on predefined principles or constitutions, reducing the need for costly human annotations. RL from Execution Feedback (RLEF) \citep{gehring2025rlefgroundingcodellms} uses environment feedback, such as pass rate on coding test cases, to calculate the reward, which it then optimized via PPO. Besides PPO, other RL optimizations, such as Direct Preference Optimization (DPO) \citep{rafailov2023direct} and its successors (e.g., \citet{azar2023generaltheoreticalparadigmunderstand,ethayarajh2024ktomodelalignmentprospect,meng2024simposimplepreferenceoptimization,lanchantin2025diversepreferenceoptimization}) as well as GRPO \citep{shao2024deepseekmathpushinglimitsmathematical} have also proven to be effective for finetuning LLMs to maximize a target reward. 
A limitation of the above approaches is that they focus on single-step optimization with the reward being calculated only at the end of the episode, leading to suboptimal performance for multi-step optimization \citep{liu2024enhancingmultistepreasoningabilities,wang2024offlinereinforcementlearningllm}. In SWiRL, we focus on scenarios where multiple steps of reasoning and tool calls are necessary prior to generating a response. Unlike the above methods, SWiRL enables the model to receive feedback on its granular stepwise actions which leads to better multi-step reasoning and tool use across longer horizons.


\noindent\textbf{Multi-step Optimization with RL.} Recent work including DQO \citep{liu2024enhancingmultistepreasoningabilities} and OREO \citep{wang2024offlinereinforcementlearningllm} propose offline reinforcement learning to improve multi-step reasoning for LLMs. However, neither focuses on enhancing a model's ability to use tools or interact with an external environment. Additionally, unlike our approach, which optimizes at the (reasoning) step level, DQO relies on token-level actions, which as shown in \citep{wang2024offlinereinforcementlearningllm}, are generally less effective than step-level actions. Moreover, OREO requires training a separate value network and policy, and relies on iterative co-optimization of both models. The process of maintaining, training, and serving these two models can be prohibitively expensive, particularly for larger models. PRIME \citep{cui2025processreinforcementimplicitrewards} proposes an online approach to improve multi-step reasoning, but does not enable tool use or offline training. Tulu-3 \citep{lambert2025tulu3pushingfrontiers} uses verifiable rewards to train a language model to do better at math, but requires access to golden labels.

\noindent\textbf{Reasoning Improvement with Synthetic Data}. Several approaches have been proposed for generating synthetic reasoning data. These methods either rely on golden labels to filter the data or use a combination of golden labels and process or outcome reward models \citep{zelikman2022starbootstrappingreasoningreasoning,singh2024humandatascalingselftraining}. For example, STaR \citep{zelikman2022starbootstrappingreasoningreasoning} generates chain-of-thoughts (CoT) for reasoning questions, filters for those that result in correct answers, and performs Supervised Fine-Tuning (SFT) on those reasoning traces. The paper also proposes an augmentation technique called ``rationalization'', in which for each question the model answered incorrectly, the model is provided with the correct answer and prompted to generate a CoT that leads to that answer. Rejection finetuning (RFT) \citep{yuan2023scalingrelationshiplearningmathematical} is another method that relies on collecting reasoning traces from the model and using those with correct outcomes for SFT. ReST \citep{gulcehre2023reinforcedselftrainingrestlanguage} demonstrates strong performance on machine translation by iteratively generating data and then finetuning on that data using either a supervised or reinforcement learning objective. $ReST^{EM}$ \citep{singh2024humandatascalingselftraining} is an extension of ReST which outperforms training on human data alone for math and coding evaluations, but which plateaus after a few iterations, presumably due to overfitting. Our method also uses a model-based approach to generate multi-step trajectories. However, we show that using a model to label the steps within each reasoning trajectory leads to higher out-of-domain generalization than using only the trajectories which contain correct final answers, meaning that we do not require golden labels. In addition, we enable the model to use tools iteratively to perform multi-hop question answering and mathematical reasoning.

\noindent\textbf{Process vs. Outcome Based Optimization}. There have been a number of attempts to compare the effectiveness of process and outcome-based approaches in the domain of math and reasoning \citep{lightman2023letsverifystepstep, uesato2022solvingmathwordproblems, snell2024scalingllmtesttimecompute}. For example, \citep{lightman2023letsverifystepstep} showed that (Outcome Reward Models) ORMs are more effective than (Process Reward Models) PRMs at the task of ranking samples from a fixed generator model, whereas \citet{uesato2022solvingmathwordproblems} demonstrated that outcome supervision yields comparable accuracy to process supervision at lower cost, but that the reasoning traces from the resulting model exhibit lower fidelity. Both rely on expensive human annotations and golden labels, and do not explore the impact of PRMs and ORMs in reinforcement learning optimization, or the differential effect of data filtering on supervised vs. RL optimization objectives.

\section{Experiments}
\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{.8}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Datasets} & HotpotQA & CofCA (Avg) & MuSiQue \\
\textbf{Metrics}  & PM$\dagger$  & PM$\dagger$  & PM$\dagger$  \\
\midrule
\multicolumn{1}{c}{\textbf{Proprietary LLMs}} \\
\midrule
\textbf{GPT-4}       & 74.8  & 51.9  & 63.9  \\
\textbf{GPT-3.5}     & 62.8  & 40.7  & 53.1  \\
\textbf{Gemini 1.0 Pro}  & 63.5  & 33.3  & 46.9  \\
\textbf{Bing Chat}   & 72.1  & 41.6  & 52.3  \\
\textbf{O1-preview}  & \textbf{76.9}  & \textbf{58.5}  & \textbf{67.9}  \\
\midrule
\multicolumn{1}{c}{\textbf{Open Source LLMs}} \\
\midrule
\textbf{Llama 2-7b}  & 38.5  & 28.9  & 34.2  \\
\textbf{Mistral-7b}  & 34.9  & 25.6  & 29.2  \\
\textbf{Qwen 2-7b}   & 39.3  & 30.7  & 33.5  \\
\textbf{Base Gemma 2-27b}  & 58.6 & 31.7 &  35.4 \\
\textbf{SWiRL Gemma 2-27b (Ours)}  & \textbf{67.8} & \textbf{39.3} & \textbf{43.6}  \\
\bottomrule
\end{tabular}
\caption{Comparison of Accuracy (\textbf{PM}$\dagger$: Partial Match) across Multiple Datasets: \textbf{HotpotQA}, \textbf{CofCA} (Average of 2-hop, 3-hop, and 4-hop), and \textbf{MuSiQue}. Baseline results were drawn from \cite{wu2024cofcastepwisecounterfactualmultihop}. The Gemma-2 models, both SWiRL and the base model, were not given access to the context documents, but were allowed to sequentially query a vector database. The SWiRL model was trained on HotPotQA using process-filtered data, and for consistency with baseline results, evaluated on GPT-4o with the same prompts as \cite{wu2024cofcastepwisecounterfactualmultihop} on 300 randomly subsampled questions. See Appendix \ref{sec:example-ids} for example ids.}
\label{tab:llm_performance_pm}
\end{table*}

\subsection{Evaluation Datasets}
To evaluate performance on multi-step search tool use, we selected five challenging multi-hop question-answering and mathematical reasoning datasets:

\begin{itemize}
    \item \noindent\textbf{HotPotQA} \citep{yang2018hotpotqadatasetdiverseexplainable} is comprised of multi-hop questions from a variety of domains. Human annotators constructed the questions to be answerable only by combining information from multiple paragraphs of Wikipedia.
    \item \noindent\textbf{MuSiQue}  \citep{trivedi2022musiquemultihopquestionssinglehop} is a multi-hop question-answering dataset constructed by chaining together multiple single-hop questions.
    \item \noindent\textbf{CofCA} \citep{wu2024cofcastepwisecounterfactualmultihop} is a multi-hop dataset constructed to be answerable only by querying a counterfactual version of Wikipedia. It contains 2- to 4-hop questions.
    \item \noindent\textbf{BeerQA}  \citep{qi2021answeringopendomainquestionsvarying} is an extension of HotPotQA designed to include an even greater number of hops than the original dataset. 
    \item \noindent\textbf{GSM8K} \citep{gsm8k2021} is a dataset composed of grade school math word problems, which typically take 2-8 steps to solve.
\end{itemize}

For question-answering datasets, we set up a vector database containing all articles from each data split using Gecko-1B with 768-dimensional embeddings (English) \citep{lee2024geckoversatiletextembeddings}.

For the experiments in Table 1, we follow the same procedure as \cite{wu2024cofcastepwisecounterfactualmultihop}, evaluating performance on 300 randomly subsampled examples from the target dataset, using the same language model as a judge (GPT4o) and the same prompt. For every other experiment in this paper, we used Gemma-2-27b as our judge, as this was more cost effective, with the exception of GSM8K for which we used Gemini 1.5 Pro as it exhibited noticeably better numeric evaluation. Model-based evaluation is emerging as a scalable and less brittle alternative to exact match and F1 metrics \citep{zheng2023judgingllmasajudgemtbenchchatbot, gu2025surveyllmasajudge}, but does introduce a new source of stochasticity into the evaluation. See Appendix \ref{sec:error-analysis} for our own manual inspection and error analysis of three different model judges.

As described in Section \ref{sec:swirl-inference}, for each question, we iteratively prompt the model to either call a tool or produce a final answer, and limit the maximum number of queries to 5 for question-answering datasets, and 10 for mathematical reasoning datasets. 

\subsection{Results and Discussion} 
\begin{figure*}[ht]
    \centering
    \includegraphics[width=.8\textwidth]{datafiltering.png}
    \caption{Impact of Data Filtering on Model Performance. Synthetic data for training is derived from HotPotQA. SWiRL learns to perform multi-hop question answering even when trained on unfiltered synthetic data. SWiRL's best performance comes from training on process-only filtered data, where the data is selected based on the soundness of each step within its reasoning traces, but which includes both correct and incorrect responses.}
    \label{fig:data_filtering}
\end{figure*}
\noindent\textbf{Impact of Data Filtering on Model Performance:} We evaluated the influence of various filtering mechanisms on downstream task accuracy, as shown in Figure \ref{fig:data_filtering}. Concretely, we consider 4 different types of filtering: no filtering, outcome-based filtering that ensures correct final answers, process-based filtering that ensure that each step is correct as judged by a model, and both process and outcome-based filtering.

In all experiments, we fix the number of trajectories used for finetuning (with the exception of our ablation study on the impact of scaling dataset size), and we provided all models with access to an appropriate tool. Notably, process-only filtering consistently yields the highest accuracy, suggesting that focusing on the procedural aspects of data refinement is more important than the correctness of a training trajectory. While both unfiltered and filtered data demonstrated an improvement over the baseline model, filtering for correctness usually harms performance; with the exception of MuSiQue, outcome-filtered or outcome and process-filtered data is less effective than unfiltered data. We hypothesize that this is because SWiRL actually benefits from having access to both positive and negative examples. These results underscore the relative unimportance of outcome-based filtering, which requires golden labels. They also demonstrate that our process RL method can effectively learn from even trajectories with incorrect final answers. 

\noindent\textbf{Generalization Across Disparate Tasks:}
To measure generalization across training tasks, we evaluated the mathematical reasoning capabilities of a model trained on multi-hop question-answering with search tool use (HotPotQA). Specifically, we evaluated the performance of this model on GSM8K, a mathematical reasoning task, providing the model with a SymPy interpreter to use as a calculator. This experiment was run on a different random subsample of 300 examples. As shown in Table \ref{tab:generalization}, applying SWiRL  on out-of-distribution data and tasks still improves performance.

\begin{table}[t] % [htbp] suggests placement: here, top, bottom, page
  \centering
  %\caption{SWiRL Generalization Performance} % Add your desired caption here
  \label{tab:transposed_metrics} % Add a label for referencing
  \begin{tabular}{lccccc}
    \toprule
    & \multicolumn{1}{c}{GSM8K} & \multicolumn{1}{c}{HotPotQA} & \multicolumn{1}{c}{CofCA} & \multicolumn{1}{c}{BeerQA} & \multicolumn{1}{c}{MuSiQue} \\
    & \multicolumn{1}{c}{(math)} & \multicolumn{1}{c}{(qa)} & \multicolumn{1}{c}{(qa)} & \multicolumn{1}{c}{(qa)} & \multicolumn{1}{c}{(qa)} \\
    \midrule
    Base Model      & 0.65 & 0.65 & 0.54 & 0.59 & 0.45 \\
    SWIRL on GSM8K (math) & 0.79 & 0.71 & 0.56 & 0.68 & 0.49 \\
    SWIRL on HotPotQA (qa) & 0.76 & 0.73 & 0.62 & 0.68 & 0.50 \\
    \bottomrule
  \end{tabular}
  \caption{SWiRL Generalization Performance. Finetuning on synthetic traces from HotPotQA or GSM8K improves performance on both in-distribution and out-of-distribution tasks. Interestingly, training on a different domain and tool (e.g. math and a calculator) improves performance on question-answering with a search engine and vice versa, suggesting the effectiveness of SWiRL in improving general multi-step reasoning and tool use capability.}
  \label{tab:generalization}
\end{table}
\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=.8\textwidth]{sft-rl.png}
    \caption{Comparison of SFT and SWiRL. Synthetic data for training is derived from HotPotQA, and to derive accuracy, Gemma 2 27b evaluates whether the model's answer matches the golden answer. SWiRL greatly benefits from process-only filtered traces, and unlike SFT, is capable of learning from traces with both correct and incorrect outcomes.}
    \label{fig:sft-vs-rl}
\end{figure*}
\noindent\textbf{Comparison of Supervised Finetuning and SWiRL:} Figure \ref{fig:sft-vs-rl} compares the performance of Supervised Fine-Tuning (SFT) and SWiRL on downstream tasks. The results show that SFT leads to worse overall performance when compared to SWiRL across all data filtering strategies. We observe that SFT performs better if we apply it to data that is both process and outcome-filtered, rather only process-filtered. However, interestingly, SWiRL learns best from data that is only process-filtered. We attribute this to SFT's tendency to memorize, rather than generalize \citep{chu2025sftmemorizesrlgeneralizes, setlur2024rlincorrectsyntheticdata}, which can hinder the model's performance on new, unseen scenarios. In contrast, SWiRL has the ability to improve model performance by targeting per-step reward maximization. SWiRL enables the model to develop a deeper understanding of the necessary steps of query generation and retrieval, which leads to enhanced planning and generalization. 

\noindent\textbf{Effect of Tool Use:} As discussed in Section \ref{sec:swirl-inference}, at inference time, we use the proposed multi-step eval as shown in Figure \ref{fig:swirl-inference} and we iteratively prompt the model to make tool calls as necessary to answer the question. 
As shown in Figure \ref{fig:tooluse}, both base and SWiRL models improve with SWiRL's multi-step tool use inference, but SWiRL-training offers even further improvements. Notably, the SWiRL model exhibits substantial improvements, even without access to a tool, suggesting that SWiRL training improves the model's ability to break down complex problems into multiple manageable subtasks.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{tooluse.png}
    \caption{Performance of SWiRL With and Without Multi-Step Tool Use. SWiRL's multi-step tool use inference improves the performance of both the base model and the SWiRL-finetuned model, but benefits the latter substantially more. Even without access to a tool, the SWiRL model outperforms, suggesting that SWiRL training improves the model's ability to break down complex problems into multiple manageable subtasks.}
    \label{fig:tooluse}
\end{figure}

\noindent\textbf{Impact of Scaling Finetuning Dataset  and Model Size:}
Our experiments on scaling the fine-tuning dataset size reveal a clear trend: SWiRL has the ability to leverage larger datasets, even when using only process-filtered data, as shown in Figure \ref{fig:impact-of-datasize}. As the fine-tuning dataset size increases, a consistent enhancement in model performance is observed across our target multi-step reasoning tasks. While a limited dataset of 100 data points appears insufficient for the model to effectively generalize, a significant improvement is evident with 1,000 data points, showing solid gains across all datasets. Furthermore, scaling up to 10,000 data points continues to yield further performance enhancements, confirming the efficacy of our method in capitalizing on larger datasets for improved reasoning capabilities.



We also varied model size, observing that smaller models (2b and 9b) may benefit from in-domain SWiRL, but do not display the same generalization as their larger counterpart, Gemma 2 27b. See results in Appendix \ref{sec:appendix-model-size}.





\noindent\textbf{Effect on Mean Process Label Accuracy}: 
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{datascaling.png}
    \caption{Performance as a Function of Synthetic Dataset Size. Synthetic training data is derived from HotPotQA, and accuracy is evaluated by Gemma 2 27b. As we scale the dataset size, we observe consistent improvements in model performance. With only 1000 data points, the model robustly improves both on in- and out- of distribution datasets.}
    \label{fig:impact-of-datasize}
\end{figure*}
In the previous subsections, we evaluated the effect of SWiRL on downstream task accuracy. Here, we take a deeper look to understand how SWiRL achieves these performance improvements. In Table \ref{tab:prm_accuracy}, we show the average process label accuracy for the baseline model vs. a SWiRL finetuned model on 500 trajectories (seeded by 100 questions) for both HotPotQA and GSM8K. To calculate the score per step, we use the same model and prompt as we used for process filtering, as described in Section \ref{fig:data_filtering}. We take a macro-average of the process label scores within and then across trajectories. We observe that both for in-distribution and out-of-distribution tasks, the SWiRL model generates trajectories with higher average process labels, suggesting that the higher final accuracies are driven by better multi-step reasoning.

\begin{table}[htbp]
  \centering
  % Reuse or adapt the original caption
  \label{tab:prm_accuracy} % Add a label for referencing
  \sisetup{output-exponent-marker=\text{e}} % Optional: format exponent for siunitx
  \begin{tabular}{lcc}
    \toprule
     & HotPotQA & GSM8K \\
    & (in distribution) & (out of distribution) \\
    \midrule
    Base (Mean Process Label)      & 82.5\% & 87.5\% \\
    SWiRL on HotPotQA (Mean Process Label) & 91.0\% & 91.6\% \\

    \bottomrule
  \end{tabular}
  \caption{Impact of SWiRL on Process Correctness. After our multi-step RL optimization, we observe that the average correctness of each step improves over the base model on both in- and out- of distribution tasks.}
\end{table}
\vspace{-.2in}
\section{Conclusion} 
\vspace{-.1in}
In this work, we propose a synthetic data generation and offline reinforcement learning approach to multi-step reasoning and tool use. This approach outperforms baselines by an average 15\% across challenging multi-hop question-answering and mathematical reasoning tasks. We explore the effect of different data filtering strategies in a multi-step, tool use setting, and find that our RL approach is effective even on unfiltered data, but performs best on process-filtered data. Unlike supervised finetuning, our RL approach can learn from trajectories with incorrect final answers and actually benefits from the presence of a mixture of both correct and incorrect final answers. SWiRL demonstrates strong generalization properties, improving performance on mathematical reasoning (GSM8K) by 16.9\% when trained on multi-hop question-answering (HotPotQA) and 9.2\% vice versa.

\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}
% \
\appendix



\renewcommand{\arraystretch}{1.4} % Improve row spacing

\newpage

\section{Prompts for Synthetic Data Generation, Filtering, and Evaluation}
\label{sec:appendix}

In this work, we use the following prompts for data generation, filtering, and evaluation.

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Prompt Type} & \textbf{Prompt Text} \\
        \hline
        \textbf{Prompt for Multi-Step Synthetic Data Generation for Question-Answering with Search Tool Use} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        Please help me answer the following question in just a few words. If you think it would help to do a search, please generate a search query enclosed by \texttt{\textless search\_query\textgreater} QUERY \texttt{\textless /search\_query\textgreater} tags. \newline
        Some questions may require multiple searches in order to answer, so I will allow you to make up to \texttt{\{\}} sequential queries before answering the question. \newline
        Please do not repeat queries you have already issued, as this is a waste of time. \newline
        I will provide search results in the following format: \newline
        \texttt{QUERY → RESULT}. \newline
        Once you have enough information, generate an answer enclosed by \texttt{\textless answer\textgreater}ANSWER\texttt{\textless /answer\textgreater} tags. \newline
        Please either issue a search query or answer the question, but not both. \newline
        The question is: \texttt{\{\}} \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Prompt Type} & \textbf{Prompt Text} \\
        \hline
        \textbf{Prompt for Multi-Step Synthetic Data Generation for Mathematical Reasoning with Calculator Tool Use} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
    Please help me answer the following question in just a few words. If you think it would help to use a calculator, please generate a mathematical query enclosed by \texttt{\textless math\_exp\textgreater} MATH EXP \texttt{\textless /math\_exp\textgreater} tags. \newline
    Some questions may benefit from using a calculator multiple times in order to answer, so I will allow you to make up to \texttt{\{\}} sequential queries before answering the question. \newline
    Please do not repeat queries you have already issued, as this is a waste of time. \newline
    I will provide results in the following format: \newline
    \texttt{QUERY → RESULT}. \newline
    Once you have enough information, generate an answer enclosed by \texttt{\textless answer\textgreater}ANSWER\texttt{\textless /answer\textgreater} tags. \newline
    Please either issue a search query or answer the question, but not both. \newline
    The question is: \texttt{\{\}} \newline
    \texttt{\textless end\_of\_turn\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Prompt Type} & \textbf{Prompt Text} \\
        \hline
        \textbf{Prompt for Process-Filtering on Multi-Step Search Tool Use Trajectories} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
    My boss asked me to answer the following question with the help of a search engine: \texttt{\{\}} \newline
    This means that I might need to decompose the question into a sequence of searches before being able to answer the question. \newline
    I am trying to learn how to do this more effectively, so please provide feedback on my last message. \newline
    Please take a look at our conversation so far: \texttt{\{\}} \newline
    When evaluating a message, please only consider the last message and do not penalize or reward me for previous messages. \newline
    When evaluating an answer, please consider only whether the answer follows from the search results, and not whether you believe the answer to be correct. \newline
    If there is not enough information from the search results to answer the question, you should rate any answer as "BAD". Pay close attention as it may initially seem like the answer is present when it is not. \newline
    When evaluating a search query, please consider whether it is likely to help me answer the original question. \newline
    Explain your reasoning and then answer with either "GOOD" or "BAD". \newline
    \texttt{\textless end\_of\_turn\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Prompt Type} & \textbf{Prompt Text} \\
        \hline
        \textbf{Prompt for Evaluation / Outcome-Filtering on Multi-Step Trajectories with Search Tool Use} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        I need you to help me grade the answer to the following question: "\texttt{\{\}}". \newline
    The answer key says: \texttt{\{\}}, and my answer is \texttt{\{\}}. Am I correct? \newline
    Please explain your reasoning and then answer "YES" or "NO". \newline
    Do not use your own knowledge to the decide, but simply check whether I gave the answer in the answer key.\newline
    \texttt{\textless end\_of\_turn\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}


\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Prompt Type} & \textbf{Prompt Text} \\
        \hline
        \textbf{Prompt for Evaluation / Outcome-Filtering on Multi-Step Trajectories with Calculator Tool Use} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        I need you to help me grade the answer to the following question: "\texttt{\{\}}". \newline
    The answer key says: \texttt{\{\}}, and my answer is \texttt{\{\}}. Am I correct? \newline
    Please explain your reasoning and then answer "YES" or "NO". \newline
    There are multiple ways to write the same answer. For example, "10", "10.00", "\$10", and "\$10.00" are all equivalent.\newline
    \texttt{\textless end\_of\_turn\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\clearpage

\section{Impact of Model Size on Effectiveness of SWiRL.}\label{sec:appendix-model-size} The trend is that models are growing in parameter count over time \citep{sevilla2022trends}, so measuring the impact of model size on the effectiveness of a method can provide insight into its longevity and future impact. It is also interesting to see whether larger models are able to learn more general patterns from the training process, and therefore exhibit greater transfer learning across datasets and even domains (e.g. math vs. question-answering). As shown in Figure \ref{fig:model-size}, SWiRL demonstrates a clear performance boost over the baseline Gemma 2-27b model, showcasing consistent improvements across both in-domain (HotPotQA) and out-of-domain datasets (MuSiQue, COFCA, and BeerQA); while the 2b and 9b Gemma models also exhibit enhanced performance on in-domain data, their generalization performance on out-of-domain data is less consistent. This suggests that the effectiveness of SWiRL grows with increased model size, which is consistent with the observation that methods such as RLHF \citep{ouyang2022traininglanguagemodelsfollow} and RLAIF \citep{bai2022constitutionalaiharmlessnessai} are more effective for larger models. 
\begin{figure}[htbp!]
    \centering
    \includegraphics[width=.6\textwidth]{modelsize-new.png}
    \caption{SWiRL Performance vs. Model Size. Synthetic data for training is derived from HotPotQA. Step-Wise RL finetuning robustly improves performance over baseline for the 27b model across both in-domain (HotPotQA) and out-of-domain datasets (MuSiQue, CofCA, and BeerQA). However, while the in-domain improvements hold for smaller models, the out-of-domain performance is mixed, suggesting that the relative effectiveness of SWiRL is higher for larger models. }
    \label{fig:model-size}
\end{figure}

\newpage\section{Error Analysis of Three LLM Judges}\label{sec:error-analysis}

\begin{table}[htbp] % Use placement specifiers like h(ere), t(op), b(ottom), p(age)
    \centering % Centers the table on the page
    \caption{Error Rates for Gemma-2-27b Judgments on HotPotQA (N=100)}
    \label{tab:gemma2_hotpotqa_rates} % Label for referencing the table in text
    % Using siunitx S column for nice decimal alignment
    \begin{tabular}{l S[table-format=2.2]} % l(eft) column for text, S column for numbers (2 digits before, 2 after decimal)
        \toprule % Top rule
        Metric & {Rate (\%)} \\ % Header for S column needs braces {}
        \midrule % Rule separating header from data
        False Positive Rate (FPR) & 4 \\
        False Negative Rate (FNR) & 1 \\
        \bottomrule % Bottom rule
    \end{tabular}
    % Optional: Add a caption* below to provide context on calculation basis
\end{table}

% Required packages:
% \usepackage{booktabs}
% \usepackage{caption}

\begin{table}[htbp]
    \centering
    \caption{Manual Analysis of LLM Math Grading Accuracy (N=100)}
    \label{tab:llm_math_grading_analysis_narrow} % Adjusted label slightly
    % Changed the last 'l' to 'p{width}'. Adjust '4.5cm' as needed.
    \begin{tabular}{l r r p{4.5cm}}
        \toprule
        Model           & FPR & FNR & Notes \\ % Shortened headers slightly
        \midrule
        Gemma-2-27b       & 15  & 0   & Overly permissive ("nice"); all errors involved units. \\
        GPT-4o          & 0   & 10  & Overly harsh; all errors involved units. \\
        Gemini 1.5 Pro  & 4   & 0   & Accurate, slightly permissive; all errors involved units. \\
        \bottomrule
    \end{tabular}
    % Add clarification that rates are percentages if needed, since (%) removed from header
\end{table}

To evaluate the suitability of language models to serve as evaluators (i.e., check the correctness of a model answer, given a golden answer), we manually checked the correctness of 100 model judgments from Gemma-2-27b on HotPotQA questions. As shown in Table \ref{tab:gemma2_hotpotqa_rates}, we found that the error was relatively low (4\% false positives and 1\% false negatives), justifying the use of this low cost open-source model as our LLM judge. 

However, we noticed that Gemma-2-27b made more errors when it came to numeric quantities, so we decided to run a separate analysis for GSM8K, manually evaluating 100 model judgments each for three language models (Gemma-2-27b, GPT-4o, and Gemini 1.5 Pro). Interestingly, we found that Gemma-2-27b tended to be overly ``permissive'' in its grading, but had zero false negatives, whereas GPT-4o had a relatively high false negative rate but no false positives. We also observed that relative results were consistent across model judges; if GPT-4o gave a higher accuracy score to a particular model, Gemma-2-27b did as well, even if the absolute scores differed. To reduce noise, we chose to use Gemini 1.5 Pro as the LLM judge for GSM8K, in spite of its higher cost.

\newpage\section{Example Trajectories}
\label{sec:example-trajectories}

For clarity, we provide example trajectories from question-answering and mathematical reasoning datasets, which showcase both search and calculator tool use.

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Trajectory Type} & \textbf{Trajectory Text} \\
        \hline
        \textbf{Example Multi-Hop Question-Answering Trajectory with Search Tool Use Generated by SWiRL model on HotPotQA} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        Please help me answer the following question in just a few words. If you think it would help to do a search, please generate a search query enclosed by \texttt{\textless search\_query\textgreater} QUERY \texttt{\textless /search\_query\textgreater} tags. \newline
        Some questions may require multiple searches in order to answer, so I will allow you to make up to 5 sequential queries before answering the question. \newline
        Please do not repeat queries you have already issued, as this is a waste of time. \newline
        I will provide search results in the following format: \newline
        \texttt{QUERY → RESULT}. \newline
        Once you have enough information, generate an answer enclosed by \texttt{\textless answer\textgreater}ANSWER\texttt{\textless /answer\textgreater} tags. \newline
        Please either issue a search query or answer the question, but not both. \newline
        The question is: What company published both The Scorch Trials and The Death Cure? \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        \texttt{\textless search\_query\textgreater}the scorch trials publisher \texttt{\textless /search\_query\textgreater} \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        the scorch trials publisher -> The Scorch Trials is a 2010 young adult post-apocalyptic dystopian science fiction novel written by American author James Dashner and the second book, fourth chronologically, in "The Maze Runner" series. The novel was published on September 18, 2010 by Delacorte Press. It is preceded by "The Maze Runner", and followed by "The Death Cure". A was released on September 18, 2015 by 20th Century Fox. \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        \texttt{\textless search\_query\textgreater}The Death cure publisher \texttt{\textless /search\_query\textgreater} \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        The Death cure publisher -> The Death Cure is a 2011 young adult dystopian science fiction novel written by American writer James Dashner and the third book, fifth chronologically, in the "Maze Runner" series. It was published on October 11, 2011 by Delacorte Press and was preceded by "The Maze Runner" and "The Scorch Trials" and followed by the series prequels, "The Kill Order and The Fever Code."
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        <answer>Delacorte Press</answer>
        \texttt{\textless end\_of\_turn\textgreater}\texttt{\textless eos\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{5cm}|p{8cm}|} % Fixed column widths
        \hline
        \textbf{Trajectory Type} & \textbf{Trajectory Text} \\
        \hline
        \textbf{Example Multi-Step Math Reasoning Trajectory with Calculator Tool Use Generated by SWiRL model on GSM8K} & 
        \raggedright 
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        Please help me answer the following question in just a few words. If you think it would help to use a calculator, please generate a mathematical query enclosed by  \texttt{\textless math\_exp\textgreater} QUERY \texttt{\textless /math\_exp\textgreater} tags. \newline
        Some questions may benefit from using a calculator multiple times in order to answer, so I will allow you to make up to 10 sequential calls before answering the question.\newline
        Please do not repeat queries you have already issued, as this is a waste of time. \newline
        I will provide search results in the following format: \newline
        \texttt{QUERY → RESULT}. \newline
        Once you have enough information, generate an answer enclosed by \texttt{\textless answer\textgreater}ANSWER\texttt{\textless /answer\textgreater} tags. \newline
        Please either issue a math query or answer the question, but not both. \newline
        The question is: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        \texttt{\textless math\_exp\textgreater}48 / 2 \texttt{\textless /math\_exp\textgreater} \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        48 / 2 -> 24.0 \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        \texttt{\textless math\_exp\textgreater}48 + 24\texttt{\textless /math\_exp\textgreater} \newline
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater user} \newline
        48 + 24 -> 72.0
        \texttt{\textless end\_of\_turn\textgreater} \\
        \texttt{\textless start\_of\_turn\textgreater model} \newline
        <answer>72</answer>
        \texttt{\textless end\_of\_turn\textgreater}\texttt{\textless eos\textgreater} \\
    \end{tabular}
    %\caption{Synthetic Data Generation with Multi-Step Search Tool Use}
\end{table}

\clearpage
\section{Example Ids}\label{sec:example-ids}

For reproducibility, we provide the ids of the examples used for evaluation.

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{12cm}|} % Fixed column widths
        \hline
        \textbf{HotPotQA Example Ids} \\
        \hline 
\tiny5ab46a295542990594ba9c1c, 5a85ad925542997b5ce40023, 5a8353285542996488c2e40d, 5a79be835542994bb9457052, 5a82a0fc55429954d2e2eb87, 5adc2c9355429947ff173904, 5ab1e71b554299340b525421, 5a7790ac5542992a6e59def9, 5a83e4195542990548d0b243, 5ab3239b554299194fa93574, 5ae1a460554299234fd042a8, 5a81e075554299676cceb128, 5a7f714c5542992097ad2f6e, 5ab639c055429953192ad2aa, 5a7c1fe4554299683c1c62cf, 5ab7cff355429928e1fe391e, 5aba5b2455429939ce03dc9c, 5a7173b45542994082a3e83c, 5a90049d55429933b8a20468, 5a8e0d7e5542995085b373b4, 5adfbf3155429906c02daa29, 5abf1fed5542990832d3a127, 5addf6415542990dbb2f7f25, 5a8138c155429938b6142300, 5a7ae77b554299042af8f6b0, 5ae293fb5542996483e649fe, 5ae40a8b55429970de88d8a9, 5ab457445542991751b4d748, 5a77d6025542995d83181301, 5a89c2715542993b751ca990, 5a7a4d845542990783324f04, 5ae0ef5e5542990adbacf6df, 5a72321f55429971e9dc934a, 5ac440355542995c82c4ad0d, 5a7dd8625542990b8f503ae8, 5ab48dd55542991779162cd9, 5abc3948554299700f9d782b, 5a8a7cb255429930ff3c0df8, 5ae1178e5542997b2ef7d0d6, 5abd08ae554299700f9d7980, 5ab1e5975542997061209590, 5a74dca85542996c70cfae1f, 5ab8348d55429934fafe6d13, 5a78f1ef55429974737f7919, 5ac16eb355429964131be1f5, 5ae5e12d55429929b08079e4, 5ade6bbf5542997c77adee24, 5adf573c5542995534e8c798, 5a8901d9554299515336125b, 5a89fd9e55429970aeb701e8, 5a7917d955429974737f7982, 5adc1017554299438c868d20, 5a8da5c355429941ae14dffe, 5a8cad265542996e8ac88b19, 5add4ae25542992200553a88, 5ae026eb55429924de1b703a, 5a74fcbe5542996c70cfae67, 5adfa8ac55429942ec259add, 5adbf4555542994650320c18, 5ac31609554299741d48a1c0, 5a7b65bf55429931da12ca86, 5a73870455429905862fe051, 5a8b009755429950cd6afc40, 5ae62b2d5542992ae0d1625b, 5a7b5d795542992d025e6825, 5ab3185755429976abd1bc5f, 5ac046475542996f0d89cb70, 5a89138255429951533612af, 5a85d69f5542997175ce2062, 5a82dfa455429940e5e1a938, 5a8730355542991e7718170f, 5a85b3455542994c784ddb4d, 5a8658c4554299211dda2b02, 5abd9fa55542996e802b4809, 5ab268aa5542993be8fa9908, 5ae5dcc755429929b08079d8, 5a727ef15542992359bc30c5, 5a8e2ba85542995a26add474, 5a84f9465542991dd0999e36, 5a87099455429960ec39b704, 5a864d835542994775f6073c, 5ab9bf3b554299743d22ebe6, 5a864dfc5542994775f6073f, 5a871ce055429960ec39b749, 5a8bd3375542997f31a41dd3, 5ab277965542993be8fa9919, 5abcea83554299114383a194, 5a897561554299515336130b, 5adfdf4a55429906c02daa7c, 5ae265bb5542992decbdccea, 5a84b3035542992a431d1a91, 5a77280b5542994aec3b71ff, 5ae4d41355429908b6326488, 5a76de035542994aec3b718d, 5a7d2045554299452d57bb09, 5abc7af15542993a06baf8ed, 5abddeb55542991f66106083, 5a8218855542990a1d231f4e, 5a732fbb5542992359bc3271, 5a8024ad5542992097ad2fde, 5ae142a4554299422ee9964a, 5a72d5155542991f9a20c5b4, 5a722a4b55429971e9dc931f, 5a7a9ca455429941d65f26f3, 5adfa5405542992d7e9f93ca, 5a7b8e3d55429927d897bfec, 5a7c6ac25542996dd594b925, 5abae9cd5542996cc5e49f04, 5ae18e37554299234fd0428f, 5a84d29d5542994c784dda60, 5ae44eeb5542995dadf2430f, 5adbe7b455429944faac23b0, 5abedd105542993fe9a41d63, 5a80a7df554299485f59867f, 5ab2f6b1554299545a2cfaea, 5ac29ddc554299657fa28fdc, 5a7222ce55429971e9dc92c7, 5ae221f15542994d89d5b366, 5a7f9cc25542995d8a8ddec2, 5abe42aa55429976d4830ac2, 5ae329e45542991a06ce993e, 5a882caa5542997e5c09a596, 5ac1a94455429964131be262, 5a762e0f5542992d0ec06052, 5a7918ec554299148911f9ef, 5a7e0bd25542997cc2c4750b, 5ab8af3c55429916710eb0ac, 5aba94465542994dbf019953, 5a82ef725542995ce29dcd0a, 5ab2a5fb554299545a2cf9ef, 5ab3d4ae5542992ade7c6ec5, 5ac25882554299636651998c, 5ae535f55542993aec5ec17c, 5ac55c915542993e66e8234f, 5adfcf7655429906c02daa49, 5a8a12555542992e4fca84f1, 5a8af82c55429950cd6afc31, 5a8c564b554299240d9c2128, 5a89efb25542992e4fca8497, 5ab58009554299637185c5b2, 5ae69a455542996d980e7c48, 5a8f8dfb5542997ba9cb32bb, 5a811e1955429903bc27b931, 5a81f2955542990a1d231eee, 5abc428955429959677d6a67, 5ac263a25542992f1f2b38a3, 5ac5190d5542996feb3fe9f8, 5a82fbfc55429954d2e2ebe5, 5abce73b5542993a06baf9a2, 5adbf672554299438c868cf0, 5a75dd02554299109176e5aa, 5a8200d055429926c1cdade2, 5a8090105542996402f6a55c, 5adfda36554299025d62a35e, 5a7f9e0155429969796c1aee, 5a7b5f64554299042af8f757, 5a8a7bfb5542996c9b8d5eff, 5ae73fae5542991bbc9761c9, 5a77b0795542992a6e59df89, 5ac178655542994ab5c67d5a, 5ab5eab35542992aa134a3dd, 5ab667be55429954757d328a, 5a7a333f5542996a35c17130, 5ac262a055429951e9e6859a, 5a87ae9d5542994846c1cdc6, 5ac1985e55429964131be248, 5a848c215542992a431d1a4f, 5a89a79c5542993b751ca970, 5a8e16d355429917b4a5bd18, 5a7289755542992359bc30d9, 5a7d1dd055429909bec76960, 5ac152e755429964131be1bb, 5ae7d4f4554299540e5a5659, 5ae21559554299492dc91bc2, 5a8935e6554299669944a506, 5a831cb955429966c78a6b3f, 5a77aa565542992a6e59df6a, 5abff5e95542997d6429596a, 5ae07634554299603e418412, 5ab4eb2b55429942dd415fa2, 5abd512655429924427fcfb4, 5a7ad0195542992d025e66fd, 5a7cf9b455429907fabef07c, 5ae0fa52554299422ee99594, 5ae24d1a5542992decbdcca6, 5a7144df5542994082a3e72f, 5ac0279c5542996f0d89cb3f, 5a88a93c5542994846c1cead, 5adec5955542992fa25da83f, 5abbfd00554299114383a0d4, 5a7b9cac554299042af8f78f, 5ab9020d5542991b5579f0ca, 5a7c1c595542990527d55456, 5a7c583e5542996dd594b910, 5a8e72f05542990e94052b13, 5a85a1015542991dd0999e6f, 5adcb8205542994ed6169bd2, 5a8cef7a554299441c6b9f8a, 5a7fee435542994857a7685b, 5a7b4f2c55429931da12ca66, 5abeaf8a5542997ec76fd346, 5abbe67e5542993f40c73c05, 5a8f4e8955429918e830d1f1, 5ac1a0e15542994ab5c67dab, 5a7a9b4755429941d65f26ef, 5a87c1ac5542997e5c09a565, 5ab962ff554299131ca4231f, 5a7b79c95542997c3ec971b0, 5abe3ac35542993f32c2a0ac, 5a7639d55542992db9473748, 5a7a2ec05542990198eaf0bc, 5ac3d31a5542995ef918c249, 5abae3eb5542996cc5e49ee2, 5adff38b55429925eb1afb7d, 5ab7530b55429928e1fe3849, 5a88dcef55429938390d3fe3, 5ae0027b55429942ec259bda, 5a85ec815542994775f606af, 5ac172a15542994d76dcce2e, 5ac073eb5542996f0d89cbd8, 5ac5262755429924173fb60f, 5a8e72fe5542990e94052b14, 5a76133755429976ec32bcff, 5ae6b38c5542992ae0d16392, 5ab98fee554299131ca4237c, 5ac0e564554299294b219045, 5a72edeb5542992359bc31da, 5a7b663355429931da12ca87, 5a7cbe0f55429909bec767ee, 5a845bdd5542996488c2e524, 5a8a28b55542996c9b8d5e23, 5ae5fb975542996de7b71aa8, 5aba9cff5542994dbf01997e, 5ae11f0b5542997b2ef7d0e0, 5abe16c655429976d4830a71, 5abbdd355542992ccd8e7fc6, 5abedbfa5542993fe9a41d5f, 5a792421554299148911fa09, 5a80c5f6554299260e20a151, 5ab4136b5542996a3a969f18, 5adc375055429944faac246c, 5ac14d9d55429964131be1ab, 5abf23a65542997ec76fd3d7, 5a7e1d4255429965cec5ea79, 5ae63c8f5542992663a4f27c, 5ae71816554299572ea546d1, 5ae4bdeb55429913cc2044ee, 5ae4a09e5542996836b02ced, 5ac2312755429964131be2c3, 5ae36d325542992e3233c3f8, 5a7d68045542995f4f40226d, 5aba88d555429901930fa811, 5a8e1e4b554299068b959e63, 5a7e6d325542991319bc94a7, 5ab96d865542996be20204df, 5ae4d2c255429960a22e01f6, 5a8053cf5542992097ad2fe0, 5a8db1b75542994ba4e3dd01, 5a8d40c95542994ba4e3dc3b, 5ae5af10554299546bf82f23, 5a8d48ff5542994ba4e3dc5a, 5ab5f694554299488d4d9a66, 5a8f99bc55429918e830d28d, 5add0ed35542990d50227dac, 5a8c38235542995e66a4755f, 5ab6ccf155429954757d3372, 5ae44fe75542995dadf24314, 5adcb67e5542994ed6169bca, 5abe833d5542993f32c2a140, 5a8b002155429950cd6afc3e, 5a76f3c65542994aec3b719a, 5ab5207c5542996a3a96a02b, 5a8a73dd5542996c9b8d5eee, 5a9063c955429933b8a2050f, 5a7b45c855429931da12ca4a, 5a8e8b6c5542990e94052b43, 5a7a57935542990783324f1d, 5abe225c5542991f661060ec, 5a72a6b65542994cef4bc3b7, 5ab7f3625542995dae37ea06, 5a7cfdda55429907fabef095, 5a8994505542993b751ca950, 5ae308775542992decbdcdcd, 5ab72f32554299110f219ac3, 5a7b93e05542995eb53be961, 5a88710b554299206df2b26b, 5ab6259855429953192ad272, 5ac29ca6554299218029dac0, 5ac0ab335542992a796ded5d, 5ade469c5542992fa25da722, 5ab318a0554299233954ff07, 5ab1f75d554299340b525443, 5ade5664554299728e26c6d5, 5ae4a3b65542995ad6573dee, 5ae40e3955429970de88d8c5, 5ab9025855429934fafe6e47, 5a82100955429926c1cdae1e, 5ac5138c5542994611c8b36a, 5ab2eb7755429929539468b9, 5ab738945542993667793f97 \\
\bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{12cm}|} % Fixed column widths
        \hline
        \textbf{CofCA Example Ids} \\
        \hline 
\tiny5a866fee5542991e77181657, 5a7db2f75542990b8f503a34, 5a8ee0a35542990e94052ba0, 5ae525835542990ba0bbb1cd, 5ac4bfd05542997ea680caab, 5ac4c61a5542996feb3fe93c, 5abbbd0f55429931dba144d5, 5a89372855429951533612e6, 5ab381b155429969a97a816b, 5ac2a912554299218029dae8, 5ae3345f55429928c4239682, 5a7bb3d9554299294a54aaa0, 5abaa25155429901930fa868, 5ac39a1c554299657fa290f9, 5a73332b5542992359bc3287, 5ae655c855429908198fa599, 5add82fc5542997545bbbd57, 5add117e5542990d50227db2, 5ab93287554299753720f78f, 5ab979da554299131ca4233a, 5a79c7f95542994bb9457099, 5ab58ae15542992aa134a357, 5ae3bdfa5542990afbd1e1c0, 5add7d055542990dbb2f7e61, 5ae136f655429920d5234325, 5a80b4635542992bc0c4a7bd, 5ab93287554299753720f78f, 5ae3b4d05542992f92d82349, 5ae77a31554299540e5a55c7, 5ae0d91e55429924de1b7198, 5ae64cab5542991bbc9760be, 5ab865be5542990e739ec8e5, 5a804fc45542992bc0c4a6f0, 5ac2ffa9554299218029dbb2, 5ae7b03e5542993210983ef6, 5a77153355429937353601c8, 5ae61be055429929b0807ace, 5a8ae6c055429950cd6afbce, 5ae64cab5542991bbc9760be, 5a72a00d5542991f9a20c53c, 5ae7b03e5542993210983ef6, 5a8eacc75542995085b37473, 5ab9253c554299131ca4227f, 5a8ee0a35542990e94052ba0, 5a866fee5542991e77181657, 5a888a8a5542997e5c09a603, 5abeed7e5542993fe9a41da0, 5ae4b3da55429913cc2044d6, 5add28c85542992ae4cec4be, 5abffc58554299012d1db552, 5a8bab4e554299240d9c207c, 5abae52a5542996cc5e49eea, 5abba27f5542996606241708, 5ab6ad2855429953192ad35e, 5aba6b2d55429901930fa7a9, 5abc145b554299658360041f, 5a7336d05542991f9a20c68d, 5ac3b0f15542995ef918c1fc, 5ac3ad225542995ef918c1da, 5a7a06935542990198eaf050, 5ae6038155429929b0807a55, 5ab3dde2554299753aec59d6, 5ab381b155429969a97a816b, 5a77bd595542995d83181291, 5a76cb6e5542994aec3b717a, 5a7524ca55429929fddd850a, 5ade025e5542997dc790711e, 5ac17f4f5542994ab5c67d70, 5ae0fa8b5542997b2ef7d0c6, 5a7336d05542991f9a20c68d, 5a8b560855429950cd6afcba, 5adce28f5542990d50227d52, 5ac491eb5542996feb3fe8d2, 5a7fe9975542994857a76847, 5a72b2695542991f9a20c56f, 5a89372855429951533612e6, 5ac219df5542992f1f2b37fc, 5a8a84775542996c9b8d5f19, 5abd7ca05542993062266cab, 5ac07a585542996f0d89cbf0, 5a8e171b554299068b959e5a, 5a79e0445542994f819ef0e7, 5ae0d26455429945ae959473, 5a8f0e065542997ba9cb319c, 5adce28f5542990d50227d52, 5a8f7de3554299458435d657, 5adc1309554299438c868d3b, 5ac219df5542992f1f2b37fc, 5a80043055429969796c1ba0, 5ac39f2a554299391541382d, 5a72b1c25542992359bc3172, 5abe3f9455429976d4830aaa, 5a8f7de3554299458435d657, 5ab74412554299110f219ae8, 5a904e725542995651fb5118, 5a7a02235542996c55b2dcd3, 5adc318c5542996e685252d5, 5a78cdf7554299029c4b5e9f, 5ade8f5e55429975fa854f11, 5ab865be5542990e739ec8e5, 5abaf9df5542996cc5e49f45, 5adcf28c5542994ed6169c30, 5a7e7bf455429949594199d6, 5adbe1e755429947ff173853, 5a83168855429966c78a6b2e, 5adc134b5542994650320c5c, 5a90c58255429916514e756c, 5a8efd3c55429918e830d179, 5abbdc135542993f40c73bf6, 5add7d055542990dbb2f7e61, 5ab344af554299753aec5969, 5a8a35625542992d82986efd, 5ab3dad4554299753aec59cb, 5a8dcd8e55429941ae14e060, 5ae377155542991a06ce99c7, 5a7cb48a5542996dd594b9a1, 5ac143535542991316484aac, 5ac31c9d554299741d48a203, 5ae5569255429908b63265e4, 5ab93287554299753720f78f, 5abd04f15542996e802b467e, 5a72b2695542991f9a20c56f, 5ab59b045542997d4ad1f190, 5a7f3d325542992e7d278cb5, 5ae061d5554299603e41840e, 5ae56d31554299546bf82ed7, 5ae255db5542992decbdccc1, 5ab6e856554299710c8d1fac, 5a7a358f5542990783324ec1, 5a7f38ae5542992e7d278c99, 5ab5c9c5554299494045f065, 5ac061ab554299294b218fac, 5a8ee0a35542990e94052ba0, 5ae3bdfa5542990afbd1e1c0, 5ab561d85542992aa134a2fc, 5ae3d8dc5542992f92d8239c, 5a7bb3d9554299294a54aaa0, 5abb1f745542996cc5e49fb5, 5adce28f5542990d50227d52, 5a904e725542995651fb5118, 5add992c5542997545bbbd83, 5adc1309554299438c868d3b, 5adfd35b55429906c02daa54, 5ab39701554299233954ff5e, 5a8b58b955429950cd6afcc2, 5ae22d035542996483e64925, 5a7fa53c5542995d8a8ddedc, 5a84322b5542996488c2e50d, 5a8d0006554299441c6b9fa8, 5add82fc5542997545bbbd57, 5a80d30655429938b61421fe, 5a72b2695542991f9a20c56f, 5a81ff1d554299676cceb1c3, 5ae755665542997b22f6a6e9, 5a79e0445542994f819ef0e7, 5ae4c2145542995dadf243e7, 5abbbd0f55429931dba144d5, 5a7ccec9554299452d57ba72, 5a7bb3d9554299294a54aaa0, 5a8355f9554299123d8c20f3, 5ab5141a5542991779162d70, 5ae4c2145542995dadf243e7, 5ae2e27155429928c423952a, 5abee5e25542994516f45473, 5ab698885542995eadef002a, 5a7f98e655429969796c1ad8, 5a77bd595542995d83181291, 5a78ed46554299148911f9a6, 5ae377155542991a06ce99c7, 5ae614055542996de7b71b2a, 5a823ae45542990a1d231f6d, 5ab520565542996a3a96a02a, 5ac168865542994ab5c67d14, 5ac1944c5542996f0d89cc90, 5a7cedca55429909bec7689c, 5ab707c05542991d32223760, 5ae27edc5542992decbdcd2d, 5ab979da554299131ca4233a, 5ab345db55429969a97a8122, 5a88fea05542997e5c09a6e9, 5ae3b4d05542992f92d82349, 5ab39701554299233954ff5e, 5add992c5542997545bbbd83, 5ab5e6d65542997d4ad1f232, 5a88b7735542993e715ac079, 5adfd35b55429906c02daa54, 5a8514545542992a431d1ad2, 5adfd35b55429906c02daa54, 5ac538ef5542994611c8b437, 5ab520565542996a3a96a02a, 5a74fbe55542996c70cfae63, 5ab55435554299488d4d9939, 5ae31a9c55429928c42395ef, 5ab67b8f55429954757d32f0, 5ae13f525542997b2ef7d169, 5a7d1f605542995ed0d165fb, 5ade52e85542997c77adedfa, 5a7607d7554299109176e61a, 5a85603a5542997b5ce3fff1, 5ac17f4f5542994ab5c67d70, 5a7fa53c5542995d8a8ddedc, 5abaef34554299660624169c, 5ae3d8dc5542992f92d8239c, 5ae0fa8b5542997b2ef7d0c6, 5ab55435554299488d4d9939, 5a904e725542995651fb5118, 5a879c8e5542994846c1cdb3, 5a870d0255429960ec39b710, 5ab3dde2554299753aec59d6, 5ac3ad225542995ef918c1da, 5ae11a6755429901ffe4ad8d, 5ab9116f5542991b5579f0db, 5ae755665542997b22f6a6e9, 5ae316f355429928c42395e3, 5abfbb455542997ec76fd440, 5a88377c5542997e5c09a5a7, 5a8099025542996402f6a588, 5a74248855429929fddd83e5, 5ac39a1c554299657fa290f9, 5abbc70d5542992ccd8e7f9b, 5ae13f525542997b2ef7d169, 5ac1f7f355429964131be2ae, 5a84322b5542996488c2e50d, 5a7738dc554299373536021f, 5a760f6855429976ec32bcf9, 5a7f38ae5542992e7d278c99, 5ae655c855429908198fa599, 5a821ffa5542990a1d231f5c, 5a90c2b35542995651fb51df, 5a78ed46554299148911f9a6, 5a8454e85542992ef85e23be, 5a8514545542992a431d1ad2, 5ac168865542994ab5c67d14, 5a88b7735542993e715ac079, 5a77aff55542992a6e59df86, 5ab39701554299233954ff5e, 5ac219df5542992f1f2b37fc, 5ab67b8f55429954757d32f0, 5a7a0d455542990783324e13, 5a8461d55542990548d0b29b, 5a879ab05542996e4f30887e, 5ae5365d5542992663a4f16d, 5a7a0d455542990783324e13, 5a7f9ee855429969796c1af3, 5ae5365d5542992663a4f16d, 5a736bfa5542991f29ee2e03, 5abfbb455542997ec76fd440, 5a8f8f345542997ba9cb32c2, 5ab9121555429919ba4e238a, 5a8dfbeb5542995085b3736e, 5a8a35625542992d82986efd, 5ac31c9d554299741d48a203, 5ae5365d5542992663a4f16d, 5add28065542990d50227e08, 5ae64cbf5542992ae0d162c1, 5adc134b5542994650320c5c, 5ac31c9d554299741d48a203, 5adf2b325542993a75d2640b, 5ae755665542997b22f6a6e9, 5a8454e85542992ef85e23be, 5a7cc5ae55429909bec767fc, 5a8a84775542996c9b8d5f19, 5ae377a35542994393b9e6db, 5ac4fa8c55429924173fb536, 5a77aff55542992a6e59df86, 5ae31a9c55429928c42395ef, 5adf5ebd5542995ec70e8fd8, 5a8a4bdc55429930ff3c0d8c, 5ae77a31554299540e5a55c7, 5ac2adf3554299657fa2900f, 5ab5a2f85542997d4ad1f197, 5abd7cb855429924427fd00a, 5ae136f655429920d5234325, 5ae525835542990ba0bbb1cd, 5a7738dc554299373536021f, 5a7a52745542996c55b2dd4f, 5ae1f61a5542994d89d5b2e1, 5add28c85542992ae4cec4be, 5a8bdef85542997f31a41dea, 5ae614055542996de7b71b2a, 5a7336d05542991f9a20c68d, 5a8eacc75542995085b37473, 5a8cdc5255429941ae14df21, 5ae664955542992ae0d1631b, 5ae2aba15542996483e64a32, 5abba27f5542996606241708, 5abd7ca05542993062266cab, 5ac1a5cd5542994d76dcce94, 5a736bfa5542991f29ee2e03, 5a8f0e065542997ba9cb319c, 5a8a2d805542996c9b8d5e2e, 5ae546e85542992663a4f1b5, 5ab6e856554299710c8d1fac, 5aba0e675542994dbf0198a0, 5ae3345f55429928c4239682, 5a7a02235542996c55b2dcd3, 5ac4fa8c55429924173fb536, 5a8beddd5542995d1e6f1468, 5abd90545542996e802b47d7, 5a7e39515542995ed0d166da\\
\bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \small % Adjust font size
    \begin{tabular}{|p{12cm}|} % Fixed column widths
        \hline
        \textbf{MuSiQue Example Ids} \\
        \hline 
\tiny2hop\_\_376129\_44537, 2hop\_\_764465\_126539, 3hop1\_\_434518\_136629\_55288, 2hop\_\_353084\_36340, 2hop\_\_344450\_160798, 2hop\_\_637856\_351187, 2hop\_\_760990\_44191, 3hop1\_\_162325\_11248\_3752, 2hop\_\_326799\_278127, 2hop\_\_239927\_62031, 2hop\_\_153813\_69936, 3hop1\_\_213491\_782843\_75255, 2hop\_\_2846\_2741, 2hop\_\_3880\_909, 2hop\_\_347735\_36735, 2hop\_\_144393\_87372, 4hop1\_\_709382\_146811\_31223\_45305, 2hop\_\_143434\_20122, 2hop\_\_21457\_74218, 3hop1\_\_129597\_517267\_451901, 2hop\_\_469317\_776926, 2hop\_\_27032\_5400, 3hop2\_\_83954\_32417\_24628, 3hop2\_\_14790\_57411\_86234, 2hop\_\_78490\_49700, 3hop1\_\_228008\_354329\_5303, 2hop\_\_631861\_160851, 3hop1\_\_662283\_507729\_351187, 2hop\_\_482727\_20661, 3hop1\_\_858308\_102146\_84004, 2hop\_\_565717\_77346, 3hop1\_\_470555\_668347\_492654, 2hop\_\_25478\_65517, 2hop\_\_129389\_31248, 2hop\_\_527889\_5365, 2hop\_\_20857\_20779, 2hop\_\_770\_919, 2hop\_\_375649\_80178, 3hop1\_\_332614\_131794\_17114, 2hop\_\_144295\_211364, 2hop\_\_108160\_159045, 2hop\_\_46545\_88521, 2hop\_\_518906\_44191, 2hop\_\_733628\_131886, 4hop1\_\_28235\_74795\_84660\_15312, 2hop\_\_104341\_92821, 2hop\_\_445544\_127008, 2hop\_\_46766\_79233, 2hop\_\_342213\_185893, 2hop\_\_528837\_126102, 2hop\_\_497897\_541630, 3hop1\_\_48619\_26424\_581618, 2hop\_\_87287\_83906, 4hop1\_\_411538\_805015\_475503\_32631, 2hop\_\_658198\_72962, 2hop\_\_42307\_120207, 2hop\_\_30878\_555599, 3hop1\_\_8373\_87072\_45358, 3hop2\_\_337255\_48727\_83343, 2hop\_\_251450\_8796, 3hop1\_\_161080\_639509\_644660, 2hop\_\_558231\_52667, 2hop\_\_424189\_49441, 3hop1\_\_821692\_74047\_756423, 2hop\_\_531731\_79705, 3hop1\_\_257981\_259472\_611044, 2hop\_\_370765\_14904, 2hop\_\_446352\_14183, 2hop\_\_81087\_13292, 2hop\_\_684971\_333904, 2hop\_\_234176\_69926, 2hop\_\_858097\_121880, 4hop2\_\_724536\_444580\_75897\_631997, 2hop\_\_492509\_70585, 4hop1\_\_405751\_4520\_65397\_49736, 2hop\_\_128610\_126060, 3hop1\_\_325154\_786384\_42990, 2hop\_\_34130\_56335, 2hop\_\_145997\_63766, 2hop\_\_146446\_690423, 2hop\_\_225632\_11125, 2hop\_\_856457\_495, 2hop\_\_129234\_330515, 2hop\_\_15674\_42467, 3hop1\_\_161946\_84298\_53741, 2hop\_\_48959\_83539, 2hop\_\_64650\_20556, 3hop1\_\_316518\_395352\_131877, 2hop\_\_136618\_92216, 2hop\_\_199336\_185893, 2hop\_\_930\_57555, 3hop1\_\_31942\_48661\_15069, 2hop\_\_35105\_160978, 2hop\_\_128804\_351187, 2hop\_\_153004\_86587, 2hop\_\_715365\_565667, 2hop\_\_401484\_135138, 2hop\_\_52622\_67783, 2hop\_\_713501\_58946, 2hop\_\_300786\_39199, 2hop\_\_5430\_5348, 3hop2\_\_29467\_132027\_73594, 3hop1\_\_225298\_755188\_480696, 2hop\_\_367037\_80178, 2hop\_\_343473\_53204, 2hop\_\_848923\_66214, 3hop1\_\_369072\_287321\_161879, 2hop\_\_250315\_64214, 3hop1\_\_104311\_833580\_61459, 2hop\_\_1835\_322987, 3hop1\_\_836616\_291186\_4303, 2hop\_\_531924\_1094, 2hop\_\_131831\_84128, 2hop\_\_328708\_90697, 2hop\_\_704691\_82816, 2hop\_\_80353\_3001, 2hop\_\_196785\_61424, 2hop\_\_130964\_47336, 3hop1\_\_761109\_548045\_159613, 3hop1\_\_4525\_52205\_55099, 3hop1\_\_58522\_787757\_69397, 2hop\_\_58284\_37793, 2hop\_\_487591\_7672, 2hop\_\_250913\_58115, 2hop\_\_131095\_85298, 2hop\_\_144937\_8600, 3hop2\_\_625639\_25582\_21116, 3hop2\_\_30023\_63595\_53125, 2hop\_\_584872\_88978, 2hop\_\_116643\_351162, 2hop\_\_826203\_62031, 2hop\_\_85036\_909, 2hop\_\_62996\_299942, 2hop\_\_236731\_229413, 2hop\_\_15169\_87091, 2hop\_\_143791\_75878, 2hop\_\_658198\_90536, 2hop\_\_70321\_15755, 2hop\_\_131105\_68117, 2hop\_\_143162\_438686, 2hop\_\_20771\_65517, 2hop\_\_65149\_46180, 2hop\_\_251426\_88653, 3hop1\_\_238983\_403313\_61770, 2hop\_\_28291\_709757, 2hop\_\_391909\_3430, 3hop1\_\_266733\_291186\_50964, 2hop\_\_205685\_160137, 2hop\_\_343141\_702969, 3hop1\_\_383692\_434040\_59381, 2hop\_\_240975\_736878, 2hop\_\_507864\_368521, 3hop1\_\_723003\_593059\_76293, 2hop\_\_109234\_62766, 4hop1\_\_16401\_4520\_65397\_52251, 2hop\_\_140591\_256194, 2hop\_\_104757\_74309, 2hop\_\_194976\_55566, 2hop\_\_361127\_140822, 3hop1\_\_108774\_104782\_14771, 4hop3\_\_393686\_620110\_61746\_261712, 2hop\_\_324178\_83854, 3hop1\_\_849536\_301867\_127418, 2hop\_\_24408\_541630, 2hop\_\_54755\_729624, 2hop\_\_693650\_61232, 3hop1\_\_89787\_49283\_632017, 4hop1\_\_104663\_221169\_833580\_61459, 2hop\_\_664573\_36741, 3hop1\_\_702271\_823374\_26254, 2hop\_\_129892\_62851, 3hop1\_\_659125\_39490\_23352, 2hop\_\_222162\_386543, 2hop\_\_446009\_412262, 2hop\_\_781841\_77980, 3hop1\_\_706183\_20196\_10585, 2hop\_\_809948\_162428, 3hop1\_\_458602\_681261\_369731, 2hop\_\_529082\_114112, 3hop1\_\_388966\_508834\_145463, 2hop\_\_582169\_370960, 2hop\_\_225632\_52135, 2hop\_\_302491\_81463, 2hop\_\_136889\_52356, 2hop\_\_81363\_42667, 3hop1\_\_599980\_544161\_92922, 2hop\_\_504710\_513189, 2hop\_\_145939\_11443, 2hop\_\_320353\_4018, 2hop\_\_27033\_85063, 2hop\_\_145110\_861627, 2hop\_\_149891\_44359, 2hop\_\_376266\_37939, 3hop2\_\_10879\_37094\_161133, 3hop2\_\_159915\_8509\_19700, 4hop1\_\_15118\_31258\_43153\_32993, 3hop1\_\_522518\_132413\_16066, 2hop\_\_129782\_517267, 3hop1\_\_252998\_715836\_26008, 4hop1\_\_205937\_144938\_83779\_44678, 2hop\_\_131318\_47465, 2hop\_\_338405\_68172, 4hop3\_\_3153\_3356\_11988\_24628, 2hop\_\_106465\_54210, 2hop\_\_397761\_404718, 4hop1\_\_632232\_164954\_6975\_6891, 2hop\_\_121872\_708662, 2hop\_\_73501\_31113, 2hop\_\_378511\_191233, 3hop1\_\_85045\_96305\_25007, 3hop1\_\_755950\_592709\_78102, 2hop\_\_811421\_377891, 3hop2\_\_63595\_391767\_53125, 2hop\_\_131380\_84859, 3hop1\_\_158678\_48408\_37793, 3hop1\_\_7312\_830682\_68600, 2hop\_\_207212\_21032, 3hop1\_\_10725\_695397\_74345, 2hop\_\_445228\_774871, 4hop1\_\_603090\_818753\_783943\_26110, 2hop\_\_177131\_646483, 3hop1\_\_801682\_192919\_16121, 2hop\_\_243908\_500443, 3hop2\_\_89818\_157704\_4107, 2hop\_\_160546\_26427, 2hop\_\_128772\_745471, 2hop\_\_62588\_20779, 2hop\_\_661636\_82027, 2hop\_\_105388\_89066, 2hop\_\_368185\_131944, 3hop1\_\_153577\_411195\_8682, 2hop\_\_327451\_90697, 2hop\_\_647590\_134798, 3hop2\_\_30796\_804098\_24137, 2hop\_\_146227\_42328, 2hop\_\_152881\_620955, 2hop\_\_11693\_42892, 2hop\_\_753498\_7606, 2hop\_\_2795\_2741, 3hop1\_\_373317\_533132\_1660, 2hop\_\_229374\_333904, 3hop1\_\_370820\_301867\_127418, 3hop1\_\_713250\_4016\_83854, 2hop\_\_130414\_68117, 4hop1\_\_7312\_84360\_334118\_41330, 2hop\_\_65149\_68376, 2hop\_\_182310\_565529, 3hop1\_\_136299\_84467\_89676, 2hop\_\_454055\_86874, 2hop\_\_604878\_40786, 2hop\_\_307569\_51671, 2hop\_\_854082\_159115, 2hop\_\_198557\_55566, 3hop1\_\_352446\_506157\_44678, 2hop\_\_468848\_44537, 2hop\_\_207571\_126101, 4hop2\_\_53235\_18485\_57802\_311656, 2hop\_\_451164\_140822, 3hop1\_\_37692\_84298\_53741, 3hop1\_\_672119\_196807\_760519, 3hop2\_\_131210\_661360\_54023, 2hop\_\_8531\_24846, 3hop2\_\_77886\_64137\_69951, 2hop\_\_730762\_8600, 2hop\_\_350323\_45731, 2hop\_\_131117\_53519, 3hop1\_\_157534\_275705\_81669, 2hop\_\_185628\_677577, 2hop\_\_77119\_20732, 2hop\_\_67755\_82010, 3hop1\_\_790278\_593059\_76293, 3hop2\_\_162189\_611045\_73761, 2hop\_\_568848\_50788, 2hop\_\_45625\_61952, 2hop\_\_146207\_30651, 2hop\_\_57439\_78714, 2hop\_\_3756\_52135, 3hop1\_\_501828\_348668\_856982, 3hop1\_\_106423\_35178\_686699, 2hop\_\_103203\_23140, 3hop1\_\_77985\_66386\_16350, 2hop\_\_664921\_579740, 2hop\_\_106125\_20644, 2hop\_\_400998\_61424, 3hop1\_\_35884\_161545\_16532, 2hop\_\_584521\_755188, 2hop\_\_80508\_400874, 2hop\_\_664137\_58115, 2hop\_\_453207\_80674, 3hop1\_\_29335\_30907\_24600, 2hop\_\_144364\_68900, 2hop\_\_226817\_482901, 4hop3\_\_39198\_75897\_8509\_19700, 2hop\_\_713863\_64008, 2hop\_\_71269\_36735, 2hop\_\_504228\_64689, 2hop\_\_604878\_18657, 2hop\_\_81372\_303417, 3hop1\_\_674688\_707133\_72062, 2hop\_\_157766\_18657\\
\bottomrule
    \end{tabular}
\end{table}

\end{document}