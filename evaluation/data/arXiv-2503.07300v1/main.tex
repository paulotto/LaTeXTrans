\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{caption}
% name tags
\newcommand{\jiarui}[1]{{\color{blue}[Jiarui: #1]}}
\newcommand{\tianfan}[1]{{\color{red}[Tianfan: #1]}}
\definecolor{darkred}{rgb}{0.7, 0.0, 0.0}
\definecolor{darkgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{darkblue}{rgb}{0.10, 0.17, 0.8}
\newcommand{\yujin}[1]{{\color{darkgreen}[Yujin: #1]}}
\newcommand{\xxx}{{\color{red}xxx\xspace}}

% macros
\newcommand{\policy}{\pi}
\newcommand{\grad}{\nabla}
\newcommand{\etal}{et al. }
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\pipeline}{image processing pipeline\xspace}
\newcommand{\tuning}{photo finishing tuning\xspace}
\newcommand{\task}{photo finishing tuning\xspace}
\newcommand{\taskPFT}{photo finishing tuning\xspace}
\newcommand{\taskPST}{photo stylization tuning\xspace}
\newcommand{\TaskPFT}{Photo finishing tuning\xspace}
\newcommand{\TaskPST}{Photo stylization tuning\xspace}
\newcommand{\styletTuning}{Photo Stylization Tuning\xspace}


\title{Goal Conditioned Reinforcement Learning for Photo Finishing Tuning}
% reinforcement learning based
% exploring goal conditioned RL for
% goal conditioned photo finishing policy
% PTA: photo tuning policy based on goal condition reinforcement learning
% photo tuning agent



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

% \makeatletter
% \renewcommand\@fnsymbol[1]{\ifcase#1\or *\or \ddagger\or \dagger\or \S\or \P\else #\fi}
% \makeatother


\def \cuhk{$^2$}
\def \pjlab{$^1$}
\def \affspace{$^,$}
\author{%
  Jiarui Wu\pjlab\affspace\cuhk\thanks{This work was done while Jiarui Wu interned at Shanghai AI Laboratory.}\hspace{4pt},
  Yujin Wang\pjlab\thanks{Corresponding authors.}\hspace{4pt},
  Lingen Li\pjlab\affspace\cuhk,
  Fan Zhang\pjlab,
  Tianfan Xue\cuhk
  \vspace{5pt}
  \\
  \pjlab Shanghai AI Laboratory \ \cuhk The Chinese University of Hong Kong \vspace{2pt} \\
   \texttt{\{wj024,tfxue\}@ie.cuhk.edu.hk, lgli@link.cuhk.edu.hk,} \\  
   \texttt{\{wangyujin,zhangfan\}@pjlab.org.cn} \\
  % \\ \ \pjlab \texttt{\{lliu,theobalt\}@mpi-inf.mpg.de} \\
}

\begin{document}


\maketitle
\vspace{-10pt}
\begin{abstract}


Photo finishing tuning aims to automate the manual tuning process of the photo finishing pipeline, like Adobe Lightroom or Darktable. Previous works either use zeroth-order optimization, which is slow when the set of parameters increases, or rely on a differentiable proxy of the target finishing pipeline, which is hard to train.
To overcome these challenges, we propose a novel goal-conditioned reinforcement learning framework for efficiently tuning parameters using a goal image as a condition. Unlike previous approaches, our tuning framework does not rely on any proxy and treats the photo finishing pipeline as a black box. Utilizing a trained reinforcement learning policy, it can efficiently find the desired set of parameters within just 10 queries, while optimization-based approaches normally take 200 queries. Furthermore, our architecture utilizes a goal image to guide the iterative tuning of pipeline parameters, allowing for flexible conditioning on pixel-aligned target images, style images, or any other visually representable goals. We conduct detailed experiments on photo finishing tuning and photo stylization tuning tasks, demonstrating the advantages of our method.
Project website: \href{https://openimaginglab.github.io/RLPixTuner/}{https://openimaginglab.github.io/RLPixTuner/}.

\end{abstract}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\section{Introduction}


Image processing pipelines (ISPs) are widely used by photographers and artists to retouch images to match their desired appearance. Existing pipelines like Adobe Lightroom and Darktable allow users to interactively tweak meaningful sliders such as exposure, white balance, and contrast, which control the pipeline to perform a series of non-destructive edits to the input image. Though users can manually tune the slider parameters, it is laborious and time-consuming even for experienced experts. To this end, automatic photo finishing algorithms have been introduced to automate the process and have drawn growing attention in the community~\cite{hu2018exposure}.

In this work, we aim to design an automatic tuning algorithm for black-box non-differentiable image processing pipelines. Although some preliminary research tries to propose fully differentiable image processing pipelines~\cite{hu2018exposure,gharbi2016deep} or approximate the existing pipelines using neural networks~\cite{tseng2022neural}, they only support a limited set of image processing operations. On the other side, most commercial image processing pipelines, like Adobe Lightroom, are still black-box and non-differentiable, with a set of tunable parameters exposed to users. Under this setup, the goal of pipeline tuning is to automatically find the set of optimal parameters to achieve a desired image appearance, named the tuning target. More specifically, in this work, we study two different tuning targets. One is a target image with the same content as the input, but with a different rendering style, and we call it \emph{\taskPFT}. The other is a target style image with different content, and the algorithm is to render the target in a similar way as the style target, and we call that \emph{\taskPST}.

One previous solution for \tuning is to use zeroth-order or first-order optimization. However, both of them are either time-consuming or limited to a small set of parameters. Zeroth-order optimizations~\cite{hansen2006cma, nishimura2018automatic, mosleh2020hardware, chen2017zoo} are gradient-free searching methods and thus are normally very slow when the search space increases.
First-order optimizations~\cite{tseng2019hyperparameter, yu2021reconfigisp, tseng2022neural} accelerate the searching process using gradient descent, but they either require the processing pipeline itself to be differentiable, or a neural proxy is pre-trained to find a differentiable proxy of the original pipeline. For a complex commercial imaging pipeline, like cellphone camera pipelines, this proxy may not fully reproduce original pipelines~\cite{tseng2019hyperparameter}. Neural photo-finishing~\cite{tseng2022neural} improves the proxy accuracy by breaking a complex pipeline into small modules, but this does not apply to black-box or dynamically reconfigurable pipelines~\cite{yu2021reconfigisp}. Considering all these limitations, this brings a challenging question: Is there an efficient parameter-searching algorithm that is applicable to non-differentiable finishing pipelines?

\begin{figure}[t]
\centering
  \includegraphics[width=1\linewidth]{figures/teaser_v3.pdf}
  \caption{In this work, we propose an RL-based photo finishing tuning algorithm that efficiently tunes the parameters of a black-box \pipeline to match any tuning target. The RL-based solution (top row) takes only about 10 iterations to achieve a similar PSNR as the 500-iteration output of a zeroth-order algorithm (bottom row). Our method demonstrates fast convergence, high quality, and no need for a proxy.\vspace{-20pt}}
  \label{fig:teaser}
  % \vspace{-20pt}
\end{figure}


To solve this challenge, we propose a novel goal-conditioned reinforcement learning (RL) approach dedicated to \tuning. At each RL iteration, the policy network takes the tuning target and the currently tuned image as input, and finds a better set of parameters that makes the finishing results closer to the target. This RL-based searching algorithm has several advantages over traditional optimization methods. Compared with the zeroth-order solution, the RL policy can more accurately predict the potential searching direction, while zeroth-order searching can only rely on less effective tries. As a result, RL searching is much more efficient. As shown in Fig.~\ref{fig:teaser}, the RL-based solution (top row) only takes about 10 iterations to reach a similar PSNR as the 500-iteration output of a zeroth-order algorithm (bottom row). Also, compared with first-order optimization, RL-based tuning directly optimizes the non-differentiable \pipeline without a differential proxy. Therefore, it is not limited by the variety and complexity of image processing operations and pipelines, achieving much better tuning results. As shown in Fig.~\ref{fig:teaser}, RL-based tuning reaches 38.92dB (top row), while the first-order solution only obtains 18.69dB (middle row).

To train an efficient RL policy for tuning, we also propose a novel state representation dedicated to this task. The state representation should model the relationship between the photo editing space and our policy. To achieve that, our state representation consists of three key components: a CNN-based feature representation to encode global and local features, a photo statistics representation to match the photographic statistics between the input and the goal, and an embedding of historical actions. These representations better fit the RL policy into our task and guide the policy to generate the next set of parameters effectively. Lastly, we also design reward functions for both \taskPFT and \taskPST, enabling our framework to tune photos to different targets.

We validate the effectiveness of our framework with extensive experiments on both \taskPFT and \taskPST. Our RL-based framework significantly outperforms previous methods in both tasks in terms of both efficiency and image quality. Experimental results demonstrate that our goal-conditioned policy is an efficient photo finishing tuner capable of performing fine-grained control on \pipeline parameters to achieve various goals.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\vspace{-5pt}
\noindent\textbf{ISP tuning.}

Recent developments in end-to-end AI-based ISP pipelines show potential as alternatives to traditional mobile ISPs~\cite{ignatov2020replacing, jeong2022rawtobit, shekhar2022transform}. Yet, traditional parametric ISPs remain preferred in consumer cameras for their controllability, efficiency, and interpretability. These systems require expert-driven, labor-intensive tuning to enhance image quality and achieve desired aesthetic effects. Efforts to streamline this process are ongoing, aiming to reduce the need for manual adjustments.
Gradient-based optimization is a notable strategy in this area. Tseng et al.~\cite{tseng2019hyperparameter} have applied differentiable black-box proxies to simplify ISP tuning. %, although the inherent complexity of these systems can complicate their application. To address this, 
Further research by Tseng et al.~\cite{tseng2022neural} has opened up the ISP pipeline into white-box manageable modules, learning differentiable proxies for each module and subsequently integrating them, which improves the tuning performance.
Additionally, Qin et al.~\cite{qin2022attention} introduced an attention-based CNN approach for scene-aware ISP tuning. However, this method lacks integration of sequence-specific prior knowledge. They later developed a framework for predicting ISP hyper-parameters sequentially~\cite{qin2023learning}, optimizing parameters based on their relationships and similarities.
In a different approach, Mosleh et al.~\cite{mosleh2020hardware} utilized a genetic evolutionary algorithm with a zero-order stochastic solver\cite{hansen2006cma} to directly optimize hardware-specific image processing pipelines, circumventing the constraints of gradient-based methods.
Moreover, Nishimura et al.~\cite{nishimura2018automatic} explored derivative-free optimization, employing nonlinear techniques and automatic reference generation for effective automation of image quality adjustments.
Despite these advancements, the complexity of the image processing pipeline and the vast parameter space continue to challenge the efficiency of tuning methods.

\noindent\textbf{Photo stylization.} 
Automating image stylization, which is straightforward for humans, poses challenges for machines. Significant research has been conducted to bridge this gap. Karras~\etal\cite{karras2019style} pioneered StyleGAN, a network manipulating the latent space to control image styles at various scales. Building on this, Brooks~\etal\cite{brooks2023instructpix2pix} developed InstructPix2Pix, which allows users to guide the stylization process through textual instructions, enhancing user-machine interaction. However, these methods often lack explainability and may alter the original image content.
Further exploring transparency, Hu~\etal\cite{hu2018exposure} proposed a reinforcement learning-based white-box photo-finisher, though its need for explicit gradients limits compatibility with traditional systems. Kosugi~\etal\cite{kosugi2020unpaired} addressed style diversity using unpaired data based on reinforcement learning.  Despite these advances, these methods are restricted to a single style during training, limiting the user's ability to control the pipeline to produce images of any desired style during inference.
Moreover, Tseng~\etal\cite{tseng2022neural} optimized proxy networks for image processing modules via a style loss function, achieving promising results but facing limitations in handling complex modules and style variability during inference. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\section{Method}
\vspace{-8pt}

\subsection{Problem Definition}
\label{sec:3.1}

Throughout this paper, we aim to tackle the \task problem. Given an input image $I_0$, an image processing pipeline $f_{\text{PIPE}}$ maps the input to a finished image $I_{\text{FINISHED}} = f_{\text{PIPE}}(I_0, P)$, controlled by a set of parameters $P$. Our task is to solve the inverse problem: given an input image $I_0$ and a tuning target $I_g$ (the goal condition in the RL framework), how to find the parameters that reach this target:
\begin{equation}
\arg \min_P \mathcal{L}(I_g, f_{\text{PIPE}}(I_0, P)).
\end{equation}
The goal image $I_{g}$ can vary between different tasks. For \taskPFT, the goal image shares the same content as the input, and the tuning target is to minimize the distance between the pipeline output and the goal image $I_g$. For \taskPST, the goal image is a style target with different content than the input, and the tuning target is to generate an output that matches the style of the goal image. Note that unlike artistic style transfer~\cite{gatys2016image}, we focus on photorealistic style transfer~\cite{xia2020joint}, where the processing pipeline does not change the content of an image.



\vspace{-4pt}
\subsection{Goal Conditioned Reinforcement Learning}
\vspace{-6pt}


We are the first to introduce a reinforcement learning (RL) approach to the \task task by formulating it as an end-to-end policy learning problem. Inspired by human experts who use iterative trial and error in photo finishing, our method models the task as a decision process with multi-step feedback, akin to a Markov Decision Process. As shown in the bottom row of Fig.~\ref{fig:framework}, our policy iteratively tunes the parameters of a given \pipeline to match a goal image. At each step, it takes the currently retouched image and the goal image as inputs, and then outputs the next action. This RL-based framework efficiently predicts pipeline parameters, requiring only minimal iterations (e.g., 10 queries to the \pipeline). Additionally, since RL optimization bypasses the need for gradient flow from the image processing pipeline, our system can handle any black-box pipeline, regardless of complexity. 

The RL process is formally defined as follows. Let $ \mathcal{S}$ be the state space, $\mathcal{O}$ be the observation space, $\mathcal{A}$ be the action space, $\mathcal{T}$ be the transition function, $\mathcal{R}$ be the reward function, $\mathcal{G}$ be the goal distribution, $\rho_0$ be the initial state distribution, and $\gamma$ the discount factor. All these forms a Goal-conditioned Partially-Observed Markov Decision Process $ \left( \mathcal{S}, \mathcal{O}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \mathcal{G}, \rho_0, \gamma \right)$ ~\cite{liu2022goalconditioned}.

In each tuning episode $t$, the agent is given a goal image $I_g \in \mathcal{G}$, as well as an observation $o_t \in \mathcal{O}$ consists of the retouched image $I_t$ at step $t$ along with all historical actions and observations.
The action $a_t$ is the parameter set $P$ used by \pipeline to generate the output image at the next step. And the transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the \pipeline $f_{\text{PIPE}}$ defined in Sec.~\ref{sec:3.1}. 
The reward function is $ \mathcal{R}(s, I_g) $ for $ s \in \mathcal{S}$ and $ I_g \in \mathcal{G}$. 
We aim to learn a goal-conditioned policy $ \pi(a | o , I_g): \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ that maps from observation $o$ and goal $I_g$ to the next action $a$, maximizing the sum of discounted rewards $ \mathbb{E}_{s_0 \sim \rho_0, I_g \sim \mathcal{G}} \sum_{t} \gamma^{t} \mathcal{R}(s_t, I_g)$.  
Our policy $\pi(a | o, I_g)$ is a deterministic policy $\mu_\theta$ parameterized by $\theta$, outputting continuous actions $a_t = \mu_\theta(o_t, I_g)$. 



\begin{figure*}[t]
\centering
  \includegraphics[width=1.0\textwidth]{figures/framework_v7.pdf}
  \vspace{-15pt}
  \caption{The overall framework. Top row: at each step, our policy maps the current image and the goal image to action (new parameters), with the help of our state representation consisting of dual-path features, photo statistics, and historical actions. Bottom row: visualization of iterative tuning trajectory of our RL-based photo finishing framework.}
  \vspace{-15pt}
  \label{fig:framework}
\end{figure*}

\vspace{-6pt}
\subsection{Photo Finishing State Representation}
\label{sec:3.3}
\vspace{-6pt}
State presentation is also critical for the success of the proposed RL policy. This is particularly important in challenging scenarios where the policy must tune parameters for unseen goals of any style. 
Our experiment in Sec.~\ref{sec:4.3} also shows that a simple concatenation of the input and goal images yields a sub-optimal result. Therefore, we design a comprehensive photo finishing state representation to extract features from observations that are critical for photo finishing tuning. 


Specifically, our representation consists of three components: a CNN-based dual-path feature representation to encode both global and local features, a photo statistics representation to match the traditional photographic statistics between input and goal images, and an embedding of historical actions. Details of each component are described below. 







\noindent\textbf{Dual-path feature representation.}
In our dual-path feature representation, we seek to extract both global and local features from both input and goal images, inspired by \cite{gharbi2017deep}. This is because the tuning task requires not only global image characteristics such as overall color, tone, average intensity, and scene category, but also local features associated with texture, highlight, and shadow. 
As shown in Fig.~\ref{fig:framework}, the architecture begins with a stride-2 convolutional encoder to reduce spatial resolution and extract initial low-level features. It then splits into two paths: a local path and a global path. The local path $\{L^i\}_{i=1...N_L}$ includes two stride-1 convolutional layers, preserving spatial resolution to extract local features. The global path $\{G^i\}_{i=1...N_G}$ has one stride-2 convolutional layer and three fully-connected layers, providing a global scene summary vector. This global feature encapsulates essential global image characteristics such as overall color and tone, as well as a global notion of scene category (light condition or indoor/outdoor).
We fuse the global and local features by adding the global feature at each $x, y$ spatial location of the local feature: $F^{D}_{x,y} = \sigma (G^{N_G} + L^{N_L}_{x,y})$, where $\sigma$ is the ReLU activation. This results in a dual-path feature representation $F^D$. 




 

\noindent\textbf{Photo statistics representation.}
Simply relying on a CNN-based policy may lead to unsatisfactory results with input and goal images outside of training distribution. To better represent invariant features across diverse styles and content of both input and goal images, we introduce a photo statistics representation, which matches traditional image statistics such as a histogram between input and goal.
Global photo statistics, such as histograms, are critical in global image processing operations, such as exposure control~\cite{onzon2021neural}, highlight, and shadow. Since they cannot be well represented by conventional convolutional neural networks due to limited receptive fields~\cite{tseng2022neural}, we propose to pre-compute these statistics and concatenate them into our state representation. Specifically, we compute histograms $H_{rgb}$ on the RGB channels of input and goal images and map these to a fixed dimension using a linear layer $H' = \text{Linear}((H_{rgb}(I_t);H_{rgb}(I_g))$. This feature is then concatenated with the luminance, median, contrast, and saturation of both input and goal images to form the photo statistics representation $F^S$.



\noindent\textbf{Policy network.} 
At last, we combine the dual-path feature representation and photo statistics representation with a historical action embedding $F^H_t = \text{Linear}(a_{1:t}; \ell_{1:t})$, where $\ell_t$ is the $\ell_2$-distance of image $I_t$ and goal image $I_g$. As shown in Fig.~\ref{fig:framework}, the input of policy network can be formulated as $s_t = \text{Concat}(F^D_t; F^S_t; F^H_t)$. The policy network is a multi-layer perceptron network (MLP) that maps from the current state and the goal representation to the next action to take. We choose deterministic policy to directly output continuous action $a_t = \mu_\theta (o_t, g) = \mu_\theta (s_t)$. We use the same architecture to estimate the value function for RL updates.

\vspace{-10pt}
\subsection{Reward Function and Training Objectives}
\label{sec:3.4}
\vspace{-2pt}
We provide a general RL-based framework for photo tuning tasks. One can train our goal-conditioned end-to-end policy with different reward functions to resolve different photo tuning tasks including \taskPFT and \taskPST. The policy is optimized with twin-delayed DDPG (TD3) algorithm~\cite{fujimoto2018addressing-td3}.

\noindent\textbf{Reward function for \taskPFT.} 
When goal image $I_g$ is the photo-finished input image, we measure the distance between current image $I_t$ and $I_g$ with PSNR metric. The reward function is calculated as the difference between PSNR values of consecutive steps:
\begin{align}\label{eq:rew_pft}
r_t = \text{PSNR}(I_{t+1}, I_g) - \text{PSNR}(I_{t}, I_g).
\end{align}
Instead of using $\ell_2$-distance to measure image distance, we use PSNR, the negative logarithm of $\ell_2$-distance. This design ensures the policy receives appropriate rewards even when the current image is close to the goal, encouraging fine-grained tuning of pipeline parameters.


\noindent\textbf{Reward function for \taskPST.} 
When goal image $I_g$ is a style image with arbitrary content and style, we measure the distance between the input and goal images with a style score:
\begin{equation}
\label{eq:rew_pst_ss}
\text{StyleScore}_t =  \sum_{i=1}^{N_S} \left\| G_i[I_g] - G_i[I_t] \right\|_{2} + 
\lambda_0 \| H(I_t^Y),  H(I_g^Y) \|_2 + 
\lambda_1 \| H(I_t^{UV}),  H(I_g^{UV}) \|_2,
\end{equation}
where $N_S$ denotes the number of layers from a pre-trained VGG-19~\cite{simonyan2014very-vgg} model used to extract features. 
Following \cite{gatys2016image}, the style is captured using Gram matrices $G_i[\cdot] = F_i[\cdot]F_i[\cdot]^T$, with $F_i$ representing feature maps. Additionally, the $\ell_2$-distances of histograms for the Y and UV channels are included to align luminance and color palettes. The overall reward is calculated by the change in style score across consecutive steps, penalized by the difference in content features $F_i$:
\begin{align}\label{eq:rew_pst}
r_t = & ~ \text{StyleScore}_t - \text{StyleScore}_{t+1} - \lambda_2\sum_{i=1}^{N_C} \left\| F_i[I_g] -F_i[I_t] \right\|_{2}.
\end{align}

\noindent\textbf{Policy optimization. } 
We optimize our goal-conditioned policy with off-policy TD3~\cite{fujimoto2018addressing-td3} algorithm. 
Specifically, TD3 learns two Q value function $Q_{\phi_{1}}$ and $Q_{\phi_{2}}$, optimized by mean square Bellman error minimization:
\begin{align}\label{eq:qloss}
y(r_t, s_{t+1}, d) = & ~ r_t + \gamma (1-d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s_{t+1}, a'(s_{t+1})), \\
L(\phi_{i}) = & ~ \underset{s_t \sim {\mathcal D}}{\mathbb{E}}  \left[ \left( Q_{\phi_i}(s_t,a_t) - y(r_t, s_{t+1}, d) \right)^2\right],
\end{align}
where $Q_{\phi_{i, \text{targ}}}$ is the exponential moving average of $Q_{\phi_{i}}$, $a'(s_{t+1})$ is given by target policy with clipped gaussian noise, and $d$ is the termination signal. The state transition pair $(s_t,a_t,r_t,s_{t+1},d)$ is sampled from a replay buffer $\mathcal{D}$. With Q functions, the policy $\mu_\theta(\cdot)$ is learned by maximizing $Q_{\phi_{1}}$:
\begin{align}
L(\theta) =  \underset{s_t \sim {\mathcal D}}{\mathbb{E}}  \left[ Q_{\phi_1}(s_t,\mu_\theta (s_t)) \right].
\end{align}
More details about our reward functions and policy optimization are in the appendix.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-8pt}
\section{Experiments}
\vspace{-6pt}
In the first subsection, we provide the details of datasets and task settings for \taskPFT and \taskPST, along with a description of the evaluation metrics. In the second subsection, we demonstrate our experimental results and compare them to zeroth-order optimization \cite{mosleh2020hardware} and first-order optimization \cite{tseng2019hyperparameter, tseng2022neural} baseline methods. 
Ablation studies are conducted in the last subsection, which investigates the impact of each component of our state representation.
We also provide supplementary qualitative results in the Appendix.

\vspace{-5pt}
\subsection{Tasks Settings and Datasets}
\label{sec:4.1}
\vspace{-3pt}


\noindent\textbf{Datasets.}  
We use the MIT-Adobe FiveK Dataset~\cite{fivek}, a renowned resource in the field of photo retouching, which comprises 5,000 photographs captured using DSLR cameras by various photographers. This dataset is notable for providing images in raw format alongside the retouching outcomes of five experts. For our study, we selected 4,500 images to serve as the training dataset, with the remaining 500 images designated as the validation dataset.
In our method, random parameters are employed to generate the target images, which are used as training data pairs. In the task of \taskPFT, the datasets including both the expert C retouched targets and randomly generated targets are utilized for evaluation. The expert C retouched targets are optimized using CMA-ES to ensure they are reachable by our \pipeline. For the \taskPST task, we have curated a collection of 200 diverse style images from the Lightroom Discover website~\footnote{https://lightroom.adobe.com/learn/discover}, following~\cite{shi2022spaceedit}.

To further evaluate our method and demonstrate its generalizability, we test our RL-based framework directly on the HDR+ dataset~\cite{hdrplus}. We used the official subset of the HDR+ dataset, which consists of 153 scenes, each containing up to 10 raw photos. The aligned and merged frames are used as the input, expertly tuned images serve as the photo-finishing targets.



\noindent\textbf{Implementation details.} We conduct all experiments on an \pipeline consisting of standard image processing operations, including exposure, color balance, saturation, contrast, tone mapping (highlight and shadow), and texture (sharpness and smoothing), with nine adjustable parameters in total, similar to~\cite{tseng2022neural}. Thus, the agent's action space is comprised of fine continuous actions corresponding to these nine pipeline parameters. 
During the policy inference, the input and goal images are resized to the resolution of 64 × 64. Additionally, a 3-level Laplacian pyramid of both input and goal images is constructed and fed into the policy network to capture high-frequency details from the original resolution. Then the output parameters from the policy network are fed to the \pipeline along with full-resolution images to produce high-resolution results. 
We train our policy using the standard TD3 algorithm~\cite{fujimoto2018addressing-td3} and set the termination of our RL policy to trigger when the episode length reaches the maximum threshold (10 steps), ensuring efficiency. Further details can be found in the appendix.


\noindent\textbf{Evaluate metrics.} 
Similar to~\cite{hu2018exposure, tseng2019hyperparameter, tseng2022neural}, we employ the Peak Signal-to-Noise Ratio (PSNR), the Structural Similarity Index Measure (SSIM)~\cite{wang2004image}, and the Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable} as our evaluation metrics. To assess the quality of stylization, we conduct user studies to evaluate our methods, offering a subjective measure of image quality based on viewer assessments.



\vspace{-5pt}
\subsection{Results}
\vspace{-5pt}

\begin{table}[t]
\caption{\taskPFT experimental results on the FiveK validation datasets with FiveK targets (expert-C) and random targets. Queries represent the times of query~\pipeline.}
\label{exp:main}
% \vspace{-0.2cm}

\centering
\setlength\tabcolsep{3pt}%调列距
\renewcommand\arraystretch{1}%调行距
\begin{adjustbox}{width=\linewidth,center=\linewidth}
\begin{tabular}{c|cccc|cccc}
\toprule

\multicolumn{1}{c|}{Eval Dataset}    & \multicolumn{4}{c|}{FiveK-Target}    & \multicolumn{4}{c}{Random-Target}     \\ \midrule%\cline{1-9}

Method & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & Queries$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & Queries$\downarrow$ \\ \midrule

CMAES~\cite{hansen2006cma, mosleh2020hardware}  & 28.53 & 0.9586 & 0.0968 & 200 &32.29   &0.9754  & 0.0827 & 200 \\
Monolithic Proxy~\cite{tseng2019hyperparameter}       & 21.71 & 0.9104 & 0.2144 & - &21.08 & 0.9251 & 0.2785 & - \\
Cascaded Proxy~\cite{tseng2022neural}   &22.31  & 0.9115  & 0.1939 & - &21.40 &0.9213 & 0.2613 & - \\
% \textbf{Ours}  & \textbf{32.47} & \textbf{0.9716}  &  \textbf{0.0279} & \textbf{10} & \textbf{38.46} & \textbf{0.9814} & \textbf{0.0128} & \textbf{10} \\
\textbf{Ours}  & \textbf{35.89} & \textbf{0.9764}  &  \textbf{0.0305} & \textbf{10} & \textbf{38.46} & \textbf{0.9814} & \textbf{0.0128} & \textbf{10} \\
% increased after non-gaussian dataset fine-tuning + Laplacian : PSNR of FiveK-Target 32.4 -> 36.68948 SSIM 9764 lpips 0.03068
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.2cm}
\label{tab:main}
\end{table}
% fivek 32.47 .9716
% baseline iteration 10


\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figures/isp-tuning-main-results-v1.pdf}
  \caption{\TaskPFT results on FiveK dataset with expert C target. The visual results of our method are closest to the target image, especially in terms of color and brightness.}
  \label{fig:pft_vis}
\end{figure*}


\noindent\textbf{Results of \taskPFT.}
To evaluate the efficacy of our framework on the~\taskPFT Task, we conduct two experiments on the FiveK validation datasets, using both expert-C targets and random targets. Our method is compared against the monolithic proxy-based approach~\cite{tseng2019hyperparameter}, the cascaded proxy-based method~\cite{tseng2022neural}, and the search-based method proposed~\cite{mosleh2020hardware}. Consistent with~\cite{tseng2022neural}, we utilize 100 iterations during inference for the proxy-based methods. Additionally, we record the number of times that the photo-finishing pipeline was queried in both search-based methods and our approach.

As illustrated in Tab.~\ref{tab:main}, our method outperforms the others across all metrics on both FiveK-Target and Random-Target. The monolithic proxy-based method struggles with accurately representing the complex~\pipeline, leading to suboptimal performance. While the cascaded proxy method can incrementally enhance tuning performance over its monolithic counterpart, it suffers from accumulated errors and difficulties in approximating certain operations, such as texture, resulting in poorer performance. Notably, our method only requires querying 10 times~\pipeline, whereas the performance of the search-based method, even with 200 queries, remained inferior to ours. Further, the visualization results depicted in Fig.~\ref{fig:pst_vis} demonstrate that our method produces visual outcomes that most closely match the target images, particularly in terms of color and brightness, when compared to all other methods. More visualization can be found in Fig.~\ref{fig:pft_vis_supp} of Appendix~\ref{sec:appen_a2}.




\noindent\textbf{Efficiency.}
To evaluate the efficiency of our approach, we conducted speed testing experiments on a system equipped with an AMD EPYC 7402 (48C) @ 2.8 GHz CPU, 8 NVIDIA RTX 4090 GPUs with 24GB of RAM each, 512 GB of memory, and running CentOS 7.9. In line with~\cite{tseng2022neural}, we applied 200 iterations during inference for both the proxy-based methods~\cite{tseng2019hyperparameter, tseng2022neural} and the search-based method~\cite{mosleh2020hardware}. We measured the execution time for each method across four different input resolutions.

As indicated in Tab.~\ref{tab:runtime}, our method demonstrated superior efficiency, requiring only 1.23 seconds per execution with 4K input. While the monolithic proxy-based method outperformed the search-based method in terms of speed, it faced limitations as input resolutions increased, leading to out-of-memory (OOM) errors once GPU memory was exceeded. The cascaded proxy-based method, which includes MLP networks, was the slowest and most prone to memory overflows due to its intensive memory demands. The search-based method primarily depends on CPU performance. Despite being executed on a high-performance server, the CMAES method~\cite{mosleh2020hardware} requires 144 seconds to process a 4K input image, which is considerably slow.


\begin{table}[t]
\caption{Experimental results demonstrating efficiency across varying input resolutions. Our method significantly outperforms other methods, achieving a speed enhancement of 260 times relative to the cascaded proxy method~\cite{tseng2022neural} at 720P resolution, and 117 times faster than the CMAES approach~\cite{hansen2006cma, mosleh2020hardware} at 4K resolution.} 
\label{tab:runtime}
% \vspace{-0.2cm}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|cccc|cccc|cccc|cccc@{}}
\toprule
Methods    & \multicolumn{4}{c|}{Monolithic Proxy~\cite{tseng2019hyperparameter}} & \multicolumn{4}{c|}{Cascaded Proxy~\cite{tseng2022neural}} & \multicolumn{4}{c|}{CMAES~\cite{hansen2006cma, mosleh2020hardware} }     & \multicolumn{4}{c}{Ours} \\ \midrule
Resolution & 720P    & 1K       & 2K       & 4K    & 720P     & 1K     & 2K     & 4K     & 720P  & 1K    & 2K    & 4K     & 720P   & 1K  & 2K  & 4K  \\
Time(s)    & 7.67    & 17.89    & 33.07    & OOM   & 70.45    & OOM    & OOM    & OOM    & 36.16 & 51.82 & 91.86 & 144.02 &   \textbf{0.27}    &   \textbf{0.33} &  \textbf{0.47}  &   \textbf{1.23}  \\ \bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\end{table}

\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figures/fig_vis_style-v2.pdf}
  \caption{\TaskPST results. Compared with CMAES~\cite{hansen2006cma, mosleh2020hardware}, monolithic proxy~\cite{tseng2019hyperparameter}, and cascaded proxy~\cite{tseng2022neural}, our output matches the best with the style goal. \vspace{-15pt}}
  % \vspace{-5pt}
  \label{fig:pst_vis}
\end{figure*}




\noindent\textbf{Results of \taskPST.} We conduct qualitative comparisons and user studies on the \taskPST task to demonstrate the effectiveness of our goal-conditioned RL framework. Our policy was trained using the FiveK training dataset, and all methods were evaluated on input and style image pairs collected from the Adobe Lightroom Discover website. For all baseline methods, we adopt the same style score described Sec.\ref{sec:3.4} as optimization objectives, ensuring fair comparison.

As shown in Fig.~\ref{fig:pst_vis}, our method produces results closer to the style goal image compared to the baseline. Notably, our method achieves better results with only 10 queries to the \pipeline, whereas the baseline methods require 200 queries, making them orders of magnitude slower.
It is important to note that the style images from the Lightroom Discover website have a different distribution than our training dataset. Despite this, our method adapts directly to the target style image distribution during testing, whereas the baseline methods require time-consuming optimization of testing data. These experimental results demonstrate that our RL-based framework can efficiently tune images to different unseen styles, showcasing that our photo finishing state representation (Sec.~\ref{sec:3.3}) has the capability to generalize to versatile goals outside of training distributions. More visualization with versatile goal images can be found in Fig.~\ref{fig:pst_vis_supp} of Appendix~\ref{sec:appen_a3}.

To rigorously evaluate the effectiveness of our method in \taskPST, we implemented a subjective user study comprising 20 questions. In each question, participants were presented with images generated by four different methods—monolithic proxy, cascaded proxy, CMAES, and our own approach. Participants were asked to identify up to two images that most closely resembled a given target image. The study was conducted online, garnering 65 responses from a diverse group of individuals selected randomly from the internet.

The aggregated preferences are visually summarized in Fig.~\ref{fig:user-study}. 
The data clearly show that our method is perceived by the majority of participants as producing results that most closely match the target images, highlighting its superiority in stylization tuning tasks.





\noindent\textbf{Cross dataset generalization.}
We conducted additional evaluations using the HDR+ dataset~\cite{hdrplus} to demonstrate our RL-based framework's ability to generalize effectively to unseen datasets. We compare to baselines including CMAES~\cite{hansen2006cma, mosleh2020hardware}, Cascaded Proxy~\cite{tseng2022neural}, Monolithic Proxy~\cite{tseng2019hyperparameter}, and Greedy Search~\cite{Kim2023LearningCI}. In Tab.~\ref{tab:hdrplus}, we report PSNR, SSIM, LPIPS, and queries to the ISP pipeline. 
The results demonstrate that our RL policy generalizes effectively to unseen data, achieving higher photo-finishing quality than methods directly tuned on the test dataset. Qualitative comparisons in Fig.~\ref{fig:hdrp_vis1} show that our results are closer to targets, even with input and target images outside the training distribution.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/nips_paper_fig-hdr.pdf}
    \vspace{-0.2cm}
    \caption{Qualitative comparison on the HDR$+$ photo finishing tuning task. These comparisons illustrate that our method remains closer to the target even when dealing with input and target images outside the training distribution. \vspace{-8pt}}
    \label{fig:hdrp_vis1}
\end{figure}


\begin{table}[t]
\caption{Photo finishing tuning experimental results on addition HDR$+$ datasets~\cite{hdrplus} with HDR$+$ expert-tuned targets.}
\label{exp:hdrplus}
% \vspace{-0.2cm}

\centering
\setlength\tabcolsep{3pt}%调列距
\renewcommand\arraystretch{1}%调行距
\begin{adjustbox}{width=0.55\linewidth,center=\linewidth}
\begin{tabular}{c|cccc}
\toprule

\multicolumn{1}{c|}{Eval Dataset}      & \multicolumn{4}{c}{HDR+ Target}     \\ \midrule%\cline{1-9}

Method &  PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & Queries$\downarrow$ \\ \midrule

CMAES~\cite{hansen2006cma, mosleh2020hardware}  & 28.08   &0.9539  & 0.1307 & 200 \\
Greedy Search~\cite{Kim2023LearningCI}  &  25.79  &0.9212  & 0.1542 & 200 \\
Monolithic Proxy~\cite{tseng2019hyperparameter}      &17.80 & 0.8940 & 0.3044 & - \\
Cascaded Proxy~\cite{tseng2022neural}  &18.90 &0.8982 & 0.2797 & - \\
\textbf{Ours}   & \textbf{31.54} & \textbf{0.9652} & \textbf{0.0563} & \textbf{10} \\    % 39+ for hdr+ random
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\label{tab:hdrplus}
\end{table}


 

In Tab.~\ref{tab:hdrplus}, our RL-based method achieves a PSNR of 31.54 on the HDR+ photo-finishing task, and it outperforms all baselines. This shows that our RL policy, trained on the FiveK dataset, effectively generalizes to the HDR+ dataset. Such out-of-distribution capability is facilitated by our proposed photo-finishing state representation, which extracts invariant features for photo finishing, allowing adaptation to diverse inputs and goals beyond the training distributions.
The CMAES~\cite{hansen2006cma, mosleh2020hardware} baseline shows consistent results on HDR+ compared to FiveK, as it is directly optimized on the test dataset without prior training. However, proxy-based methods~\cite{tseng2019hyperparameter, tseng2022neural} perform worse because the proxy network trained on FiveK does not generalize well to HDR+, leading to incorrect gradients and poorer photo-finishing quality.





\begin{figure}[t]
\begin{minipage}{0.48\linewidth}
% \begin{figure}[t]
\centering
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/user_study_results_v2.pdf}
    \captionof{figure}{Results of the user study on the photo stylization tuning task. Each bar represents the number of votes each method received, where participants select the images they believed most closely resembled target images. }
    \label{fig:user-study}
% \end{figure}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}{0.50\linewidth}
% \begin{table}[h]
% \centering
    \centering
    \small
    \captionof{table}{Ablation study on each component of our finishing state representation. $RL$ denotes a baseline naively using CNN trained with RL, $F^D, F^S, F^H$ denotes each of our state representation respectively.}
    \vspace{+2pt}
    % \setlength\tabcolsep{5pt}%调列距
    % \renewcommand\arraystretch{1}%调行距
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c|cccc|cc}
    \toprule
     & $RL$ & $F^D$ & $F^S$ & $F^H$   & PSNR$\uparrow$ & SSIM$\uparrow$\\ \midrule
    $Ex_{1}$& \checkmark &   & & & 32.17 & 0.968\\
    $Ex_{2}$ &\checkmark  & \checkmark & &  & 35.15 &0.973\\
    $Ex_{3}$ &\checkmark  & \checkmark & \checkmark & &37.61 &  0.980\\
    $Ex_{4}$  & \checkmark & \checkmark &  & \checkmark & 35.96& 0.976\\ 
    $Ex_{5}$  & \checkmark & \checkmark & \checkmark &\checkmark &  38.46& 0.983\\
    % 0.9815
    \bottomrule
    \end{tabular}
    }
    % \vspace{-0.2cm}
    \label{tab:ablation}
% \end{tabl}
\end{minipage}
\end{figure}


\vspace{-5pt}
\subsection{Ablation Study on State Representation}
\label{sec:4.3}
\vspace{-7pt}
As has been shown in our main experiment, our RL-based approach significantly outperforms the previous method in terms of photo finishing quality and efficiency, demonstrating that RL is more suitable for the \taskPFT task. In this subsection, we focus on the photo finishing state representation we propose to better fit RL in our task. Specifically, we conduct experiments on \taskPFT task using FiveK Random-Target dataset, in order to study the contribution of each specific representation in our photo finishing state representation. 


As shown in Tab.~\ref{tab:ablation}, we set our baseline $RL$ as RL policy trained with a naive CNN-based encoder taking the concatenated input and goal images as input. 
This baseline achieves only 32.17 dB of PSNR. In $Ex_{2}$, our dual-path feature representation $F^D$ improves the photo finishing quality, as it better encodes local features and global image characteristics critical to our photo tuning task. 
Since traditional photo statistics contain invariant features about global statistics that are difficult to learn solely using a network, the photo statistics representation $F^S$ also helps to boost photo finishing quality as shown in $Ex_{3}$. 
Moreover, as parameters and results from all previous RL steps help in the decision process, the historical action representation is also useful as shown in $Ex_{4}$. 
With the proposed three representations combined, our RL policy can be guided to effectively tune \pipeline parameters given input and goal images of any photo finishing style, as evidenced by the superior performance in $Ex_{5}$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-4pt}
In this work, we propose an RL-based photo finishing tuning algorithm that efficiently tunes the parameters of a black-box \pipeline to match any tuning target. Our approach encompasses several key innovations. Firstly, we integrate goal-conditioned RL into the realm of photo-finishing tuning. Secondly, we propose a photo finishing state representation comprising three principal components essential for training an effective RL policy network: a CNN-based feature representation that encodes both global and local image features, a photo statistics representation designed to align the photographic statistics between the input and the target, and an embedding of historical actions. We assess the effectiveness of our framework through comprehensive experiments on both \taskPFT and \taskPST. The experimental results affirm that our goal-conditioned policy is an adept photo-finishing tuner, capable of exerting efficient and fine-grained control over image processing pipeline parameters to fulfill a variety of objectives.
Currently, our method exclusively supports conditional inputs in the form of images and lacks the capability to process non-image types such as textual inputs. Moving forward, we intend to broaden our research to include multi-modal conditional inputs.


\clearpage

\section*{Acknowledgements}
This work is supported by Shanghai Artificial Intelligence Laboratory and RGC Early Career Scheme (ECS) No. 24209224. We also extend our gratitude to Quanyi Li for his insightful discussions and valuable comments.


%%% some writing rules

%% Writing rule for creating tags.
%% Tags :
%% Theorem    \ref{thm:bla_bla}
%% Table      \ref{tab:bla_bla}
%% Lemma      \ref{lem:bla_bla}
%% Claim      \ref{cla:bla_bla}
%% Corollary  \ref{cor:bla_bla}
%% Fact       \ref{fac:bla_bla}
%% Definition \ref{def:bla_bla}
%% Section    \ref{sec:bla_bla}
%% Subsection \ref{sub:bla_bla}
%% Equation   \ref{eq:bla_bla}


\begin{thebibliography}{10}

\bibitem{pymoo}
J.~{Blank} and K.~{Deb}.
\newblock pymoo: Multi-objective optimization in python.
\newblock {\em IEEE Access}, 8:89497--89509, 2020.

\bibitem{brooks2023instructpix2pix}
Tim Brooks, Aleksander Holynski, and Alexei~A Efros.
\newblock Instructpix2pix: Learning to follow image editing instructions.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 18392--18402, 2023.

\bibitem{fivek}
Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr{\'e}do Durand.
\newblock Learning photographic global tonal adjustment with a database of input / output image pairs.
\newblock In {\em CVPR}, 2011.

\bibitem{chen2017zoo}
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.
\newblock In {\em Proceedings of the 10th ACM workshop on artificial intelligence and security}, pages 15--26, 2017.

\bibitem{fujimoto2018addressing-td3}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages 1587--1596. PMLR, 2018.

\bibitem{gatys2016image}
Leon~A Gatys, Alexander~S Ecker, and Matthias Bethge.
\newblock Image style transfer using convolutional neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2414--2423, 2016.

\bibitem{gharbi2016deep}
Micha{\"e}l Gharbi, Gaurav Chaurasia, Sylvain Paris, and Fr{\'e}do Durand.
\newblock Deep joint demosaicking and denoising.
\newblock {\em ACM Transactions on Graphics (ToG)}, 35(6):1--12, 2016.

\bibitem{gharbi2017deep}
Micha{\"e}l Gharbi, Jiawen Chen, Jonathan~T Barron, Samuel~W Hasinoff, and Fr{\'e}do Durand.
\newblock Deep bilateral learning for real-time image enhancement.
\newblock {\em TOG}.

\bibitem{hansen2006cma}
Nikolaus Hansen.
\newblock The cma evolution strategy: a comparing review.
\newblock {\em Towards a new evolutionary computation: Advances in the estimation of distribution algorithms}, pages 75--102, 2006.

\bibitem{hdrplus}
Sam Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan~T. Barron, Florian Kainz, Jiawen Chen, and Marc Levoy.
\newblock Burst photography for high dynamic range and low-light imaging on mobile cameras.
\newblock {\em SIGGRAPH Asia}, 2016.

\bibitem{hu2018exposure}
Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, and Stephen Lin.
\newblock Exposure: A white-box photo post-processing framework.
\newblock {\em TOG}.

\bibitem{huang2017arbitrary-adain}
Xun Huang and Serge Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance normalization.
\newblock In {\em {ICCV}}, 2017.

\bibitem{ignatov2020replacing}
Andrey Ignatov, Luc Van~Gool, and Radu Timofte.
\newblock Replacing mobile camera isp with a single deep learning model.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops}, pages 536--537, 2020.

\bibitem{jeong2022rawtobit}
Wooseok Jeong and Seung-Won Jung.
\newblock Rawtobit: A fully end-to-end camera isp network.
\newblock In {\em European Conference on Computer Vision}, pages 497--513. Springer, 2022.

\bibitem{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4401--4410, 2019.

\bibitem{Kim2023LearningCI}
Heewon Kim and Kyoung~Mu Lee.
\newblock Learning controllable isp for image enhancement.
\newblock {\em IEEE Transactions on Image Processing}, 33:867--880, 2023.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kosugi2020unpaired}
Satoshi Kosugi and Toshihiko Yamasaki.
\newblock Unpaired image enhancement featuring reinforcement-learning-controlled image editing software.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 11296--11303, 2020.

\bibitem{liu2022goalconditioned}
Minghuan Liu, Menghui Zhu, and Weinan Zhang.
\newblock Goal-conditioned reinforcement learning: Problems and solutions, 2022.

\bibitem{mosleh2020hardware}
Ali Mosleh, Avinash Sharma, Emmanuel Onzon, Fahim Mannan, Nicolas Robidoux, and Felix Heide.
\newblock Hardware-in-the-loop end-to-end optimization of camera image processing pipelines.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7529--7538, 2020.

\bibitem{nishimura2018automatic}
Jun Nishimura, Timo Gerasimow, Rao Sushma, Aleksandar Sutic, Chyuan-Tyng Wu, and Gilad Michael.
\newblock Automatic isp image quality tuning using nonlinear optimization.
\newblock In {\em 2018 25th IEEE International Conference on Image Processing (ICIP)}, pages 2471--2475. IEEE, 2018.

\bibitem{onzon2021neural}
Emmanuel Onzon, Fahim Mannan, and Felix Heide.
\newblock Neural auto-exposure for high-dynamic range object detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021.

\bibitem{qin2022attention}
Haina Qin, Longfei Han, Juan Wang, Congxuan Zhang, Yanwei Li, Bing Li, and Weiming Hu.
\newblock Attention-aware learning for hyperparameter prediction in image processing pipelines.
\newblock In {\em European Conference on Computer Vision}, pages 271--287. Springer, 2022.

\bibitem{qin2023learning}
Haina Qin, Longfei Han, Weihua Xiong, Juan Wang, Wentao Ma, Bing Li, and Weiming Hu.
\newblock Learning to exploit the sequence-specific prior knowledge for image processing pipelines optimization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22314--22323, 2023.

\bibitem{shekhar2022transform}
Ardhendu Shekhar~Tripathi, Martin Danelljan, Samarth Shukla, Radu Timofte, and Luc Van~Gool.
\newblock Transform your smartphone into a dslr camera: Learning the isp in the wild.
\newblock In {\em European Conference on Computer Vision}, pages 625--641. Springer, 2022.

\bibitem{shi2022spaceedit}
Jing Shi, Ning Xu, Haitian Zheng, Alex Smith, Jiebo Luo, and Chenliang Xu.
\newblock Spaceedit: Learning a unified editing space for open-domain image color editing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition}, pages 19730--19739, 2022.

\bibitem{simonyan2014very-vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognitio.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2015.

\bibitem{tseng2019hyperparameter}
Ethan Tseng, Felix Yu, Yuting Yang, Fahim Mannan, Karl~ST Arnaud, Derek Nowrouzezahrai, Jean-Fran{\c{c}}ois Lalonde, and Felix Heide.
\newblock Hyperparameter optimization in black-box image processing using differentiable proxies.
\newblock {\em ACM Trans. Graph.}, 38(4):27--1, 2019.

\bibitem{tseng2022neural}
Ethan Tseng, Yuxuan Zhang, Lars Jebe, Xuaner Zhang, Zhihao Xia, Yifei Fan, Felix Heide, and Jiawen Chen.
\newblock Neural photo-finishing.
\newblock {\em ACM Transactions on Graphics}, 41(6):3555526, 2022.

\bibitem{wang2004image}
Zhou Wang, Alan~C Bovik, Hamid~R Sheikh, and Eero~P Simoncelli.
\newblock Image quality assessment: from error visibility to structural similarity.
\newblock {\em IEEE transactions on image processing}, 13(4):600--612, 2004.

\bibitem{xia2020joint}
Xide Xia, Meng Zhang, Tianfan Xue, Zheng Sun, Hui Fang, Brian Kulis, and Jiawen Chen.
\newblock Joint bilateral learning for real-time universal photorealistic style transfer.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VIII 16}, pages 327--342. Springer, 2020.

\bibitem{yu2021reconfigisp}
Ke~Yu, Zexian Li, Yue Peng, Chen~Change Loy, and Jinwei Gu.
\newblock Reconfigisp: Reconfigurable camera image processing pipeline.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4248--4257, 2021.

\bibitem{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 586--595, 2018.

\end{thebibliography}



\clearpage
\input{supp}




\end{document}