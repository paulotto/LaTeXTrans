\section{Methodology}
\vspace{-5pt}


In this section, we introduce our formulation formally and present \ours for solving the alignment problem by optimizing the alignment instruction.

%Most existing methods for alignment use techniques like PPO \cite{Schulman2017ProximalPO} and DPO \cite{rafailov2023direct} which are very costly to execute as they need large amounts of data to be successful. 

% Most existing alignment methods, such as RLHF~\cite{ouyang2022training} and DPO~\cite{rafailov2023direct}, are generally expensive to execute, requiring not only large amounts of data but also substantial computational resources and human labeling efforts.
% Research on using in-context learning for alignment has been limited and primarily relies on human-generated prompts and in-context learning examples. To address this limitation, we introduce a systematic method to optimize prompts and in-context learning examples for LLMs without any human effort or annotation.


\subsection{Problem Formulation}

% \noindent \textbf{Problem Formulation}.
Given an LLM $\mathcal{B}$, an alignment instruction consists of two parts: a system prompt $\mathcal{P}$ and a set of $N$ in-context learning (ICL) examples $\mathcal{I}$.
The system prompt $\mathcal{P}$ serves as a prefix that provides high-level instructions, sets the tone, and imposes constraints on the model's responses. Each ICL example $\mathcal{I}_i$ consists of a pair $(q_i, d_i)$, where $q_i$ is an input query and $d_i$ is the corresponding desired response, so we can represent $\mathcal{I} = \{(q_1, d_1), (q_2, d_2), \ldots, (q_N, d_N)\}$. 

Conditioning on the system prompt $\mathcal{P}$ and a selected subset of $K$ ICL examples $\mathcal{I}_K \subseteq \mathcal{I}$, the aligned model response $y$ to an input $x$ is generated as:
\[
y = \mathcal{B}(x \mid \mathcal{P}, \mathcal{I}_K)
\]

\ours aims to optimize both system prompt $\mathcal{P}$ and ICL examples $\mathcal{I}_K$ to enhance alignment. This involves finding the best possible $\mathcal{P}^*$ and $\mathcal{I}_K^*$ that maximize the alignment of the model's responses. This optimization problem can be formulated as follows:
\[
(\mathcal{P}^*, \mathcal{I}_K^*) = \arg\max_{\mathcal{P}, \mathcal{I}_K} \mathbb{E}_{x \sim \mathcal{D}_x} \left[\mathcal{B}(x \mid \mathcal{P}, \mathcal{I}_K) \right]
\]
\noindent where $\mathcal{D}_x$ denotes the distribution of input queries, and the expectation $\mathbb{E}$ represents the alignment performance for responses based on specific metrics.


% We propose a two-step approach to the optimization problem: 
% \begin{enumerate}
%     \item Estimating $\mathcal{I}^*$ (universal).
%     \item Estimating $\mathcal{P}^*$ given ${\mathcal{I}}^*$. (model specific)
% \end{enumerate}


% \subsection{Systematic Optimization Framework}

% We approach the optimization of the system prompt $\mathcal{P}$ and in-context learning examples $\mathcal{E}$ using the unified framework LLM Reasoners~\cite{hao2024llm} that involves a base model $\mathcal{B}$, an optimizer $\mathcal{O}$, and an evaluator $\mathcal{E}$. This framework is treated as a search process that iteratively interacts with the model's environment and adjusts the prompt $\mathcal{P}$ based on a reward function $\mathcal{R}$. The main challenge for our problem is -the design of a reward function for a problem as broad and general as Alignment. To overcome this challenge we introduce `Dynamic Rewarding', which can effectively handle tasks as broad and challenging as alignment.


\subsection{Dynamic Rewarding with Prompt Optimization (\ours)}

Given the distinct nature of the system prompt and ICL examples, we propose to optimize them separately, resulting in a two-step optimization approach. We first construct a universal set of ICL examples and optimize their responses to obtain $\mathcal{I}^*$. Next, we estimate a model-specific system prompt $\mathcal{P}^*$ based on the optimized universal set $\mathcal{I}^*$. Notably, we leverage the \texttt{LLM Reasoners}\footnote{\url{https://github.com/maitrix-org/llm-reasoners}} framework~\cite{hao2023reasoning, hao2024llm} as the prompt optimization (PO) framework. Specifically, \texttt{LLM Reasoners} incorporates a base model $\mathcal{B}$, an optimizer $\mathcal{O}$, and an evaluator $\mathcal{E}$. It operates as a search agent that iteratively interacts with the model's environment, using the optimizer $\mathcal{O}$ to adjust the prompt $\mathcal{P}$ or ICL examples $\mathcal{I}$ based on a reward function $\mathcal{R}$. For further details, we refer readers to the original references. In the following, we introduce the core component of \ours. 


\subsubsection{Dynamic Rewarding for Alignment}

We formulate this optimization problem as a Markov Decision Process (MDP). In this framework, the states $s\in \mathcal{S}$ represent our optimization goal, which could be either a system prompt or an in-context example. Actions $a \in \mathcal{A}$ are defined based on the alignment feedback obtained during the evaluation of any given state. The key motivation is to leverage the superior generalization capabilities of LLMs to evaluate and analyze states, guiding state transitions toward an optimal state. We employ different evaluation techniques for system prompt and in-context example optimization, which are detailed in subsequent sections. Efficient traversal of this state space is crucial, and for this purpose, we adopt beam search due to its effectiveness and low computational cost.


One of the key challenges in our optimization task is designing a reward function capable of handling a problem as broad and generalized as alignment. As illustrated in Figure~\ref{fig:dynamic_rewarding}, a single, unified reward function is impractical due to the vast query space we aim to align with the base LLM $\mathcal{B}$. Different queries emphasize different focal points, meaning that certain evaluation criteria might be appropriate for some queries but not for others. To overcome this, we introduce a dynamic reward function $\mathcal{R}$, which can dynamically adapt to the specific query being evaluated. Notably, our approach shares conceptual similarities with a few recent alignment research, which also advocate for adaptable and query-sensitive alignment strategies~\cite{bai2022constitutional, sun2024principle}. However, the key distinction lies in our dynamic reward functionâ€™s ability to not only enable flexible evaluation but also integrate seamlessly into a formally defined optimization framework.



Specifically, we first predefined a set of reward criteria $\mathbb{R}$, from which the model dynamically selects the most relevant rewards, while also retaining the flexibility to propose new ones when necessary. Formally, for a given query \( q \), the dynamic reward function $\mathcal{R}$ evaluates the model's response $\sigma$ based on a dynamically selected or proposed rewards $\mathbb{R}_q$, where $\mathbb{R}_q \subseteq \mathbb{R} \cup \mathbb{R}^*$ and $\mathbb{R}^*$ represents newly proposed rewards. The reward function is defined as:

\[
\mathcal{R}(\sigma \mid \mathbb{R}_q) = \frac{1}{|\mathbb{R}_q|} \sum_{r \in \mathbb{R}_q} r(\sigma)
\]

Here, $\mathbb{R}_q$ denotes relevant rewards tailored for the given query \( q \) and \(r(\sigma)\) denotes the score of a specific reward when evaluating any response \(\sigma\). 

This allows us to flexibly score and evaluate responses based on the most relevant criteria for each specific query, ensuring that the evaluation remains contextually appropriate and comprehensive.




\subsubsection{ICL Example Optimization}

To optimize in-context learning examples, we start with a set of base ICL examples $\mathcal{I}_{\text{base}} = \{(q_1, b_1), (q_2, b_2), \ldots, (q_N, b_N)\} $, where $q_i$ is a query and $b_i$ is a base response to the query, $N$ is the total number of in-context examples. Our overall goal is to find a universal set $\mathcal{I}^{*}$ that maximizes alignment across various models.


We specifically optimize each ICL example $(q_i, b_i)$ individually. The initial state of the search tree for an ICL example is defined as the base response to the query, i.e.,  $s_0 = b_i$. At any time $t$, the state of the search tree, $s_t$, is the response of the example. This allows us to systematically monitor and evaluate the response at any given time $t$. The state space $\mathcal{S}$ encompasses all possible responses to the query $q_i$.


To evaluate and improve the alignment, we use the dynamic reward function $\mathcal{R}$. The relevant rewards $\mathbb{R}_{q_i}$ for the query $q_i$ are specifically selected or potentially proposed new rewards. The reward function $\mathcal{R}$ and evaluator $\mathcal{E}$ then evaluates the state $s_t$ based on these rewards, providing a reward $r_t$ and alignment feedback $a_t$:

\[
\begin{aligned}
& r_t = \mathcal{R}(s_t \mid \mathbb{R}_{q_i}) \\
& a_t = \mathcal{E}(s_t \mid \mathbb{R}_{q_i})
\end{aligned}
\]

Note that, in practice, evaluation and reward generation are performed simultaneously using one single prompt, so the evaluation can also be considered dynamic. The transition function $\mathcal{T}$, implemented by optimizer $\mathcal{O}$, then updates the state:
\[
s_{t+1} = \mathcal{T}(s_t, a_t)
\]

The detailed pseudo-code for this optimization process is provided in Algorithm \ref{alg:icl_opti} in Appendix \ref{sec:opti_algo} and the prompts used by our algorithm can be found in Appendix \ref{sec:meta_prompts}.



\subsubsection{System Prompt Optimization}

The optimization process for the system prompt is similar to that of the ICL example optimization. For the system prompt optimization, we use $K$ optimized ICL examples $\mathcal{I}_K^*  \subseteq \mathcal{I}^*$, where the $K$ ICL examples are chosen using similarity-based retrieval. We collect a set of seed samples $\mathcal{X} = \{x_1, x_2, \ldots, x_N \}$, where $x_i$ is a query that will be used to test the alignment of the base model $\mathcal{B}$. The goal of this process is to find the optimal prompt $\mathcal{P}^*$ (given that we already have access to $\mathcal{I}_K^*$), such that alignment of LLM $\mathcal{B}$ is maximized. This prompt is specific to the base model $\mathcal{B}$ and will provide the model with actionable insights and guidance to improve its alignment.


The optimization process begins by defining the initial state $s_0$ as the basic system prompt (e.g., ``You are a helpful assistant.''). At any time $t$, the state $s_t$ represents the current system prompt, and the state space $\mathcal{S}$ includes all possible system prompts for the given LLM $\mathcal{B}$.


For a given state $s_t$, we sample a query $x_t$ from the seed samples $\mathcal{X}$. The relevant rewards $\mathbb{R}_{x_t}$ for the query $x_t$ are specifically selected or potentially proposed new rewards. The reward function $\mathcal{R}$ and the evaluator $\mathcal{E}$ then evaluate the response generated by the model $\mathcal{B}$ given the system prompt $s_t$ and the selected in-context examples $\mathcal{I}_K^*$, providing a reward $r_t$ and alignment feedback $a_t$:

\[
\begin{aligned}
& r_t = \mathcal{R}(\mathcal{B}(x_t \mid s_t, \mathcal{I}_K^*)\mid \mathbb{R}_{x_t}) \\
& a_t = \mathcal{E}(\mathcal{B}(x_t \mid s_t, \mathcal{I}_K^*)\mid \mathbb{R}_{x_t})
\end{aligned}
\]

The optimizer $\mathcal{O}$ as a transition function then updates the state, $ s_{t+1} = \mathcal{T}(s_t, a_t) $. The detailed pseudo-code for this optimization process is provided in Algorithm \ref{alg:prompt_opti} in Appendix \ref{sec:opti_algo}.