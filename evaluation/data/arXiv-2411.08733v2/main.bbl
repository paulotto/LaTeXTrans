\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{AI@Meta(2024)}]{llama3modelcard}
AI@Meta. 2024.
\newblock \href {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} {Llama 3 model card}.

\bibitem[{Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022{\natexlab{a}}.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}.

\bibitem[{Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 2022{\natexlab{b}}.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike et~al.}]{burns2023weak}
Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al. 2023.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.
\newblock \emph{arXiv preprint arXiv:2312.09390}.

\bibitem[{Cao et~al.(2024)Cao, Lu, Lu, Chen, Ren, Xiang, Liu, Lu, He, Han et~al.}]{cao2024towards}
Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, et~al. 2024.
\newblock Towards scalable automated alignment of llms: A survey.
\newblock \emph{arXiv preprint arXiv:2406.01252}.

\bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2023.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24(240):1--113.

\bibitem[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022.
\newblock A survey on in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}.

\bibitem[{Fernando et~al.(2023)Fernando, Banarse, Michalewski, Osindero, and Rockt{\"a}schel}]{fernando2023promptbreeder}
Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt{\"a}schel. 2023.
\newblock Promptbreeder: Self-referential self-improvement via prompt evolution.
\newblock \emph{arXiv preprint arXiv:2309.16797}.

\bibitem[{Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, Jones, Bowman, Chen, Conerly, Dassarma, Drain, Elhage, El-Showk, Fort, Dodds, Henighan, Hernandez, Hume, Jacobson, Johnston, Kravec, Olsson, Ringer, Tran-Johnson, Amodei, Brown, Joseph, McCandlish, Olah, Kaplan, and Clark}]{Ganguli2022RedTL}
Deep Ganguli, Liane Lovitt, John Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Benjamin Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zachary Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom~B. Brown, Nicholas Joseph, Sam McCandlish, Christopher Olah, Jared Kaplan, and Jack Clark. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:252355458} {Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned}.
\newblock \emph{ArXiv}, abs/2209.07858.

\bibitem[{Guo et~al.(2024)Guo, Yao, Shen, Wei, Zhang, Wang, and Liu}]{guo2024human}
Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang, and Yang Liu. 2024.
\newblock Human-instruction-free llm self-alignment with limited samples.
\newblock \emph{arXiv preprint arXiv:2401.06785}.

\bibitem[{Han(2023)}]{han2023context}
Xiaochuang Han. 2023.
\newblock In-context alignment: Chat with vanilla language models before fine-tuning.
\newblock \emph{arXiv preprint arXiv:2308.04275}.

\bibitem[{Hao et~al.(2024)Hao, Gu, Luo, Liu, Shao, Wang, Xie, Ma, Samavedhi, Gao, Wang, and Hu}]{hao2024llm}
Shibo Hao, Yi~Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, and Zhiting Hu. 2024.
\newblock \href {https://arxiv.org/abs/2404.05221} {Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models}.
\newblock \emph{Preprint}, arXiv:2404.05221.

\bibitem[{Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu}]{hao2023reasoning}
Shibo Hao, Yi~Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023.
\newblock Reasoning with language model is planning with world model.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 8154--8173.

\bibitem[{Huang et~al.(2024)Huang, Sengupta, Bonadiman, Lai, Gupta, Pappas, Mansour, Kirchoff, and Roth}]{huang2024deal}
James~Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, and Dan Roth. 2024.
\newblock Deal: Decoding-time alignment for large language models.
\newblock \emph{arXiv preprint arXiv:2402.06147}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed}]{Jiang2023Mistral7}
Albert~Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'e}e Lacroix, and William~El Sayed. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:263830494} {Mistral 7b}.
\newblock \emph{ArXiv}, abs/2310.06825.

\bibitem[{Khanov et~al.(2024)Khanov, Burapacheep, and Li}]{khanov2024args}
Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. 2024.
\newblock Args: Alignment as reward-guided search.
\newblock \emph{arXiv preprint arXiv:2402.01694}.

\bibitem[{Kim et~al.(2023)Kim, Bae, Shin, Kang, Kwak, Yoo, and Seo}]{kim2023aligning}
Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang~Min Yoo, and Minjoon Seo. 2023.
\newblock Aligning large language models through synthetic feedback.
\newblock \emph{arXiv preprint arXiv:2305.13735}.

\bibitem[{Kong et~al.(2024)Kong, Wang, Mu, Du, Zhuang, Zhou, Song, Zhang, Wang, and Zhang}]{kong2024aligning}
Lingkai Kong, Haorui Wang, Wenhao Mu, Yuanqi Du, Yuchen Zhuang, Yifei Zhou, Yue Song, Rongzhi Zhang, Kai Wang, and Chao Zhang. 2024.
\newblock Aligning large language models with representation editing: A control perspective.
\newblock \emph{arXiv preprint arXiv:2406.05954}.

\bibitem[{Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi}]{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
\newblock \emph{arXiv preprint arXiv:2309.00267}.

\bibitem[{Li et~al.(2024)Li, Patel, Vi{\'e}gas, Pfister, and Wattenberg}]{li2024inference}
Kenneth Li, Oam Patel, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg. 2024.
\newblock Inference-time intervention: Eliciting truthful answers from a language model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis}]{li2023self}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023{\natexlab{a}}.
\newblock Self-alignment with instruction backtranslation.
\newblock \emph{arXiv preprint arXiv:2308.06259}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto}]{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023{\natexlab{b}}.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}.

\bibitem[{Li et~al.(2023{\natexlab{c}})Li, Wei, Zhao, Zhang, and Zhang}]{li2023rain}
Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023{\natexlab{c}}.
\newblock Rain: Your language models can align themselves without finetuning.
\newblock \emph{arXiv preprint arXiv:2309.07124}.

\bibitem[{Lin et~al.(2024{\natexlab{a}})Lin, Ravichander, Lu, Dziri, Sclar, Chandu, Bhagavatula, and Choi}]{Lin2024ReAlign}
Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2024{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2312.01552} {The unlocking spell on base llms: Rethinking alignment via in-context learning}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Lin et~al.(2024{\natexlab{b}})Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han}]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024{\natexlab{b}}.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock In \emph{MLSys}.

\bibitem[{Madaan et~al.(2024)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang et~al.}]{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al. 2024.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {https://arxiv.org/abs/2203.02155} {Training language models to follow instructions with human feedback}.
\newblock \emph{Preprint}, arXiv:2203.02155.

\bibitem[{Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and Zeng}]{pryzant2023automatic}
Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng. 2023.
\newblock Automatic prompt optimization with" gradient descent" and beam search.
\newblock \emph{arXiv preprint arXiv:2305.03495}.

\bibitem[{Reimers and Gurevych(2019)}]{reimers-2019-sentence-bert}
Nils Reimers and Iryna Gurevych. 2019.
\newblock \href {https://arxiv.org/abs/1908.10084} {Sentence-bert: Sentence embeddings using siamese bert-networks}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics.

\bibitem[{Rubin et~al.(2021)Rubin, Herzig, and Berant}]{rubin2021learning}
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021.
\newblock Learning to retrieve prompts for in-context learning.
\newblock \emph{arXiv preprint arXiv:2112.08633}.

\bibitem[{Sun et~al.(2024)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan}]{sun2024principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2024.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{Touvron2023Llama2O}
Hugo Touvron, Louis Martin, Kevin~R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel~M. Bikel, Lukas Blecher, Cristian~Cant{\'o}n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony~S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel~M. Kloumann, A.~V. Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, R.~Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and
  Thomas Scialom. 2023{\natexlab{b}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:259950998} {Llama 2: Open foundation and fine-tuned chat models}.
\newblock \emph{ArXiv}, abs/2307.09288.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Ma, Meng, Qin, Shen, Zhang, Wu, Liu, Bian, Xu et~al.}]{wang2024step}
Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li~Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, et~al. 2024{\natexlab{a}}.
\newblock Step-on-feet tuning: Scaling self-alignment of llms via bootstrapping.
\newblock \emph{arXiv preprint arXiv:2402.07610}.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Zhang, Li, Tan, Wang, Ren, Jiang, and Qiu}]{wang2024inferaligner}
Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke~Ren, Botian Jiang, and Xipeng Qiu. 2024{\natexlab{b}}.
\newblock Inferaligner: Inference-time alignment for harmlessness through cross-model guidance.
\newblock \emph{arXiv preprint arXiv:2401.11206}.

\bibitem[{Wang et~al.(2023)Wang, Li, Wang, Bai, Luo, Zhang, Jojic, Xing, and Hu}]{wang2023promptagent}
Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. 2023.
\newblock Promptagent: Strategic planning with language models enables expert-level prompt optimization.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi}]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}.

\bibitem[{Wu et~al.(2024)Wu, Arora, Wang, Geiger, Jurafsky, Manning, and Potts}]{wu2024reft}
Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher~D Manning, and Christopher Potts. 2024.
\newblock Reft: Representation finetuning for language models.
\newblock \emph{arXiv preprint arXiv:2404.03592}.

\bibitem[{Xu et~al.(2022)Xu, Chen, Du, Shao, Wang, Li, and Yang}]{xu2022gps}
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022.
\newblock Gps: Genetic prompt search for efficient few-shot learning.
\newblock \emph{arXiv preprint arXiv:2210.17041}.

\bibitem[{Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{yang2023large}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2023.
\newblock Large language models as optimizers.
\newblock \emph{arXiv preprint arXiv:2309.03409}.

\bibitem[{Zhao et~al.(2024)Zhao, Andriushchenko, Croce, and Flammarion}]{zhao2024context}
Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. 2024.
\newblock Is in-context learning sufficient for instruction following in llms?
\newblock \emph{arXiv preprint arXiv:2405.19874}.

\bibitem[{Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu et~al.}]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2024.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba}]{zhou2022large}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.
\newblock Large language models are human-level prompt engineers.
\newblock \emph{arXiv preprint arXiv:2211.01910}.

\bibitem[{Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski et~al.}]{zou2023representation}
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et~al. 2023.
\newblock Representation engineering: A top-down approach to ai transparency.
\newblock \emph{arXiv preprint arXiv:2310.01405}.

\end{thebibliography}
