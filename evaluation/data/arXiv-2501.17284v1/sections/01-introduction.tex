\section{Introduction}
\label{sec:introduction}

A striking feature of peripheral responses in the animal nervous system is \emph{localization}---that is,
the linear receptive fields of simple-cell neurons often respond to contiguous regions much smaller than their full input domain. 
In vision, retinal ganglion cells approximate localized center-surround filters that tile the input~\parencite{dacey2000center,doi2012efficient,knudsen1978space},
and simple cells downstream in primary visual cortex have localized filters that are selective for spatial frequency and orientation~\parencite{hubel1959receptive,hubel1968receptive,rolls1995sparseness,niell2008highly,willmore2011sparse,ringach2002orientation,ringach2002spatial}.
In primary somatosensory cortex, neurons respond to stimulation of restricted regions of skin~\parencite{crochet2011synaptic} and
in primary auditory cortex, spatiotemporal receptive fields are typically localized in both time and frequency domains~\parencite{deweese2003binary,hromadka2008sparse};
see \cref{fig:sim-real-gabors} (left).

By contrast, artificial learning systems do not always learn localized filters. 
Principal component analysis tends to fit weights that span the entire input signal, as do unregularized autoencoder neural network architectures and restricted Boltzmann machines~\parencite{saxe2011unsupervised}.
This difference has prompted the search for artificial learning models that can learn 
localized receptive fields from naturalistic stimuli,
the most notable of which are sparse coding~\parencite{olshausen1996emergence,olshausen1997sparse}
and independent component analysis~\parencite[ICA;~][]{bell1997independent,vanhateren1998independent}.
Sparse coding, ICA, and related compression methods that produce localized receptive fields from naturalistic data share a top-down approach---they find an efficient representation of the input signal by optimizing an explicit sparsity criterion, or an independence criterion that necessitates sparsity in a critically parameterized regime~\cite{field1999wavelets,saxe2011unsupervised}.

Though sparsity is appealing as a potentially unifying explanation for localization, localization also emerges naturally in networks trained to perform classification tasks without any explicit sparsity regularization~\parencite{krizhevsky2012imagenet,zeiler2013visualizing,yosinski2015understanding,sengupta2018manifoldtiling}; see \cref{fig:sim-real-gabors} (center) for an example.
\textcite{ingrosso2022data} distilled such examples of emergent localization by demonstrating that localized receptive fields emerge in simple feedforward neural networks trained on a data model with properties meant to approximate natural visual input, in particular,
locality structure (statistical independence of non-collocated dimensions)
and non-Gaussianity (higher-order cumulants are non-null).
In simulations, \textcite{ingrosso2022data} tie the dynamical emergence of localization to increased tuning to higher-order statistics of the input, and demonstrate that even a single neuron is sufficient to learn a localized receptive field in this setting.

In this work, we build on the demonstration of \textcite{ingrosso2022data} with the aim of describing the mechanisms behind the emergence of a localized receptive field in this minimal setting.
The higher-order input statistics that drive localization are challenging to analyze with existing tools that exploit implied Gaussianity~\parencite{goldt2020modelling}.
By separating two stages of learning, we are able to derive equations for the effective early-time learning dynamics of the single neuron model that learns a localized receptive field from idealized naturalistic data.
Our analytical model identifies a concise description of the higher-order statistics that drive emergence, 
and we validate both positive and negative predictions of this analytical model via simulations with many neurons; see \cref{fig:sim-real-gabors} (right).
These findings suggest an alternative path to account for the ubiquity of localization in early neural responses as resulting from the interaction of the nonlinear dynamics of learning in neural circuits and naturalistic data with higher-order statitistical structure,
rather than an explicit efficiency criterion.
