\begin{thebibliography}{128}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O'Connor, and McGuinness]{arazo19}
E.~Arazo, D.~Ortego, P.~Albert, N.~E. O'Connor, and K.~McGuinness.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{ICML}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal, Maharaj, Fischer, Courville, and Bengio]{arpit17memory}
D.~Arpit, S.~Jastrzebski, N.~Ballas, D.~Krueger, E.~Bengio, M.~Kanwal, T.~Maharaj, A.~Fischer, A.~Courville, and Y.~Bengio.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Bae et~al.(2022)Bae, Shin, Jang, Song, and Moon]{bae22icml}
H.~Bae, S.~Shin, J.~Jang, K.~Song, and I.~Moon.
\newblock From noisy prediction to true label: Noisy prediction calibration via generative model.
\newblock In \emph{ICML}, 2022.

\bibitem[Bai et~al.(2021)Bai, Yang, Han, Yang, Li, Mao, Niu, and Liu]{bai2021understanding}
Y.~Bai, E.~Yang, B.~Han, Y.~Yang, J.~Li, Y.~Mao, G.~Niu, and T.~Liu.
\newblock Understanding and improving early stopping for learning with noisy labels.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock21nips}
R.~J.~N. Baldock, H.~Maennel, and B.~Neyshabur.
\newblock Deep learning through the lens of example difÔ¨Åculty.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Bertin-Mahieux et~al.(2011)Bertin-Mahieux, Ellis, Whitman, and Lamere]{bertin11msd}
T.~Bertin-Mahieux, D.~P.~W. Ellis, B.~Whitman, and P.~Lamere.
\newblock The million song dataset.
\newblock In \emph{ISMIR}, 2011.

\bibitem[Boudiat et~al.(2020)Boudiat, Rony, Ziko, Granger, Pedersoli, Piantanida, and Ayed]{boudiaf2020}
M.~Boudiat, J.~Rony, I.~M Ziko, E.~Granger, M.~Pedersoli, P.~Piantanida, and I.~B. Ayed.
\newblock A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses.
\newblock In \emph{ECCV}, 2020.

\bibitem[Castells et~al.(2020)Castells, Weinzaepfel, and Revaud]{castells20}
T.~Castells, P.~Weinzaepfel, and J.~Revaud.
\newblock Superloss: A generic loss for robust curriculum learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[de~Vente et~al.(2021)de~Vente, Vos, Hosseinzadeh, Pluim, and Veta]{coen21medicine}
C.~de~Vente, P.~Vos, M.~Hosseinzadeh, J.~Pluim, and M.~Veta.
\newblock Deep learning regression for prostate cancer detection and grading in bi-parametric mri.
\newblock \emph{IEEE Transactions on Biomedical Engineering}, 68\penalty0 (2):\penalty0 374--383, 2021.

\bibitem[Doi et~al.(2022)Doi, Takahashi, Yasuoka, Fukuda, and Aoyagi]{doi22physics}
H.~Doi, K.~Z. Takahashi, H.~Yasuoka, J.~Fukuda, and T.~Aoyagi.
\newblock Regression analysis for predicting the elasticity of liquid crystal elastomers.
\newblock \emph{Scientific Reports}, 12\penalty0 (19788), 2022.

\bibitem[Dukler et~al.(2023)Dukler, Bowman, Achille, Golatkar, Swaminathan, and Soatto]{dukler23}
Y.~Dukler, B.~Bowman, A.~Achille, A.~Golatkar, A.~Swaminathan, and S.~Soatto.
\newblock Safe: Machine unlearning with shard graphs.
\newblock \emph{arXiv preprint arXiv: 2304.13169}, 2023.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus21}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Fu et~al.(2018)Fu, Gong, Wang, Batmanghelich, and Tao]{fu2018deep}
H.~Fu, M.~Gong, C.~Wang, K.~Batmanghelich, and D.~Tao.
\newblock Deep ordinal regression network for monocular depth estimation.
\newblock In \emph{CVPR}, 2018.

\bibitem[Gao et~al.(2019)Gao, Wang, Dai, Li, and Nevatia]{gao2018notercnn}
J.~Gao, J.~Wang, S.~Dai, L.~J. Li, and R.~Nevatia.
\newblock Note-rcnn: Noise tolerant ensemble rcnn for semi-supervised object detection.
\newblock In \emph{ICCV}, 2019.

\bibitem[Gao et~al.(2016)Gao, Yang, and Zhou]{gao16knn}
W.~Gao, B.~Yang, and Z.~Zhou.
\newblock On the resistance of nearest neighbor to random noisy labels.
\newblock \emph{arXiv preprint arXiv:1607.07526}, 2016.

\bibitem[Gao et~al.(2020)Gao, Cheng, He, Xie, Zhao, Lu, and Xiang]{gao20mpo}
Z.~Gao, S.~Cheng, R.~He, Z.~Xie, H.~Zhao, Z.~Lu, and T.~Xiang.
\newblock Compressing deep neural networks by matrix product operators.
\newblock In \emph{Physical Review Research}, 2020.

\bibitem[Gao et~al.(2022)Gao, Liu, Zhao, Lu, and Wen]{gao22mpo}
Z.~Gao, P.~Liu, W.~X. Zhao, Z.~Lu, and J.~Wen.
\newblock Parameter-efficient mixture-of-experts architecture for pre-trained language models.
\newblock In \emph{Proceedings of the 29th International Conference on Computational Linguistics}, 2022.

\bibitem[Garg and Manwani(2020)]{garg2020robust}
B.~Garg and N.~Manwani.
\newblock Robust deep ordinal regression under label noise.
\newblock In \emph{Asian Conference on Machine Learning}, 2020.

\bibitem[Geng and Xia(2014)]{geng2014headPE}
X.~Geng and Y.~Xia.
\newblock Head pose estimation based on multivariate label distribution.
\newblock In \emph{CVPR}, 2014.

\bibitem[Gibbons(1985)]{gibbons85pm}
A.~Gibbons, editor.
\newblock \emph{Algorithmic Graph Theory}.
\newblock Cambridge University Press, London, England, 1985.

\bibitem[Gong et~al.(2022)Gong, Mori, and Tung]{gong22rank}
Y.~Gong, G.~Mori, and F.~Tung.
\newblock Ranksim: Ranking similarity regularization for deep imbalanced regression.
\newblock In \emph{ICML}, 2022.

\bibitem[Gorishniy et~al.(2021)Gorishniy, Rubachev, Khrulkov, and Babenko]{gorishniy21nips}
Y.~Gorishniy, I.~Rubachev, V.~Khrulkov, and A.~Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Grinsztajn et~al.(2022)Grinsztajn, Oyallon, and Varoquaux]{grinsztajn22nips}
L.~Grinsztajn, E.~Oyallon, and G.~Varoquaux.
\newblock Why do tree-based models still outperform deep learning on tabular data?
\newblock In \emph{NeurIPS Track Datasets and Benchmarks}, 2022.

\bibitem[Gr{\o}nlund et~al.(2019)Gr{\o}nlund, Kamma, Larsen, Mathiasen, and Nelson]{gronlund2019margin}
A.~Gr{\o}nlund, L.~Kamma, K.~G. Larsen, A.~Mathiasen, and J.~Nelson.
\newblock Margin-based generalization lower bounds for boosted classifiers.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Gr{\o}nlund et~al.(2020)Gr{\o}nlund, Kamma, and Larsen]{gronlund2020near}
A.~Gr{\o}nlund, L.~Kamma, and K.~G. Larsen.
\newblock Near-tight margin-based generalization bounds for support vector machines.
\newblock In \emph{ICML}, 2020.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and Sugiyama]{han18coteaching}
B.~Han, Q.~Yao, X.~Yu, G.~Niu, M.~Xu, W.~Hu, I.~Tsang, and M.~Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely noisy labels.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Han et~al.(2020)Han, Niu, Yu, Yao, Xu, Tsang, and Sugiyama]{han20sigua}
B.~Han, G.~Niu, X.~Yu, Q.~Yao, M.~Xu, I.~W. Tsang, and M.~Sugiyama.
\newblock Sigua: Forgetting may make learning with noisy labels more robust.
\newblock In \emph{ICML}, 2020.

\bibitem[He et~al.(2021)He, Qiu, Zeng, Yang, Zhai, and Tang]{he21fastmoe}
J.~He, J.~Qiu, A.~Zeng, Z.~Yang, J.~Zhai, and J.~Tang.
\newblock Fastmoe: A fast mixture-of-expert training system.
\newblock \emph{arXiv preprint arXiv:2103.13262}, 2021.

\bibitem[Heller and Ghahramani(2007{\natexlab{a}})]{heller2007}
K.~A. Heller and Z.~A. Ghahramani.
\newblock Nonparametric bayesian approach to modeling overlapping clusters.
\newblock In \emph{Artificial Intelligence and Statistics}, 2007{\natexlab{a}}.

\bibitem[Heller and Ghahramani(2007{\natexlab{b}})]{heller07}
Katherine~A. Heller and Zoubin Ghahramani.
\newblock A nonparametric bayesian approach to modeling overlapping clusters.
\newblock In \emph{Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics}, volume~2, pages 187--194. PMLR, 2007{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and Gimpel]{hendrycks18nips}
D.~Hendrycks, M.~Mazeika, D.~Wilson, and K.~Gimpel.
\newblock Using trusted data to train deep networks on labels corrupted by severe noise.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Hinton(2002)]{hinton02poe}
G.~E. Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock In \emph{Neural Computation}, 2002.

\bibitem[Hirk et~al.(2019)Hirk, Hornik, and Vana]{hirk2019multivariate}
R.~Hirk, K.~Hornik, and L.~Vana.
\newblock Multivariate ordinal regression models: an analysis of corporate credit ratings.
\newblock \emph{Statistical Methods \& Applications}, 28:\penalty0 507--539, 2019.

\bibitem[Hu et~al.(2020)Hu, Li, and Y]{hu20rdiaux}
W.~Hu, Z.~Li, and D.~Y.
\newblock Simple and effective regularization methods for training on noisily labeled data with generalization guarantee.
\newblock In \emph{ICLR}, 2020.

\bibitem[Huang et~al.(2020)Huang, Zhang, and Zhang]{huang2020self}
L.~Huang, C.~Zhang, and H.~Zhang.
\newblock Self-adaptive training: beyond empirical risk minimization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Huang et~al.(2023)Huang, Zhang, and Shan]{huang2023twin}
Z.~Huang, J.~Zhang, and H.~Shan.
\newblock Twin contrastive learning with noisy labels.
\newblock In \emph{CVPR}, 2023.

\bibitem[J.~Li(2021)]{li2021learning}
S.~C. H.~Hoi J.~Li, C.~Xiong.
\newblock Learning from noisy data with robust representation learning.
\newblock In \emph{ICCV}, 2021.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{jacobs1991MoE}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Comput}, 3:\penalty0 79--87, 1991.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and Fei-Fei]{jiang17icml}
L.~Jiang, Z.~Zhou, T.~Leung, L.~Li, and L.~Fei-Fei.
\newblock Mentornet:learning data-driven curriculum for very deep neural networks on corrupted labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Karim et~al.(2022)Karim, Rizve, Rahnavard, Mian, and Shah]{karim2022unicon}
N.~Karim, M.~N. Rizve, N.~Rahnavard, A.~Mian, and M.~Shah.
\newblock Unicon: Combating label noise through uniform selection and contrastive learning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Kim et~al.(2021)Kim, Ko, Cho, Choi, and Yun]{kim21fine}
T.~Kim, J.~Ko, S.~Cho, J.~Choi, and S.~Yun.
\newblock Fine samples for learning with noisy labels.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Kim et~al.(2023)Kim, Awan, Muzio, Salinas, Lu, Hendy, Rajbhandari, He, and Awadalla]{yjkim21}
Y.~J. Kim, A.~A. Awan, A.~Muzio, A.~Salinas, L.~Lu, A.~Hendy, S.~Rajbhandari, Y.~He, and H.~H. Awadalla.
\newblock Scalable and efficient moe training for multitask multilingual models.
\newblock \emph{arXiv preprint arXiv:2109.10465}, 2023.

\bibitem[Kimura et~al.(2021)Kimura, Nakamura, and Saito]{kimura21shift15m}
M.~Kimura, T.~Nakamura, and Y.~Saito.
\newblock Shift15m: Multiobjective large-scale fashiondataset with distributional shifts.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop}, 2021.

\bibitem[Kingma and Ba(2015)]{kingma15adam}
D.~Kingma and J.~Ba.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kye et~al.(2022)Kye, Choi, Yi, and Chang]{kye2021learning}
S.~M. Kye, K.~Choi, J.~Yi, and B.~Chang.
\newblock Learning with noisy labels by efficient transition matrix estimation to combat label miscorrection.
\newblock In \emph{ECCV}, 2022.

\bibitem[Lee et~al.(2018)Lee, He, Zhang, and Yang]{lee18cleannet}
K.~Lee, X.~He, L.~Zhang, and L.~Yang.
\newblock Cleannet: Transfer learning for scalable image classfier training with label noise.
\newblock In \emph{CVPR}, 2018.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{kmlee19}
K.~Lee, S.~Yun, K.~Lee, H.~Lee, B.~Li, and J.~Shin.
\newblock Robust inference via generative classiÔ¨Åers for handling noisy labels.
\newblock In \emph{ICML}, 2019.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, and Huang]{lepikhin21}
D.~Lepikhin, H.~J. Lee, Y.~Xu, D.~Chen, O.~Firat, and Y.~Huang.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{ICLR}, 2021.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and Zettlemoyer]{lewis21}
M.~Lewis, S.~Bhosale, T.~Dettmers, N.~Goyal, and L.~Zettlemoyer.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock \emph{arXiv preprint arXiv:2103.16716}, 2021.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Socher, and Hoi]{li2020dividemix}
J.~Li, R.~Socher, and S.~C. Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Xiong, Socher, and Hoi]{li2020towards}
J.~Li, C.~Xiong, R.~Socher, and S.~Hoi.
\newblock Towards noise-resistant object detection with noisy annotations.
\newblock \emph{arXiv preprint arXiv: 2003.01285}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Liu, and Yu]{li22neighbor}
J.~Li, G.~Li, F.~Liu, and Y.~Yu.
\newblock Neighborhood collective estimation for noisy label identification and correction.
\newblock In \emph{ECCV}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Xia, Ge, and Liu]{li2022selective}
S.~Li, X.~Xia, S.~Ge, and T.~Liu.
\newblock Selective-supervised contrastive learning with noisy labels.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Xia, Zhang, Zhan, Ge, and Liu]{li2022estimating}
S.~Li, X.~Xia, H.~Zhang, Y.~Zhan, S.~Ge, and T.~Liu.
\newblock Estimating noise transition matrix with label correlations for noisy multi-label learning.
\newblock In \emph{NeurIPS}, 2022{\natexlab{c}}.

\bibitem[Li et~al.(2019)Li, Lu, Feng, Xu, Zhou, and Tian]{li19bridge}
W.~Li, J.~Lu, J.~Feng, C.~Xu, J.~Zhou, and Q.~Tian.
\newblock Bridgenet: A continuity-aware probabilistic network for age estimation.
\newblock In \emph{CVPR}, 2019.

\bibitem[Lim et~al.(2020)Lim, Shin, Lee, and Kim]{lim20order}
K.~Lim, N.~H. Shin, Y.~Y. Lee, and C.~S. Kim.
\newblock Order learning and its application to age estimation.
\newblock In \emph{ICLR}, 2020.

\bibitem[Liu et~al.(2022)Liu, Wang, Lu, Cao, and Zhang]{liu2022robust}
C.~Liu, K.~Wang, H.~Lu, Z.~Cao, and Z.~Zhang.
\newblock Robust object detection with inaccurate bounding boxes.
\newblock In \emph{ECCV}, 2022.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and Fernandez-Granda]{liu2020early}
S.~Liu, J.~Niles-Weed, N.~Razavian, and C.~Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Liu et~al.(2023)Liu, Duffy, Dy, and Gaunguly]{liu23elnino}
Y.~Liu, K.~Duffy, J.~G. Dy, and A.~R. Gaunguly.
\newblock Explainable deep learning for insights in el ni√±o and river flows.
\newblock \emph{Nature Communications}, 14\penalty0 (339), 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov17iclr}
I.~Loshchilov and F.~Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.

\bibitem[Ma et~al.(2022)Ma, Ushiku, and Sagara]{ma2022TheEO}
J.~Ma, Y.~Ushiku, and M.~Sagara.
\newblock The effect of improving annotation quality on object detection datasets: A preliminary study.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop}, 2022.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and Bailey]{ma18d2l}
X.~Ma, Y.~Wang, M.~E. Houle, S.~Zhou, S.~M. Erfani, S.~T. Xia, S.~Wijewickrema, and J.~Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Mao et~al.(2021)Mao, Yu, Yamakata, and Aizawa]{mao2021noisy}
J.~Mao, Q.~Yu, Y.~Yamakata, and K.~Aizawa.
\newblock Noisy annotation refinement for object detection.
\newblock \emph{British Machine Vision Conference}, 2021.

\bibitem[Masoudnia and Ebrahimpour(2014)]{masoudnia2014Moe}
S.~Masoudnia and R.~Ebrahimpour.
\newblock Mixture of experts: a literature survey.
\newblock In \emph{Artificial Intelligence Review}, 2014.

\bibitem[Menon et~al.(2020)Menon, Rawat, Reddi, and Kumar]{menon20phuber}
A.~K. Menon, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Can gradient clipping mitigate label noise?
\newblock In \emph{ICLR}, 2020.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Cao, and Leskovec]{mirzasoleiman20crust}
B.~Mirzasoleiman, K.~Cao, and J.~Leskovec.
\newblock Coresets for robust training of neural networks against noisy labels.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Monfared and Mallik(2016)]{monfared16pm}
K.~H. Monfared and S.~Mallik.
\newblock Spectral characterization of matchings in graphs.
\newblock \emph{Linear Algebra and its Applciations}, 496\penalty0 (1):\penalty0 234--778, 2016.

\bibitem[Niu et~al.(2016)Niu, Zhou, Gao, and Hua]{niu16afad}
Z.~Niu, M.~Zhou, X.~Gao, and G.~Hua.
\newblock Ordinal regression with a multiple output cnn for age estimation.
\newblock In \emph{CVPR}, 2016.

\bibitem[Ortego et~al.(2021)Ortego, Arazo, Albert, O‚ÄôConnor, and McGuinness]{ortego2020multi}
D.~Ortego, E.~Arazo, P.~Albert, N.~E. O‚ÄôConnor, and K.~McGuinness.
\newblock Multi-objective interpolation training for robustness to label noise.
\newblock In \emph{CVPR}, 2021.

\bibitem[Ostyakov et~al.(2018)Ostyakov, Logacheva, Suvorov, Aliev, Sterkin, Khomenko, and Nikolenko]{ostyakov18eccv}
P.~Ostyakov, E.~Logacheva, R.~Suvorov, V.~Aliev, G.~Sterkin, O.~Khomenko, and S.~I. Nikolenko.
\newblock Label denoising with large ensembles of heterogeneous neural networks.
\newblock In \emph{ECCV}, 2018.

\bibitem[Papadopoulos et~al.(2022)Papadopoulos, Koutlis, Papadopoulos, and Kompatsiaris]{papadopoulos22fashion}
S.~Papadopoulos, C.~Koutlis, S.~Papadopoulos, and I.~Kompatsiaris.
\newblock Multimodal quasi-autoregression: forecasting the visual popularity of new fashion products.
\newblock \emph{International Journal of Multimedia Information Retrieval}, 11:\penalty0 717--729, 2022.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Menon, Nock, and Qu]{patrini17}
G.~Patrini, A.~Rozza, A.~Menon, R.~Nock, and L.~Qu.
\newblock Making deep neural networks robust to label noise: a loss correction approach.
\newblock In \emph{CVPR}, 2017.

\bibitem[Pleiss et~al.(2020)Pleiss, Zhang, Elenberg, and Weinberger]{pleiss20aum}
G.~Pleiss, T.~Zhang, E.~R. Elenberg, and K.~Q. Weinberger.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Reed et~al.(2015)Reed, Lee, Anguelov, Szegedy, Erhan, and Rabinovich]{reed15}
S.~Reed, H.~Lee, D.~Anguelov, C.~Szegedy, D.~Erhan, and A.~Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock In \emph{ICLR workshop}, 2015.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren18l2r}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Rothe et~al.(2018)Rothe, Timofte, and Gool]{rothe18imdb}
R.~Rothe, R.~Timofte, and L.~V. Gool.
\newblock Deep expectation of real and apparent age from a single image without facial landmarks.
\newblock \emph{International Journal of Computer Vision}, 126\penalty0 (2-4):\penalty0 144--157, 2018.

\bibitem[Rothe et~al.(2015)Rothe, Timofte, and Van~Gool]{rothe2015imdb-wiki}
Rasmus Rothe, Radu Timofte, and Luc Van~Gool.
\newblock Dex: Deep expectation of apparent age from a single image.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision workshops}, pages 10--15, 2015.

\bibitem[Sarfi et~al.(2023)Sarfi, Karimpour, Chaudhary, Khalid, Ravanelli, Mudur, and Belilovsky]{sarfi2023simulated}
Amir~M Sarfi, Zahra Karimpour, Muawiz Chaudhary, Nasir~M Khalid, Mirco Ravanelli, Sudhir Mudur, and Eugene Belilovsky.
\newblock Simulated annealing in early layers leads to better generalization.
\newblock In \emph{CVPR}, pages 20205--20214, 2023.

\bibitem[Schubert et~al.(2023)Schubert, Riedlinger, Kahl, Kr√∂ll, Schoenen, ≈†egviƒá, and Rottmann]{schubert2023identifying}
M.~Schubert, T.~Riedlinger, K.~Kahl, D.~Kr√∂ll, S.~Schoenen, S.~≈†egviƒá, and M.~Rottmann.
\newblock Identifying label errors in object detection datasets by loss inspection.
\newblock \emph{arXiv preprint arXiv: 2303.06999}, 2023.

\bibitem[Shah et~al.(2022)Shah, Xue, and Aamodt]{shah2022label}
D.~Shah, Z.~Y. Xue, and T.~M. Aamodt.
\newblock Label encoding for regression networks.
\newblock \emph{arXiv preprint arXiv:2212.01927}, 2022.

\bibitem[Sharkey and Sharkey(1997)]{sharkey1997}
A.~Sharkey and N.~Sharkey.
\newblock Combining diverse neural nets.
\newblock In \emph{The Knowledge Engineering Review}, 1997.

\bibitem[Shawe-Taylor and Cristianini(1998)]{shawe1998robust}
J.~Shawe-Taylor and N.~Cristianini.
\newblock Robust bounds on generalization from the margin distribution.
\newblock 1998.

\bibitem[Shen and Sanghavi(2019)]{shen19icml}
Y.~Shen and S.~Sanghavi.
\newblock Learning with bad training data via iterative trimmed loss minimization.
\newblock In \emph{ICML}, 2019.

\bibitem[Shen et~al.(2020)Shen, Ji, Chen, Hong, Zheng, Liu, Xu, and Tian]{shen2020noise}
Y.~Shen, R.~Ji, Z.~Chen, X.~Hong, F.~Zheng, J.~Liu, M.~Xu, and Q.~Tian.
\newblock Noise-aware fully webly supervised object detection.
\newblock In \emph{CVPR}, 2020.

\bibitem[Shin et~al.(2022)Shin, Lee, and Kim]{shin2022moving}
N.~Shin, S.~Lee, and C.~Kim.
\newblock Moving window regression: a novel approach to ordinal regression.
\newblock In \emph{CVPR}, 2022.

\bibitem[Sia et~al.(2020)Sia, Baldrich, Vanrell, and Samaras]{sial2020light}
H.~A. Sia, R.~Baldrich, M.~Vanrell, and D.~Samaras.
\newblock Light direction and color estimation from single image with deep regression.
\newblock \emph{arXiv preprint arXiv:2009.08941}, 2020.

\bibitem[Song et~al.(2019)Song, Kim, and Lee]{song19b}
H.~Song, M.~Kim, and J.~Lee.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Stephenson et~al.(2021)Stephenson, Ganesh, Hui, Tang, Chung, et~al.]{stephenson2021geometry}
Cory Stephenson, Abhinav Ganesh, Yue Hui, Hanlin Tang, SueYeon Chung, et~al.
\newblock On the geometry of generalization and memorization in deep neural networks.
\newblock In \emph{ICLR}, 2021.

\bibitem[Su et~al.(2012)Su, Deng, and Fei-Fei]{su2012crowdsourcingAF}
H.~Su, J.~Deng, and L.~Fei-Fei.
\newblock Crowdsourcing annotations for visual object detection.
\newblock In \emph{HCOMP@AAAI}, 2012.

\bibitem[Tanaka et~al.(2022)Tanaka, Kadoya, Sugai, Umeda, Ishizawa, Katsuta, Ito, Takeda, and Jingu]{tanaka22medicine}
S.~Tanaka, N.~Kadoya, Y.~Sugai, M.~Umeda, M.~Ishizawa, Y.~Katsuta, K.~Ito, K.~Takeda, and K.~Jingu.
\newblock A deep learning-based radiomics approach to predict head and neck tumor regression for adaptive radiotherapy.
\newblock \emph{Scientific Reports}, 12\penalty0 (8899), 2022.

\bibitem[Wan et~al.(2024)Wan, Wang, Xie, Li, Huang, and Chen]{wan2024unlocking}
Wenhai Wan, Xinrui Wang, Ming-Kun Xie, Shao-Yuan Li, Sheng-Jun Huang, and Songcan Chen.
\newblock Unlocking the power of open set: A new perspective for open-set noisy label learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 15438--15446, 2024.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and Bailey]{wang19sce}
Y.~Wang, X.~Ma, Z.~Chen, Y.~Luo, J.~Yi, and J.~Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{ICCV}, 2019.

\bibitem[Wang et~al.(2022)Wang, Sun, and Fu]{wang22spr}
Y.~Wang, X.~Sun, and Y.~Fu.
\newblock Scalable penalized regression for noise detection in learning with noisy labels.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wang et~al.(2020)Wang, Hu, and Hu]{wang2020training}
Z.~Wang, G.~Hu, and Q.~Hu.
\newblock Training noise-robust deep neural networks via meta-learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{wei20jocor}
H.~Wei, L.~Feng, X.~Chen, and B.~An.
\newblock Combating noisy labels by agreement: A joint training method with co-regularization.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wei et~al.(2021)Wei, Tao, Xie, and An]{wei2021open}
Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo~An.
\newblock Open-set label noise can improve robustness against inherent label noise.
\newblock \emph{NeurIPS}, 34:\penalty0 7978--7992, 2021.

\bibitem[Wen-Huang et~al.(2021)Wen-Huang, Sijie, Chieh-Yun, Chusnul, and Jiaying]{wen2021fashion}
Cheng Wen-Huang, Song Sijie, Chen Chieh-Yun, Hidayati~Shintami Chusnul, and Liu Jiaying.
\newblock Fashion meets computer vision: A survey.
\newblock \emph{ACM Computing Surveys}, 2021.

\bibitem[Wu et~al.(2020{\natexlab{a}})Wu, Zheng, Goswami, Metaxas, and Chen]{wu20topo}
P.~Wu, S.~Zheng, M.~Goswami, D.~N. Metaxas, and C.~Chen.
\newblock A topological Ô¨Ålter for learning with label noise.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Wu et~al.(2020{\natexlab{b}})Wu, Pan, Long, Jiang, Chang, and Zhang]{wu20finance}
Z.~Wu, S.~Pan, G.~Long, J.~Jiang, X.~Chang, and C.~Zhang.
\newblock Connecting the dots: Multivariate time series forecasting with graph neural networks.
\newblock In \emph{KDD}, 2020{\natexlab{b}}.

\bibitem[Xia et~al.(2020)Xia, Liu, Han, Wang, Gong, Liu, Niu, Tao, and Sugiyama]{xia2020part}
X.~Xia, T.~Liu, B.~Han, N.~Wang, M.~Gong, H.~Liu, G.~Niu, D.~Tao, and M.~Sugiyama.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xia et~al.(2021)Xia, Liu, Han, Gong, Wang, Ge, and Chang]{xia21cdr}
X.~Xia, T.~Liu, B.~Han, C.~Gong, N.~Wang, Z.~Ge, and Y.~Chang.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In \emph{ICLR}, 2021.

\bibitem[Xia et~al.(2022)Xia, Liu, Han, Gong, Yu, Niu, and Sugiyama]{xia22}
X.~Xia, T.~Liu, B.~Han, M.~Gong, J.~Yu, G.~Niu, and M.~Sugiyama.
\newblock Sample selection with uncertainty of losses for learning with noisy labels.
\newblock In \emph{ICLR}, 2022.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Yang, Han, Liu, Xu, Niu, and Liu]{yang2021estimating}
S.~Yang, E.~Yang, B.~Han, Y.~Liu, M.~Xu, G.~Niu, and T.~Liu.
\newblock Estimating instance-dependent label-noise transition matrix using dnns.
\newblock In \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Zha, Chen, and Katabi]{yang21ldsfds}
Y.~Yang, K.~Zha, Y.~Chen, and H.~Wang~D. Katabi.
\newblock Delving into deep imbalanced regression.
\newblock In \emph{ICML}, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2022)Yao, Wang, Zhang, Zou, and Finn]{yao22cmixup}
H.~Yao, Y.~Wang, L.~Zhang, J.~Zou, and C.~Finn.
\newblock C-mixup: Improving generalization in regression.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Yao et~al.(2020)Yao, Liu, Han, Gong, Deng, Niu, and Sugiyama]{yao2020dual}
Y.~Yao, T.~Liu, B.~Han, M.~Gong, J.~Deng, G.~Niu, and M.~Sugiyama.
\newblock Dual t: Reducing estimation error for transition matrix in label-noise learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Yi and Wu(2019)]{yi19pencil}
K.~Yi and J.~Wu.
\newblock Probabilistic end-to-end noise correction for learning with noisy labels.
\newblock In \emph{CVPR}, 2019.

\bibitem[Yi et~al.(2022)Yi, Liu, She, McLeod, and Wang]{yi2022onleraning}
L.~Yi, S.~Liu, Q.~She, A.~McLeod, and B.~Wang.
\newblock On learning contrastive representations for learning with noisy labels.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yiming et~al.(2021)Yiming, Jie, Yujiang, and Maja]{lin2021imdbclean}
L.~Yiming, S.~Jie, W.~Yujiang, and P.~Maja.
\newblock Fp-age: Leveraging face parsing attention for facial age estimation in the wild.
\newblock \emph{arXiv}, 2021.

\bibitem[Yuksel et~al.(2012)Yuksel, Wilson, and Gader]{yuksel2012Moe}
S.~E. Yuksel, J.~N. Wilson, and P.~D. Gader.
\newblock Twenty years of mixture of experts.
\newblock In \emph{Transactions on neural networks and learning systems}, 2012.

\bibitem[Zadouri et~al.(2023)Zadouri, {\"U}st{\"u}n, Ahmadian, Ermis, Locatelli, and Hooker]{zadouri23}
T.~Zadouri, A.~{\"U}st{\"u}n, A.~Ahmadian, B.~Ermis, A.~Locatelli, and S.~Hooker.
\newblock Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.
\newblock \emph{arXiv preprint arXiv:2309.05444}, 2023.

\bibitem[Zang et~al.(2019)Zang, Ding, Smith, Tyler, Rakotoarivelo, and K{\^a}afar]{zang2019TheIO}
S.~Zang, M.~Ding, D.~B. Smith, P.~Tyler, T.~Rakotoarivelo, and M.~Ali K{\^a}afar.
\newblock The impact of adverse weather conditions on autonomous vehicles: How rain, snow, fog, and hail affect the performance of a self-driving car.
\newblock \emph{IEEE Vehicular Technology Magazine}, 14:\penalty0 103--111, 2019.

\bibitem[Zha et~al.(2022)Zha, Cao, Yang, and Katabi]{zha22supcr}
K.~Zha, P.~Cao, Y.~Yang, and D.~Katabi.
\newblock Supervised contrastive regression.
\newblock \emph{arXiv preprint arXiv:2210.01189}, 2022.

\bibitem[Zhang et~al.(2017{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang17memory}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and Lopez-Paz]{zhang18mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2017{\natexlab{b}})Zhang, Aggarwal, and Qi]{zhang17finance}
L.~Zhang, C.~Aggarwal, and G.~Qi.
\newblock Stock price prediction via discovering multi-frequency trading patterns.
\newblock In \emph{KDD}, 2017{\natexlab{b}}.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Mi, Zheng, and Yao]{zhang2023}
S.~Zhang, L.~Yang, M.~B. Mi, X.~Zheng, and A.~Yao.
\newblock Improving deep regression with ordinal entropy.
\newblock In \emph{ICLR}, 2023.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Liu, Xiao, Shen, Huang, Yang, Samaras, and Han]{zhang2021codim}
X.~Zhang, Z.~Liu, K.~Xiao, T.~Shen, J.~Huang, W.~Yang, D.~Samaras, and X.~Han.
\newblock Codim: Learning with noisy labels via contrastive semi-supervised learning.
\newblock \emph{arXiv preprint arXiv: 2111.11652}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2022)Zhang, Sun, Gao, Shou, and Wu]{zhang22economics}
Y.~Zhang, H.~Sun, G.~Gao, L.~Shou, and D.~Wu.
\newblock Developing spatio-temporal approach to predict economic dynamics based on online news.
\newblock \emph{Scientific Reports}, 12\penalty0 (16158), 2022.

\bibitem[Zhang and Sabuncu(2018)]{zhang18nips}
Z.~Zhang and M.~Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with noisy labels.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Lin, Liu, Li, Sun, and Zhou]{zhang21b}
Z.~Zhang, Y.~Lin, Z.~Liu, P.~Li, M.~Sun, and J.~Zhou.
\newblock Moefication: Conditional computation of transformer models for efficient inference.
\newblock \emph{arXiv preprint arXiv:2110.01786}, 2021{\natexlab{b}}.

\bibitem[Zhifei et~al.(2017)Zhifei, Yang, and Hairong]{zhifei2017utkface}
Z.~Zhifei, S.~Yang, and Q.~Hairong.
\newblock Age progression regression by conditional adversarial autoencoder.
\newblock In \emph{CVPR}, 2017.

\bibitem[Zhou et~al.(2022)Zhou, Vani, Larochelle, and Courville]{zhou2021fortuitous}
Hattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville.
\newblock Fortuitous forgetting in connectionist networks.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zhou et~al.(2012)Zhou, Xu, Ma, and Tian]{zhou2012onTS}
M.~Zhou, Y.~Xu, L.~Ma, and S.~Tian.
\newblock On the statistical errors of radar location sensor networks with built-in wi-fi gaussian linear fingerprints.
\newblock \emph{Sensors (Basel, Switzerland)}, 12:\penalty0 3605 -- 3626, 2012.

\bibitem[Zhu et~al.(2022)Zhu, Wang, and Liu]{zhu2022beyond}
Z.~Zhu, J.~Wang, and Y.~Liu.
\newblock Beyond images: Label noise transition matrix estimation for tasks with lower-quality features.
\newblock In \emph{ICML}, 2022.

\bibitem[Zong et~al.(2024)Zong, Wang, Xie, and Huang]{zong2024dirichlet}
Chen-Chen Zong, Ye-Wen Wang, Ming-Kun Xie, and Sheng-Jun Huang.
\newblock Dirichlet-based prediction calibration for learning with noisy labels.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 38\penalty0 (15):\penalty0 17254--17262, 2024.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{zoph22}
B.~Zoph, I.~Bello, S.~Kumar, N.~Du, Y.~Huang, J.~Dean, N.~Shazeer, and W.~Fedus.
\newblock Designing effective sparse expert models.
\newblock \emph{arXiv preprint arXiv:2202.08906}, 2022.

\bibitem[Zuo et~al.(2021)Zuo, Liu, Jiao, Kim, Hassan, Zhang, Zaho, and Gao]{zuo21}
S.~Zuo, X.~Liu, J.~Jiao, Y.~J. Kim, H.~Hassan, R.~Zhang, T.~Zaho, and J.~Gao.
\newblock Taming sparsely activated transformers with stochastic experts.
\newblock \emph{CoRR CoRR:2110.04260}, 2021.

\end{thebibliography}
