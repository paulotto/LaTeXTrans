@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      volume={arXiv:2412.19437},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2412.19437}, 
}

@article{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      volume={arXiv:2501.12948},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.12948}, 
}

@article{liu2024enablingweakllmsjudge,
      title={Enabling Weak LLMs to Judge Response Reliability via Meta Ranking}, 
      author={Zijun Liu and Boqun Kou and Peng Li and Ming Yan and Ji Zhang and Fei Huang and Yang Liu},
      year={2024},
      volume={arXiv:2402.12146},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2402.12146}, 
}

@inproceedings{instructgpt,
author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
title = {Training language models to follow instructions with human feedback},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2011},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{bai2022traininghelpfulharmlessassistant,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      volume={arXiv:2204.05862},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2204.05862}, 
}

@article{wang2024selftaughtevaluators,
      title={Self-Taught Evaluators}, 
      author={Tianlu Wang and Ilia Kulikov and Olga Golovneva and Ping Yu and Weizhe Yuan and Jane Dwivedi-Yu and Richard Yuanzhe Pang and Maryam Fazel-Zarandi and Jason Weston and Xian Li},
      year={2024},
      volume={arXiv:2408.02666},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2408.02666}, 
}

@inproceedings{
rafailov2023direct,
title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=HPuSIXJaa9}
}

@inproceedings{
kamoi2024evaluating,
title={Evaluating {LLM}s at Detecting Errors in {LLM} Responses},
author={Ryo Kamoi and Sarkar Snigdha Sarathi Das and Renze Lou and Jihyun Janice Ahn and Yilun Zhao and Xiaoxin Lu and Nan Zhang and Yusen Zhang and Haoran Ranran Zhang and Sujeeth Reddy Vummanthala and Salika Dave and Shaobo Qin and Arman Cohan and Wenpeng Yin and Rui Zhang},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=dnwRScljXr}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{alexandru2025atlaseleneminigeneral,
      title={Atla Selene Mini: A General Purpose Evaluation Model}, 
      author={Andrei Alexandru and Antonia Calvi and Henry Broomfield and Jackson Golden and Kyle Dai and Mathias Leys and Maurice Burger and Max Bartolo and Roman Engeler and Sashank Pisupati and Toby Drane and Young Sun Park},
      year={2025},
      volume={arXiv:2501.17195},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.17195}, 
}

@article{sharma2025constitutionalclassifiersdefendinguniversal,
      title={Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming}, 
      author={Mrinank Sharma and Meg Tong and Jesse Mu and Jerry Wei and Jorrit Kruthoff and Scott Goodfriend and Euan Ong and Alwin Peng and Raj Agarwal and Cem Anil and Amanda Askell and Nathan Bailey and Joe Benton and Emma Bluemke and Samuel R. Bowman and Eric Christiansen and Hoagy Cunningham and Andy Dau and Anjali Gopal and Rob Gilson and Logan Graham and Logan Howard and Nimit Kalra and Taesung Lee and Kevin Lin and Peter Lofgren and Francesco Mosconi and Clare O'Hara and Catherine Olsson and Linda Petrini and Samir Rajani and Nikhil Saxena and Alex Silverstein and Tanya Singh and Theodore Sumers and Leonard Tang and Kevin K. Troy and Constantin Weisser and Ruiqi Zhong and Giulio Zhou and Jan Leike and Jared Kaplan and Ethan Perez},
      year={2025},
      volume={arXiv:2501.18837},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.18837}, 
}

@article{bai2022constitutionalaiharmlessnessai,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      volume={arXiv:2212.08073},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2212.08073}, 
}

@inproceedings{
winata2025metametrics,
title={MetaMetrics: Calibrating Metrics for Generation Tasks Using Human Preferences},
author={Genta Indra Winata and David Anugraha and Lucky Susanto and Garry Kuwanto and Derry Tanti Wijaya},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=slO3xTt4CG}
}

@article{baker2025monitoring,
  title = {Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation},
  author = {Baker, Bowen and Huizinga, Joost and Gao, Leo and Dou, Zehao and Guan, Melody Y. and Madry, Aleksander and Zaremba, Wojciech and Pachocki, Jakub and Farhi, David},
  year = {2025},
  journal={OpenAI Publication},
  url = {https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf}
}

@article{o3-system-card,
  title = {OpenAI o3-mini System Card},
  author = {OpenAI},
  year = {2025},
  journal={OpenAI Publication},
  url = {https://cdn.openai.com/o3-mini-system-card-feb10.pdf}
}

@article{gpt45-system-card,
  title = {OpenAI GPT-4.5 System Card},
  author = {OpenAI},
  year = {2025},
  journal={OpenAI Publication},
  url = {https://cdn.openai.com/gpt-4-5-system-card-2272025.pdf}
}

@article{deep-research-system-card,
  title = {Deep Research System Card},
  author = {OpenAI},
  year = {2025},
  journal={OpenAI Publication},
  url = {https://cdn.openai.com/deep-research-system-card.pdf}
}

@article{lifshitz2025multiagentverificationscalingtesttime,
      title={Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers}, 
      author={Shalev Lifshitz and Sheila A. McIlraith and Yilun Du},
      year={2025},
      volume={arXiv:2502.20379},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2502.20379}, 
}

@inproceedings{
chen2023codet,
title={CodeT:  Code Generation with Generated Tests},
author={Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ktrw68Cmu9c}
}

@inproceedings{
zhou2025rmb,
title={{RMB}: Comprehensively benchmarking reward models in {LLM} alignment},
author={Enyu Zhou and Guodong Zheng and Binghai Wang and Zhiheng Xi and Shihan Dou and Rong Bao and Wei Shen and Limao Xiong and Jessica Fan and Yurong Mou and Rui Zheng and Tao Gui and Qi Zhang and Xuanjing Huang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=kmgrlG9TR0}
}

@inproceedings{
frick2025how,
title={How to Evaluate Reward Models for {RLHF}},
author={Evan Frick and Tianle Li and Connor Chen and Wei-Lin Chiang and Anastasios Nikolas Angelopoulos and Jiantao Jiao and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=cbttLtO94Q}
}

@inproceedings{
zhang2025generative,
title={Generative Verifiers: Reward Modeling as Next-Token Prediction},
author={Lunjun Zhang and Arian Hosseini and Hritik Bansal and Mehran Kazemi and Aviral Kumar and Rishabh Agarwal},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Ccwp4tFEtE}
}

@InProceedings{pmlr-v202-gao23h,
  title = 	 {Scaling Laws for Reward Model Overoptimization},
  author =       {Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {10835--10866},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  url = 	 {https://proceedings.mlr.press/v202/gao23h.html},
  abstract = 	 {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart’s law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed “gold-standard” reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.}
}

@article{an2024fireflyeraihpccosteffectivesoftwarehardware,
      title={Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning}, 
      author={Wei An and Xiao Bi and Guanting Chen and Shanhuang Chen and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Wenjun Gao and Kang Guan and Jianzhong Guo and Yongqiang Guo and Zhe Fu and Ying He and Panpan Huang and Jiashi Li and Wenfeng Liang and Xiaodong Liu and Xin Liu and Yiyuan Liu and Yuxuan Liu and Shanghao Lu and Xuan Lu and Xiaotao Nie and Tian Pei and Junjie Qiu and Hui Qu and Zehui Ren and Zhangli Sha and Xuecheng Su and Xiaowen Sun and Yixuan Tan and Minghui Tang and Shiyu Wang and Yaohui Wang and Yongji Wang and Ziwei Xie and Yiliang Xiong and Yanhong Xu and Shengfeng Ye and Shuiping Yu and Yukun Zha and Liyue Zhang and Haowei Zhang and Mingchuan Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Yuheng Zou},
      year={2024},
      volume={arXiv:2408.14158},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2408.14158}, 
}

@article{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      volume={arXiv:2110.14168},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2110.14168}, 
}

@inproceedings{
frnken2024selfsupervised,
title={Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels},
author={Jan-Philipp Fr{\"a}nken and Eric Zelikman and Rafael Rafailov and Kanishk Gandhi and Tobias Gerstenberg and Noah Goodman},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=UvbpbEhGaw}
}

@inproceedings{10.5555/3495724.3495977,
author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
title = {Learning to summarize from human feedback},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about—summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {253},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{xie2025teachinglanguagemodelscritique,
      title={Teaching Language Models to Critique via Reinforcement Learning}, 
      author={Zhihui Xie and Jie chen and Liyu Chen and Weichao Mao and Jingjing Xu and Lingpeng Kong},
      year={2025},
      volume={arXiv:2502.03492},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2502.03492}, 
}

@article{wang2025critiquefinetuninglearningcritique,
      title={Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate}, 
      author={Yubo Wang and Xiang Yue and Wenhu Chen},
      year={2025},
      volume={arXiv:2501.17703},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.17703}, 
}

@article{yu2025selfgeneratedcritiquesboostreward,
      title={Self-Generated Critiques Boost Reward Modeling for Language Models}, 
      author={Yue Yu and Zhengxing Chen and Aston Zhang and Liang Tan and Chenguang Zhu and Richard Yuanzhe Pang and Yundi Qian and Xuewei Wang and Suchin Gururangan and Chao Zhang and Melanie Kambadur and Dhruv Mahajan and Rui Hou},
      year={2025},
      volume={arXiv:2411.16646},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2411.16646}, 
}

@article{ye2024improvingrewardmodelssynthetic,
      title={Improving Reward Models with Synthetic Critiques}, 
      author={Zihuiwen Ye and Fraser Greenlee-Scott and Max Bartolo and Phil Blunsom and Jon Ander Campos and Matthias Gallé},
      year={2024},
      volume={arXiv:2405.20850},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2405.20850}, 
}

@article{mahan2024generativerewardmodels,
      title={Generative Reward Models}, 
      author={Dakota Mahan and Duy Van Phung and Rafael Rafailov and Chase Blagden and Nathan Lile and Louis Castricato and Jan-Philipp Fränken and Chelsea Finn and Alon Albalak},
      year={2024},
      volume={arXiv:2410.12832},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2410.12832}, 
}

@InProceedings{pmlr-v235-yuan24d,
  title = 	 {Self-Rewarding Language Models},
  author =       {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Li, Xian and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason E},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {57905--57923},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/yuan24d/yuan24d.pdf},
  url = 	 {https://proceedings.mlr.press/v235/yuan24d.html},
  abstract = 	 {We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these reward models require additional human preferences data to further improve.In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training, not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.}
}

@inproceedings{jiang-etal-2023-llm,
    title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
    author = "Jiang, Dongfu  and
      Ren, Xiang  and
      Lin, Bill Yuchen",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.792/",
    doi = "10.18653/v1/2023.acl-long.792",
    pages = "14165--14178",
    abstract = "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap."
}

@misc{kyle2024judgearena,
  title = {Judge Arena: Benchmarking LLMs as Evaluators},
  author = {kyle and Maurice and Engeler, Roman and Bartolo, Max and Fourrier, Clémentine and Drane, Toby and Leys, Mathias and Golden, Jake},
  year = {2024},
  howpublished = {\url{https://huggingface.co/blog/arena-atla}},
  note = {Accessed: 2025-03-11}
}

@article{wang2024directjudgementpreferenceoptimization,
      title={Direct Judgement Preference Optimization}, 
      author={Peifeng Wang and Austin Xu and Yilun Zhou and Caiming Xiong and Shafiq Joty},
      year={2024},
      volume={arXiv:2409.14664},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2409.14664}, 
}

@inproceedings{park-etal-2024-offsetbias,
    title = "{O}ffset{B}ias: Leveraging Debiased Data for Tuning Evaluators",
    author = "Park, Junsoo  and
      Jwa, Seungyeon  and
      Meiying, Ren  and
      Kim, Daeyoung  and
      Choi, Sanghyuk",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.57/",
    doi = "10.18653/v1/2024.findings-emnlp.57",
    pages = "1043--1067",
    abstract = "Employing Large Language Models (LLMs) to assess the quality of generated responses has become a widely adopted evaluation method. Specifically, instruct-tuned models and fine-tuned judge models based on open-source LLMs have been reported. While it is known that judge models are vulnerable to certain biases, such as favoring longer answers regardless of content, the specifics of these biases remain under-explored. In this work, we qualitatively identify six types of biases inherent in various judge models. We propose EvalBiasBench as a meta-evaluation collection of hand-crafted test cases for each bias type. Additionally, we present de-biasing dataset construction methods and the associated preference dataset OffsetBias. Experimental results demonstrate that fine-tuning on our dataset significantly enhances the robustness of judge models against biases and improves performance across most evaluation scenarios. We release our datasets and the fine-tuned judge model to public."
}

@inproceedings{vu-etal-2024-foundational,
    title = "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
    author = "Vu, Tu  and
      Krishna, Kalpesh  and
      Alzubi, Salaheddin  and
      Tar, Chris  and
      Faruqui, Manaal  and
      Sung, Yun-Hsuan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.949/",
    doi = "10.18653/v1/2024.emnlp-main.949",
    pages = "17086--17105",
    abstract = "As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8{\%} accuracy, surpassing GPT-4-0125 (85.9{\%}) and GPT-4o (84.7{\%}). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25{\texttimes}fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark."
}

@inproceedings{
li2024generative,
title={Generative Judge for Evaluating Alignment},
author={Junlong Li and Shichao Sun and Weizhe Yuan and Run-Ze Fan and hai zhao and Pengfei Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gtkFw6sZGS}
}

@article{mcaleese2024llmcriticshelpcatch,
      title={LLM Critics Help Catch LLM Bugs}, 
      author={Nat McAleese and Rai Michael Pokorny and Juan Felipe Ceron Uribe and Evgenia Nitishinskaya and Maja Trebacz and Jan Leike},
      year={2024},
      volume={arXiv:2407.00215},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2407.00215}, 
}

@article{ankner2024critiqueoutloudrewardmodels,
      title={Critique-out-Loud Reward Models}, 
      author={Zachary Ankner and Mansheej Paul and Brandon Cui and Jonathan D. Chang and Prithviraj Ammanabrolu},
      year={2024},
      volume={arXiv:2408.11791},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2408.11791}, 
}

@inproceedings{
ye2025learning,
title={Learning {LLM}-as-a-Judge for Preference Alignment},
author={Ziyi Ye and Xiangsheng Li and Qiuchi Li and Qingyao Ai and Yujia Zhou and Wei Shen and Dong Yan and Yiqun LIU},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=HZVIQE1MsJ}
}

@article{yu2025improvellmasajudgeabilitygeneral,
      title={Improve LLM-as-a-Judge Ability as a General Ability}, 
      author={Jiachen Yu and Shaoning Sun and Xiaohui Hu and Jiaxu Yan and Kaidong Yu and Xuelong Li},
      year={2025},
      volume={arXiv:2502.11689},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2502.11689}, 
}

@article{cao2024compassjudger1allinonejudgemodel,
      title={CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution}, 
      author={Maosong Cao and Alexander Lam and Haodong Duan and Hongwei Liu and Songyang Zhang and Kai Chen},
      year={2024},
      volume={arXiv:2410.16256},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2410.16256}, 
}

@article{shao2024deepseekmathpushinglimitsmathematical,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      volume={arXiv:2402.0330},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2402.03300}, 
}

@inproceedings{kim-etal-2024-prometheus,
    title = "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    author = "Kim, Seungone  and
      Suk, Juyoung  and
      Longpre, Shayne  and
      Lin, Bill Yuchen  and
      Shin, Jamin  and
      Welleck, Sean  and
      Neubig, Graham  and
      Lee, Moontae  and
      Lee, Kyungjae  and
      Seo, Minjoon",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.248/",
    doi = "10.18653/v1/2024.emnlp-main.248",
    pages = "4334--4353",
    abstract = "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available."
}

@inproceedings{sun-etal-2024-critique,
    title = "The Critique of Critique",
    author = "Sun, Shichao  and
      Li, Junlong  and
      Yuan, Weizhe  and
      Yuan, Ruifeng  and
      Li, Wenjie  and
      Liu, Pengfei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.538/",
    doi = "10.18653/v1/2024.findings-acl.538",
    pages = "9077--9096"
}

@inproceedings{hu-etal-2024-themis,
    title = "Themis: A Reference-free {NLG} Evaluation Language Model with Flexibility and Interpretability",
    author = "Hu, Xinyu  and
      Lin, Li  and
      Gao, Mingqi  and
      Yin, Xunjian  and
      Wan, Xiaojun",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.891/",
    doi = "10.18653/v1/2024.emnlp-main.891",
    pages = "15924--15951",
    abstract = "The evaluation of natural language generation (NLG) tasks is a significant and longstanding research area. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics. However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus **NLG-Eval** with annotations from both human and GPT-4 to alleviate the lack of relevant data in this field. Furthermore, we propose **Themis**, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency verification and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4."
}

@inproceedings{
wu2025inference,
title={Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for {LLM} Problem-Solving},
author={Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=VNckp7JEHn}
}

@inproceedings{
snell2025scaling,
title={Scaling {LLM} Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning},
author={Charlie Victor Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=4FWAwZtd2n}
}

@article{brown2024largelanguagemonkeysscaling,
      title={Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}, 
      author={Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher Ré and Azalia Mirhoseini},
      year={2024},
      volume={arXiv:2407.21787},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2407.21787}, 
}

@article{liu2025pairjudgermperformbestofn,
      title={PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament}, 
      author={Yantao Liu and Zijun Yao and Rui Min and Yixin Cao and Lei Hou and Juanzi Li},
      year={2025},
      volume={arXiv:2501.13007},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.13007}, 
}

@article{peng2025agenticrewardmodelingintegrating,
      title={Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems}, 
      author={Hao Peng and Yunjia Qi and Xiaozhi Wang and Zijun Yao and Bin Xu and Lei Hou and Juanzi Li},
      year={2025},
      volume={arXiv:2502.19328},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2502.19328}, 
}

@article{dou2024metarmshifteddistributionsalignment,
      title={MetaRM: Shifted Distributions Alignment via Meta-Learning}, 
      author={Shihan Dou and Yan Liu and Enyu Zhou and Tianlong Li and Haoxiang Jia and Limao Xiong and Xin Zhao and Junjie Ye and Rui Zheng and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2024},
      volume={arXiv:2405.00438},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2405.00438}, 
}

@article{wu2024metarewardinglanguagemodelsselfimproving,
      title={Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge}, 
      author={Tianhao Wu and Weizhe Yuan and Olga Golovneva and Jing Xu and Yuandong Tian and Jiantao Jiao and Jason Weston and Sainbayar Sukhbaatar},
      year={2024},
      volume={arXiv:2407.19594},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2407.19594}, 
}

@inproceedings{
chow2025inferenceaware,
title={Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models},
author={Yinlam Chow and Guy Tennenholtz and Izzeddin Gur and Vincent Zhuang and Bo Dai and Aviral Kumar and Rishabh Agarwal and Sridhar Thiagarajan and Craig Boutilier and Aleksandra Faust},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=77gQUdQhE7}
}

@inproceedings{
lightman2024lets,
title={Let's Verify Step by Step},
author={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=v8L0pN6EOi}
}

@inproceedings{chatbot-arena,
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2020},
numpages = {29},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{
wang2024helpsteer,
title={HelpSteer 2: Open-source dataset for training top-performing reward models},
author={Zhilin Wang and Yi Dong and Olivier Delalleau and Jiaqi Zeng and Gerald Shen and Daniel Egert and Jimmy J. Zhang and Makesh Narsimhan Sreedhar and Oleksii Kuchaiev},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=PvVKUFhaNy}
}

@inproceedings{wang-etal-2024-interpretable,
    title = "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
    author = "Wang, Haoxiang  and
      Xiong, Wei  and
      Xie, Tengyang  and
      Zhao, Han  and
      Zhang, Tong",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.620/",
    doi = "10.18653/v1/2024.findings-emnlp.620",
    pages = "10582--10592",
    abstract = "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, it is desirable for them to be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model."
}

@article{0627eaad-0ecb-353b-9c3d-81e29de3658f,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2332613},
 author = {M. G. Kendall and B. Babington Smith},
 journal = {Biometrika},
 number = {3/4},
 pages = {324--345},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Method of Paired Comparisons},
 urldate = {2025-03-18},
 volume = {31},
 year = {1940}
}

@article{liu2024skyworkrewardbagtricksreward,
      title={Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs}, 
      author={Chris Yuhao Liu and Liang Zeng and Jiacai Liu and Rui Yan and Jujie He and Chaojie Wang and Shuicheng Yan and Yang Liu and Yahui Zhou},
      year={2024},
      volume={arXiv:2410.18451},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2410.18451}, 
}

@inproceedings{
sun2023principledriven,
title={Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
author={Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David Daniel Cox and Yiming Yang and Chuang Gan},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=p40XRfBX96}
}

@article{cai2024internlm2technicalreport,
      title={InternLM2 Technical Report}, 
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      volume={arXiv:2403.17297},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2403.17297}, 
}

@inproceedings{
wang2025helpsteerpreference,
title={HelpSteer2-Preference: Complementing Ratings with Preferences},
author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=MnfHxPP5gs}
}

@inproceedings{wang-etal-2024-math,
    title = "Math-Shepherd: Verify and Reinforce {LLM}s Step-by-step without Human Annotations",
    author = "Wang, Peiyi  and
      Li, Lei  and
      Shao, Zhihong  and
      Xu, Runxin  and
      Dai, Damai  and
      Li, Yifei  and
      Chen, Deli  and
      Wu, Yu  and
      Sui, Zhifang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.510/",
    doi = "10.18653/v1/2024.acl-long.510",
    pages = "9426--9439",
    abstract = "In this paper, we present an innovative process-oriented math process reward model called Math-shepherd, which assigns a reward score to each step of math problem solutions. The training of Math-shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-shepherd in two scenarios: 1) $\textit{Verification}$: Math-shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) $\textit{Reinforcement Learning (RL)}$: Math-shepherd is employed to reinforce LLMs.With Math-shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, process RL with Math-shepherd significantly enhances Mistral-7B (77.9{\%}$\to$84.1{\%} on GSM8K and 28.6{\%}$\to$33.0{\%} on MATH).The accuracy can be further improved to 89.1{\%} and 43.5{\%} on two benchmarks with verification of Math-shepherd.We believe that automatic process supervision holds significant potential for the future evolution of LLMs."
}

@article{openai2024openaio1card,
      title={OpenAI o1 System Card}, 
      author={OpenAI},
      year={2024},
      volume={arXiv:2412.16720},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2412.16720}, 
}

@article{zhang2025lessonsdevelopingprocessreward,
      title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, 
      author={Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin},
      year={2025},
      volume={arXiv:2501.07301},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2501.07301}, 
}

@inproceedings{
xu2025magpie,
title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned {LLM}s with Nothing},
author={Zhangchen Xu and Fengqing Jiang and Luyao Niu and Yuntian Deng and Radha Poovendran and Yejin Choi and Bill Yuchen Lin},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Pnk7vMbznK}
}

@inproceedings{10.1145/3701551.3703583,
author = {Guo, Fang and Li, Wenyu and Zhuang, Honglei and Luo, Yun and Li, Yafu and Yan, Le and Zhu, Qi and Zhang, Yue},
title = {MCRanker: Generating Diverse Criteria On-the-Fly to Improve Pointwise LLM Rankers},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3703583},
doi = {10.1145/3701551.3703583},
abstract = {The most recent pointwise Large Language Model (LLM) rankers have achieved remarkable ranking results. However, these rankers are hindered by two major drawbacks: (1) they fail to follow a standardized comparison guidance during the ranking process, and (2) they struggle with comprehensive considerations when dealing with diverse semantics of the query and complicated info in the passages. To address these shortcomings, we propose to build a zero-shot pointwise ranker that first recruits a virtual annotation team to generate query-based criteria from various perspectives and then uses these criteria to conduct an ensemble passage evaluation. Additionally, we are among the first to explore how criteria can be generated automatically and used in text ranking tasks. Our method, tested on eight datasets from the BEIR benchmark, demonstrates that incorporating this multi-perspective criteria ensemble approach significantly enhanced the performance of pointwise LLM rankers.},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {944–953},
numpages = {10},
keywords = {agent, llm ranking, pointwise ranking, text ranking},
location = {Hannover, Germany},
series = {WSDM '25}
}

@inproceedings{arabzadeh-etal-2024-assessing,
    title = "Assessing and Verifying Task Utility in {LLM}-Powered Applications",
    author = "Arabzadeh, Negar  and
      Huo, Siqing  and
      Mehta, Nikhil  and
      Wu, Qingyun  and
      Wang, Chi  and
      Awadallah, Ahmed Hassan  and
      Clarke, Charles L. A.  and
      Kiseleva, Julia",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1219/",
    doi = "10.18653/v1/2024.emnlp-main.1219",
    pages = "21868--21888",
    abstract = "The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application`s functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://github.com/Narabzad/AgentEval"
}

@article{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team},
      year={2024},
      volume={arXiv:2408.0011},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2408.00118}, 
}

@inproceedings{
li2024toolaugmented,
title={Tool-Augmented Reward Modeling},
author={Lei Li and Yekun Chai and Shuohuan Wang and Yu Sun and Hao Tian and Ningyu Zhang and Hua Wu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=d94x0gWTUX}
}

@article{he2025enhancinglanguagemultiagentlearning,
      title={Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization}, 
      author={Zhitao He and Zijun Liu and Peng Li and May Fung and Ming Yan and Ji Zhang and Fei Huang and Yang Liu},
      year={2025},
      volume={arXiv:2502.14496},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2502.14496}, 
}

@article{lambert2024rewardbenchevaluatingrewardmodels,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      volume={arXiv:2403.13787},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2403.13787}, 
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Mathematical Problem Solving With the {MATH} Dataset},
author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=7Bywt2mQsCe}
}

@misc{aime_1983_2024,
  author = {Hemish Veeraboina},
  title = {AIME Problem Set 1983-2024},
  year = {2023},
  publisher = {Kaggle},
  url = {https://www.kaggle.com/datasets/hemishveeraboina/aime-problem-set-1983-2024}
}

@inproceedings{
jimenez2024swebench,
title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=VTF8yNQM66}
}

@inproceedings{
zhuo2025bigcodebench,
title={BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions},
author={Terry Yue Zhuo and Vu Minh Chien and Jenny Chim and Han Hu and Wenhao Yu and Ratnadira Widyasari and Imam Nur Bani Yusuf and Haolan Zhan and Junda He and Indraneil Paul and Simon Brunner and Chen GONG and James Hoang and Armel Randy Zebaze and Xiaoheng Hong and Wen-Ding Li and Jean Kaddour and Ming Xu and Zhihan Zhang and Prateek Yadav and Naman Jain and Alex Gu and Zhoujun Cheng and Jiawei Liu and Qian Liu and Zijian Wang and David Lo and Binyuan Hui and Niklas Muennighoff and Daniel Fried and Xiaoning Du and Harm de Vries and Leandro Von Werra},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=YrycTjllL0}
}

@inproceedings{
yao2022webshop,
title={WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
author={Shunyu Yao and Howard Chen and John Yang and Karthik R Narasimhan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=R9KnuFlvnU}
}

@inproceedings{
xie2024osworld,
title={{OSW}orld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=tN61DTr4Ed}
}

@article{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI},
      year={2024},
      volume={arXiv:2405.04434},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2405.04434}, 
}

@inproceedings{
mu2024rule,
title={Rule Based Rewards for Language Model Safety},
author={Tong Mu and Alec Helyar and Johannes Heidecke and Joshua Achiam and Andrea Vallone and Ian D Kivlichan and Molly Lin and Alex Beutel and John Schulman and Lilian Weng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=QVtwpT5Dmg}
}

@article{glaese2022improvingalignmentdialogueagents,
      title={Improving alignment of dialogue agents via targeted human judgements}, 
      author={Amelia Glaese and Nat McAleese and Maja Trębacz and John Aslanides and Vlad Firoiu and Timo Ewalds and Maribeth Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and Jonathan Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and Abigail See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Soňa Mokrá and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William Isaac and John Mellor and Demis Hassabis and Koray Kavukcuoglu and Lisa Anne Hendricks and Geoffrey Irving},
      year={2022},
      volume={arXiv:2209.14375},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2209.14375}, 
}


@InProceedings{pmlr-v235-cui24f,
  title = 	 {{ULTRAFEEDBACK}: Boosting Language Models with Scaled {AI} Feedback},
  author =       {Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {9722--9744},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/cui24f/cui24f.pdf},
  url = 	 {https://proceedings.mlr.press/v235/cui24f.html},
  abstract = 	 {Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present UltraFeedback, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon UltraFeedback, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  month = {5},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@inproceedings{
zeng2024evaluating,
title={Evaluating Large Language Models at Evaluating Instruction Following},
author={Zhiyuan Zeng and Jiatong Yu and Tianyu Gao and Yu Meng and Tanya Goyal and Danqi Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tr0KidwPLc}
}

@inproceedings{
muennighoff2024octopack,
title={OctoPack: Instruction Tuning Code Large Language Models},
author={Niklas Muennighoff and Qian Liu and Armel Randy Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro Von Werra and Shayne Longpre},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mw1PWNSWZP}
}

@inproceedings{rottger-etal-2024-xstest,
    title = "{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
    author = {R{\"o}ttger, Paul  and
      Kirk, Hannah  and
      Vidgen, Bertie  and
      Attanasio, Giuseppe  and
      Bianchi, Federico  and
      Hovy, Dirk},
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.301/",
    doi = "10.18653/v1/2024.naacl-long.301",
    pages = "5377--5400",
    abstract = "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest`s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models."
}

@inproceedings{wang-etal-2024-answer,
    title = "Do-Not-Answer: Evaluating Safeguards in {LLM}s",
    author = "Wang, Yuxia  and
      Li, Haonan  and
      Han, Xudong  and
      Nakov, Preslav  and
      Baldwin, Timothy",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.61/",
    pages = "896--911",
    abstract = "With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of {\textquotedblleft}dangerous capabilities{\textquotedblright} in order to responsibly deploy LLMs. Here we aim to facilitate this process. In particular, we collect an open-source dataset to evaluate the safeguards in LLMs, to facilitate the deployment of safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We assess the responses of six popular LLMs to these instructions, and we find that simple BERT-style classifiers can achieve results that are comparable to GPT-4 on automatic safety evaluation. Our data and code are available at https://github.com/Libr-AI/do-not-answer"
}

@article{askell2021generallanguageassistantlaboratory,
      title={A General Language Assistant as a Laboratory for Alignment}, 
      author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2021},
      volume={arXiv:2112.00861},
      journal={Computing Research Repository},
      url={https://arxiv.org/abs/2112.00861}, 
}


@InProceedings{pmlr-v162-ethayarajh22a,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ethayarajh22a.html},
  abstract = 	 {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty—w.r.t. a model $\mathcal{V}$—as the lack of $\mathcal{V}$-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\mathcal{V}$. We further introduce pointwise $\mathcal{V}$-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\mathcal{V}$-usable information and PVI also permit the converse: for a given model $\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.}
}

