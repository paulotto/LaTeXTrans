\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Awadalla, Awadallah, Awan, Bach, Bahree, Bakhtiari, Bao, Behl, et~al.]{abdin2024phi3}
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar~Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}, 2024.

\bibitem[Abnar et~al.(2025)Abnar, Shah, Busbridge, Ali, Susskind, and Thilak]{abnar2025parameters}
Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed~Elnouby Ali, Josh Susskind, and Vimal Thilak.
\newblock Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models.
\newblock \emph{arXiv preprint arXiv:2501.12370}, 2025.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal, Okhonko, Joshi, Ghosh, Lewis, et~al.]{aghajanyan2022cm3}
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et~al.
\newblock Cm3: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}, 2022.

\bibitem[Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer]{aghajanyan2023scalingmm}
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer.
\newblock Scaling laws for generative mixed-modal language models.
\newblock In \emph{International Conference on Machine Learning}, pages 265--279. PMLR, 2023.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Bao et~al.(2021)Bao, Wang, Dong, Liu, Mohammed, Aggarwal, Som, and Wei]{bao2021vlmo}
Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais~Khan Mohammed, Kriti Aggarwal, Subhojit Som, and Furu Wei.
\newblock Vlmo: Unified vision-language pre-training with mixture-of-modality-experts.
\newblock \emph{arXiv preprint arXiv:2111.02358}, 2021.

\bibitem[Bavishi et~al.(2023)Bavishi, Elsen, Hawthorne, Nye, Odena, Somani, and Ta\c{s}\i{}rlar]{fuyu8b}
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa\u{g}nak Ta\c{s}\i{}rlar.
\newblock Introducing our multimodal models, 2023.

\bibitem[Beyer et~al.(2024)Beyer, Steiner, Pinto, Kolesnikov, Wang, Salz, Neumann, Alabdulmohsin, Tschannen, Bugliarello, et~al.]{beyer2024paligemma}
Lucas Beyer, Andreas Steiner, Andr{\'e}~Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et~al.
\newblock Paligemma: A versatile 3b vlm for transfer.
\newblock \emph{arXiv preprint arXiv:2407.07726}, 2024.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Byeon et~al.(2022)Byeon, Park, Kim, Lee, Baek, and Kim]{kakaobrain2022coyo700m}
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
\newblock Coyo-700m: Image-text pair dataset.
\newblock \url{https://github.com/kakaobrain/coyo-dataset}, 2022.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Guo, Sun, Shao, Yuan, Lin, and Zhang]{chen2024eve}
Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, and Dongyu Zhang.
\newblock Eve: Efficient vision-language pre-training with masked prediction and modality-aware moe.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 1110--1119, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Wang, Chen, Wu, Liu, Chen, Li, Kanda, Yoshioka, Xiao, et~al.]{chen2022wavlm}
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et~al.
\newblock Wavlm: Large-scale self-supervised pre-training for full stack speech processing.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing}, 16\penalty0 (6):\penalty0 1505--1518, 2022.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Wu, Wang, Su, Chen, Xing, Zhong, Zhang, Zhu, Lu, et~al.]{chen2024internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24185--24198, 2024{\natexlab{b}}.

\bibitem[Cheng et~al.(2024)Cheng, Chen, Li, Gong, Tang, and Song]{scalingprotein}
Xingyi Cheng, Bo Chen, Pan Li, Jing Gong, Jie Tang, and Le Song.
\newblock Training compute-optimal protein language models.
\newblock \emph{bioRxiv}, 2024.

\bibitem[Clark et~al.(2022)Clark, de~Las~Casas, Guy, Mensch, Paganini, Hoffmann, Damoc, Hechtman, Cai, Borgeaud, et~al.]{clark2022unifiedscalingmoe}
Aidan Clark, Diego de Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock In \emph{International conference on machine learning}, pages 4057--4086. PMLR, 2022.

\bibitem[Dai et~al.(2024)Dai, Lee, Wang, Yang, Liu, Barker, Rintamaki, Shoeybi, Catanzaro, and Ping]{dai2024nvlm}
Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.
\newblock Nvlm: Open frontier-class multimodal llms.
\newblock \emph{arXiv preprint arXiv:2409.11402}, 2024.

\bibitem[D{\'e}fossez et~al.(2024)D{\'e}fossez, Mazar{\'e}, Orsini, Royer, P{\'e}rez, J{\'e}gou, Grave, and Zeghidour]{defossez2024moshi}
Alexandre D{\'e}fossez, Laurent Mazar{\'e}, Manu Orsini, Am{\'e}lie Royer, Patrick P{\'e}rez, Herv{\'e} J{\'e}gou, Edouard Grave, and Neil Zeghidour.
\newblock Moshi: a speech-text foundation model for real-time dialogue.
\newblock \emph{arXiv preprint arXiv:2410.00037}, 2024.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pages 7480--7512. PMLR, 2023.

\bibitem[Diao et~al.(2024)Diao, Cui, Li, Wang, Lu, and Wang]{diao2024unveiling}
Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang.
\newblock Unveiling encoder-free vision-language models.
\newblock \emph{arXiv preprint arXiv:2406.11832}, 2024.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama3}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Elizalde et~al.(2023)Elizalde, Deshmukh, Al~Ismail, and Wang]{elizalde2023clap}
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al~Ismail, and Huaming Wang.
\newblock Clap learning audio concepts from natural language supervision.
\newblock In \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2023.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{vqgan}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 12873--12883, 2021.

\bibitem[Fang et~al.(2023)Fang, Jose, Jain, Schmidt, Toshev, and Shankar]{fang2023data}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
\newblock Data filtering networks.
\newblock \emph{arXiv preprint arXiv:2309.17425}, 2023.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (120):\penalty0 1--39, 2022.

\bibitem[Fini et~al.(2024)Fini, Shukor, Li, Dufter, Klein, Haldimann, Aitharaju, da~Costa, BÃ©thune, Gan, Toshev, Eichner, Nabi, Yang, Susskind, and El-Nouby]{fini2024multimodalaimv2}
Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme~Turrisi da Costa, Louis BÃ©thune, Zhe Gan, Alexander~T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua~M. Susskind, and Alaaeldin El-Nouby.
\newblock Multimodal autoregressive pre-training of large vision encoders, 2024.

\bibitem[Gale et~al.(2023)Gale, Narayanan, Young, and Zaharia]{gale2023megablocks}
Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia.
\newblock Megablocks: Efficient sparse training with mixture-of-experts.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0 288--304, 2023.

\bibitem[H{\"a}gele et~al.(2024)H{\"a}gele, Bakouch, Kosson, Allal, Von~Werra, and Jaggi]{hagele2024scaling}
Alexander H{\"a}gele, Elie Bakouch, Atli Kosson, Loubna~Ben Allal, Leandro Von~Werra, and Martin Jaggi.
\newblock Scaling laws and compute-optimal training beyond fixed training durations.
\newblock \emph{arXiv preprint arXiv:2405.18392}, 2024.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{Proceedings of the 36th International Conference on Neural Information Processing Systems}, pages 30016--30030, 2022.

\bibitem[Hsu et~al.(2021)Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and Mohamed]{hsu2021hubert}
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung~Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.
\newblock Hubert: Self-supervised speech representation learning by masked prediction of hidden units.
\newblock \emph{IEEE/ACM transactions on audio, speech, and language processing}, 29:\penalty0 3451--3460, 2021.

\bibitem[Huang et~al.(2022)Huang, Xu, Li, Baevski, Auli, Galuba, Metze, and Feichtenhofer]{huang2022masked}
Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer.
\newblock Masked autoencoders that listen.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28708--28720, 2022.

\bibitem[Huang et~al.(2023)Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui, Mohammed, Patra, et~al.]{kosmoshuang2023language}
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Barun Patra, et~al.
\newblock Language is not all you need: Aligning perception with language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 72096--72109, 2023.

\bibitem[Huber(1992)]{Huber1992}
Peter~J. Huber.
\newblock \emph{Robust Estimation of a Location Parameter}, pages 492--518.
\newblock Springer New York, New York, NY, 1992.

\bibitem[Hurst et~al.(2024)Hurst, Lerer, Goucher, Perelman, Ramesh, Clark, Ostrow, Welihinda, Hayes, Radford, et~al.]{hurst2024gpt4o}
Aaron Hurst, Adam Lerer, Adam~P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et~al.
\newblock Gpt-4o system card.
\newblock \emph{arXiv preprint arXiv:2410.21276}, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Koh et~al.(2023)Koh, Salakhutdinov, and Fried]{koh2023grounding}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
\newblock Grounding language models to images for multimodal inputs and outputs.
\newblock In \emph{International Conference on Machine Learning}, pages 17283--17300. PMLR, 2023.

\bibitem[Kong et~al.(2024)Kong, Goel, Badlani, Ping, Valle, and Catanzaro]{kong2024audioflam}
Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro.
\newblock Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities.
\newblock In \emph{International Conference on Machine Learning}, pages 25125--25148. PMLR, 2024.

\bibitem[Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro, Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski, et~al.]{krajewski2024scalingmoe}
Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi{\'o}ro, Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr{\'o}l, Tomasz Odrzyg{\'o}{\'z}d{\'z}, Piotr Sankowski, et~al.
\newblock Scaling laws for fine-grained mixture of experts.
\newblock \emph{arXiv preprint arXiv:2402.07871}, 2024.

\bibitem[Lauren{\c{c}}on et~al.(2024{\natexlab{a}})Lauren{\c{c}}on, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, Kiela, et~al.]{laurenccon2024obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et~al.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Lauren{\c{c}}on et~al.(2024{\natexlab{b}})Lauren{\c{c}}on, Tronchon, Cord, and Sanh]{laurenccon2024mattersidefics2}
Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock \emph{arXiv preprint arXiv:2405.02246}, 2024{\natexlab{b}}.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and Zettlemoyer]{lewis2021base}
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock In \emph{International Conference on Machine Learning}, pages 6265--6274. PMLR, 2021.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Liu, Wu, Wang, Shen, Qu, Niu, Wang, Chen, and Li]{li2024aria}
Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li.
\newblock Aria: An open multimodal native mixture-of-experts model.
\newblock \emph{arXiv preprint arXiv:2410.05993}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora, et~al.]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}, 2024{\natexlab{b}}.

\bibitem[Lin et~al.(2024{\natexlab{a}})Lin, Tang, Ye, Cui, Zhu, Jin, Zhang, Ning, and Yuan]{lin2024moe}
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.
\newblock Moe-llava: Mixture of experts for large vision-language models.
\newblock \emph{arXiv preprint arXiv:2401.15947}, 2024{\natexlab{a}}.

\bibitem[Lin et~al.(2024{\natexlab{b}})Lin, Yin, Ping, Molchanov, Shoeybi, and Han]{lin2024vila}
Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han.
\newblock Vila: On pre-training for visual language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26689--26699, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan, et~al.]{liu2024deepseekv3}
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et~al.
\newblock Deepseek-v3 technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Li, Li, and Lee]{liu2024improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26296--26306, 2024{\natexlab{b}}.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2022)Lu, Clark, Zellers, Mottaghi, and Kembhavi]{lu2022unified}
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
\newblock Unified-io: A unified model for vision, language, and multi-modal tasks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[McKinzie et~al.(2025)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Belyi, et~al.]{mckinzie2025mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et~al.
\newblock Mm1: methods, analysis and insights from multimodal llm pre-training.
\newblock In \emph{European Conference on Computer Vision}, pages 304--323. Springer, 2025.

\bibitem[Merullo et~al.(2023)Merullo, Castricato, Eickhoff, and Pavlick]{merullo2023linearly}
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.
\newblock Linearly mapping from image to text space.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Mizrahi et~al.(2023)Mizrahi, Bachmann, Kar, Yeo, Gao, Dehghan, and Zamir]{mizrahi20234m}
David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir.
\newblock 4m: Massively multimodal masked modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 58363--58408, 2023.

\bibitem[Moon et~al.(2024)Moon, Madotto, Lin, Nagarajan, Smith, Jain, Yeh, Murugesan, Heidari, Liu, et~al.]{moon2024anymal}
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et~al.
\newblock Anymal: An efficient and scalable any-modality augmented language model.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pages 1314--1332, 2024.

\bibitem[Mustafa et~al.(2022)Mustafa, Riquelme, Puigcerver, Jenatton, and Houlsby]{mustafa2022multimodal}
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby.
\newblock Multimodal contrastive learning with limoe: the language-image mixture of experts.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9564--9576, 2022.

\bibitem[Nocedal(1980)]{lbfgs}
Jorge Nocedal.
\newblock Updating quasi newton matrices with limited storage.
\newblock \emph{Mathematics of Computation}, 35\penalty0 (151):\penalty0 951--958, 1980.

\bibitem[Oquab et~al.(2023)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov, Fernandez, Haziza, Massa, El-Nouby, et~al.]{oquab2023dinov2}
Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et~al.
\newblock Dinov2: Learning robust visual features without supervision.
\newblock \emph{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem[Pearce et~al.(2024)Pearce, Rashid, Bignell, Georgescu, Devlin, and Hofmann]{pearce2024scaling}
Tim Pearce, Tabish Rashid, Dave Bignell, Raluca Georgescu, Sam Devlin, and Katja Hofmann.
\newblock Scaling laws for pre-training agents and world models.
\newblock \emph{arXiv preprint arXiv:2411.04434}, 2024.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Rajasegaran et~al.(2025)Rajasegaran, Radosavovic, Ravishankar, Gandelsman, Feichtenhofer, and Malik]{rajasegaran2025empirical}
Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, and Jitendra Malik.
\newblock An empirical study of autoregressive pre-training from videos.
\newblock \emph{arXiv preprint arXiv:2501.05453}, 2025.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Shen et~al.(2023)Shen, Yao, Li, Darrell, Keutzer, and He]{shen2023scaling}
Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He.
\newblock Scaling vision-language models with sparse mixture of experts.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Shukor and Cord(2024)]{shukor2024implicit}
Mustafa Shukor and Matthieu Cord.
\newblock Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs.
\newblock \emph{arXiv preprint arXiv:2405.16700}, 2024.

\bibitem[Shukor et~al.(2023{\natexlab{a}})Shukor, Dancette, and Cord]{shukor2023epalm}
Mustafa Shukor, Corentin Dancette, and Matthieu Cord.
\newblock ep-alm: Efficient perceptual augmentation of language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 22056--22069, 2023{\natexlab{a}}.

\bibitem[Shukor et~al.(2023{\natexlab{b}})Shukor, Dancette, Rame, and Cord]{shukor2023unival}
Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord.
\newblock Unival: Unified model for image, video, audio and language tasks.
\newblock \emph{Transactions on Machine Learning Research Journal}, 2023{\natexlab{b}}.

\bibitem[Sun et~al.(2024)Sun, Chen, Huang, Xie, Zhu, Zhang, Li, Yang, Han, Shu, et~al.]{sun2024hunyuan}
Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et~al.
\newblock Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent.
\newblock \emph{arXiv preprint arXiv:2411.02265}, 2024.

\bibitem[Team(2024)]{team2024chameleon}
Chameleon Team.
\newblock Chameleon: Mixed-modal early-fusion foundation models.
\newblock \emph{arXiv preprint arXiv:2405.09818}, 2024.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, Katie Millican, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals, and Hill]{tsimpoukelli2021multimodalfrozen}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 200--212, 2021.

\bibitem[Vallaeys et~al.(2024)Vallaeys, Shukor, Cord, and Verbeek]{vallaeys2024improveddepalm}
Th{\'e}ophane Vallaeys, Mustafa Shukor, Matthieu Cord, and Jakob Verbeek.
\newblock Improved baselines for data-efficient perceptual augmentation of llms.
\newblock \emph{arXiv preprint arXiv:2403.13499}, 2024.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and kavukcuoglu]{vqvae}
Aaron van~den Oord, Oriol Vinyals, and koray kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2017.

\bibitem[Vaswani(2017)]{vaswani2017attention}
A Vaswani.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yang, Men, Lin, Bai, Li, Ma, Zhou, Zhou, and Yang]{wang2022ofa}
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.
\newblock In \emph{International conference on machine learning}, pages 23318--23340. PMLR, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, et~al.]{wang2024qwen2}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et~al.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Li, He, Zhang, and Wang]{wangscalingmoe}
Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min Zhang, and Jingang Wang.
\newblock Scaling laws across model architectures: A comparative analysis of dense and {M}o{E} models in large language models.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 5583--5595, Miami, Florida, USA, 2024{\natexlab{b}}. Association for Computational Linguistics.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal, Mohammed, Singhal, Som, et~al.]{wang2022image}
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais~Khan Mohammed, Saksham Singhal, Subhojit Som, et~al.
\newblock Image as a foreign language: Beit pretraining for all vision and vision-language tasks.
\newblock \emph{arXiv preprint arXiv:2208.10442}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Zhang, Luo, Sun, Cui, Wang, Zhang, Wang, Li, Yu, et~al.]{wang2024emu3}
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et~al.
\newblock Emu3: Next-token prediction is all you need.
\newblock \emph{arXiv preprint arXiv:2409.18869}, 2024{\natexlab{c}}.

\bibitem[Wei et~al.(2024)Wei, Zhu, Zhao, Cheng, Li, L{\"u}, Cheng, Zhang, Zhang, Zeng, et~al.]{wei2024skywork}
Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei L{\"u}, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, et~al.
\newblock Skywork-moe: A deep dive into training techniques for mixture-of-experts language models.
\newblock \emph{arXiv preprint arXiv:2406.06563}, 2024.

\bibitem[Xue et~al.(2024)Xue, Shu, Awadalla, Wang, Yan, Purushwalkam, Zhou, Prabhu, Dai, Ryoo, et~al.]{xue2024xgenblip3}
Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael~S Ryoo, et~al.
\newblock xgen-mm (blip-3): A family of open large multimodal models.
\newblock \emph{arXiv preprint arXiv:2408.08872}, 2024.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and Wu]{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{zhai2023sigmoidsiglip}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11975--11986, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Li, and Bing]{zhang2023videollama}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 543--553, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Gao, Gan, Dufter, Wenzel, Huang, Shah, Du, Zhang, Li, et~al.]{zhang2024mm1_5}
Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et~al.
\newblock Mm1. 5: Methods, analysis \& insights from multimodal llm fine-tuning.
\newblock \emph{arXiv preprint arXiv:2409.20566}, 2024.

\bibitem[Zhao et~al.(2023)Zhao, Gu, Varma, Luo, Huang, Xu, Wright, Shojanazeri, Ott, Shleifer, et~al.]{zhao2023pytorch}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et~al.
\newblock Pytorch fsdp: experiences on scaling fully sharded data parallel.
\newblock \emph{arXiv preprint arXiv:2304.11277}, 2023.

\bibitem[Zhu et~al.(2024)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2024minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Mini{GPT}-4: Enhancing vision-language understanding with advanced large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{zoph2022st}
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus.
\newblock St-moe: Designing stable and transferable sparse expert models.
\newblock \emph{arXiv preprint arXiv:2202.08906}, 2022.

\end{thebibliography}
