
\begin{table}[h!]
    \centering
    \setlength{\tabcolsep}{12pt} %
    \renewcommand{\arraystretch}{1} %
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{lcccccccc}
        & \multicolumn{6}{c}{Accuracy} & \multicolumn{2}{c}{CIDEr}  \\
        \cmidrule(lr){2-7} \cmidrule(lr){8-9}
        & AVG & VQAv2 & TextVQA & OKVQA & GQA & VizWiz & COCO & TextCaps \\
        \shline
        Late-fusion   & 46.8  & 69.4  & 25.8  & 50.1  & \textbf{65.8}  & 22.8  & 70.7  & 50.9 \\
        Early-fusion  & 47.6  & 69.3  & 28.1  & \textbf{52.1}  & 65.4  & 23.2  & \textbf{72.0}  & 53.8 \\
        Early-MoEs    & \textbf{48.2}  & \textbf{69.8}  & \textbf{30.0}  & \textbf{52.1}  & 65.4  & \textbf{23.6}  & 69.6  & \textbf{55.7} \\
    \end{tabular}%
    }
    \caption{\textbf{Supervised finetuning on the LLaVA mixture.} All models are native at 1.5B scale and pre-trained on 300B tokens.}
    \vspace{-7pt}
    \label{tab:sft}
\end{table}
