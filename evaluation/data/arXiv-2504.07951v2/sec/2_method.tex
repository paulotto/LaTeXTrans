





 











\section{Scaling  native multimodal models}








In this section, we present a scaling laws study of native multimodal models,
examining various architectural choices~\cref{sec:scaling_laws_early}, exploring
different data mixtures~\cref{sec:scaling_data_mix}, analyzing the practical
trade-offs between late and early fusion
NMMs, and comparing the performance of native
pre-training and continual pre-training of NMMs~\cref{sec:native_vs_continual}.  

\cpar{Setup.} We train models ranging from 0.3B to 4B active parameters,
scaling the width while keeping the depth constant. For smaller training token
budgets, we reduce the warm-up phase to 1K steps while maintaining 5K steps for
larger budgets.  
Following~\citet{hagele2024scaling}, models are trained with a constant learning
rate, followed by a cool-down phase using an inverse square root scheduler. The
cool-down phase spans 20\% of the total steps spent at the constant learning
rate.  To estimate the scaling coefficients in \Cref{eq:scaling_laws}, we apply the
L-BFGS algorithm~\citep{lbfgs} and Huber loss~\citep{Huber1992} (with $\delta =
10^{-3}$), performing a grid search over initialization ranges.  







\input{tables/scaling_laws_coeffs}       

\input{figs/latr_vs_early_equal_flops}




\vspace{-0.5cm}
\subsection{Scaling laws of NMMs}
\label{sec:scaling_laws_early}

\cpar{Scaling laws for early-fusion and late-fusion models.}  
\Cref{fig:early_vs_late_scaleflops_3d}~(left) presents the final loss averaged
across interleaved, image-caption, and text datasets for early-fusion NMMs. The
lowest-loss frontier follows a power law as a function of FLOPs. Fitting the
power law yields the expression $L \propto C^{-0.049}$, indicating the rate of
improvement with increasing compute. When analyzing the scaling laws per data
type (\eg, image-caption, interleaved, text), we observe that the exponent
varies (\cref{tab:early_vs_late_coeffs}). For instance, the model achieves a
higher rate of improvement for image-caption data $(L \propto C^{-0.061})$ when
compared to interleaved documents $(L \propto C^{-0.046}$).  

To model the loss as a function of the number of training tokens $D$ and model
parameters $N$, we fit the parametric function in \cref{eq:scaling_laws}, obtaining
scaling exponents $\alpha = 0.301$ and $\beta = 0.335$. These describe the rates
of improvement when scaling the number of model parameters and training tokens, respectively.
Assuming a linear relationship between compute, $N$, and $D$ (\ie, $C \propto ND$),
we derive the law relating model parameters to the compute budget (see
\cref{app:scaling_laws} for details). Specifically, for a given compute budget
$C$, we compute the corresponding model size $N$ at logarithmically spaced $D$
values and determine $N_{opt}$, the parameter count that minimizes loss.
Repeating this across different FLOPs values produces a dataset of $(C,
N_{opt})$, to which we fit a power law predicting the compute-optimal model size
as a function of compute: $N^* \propto C^{0.526}.$

Similarly, we fit power laws to estimate the compute-optimal training dataset
size as a function of compute and model size:  
\[
D_{opt} \propto C^{0.473}, \quad D_{opt} \propto N^{0.899}.
\]  
These relationships allow practitioners to determine the optimal model and
dataset size given a fixed compute budget. When analyzing by data type, we find
that interleaved data benefits more from larger models ($a=0.532$) compared to
image-caption data ($a=0.520$), whereas the opposite trend holds for training
tokens.  





\input{figs/data_mixtures_scaling}

We conduct a similar study on late-fusion models
in~\cref{fig:early_vs_late_scaleflops_3d}~(right) and observe comparable scaling
behaviors. In particular, the loss scaling exponent ($c = -0.0494$) is nearly
identical to that of early fusion ($c = -0.0492$).  
This trend is evident in \cref{fig:early_vs_late_scaleflops}, where early fusion
outperforms late fusion at smaller model scales, while both architectures
converge to similar performance at larger model sizes. We also observe similar
trends when varying late-fusion configurations, such as using a smaller vision
encoder with a larger text decoder~\cref{app:late_vs_early}.


\begin{figure}[t!]
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/late_vs_early_efficiency}
    \end{minipage}        
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/training_mixtures}
    \end{minipage}
\end{figure}



\cpar{Scaling laws of NMMs \textit{vs} LLMs.}
Upon comparing the scaling law coefficients of our NMMs to those reported for
text-only LLMs (\eg, GPT-3, Chinchilla), we find them to be within similar
ranges. In particular, for predicting the loss as a function of compute,
GPT-3~\citep{brown2020language} follows $L \propto C^{-0.048}$, while our models
follow $L \propto C^{-0.049}$, suggesting that the performance of NMMs adheres
to similar scaling laws as LLMs.  
Similarly, our estimates of the $\alpha$ and $\beta$ parameters in
\cref{eq:scaling_laws} ($\alpha=0.301$, $\beta=0.335$) closely match those
reported by~\citet{hoffmann2022training} ($\alpha=0.339$, $\beta=0.285$).
Likewise, our computed values of $a=0.526$ and $b=0.473$ align closely with
$a=0.46$ and $b=0.54$ from~\citet{hoffmann2022training}, reinforcing the idea
that, for native multimodal models, the number of training tokens and model
parameters should be scaled proportionally.  
However, since the gap between $a$ and $b$ is smaller than in LLMs, this
principle holds even more strongly for NMMs. Additionally, as $a=0.526$ is
greater than $b=0.473$ in our case, the optimal model size for NMMs is larger
than that of LLMs, while the optimal number of training tokens is lower, given
a fixed compute budget. 



\cpar{Compute-optimal trade-offs for early \textit{vs.} late fusion NMMs.}  
While late- and early-fusion models reduce loss at similar rates with increasing
FLOPs, we observe distinct trade-offs in their compute-optimal models.
Specifically, $N_{opt}$ is larger for late-fusion models, whereas $D_{opt}$ is
larger for early-fusion models. This indicates that, given a fixed compute
budget, late-fusion models require a higher number of parameters, while early-fusion
models benefit more from a higher number of training tokens.  
This trend is also reflected in the lower $\frac{N_{opt}}{D_{opt}} \propto
C^{0.053}$ for early fusion compared to $\frac{N_{opt}}{D_{opt}} \propto
C^{0.076}$ for late fusion. As shown in \cref{fig:teaser}~(right), when scaling FLOPs,
the number of parameters of early fusion models becomes significantly lower, which is crucial
for reducing inference costs and, consequently, lowering serving costs after
deployment.  


\input{tables/scaling_laws_coeffs_datamixture}

\cpar{Early-fusion is more efficient to train.}
We compare the training efficiency of late- and early-fusion architectures. As shown in \cref{fig:early_vs_late_efficiency}, early-fusion models consume less memory and train faster under the same compute budget. This advantage becomes even more pronounced as compute increases, highlighting the superior training efficiency of early fusion while maintaining comparable performance to late fusion at scale. Notably, for the same FLOPs, late-fusion models have a higher parameter count and higher effective depth (\ie, additional vision encoder layers alongside decoder layers) compared to early-fusion models.




\begin{figure*}[h!]
    \centering
    \captionsetup{type=figure}
    
    \begin{minipage}[t]{0.55\linewidth}
        \centering
        \input{graphs/early_late/pred_loss_vs_loss_early_extrapolation}
        \caption{\textbf{Observed vs predicted loss.} We visualize the loss predicted by our scaling laws \cref{eq:scaling_laws} and the actual loss achieved by each run. We can reliably predict the performance of models larger (8B params) than those used to fit the scaling laws.}
        \label{fig:observed_vs_predicted_loss_extrapolation}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\linewidth}
        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-5.5cm}
            \setlength{\tabcolsep}{8pt} %
            \renewcommand{\arraystretch}{1} %
            \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{lccc}
                Parameter & MSE & R2 & MAE (\%) \\
                \shline
                Held-in      & 0.0029  & 0.9807 & 0.8608 \\
                Held-out     & 0.0004 & 0.9682 & 0.5530 \\
            \end{tabular}%
            }
            \captionof{table}{\textbf{Scaling laws prediction errors.} We report the mean square error, R2 and mean absolute error for the loss prediction for held-in and held-out (8B model) data.}
            \label{tab:scaling_laws_errors_main}
        \end{minipage}
        
        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-2.8cm}
            \setlength{\tabcolsep}{12pt} %
            \renewcommand{\arraystretch}{1} %
            \resizebox{\linewidth}{!}{
            \begin{tabular}{lcc}
                Parameter & Avg & Std \\
                \shline
                $E$      & 1.80922  & 0.33811  \\
                $\alpha$ & 0.29842  & 0.10101  \\
                $\beta$  & 0.33209  & 0.02892  \\
                $a$      & 0.54302  & 0.08813  \\
                $b$      & 0.48301  & 0.05787  \\
                $d$      & 0.92375  & 0.23296  \\
            \end{tabular}%
            }
            \captionof{table}{\textbf{Scaling laws sensitivity.} We report the mean and standard deviation after bootstrapping with 100 iterations.}
            \label{tab:scaling_laws_sensitivity_main}
        \end{minipage}
    \end{minipage}
\end{figure*}


    

\subsection{\edit{Scaling laws evaluation}}
\label{sec:scaling_laws_evaluation}
For each model size and number of training tokens, we compute the loss using the
estimated functional form in \cref{eq:scaling_laws} and compare it to the actual
loss observed in our runs. \Cref{fig:observed_vs_predicted_loss_extrapolation}
and \Cref{tab:scaling_laws_errors_main} visualizes these comparisons, showing
that our estimation is highly accurate, particularly for lower loss values and
larger FLOPs. We also assess our scaling laws in an extrapolation setting,
predicting performance beyond the model sizes used for fitting. Notably, our
approach estimates the performance of an 8B model with reasonable accuracy.  

Additionally, we conduct a sensitivity analysis using bootstrapping.
Specifically, we sample \( P \) points with replacement (\( P \) being the total
number of trained models) and re-estimate the scaling law coefficients. This
process is repeated 100 times, and we report the mean and standard deviation of
each coefficient. \Cref{tab:scaling_laws_sensitivity_main} shows that our
estimation is more precise for \(\beta\) than for \(\alpha\), primarily due to
the smaller number of model sizes relative to the number of different token
counts used to derive the scaling laws.





\subsection{Scaling laws for different data mixtures}
\label{sec:scaling_data_mix}
We investigate how variations in the training mixture affect the scaling laws of
native multimodal models. To this end, we study four different mixtures that
reflect common community
practices~\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila},
with Image Caption-Interleaved-Text ratios of \colorbox{blue!10}{45-45-10} (our default setup),
\colorbox{red!10}{30-30-40}, \colorbox{green!10}{40-20-40}, and \colorbox{orange!10}{20-40-40}.  
For each mixture, we conduct a separate scaling study by training 76 different
models, following our setup in \cref{sec:scaling_laws_early}. Overall,
\cref{fig:early_scaleflops_data_mixtures} shows that different mixtures follow
similar scaling trends; however, the scaling coefficients vary depending on the
mixture (\cref{tab:scaling_laws_coeffs_data_mixtures}). Interestingly,
increasing the proportion of image-caption data (mixtures 1 and 2) leads to
lower $a$ and higher $b$, whereas increasing the ratio of interleaved and text
data (mixtures 3 and 4) have the opposite effect.  
Notably, image-caption data contains more image tokens than text
tokens; therefore, increasing its proportion results in more
image tokens, while increasing interleaved and text data increases text token
counts. This suggests that, when image tokens are prevalent, training for longer decreases the loss faster than increasing the model size. 
We also found that for a fixed model size, increasing text-only and interleaved data ratio is in favor of
early-fusion \cref{fig:early_vs_late_datatype_interleaved_text_main}.


























\subsection{Native multimodal pre-training \textbf{\vs} continual training of
LLMs}
\label{sec:native_vs_continual}
In this section, we compare training natively from scratch to continual training
after initializing from a pre-trained LLM. We initialize the model from DCLM-1B~\citep{fang2023data} that is trained on more than 2T tokens.
\Cref{fig:early_vs_early_init_scaledata} shows that native multimodal models can
close the gap with initialized models when trained for longer.
Specifically, on image captioning data, the model requires fewer than 100B
multimodal tokens to reach comparable performance. However, on interleaved and
text data, the model may need longer trainingâ€”up to 1T tokens.
Considering the cost of pre-training, these results suggest that training
natively could be a more efficient approach for achieving the same performance on multimodal benchmarks.

\input{figs/early_vs_early_init_scaledata}




\section{\edit{Towards multimodal specialization}}
Previously, we demonstrated that early-fusion models achieve performance on par
with late-fusion models under a fixed compute budget. However, multimodal data
is inherently heterogeneous, and training a unified model to fit such diverse
distributions may be suboptimal.  
Here, we argue for multimodal specialization within a unified architecture.
Ideally, the model should implicitly adapt to each modality, for instance, by
learning modality-specific weights or specialized experts. MoEs is
a strong candidate for this approach, having demonstrated effectiveness in LLMs.  
In this section, we highlight the advantages of sparse early-fusion models over
their dense counterparts.



\cpar{Setup.} Our sparse models are based on the dropless-MoE implementation
of~\citet{gale2023megablocks}, which eliminates token dropping during training
caused by expert capacity constraints. We employ a top-$k$ expert-choice routing
mechanism, where each token selects its top-$k$ experts among the $E$ available
experts. Specifically, we set $k=1$ and $E=8$, as we find this configuration to
work effectively.  
Additionally, we incorporate an auxiliary load-balancing
loss~\citep{shazeer2017outrageously} with a weight of 0.01 to ensure a balanced
expert utilization. Following~\citet{abnar2025parameters}, we compute training
FLOPs as $6ND$, where $N$ represents the number of active parameters.  



\subsection{Sparse vs dense NMMs when scaling FLOPs}  
We compare sparse MoE models to their dense counterparts by training models with different numbers of active parameters and varying amounts of training tokens. \cref{fig:dense_vs_moe_scaledata} shows that, under the same inference cost (or number of active parameters), MoEs significantly outperform dense models.  
Interestingly, this performance gap is more pronounced for smaller model sizes. This suggests that MoEs enable models to handle heterogeneous data more effectively and specialize in different modalities. However, as dense models become sufficiently large, the gap between the two architectures gradually closes.


\vspace{15pt}
\subsection{Scaling laws for sparse early-fusion models}
We train different models (ranging from 300M to 3.4B active parameters) on
varying amounts of tokens (ranging from 250M to 600B) and report the final loss
in \cref{fig:early_scaleflops_moe_avg}. We fit a power law to the convex hull of
the lowest loss as a function of compute (FLOPs). Interestingly, the exponent
($-0.047$) is close to that of dense NMMs ($-0.049$), indicating that both
architectures scale similarly. However, the multiplicative constant is smaller
for MoEs ($26.287$) compared to dense models ($29.574$), revealing lower loss.
Additionally, MoEs require longer training to reach saturation compared to dense
models (\cref{app:scaling_laws} for more details). \edit{We also predict the
coefficients of \cref{eq:scaling_laws} by \edit{considering $N$ as the number of
active parameters. \Cref{tab:early_vs_late_coeffs} shows significantly higher
$\alpha$ compared to dense models. Interestingly, $b$ is significantly higher
than $a$, revealing that the training tokens should be scaled at a higher rate
than the number of parameters when training sparse NMMs. We also experiment with a
scaling law that takes into account the sparsity~\citep{abnar2025parameters} and
reached similar conclusions \Cref{app:scaling_laws_moes}.}}


\subsection{Modality-aware \vs Modality-agnostic routing}

Another alternative to MoEs is modality-aware routing, where multimodal tokens are assigned to experts based on their modalities, similar
to previous works~\citep{bao2021vlmo,wang2022image}. We train models with
distinct image and text experts in the form of FFNs, where image tokens are
processed only by the image FFN and text tokens only by the text FFN. Compared to modality-aware routing, MoEs exhibit significantly better performance on both image-caption and interleaved data as presented in~\cref{fig:hard_vs_moe_scaledata}.


\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/dense_vs_moe_scaledata}            
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/scaling_laws_moes}
    \end{minipage}
\end{figure}


\subsection{Emergence of expert specialization and sharing}
\label{sec:specialization}
We investigate multimodal specialization in MoE architectures. In~\cref{fig:tokens_assignment}, we visualize the normalized number of text and image tokens assigned to each expert across layers.  To quantify this specialization, we compute a specialization score, defined as the average, across all experts within a layer, of $1-H(p)$, where $H$ is the binary entropy of each expert's text/image token distribution. We plot this specialization score in~\cref{fig:tokens_specialization}.  Higher specialization scores indicate a tendency for experts to focus on either text or image tokens, while lower scores indicate a shared behavior.  These visualizations provide clear evidence of modality-specific experts, particularly in the early layers. Furthermore, the specialization score decreases as the number of layers increases, before rising again in the last layers. This suggests that early and final layers exhibit higher modality specialization compared to mid-layers. This behavior is intuitive, as middle layers are expected to hold higher-level features that may generalize across modalities, \edit{and consistent with findings in \citep{shukor2024implicit} that shows increasing alignment between modalities across layers}. The emergence of both expert specialization and cross-modality sharing in our modality-agnostic MoE, suggests it may be a preferable approach compared to modality-aware sparsity. All data displayed here is from an early-fusion MoE model with 1B active parameters trained for 300B tokens. 





\input{tables/sft_results}

\vspace{-1cm}
\section{Evaluation on downstream tasks with SFT} 
Following previous work on scaling laws, we primarily rely on validation losses. However, we generally find that this evaluation correlates well
with performance on downstream tasks. To validate this, we conduct a multimodal
instruction tuning stage (SFT) on the LLaVA mixture \citep{liu2024improvedllava} and report
accuracy and CIDEr scores across several VQA and captioning tasks.
\cref{tab:sft} confirms the ranking of different model configurations.
Specifically, early fusion outperforms late fusion, and MoEs outperform dense
models. However, since the models are relatively small (1.5B scale), trained
from scratch, and fine-tuned on a small dataset, the overall scores
are lower than the current state of the art.  Further implementation
details can be found in~\Cref{app:implementation_details}.  


\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/soft_vs_hard_routing}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/moe_entropy}
    \end{minipage}
    \vspace{3mm}
\end{figure}

