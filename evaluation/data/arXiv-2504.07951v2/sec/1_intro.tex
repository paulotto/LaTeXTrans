
\section{Introduction}
\label{sec:intro}



Multimodality provides a rich signal for perceiving and understanding the world.
Advances in vision  
\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2},
\edit{audio \citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}} 
and language models \citep{achiam2023gpt4,team2023gemini,dubey2024llama3}  
have enabled the development of powerful multimodal models that understand
language, images, and audio. A common approach involves grafting separately
pre-trained unimodal models, \edit{such as connecting a vision encoder to the input
layer of an
LLM~\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,
xue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}.}


Although this seems like a convenient approach, it remains an open question
whether such late-fusion strategies are inherently optimal \edit{for
understanding multimodal signals}.  Moreover, with abundant multimodal data
available, initializing from unimodal pre-training is potentially detrimental,
as it may introduce biases that prevent the model from \edit{fully leveraging
cross-modality co-dependancies}.
An additional challenge is scaling such systems;  each component (e.g., vision
encoder, LLM) has its own set of hyperparameters, \edit{pre-training data
mixtues}, and \edit{scaling properties with respect to the amount} of data and
compute applied. A more flexible architecture might allow the model to
dynamically allocate its capacity across modalities, simplifying scaling
efforts.


In this work, we focus on the scaling properties of native multimodal models
trained from the ground up on multimodal data. We first investigate whether
\edit{the commonly adopted} late-fusion architectures hold an intrinsic
advantage by comparing them to early-fusion models, which process raw multimodal
inputs without relying on \edit{dedicated vision encoders}.  
We conduct scaling experiments on early and late fusion architectures, deriving
scaling laws to predict their performance and compute-optimal configurations.
Our findings indicate that late fusion offers no inherent advantage when
\edit{trained} from scratch. Instead, early-fusion models are more efficient and
are easier to scale. Furthermore, we observe that native multimodal models
follow scaling laws similar to those of LLMs~\citep{hoffmann2022training},
albeit with slight variations in scaling coefficients across modalities and
datasets. Our results suggest that model parameters and training tokens should
be scaled \edit{roughly equally} for optimal performance.
Moreover, we find that different \edit{multimodal} training mixtures exhibit
similar overall trends, indicating that our findings are likely to generalize to
a broader range of settings.


While our findings favor early fusion, multimodal data is inherently
heterogeneous, suggesting that some degree of parameter specialization may still
offer benefits. To \edit{investigate} this, we \edit{explore leveraging} Mixture
of Experts (MoEs)~\citep{shazeer2017outrageously}, a technique that enables the
model to dynamically allocate specialized parameters across modalities in a
symmetric and parallel manner, in contrast to late-fusion models, which are
asymmetric and process data sequentially. Training native multimodal models with
MoEs results in significantly improved performance and \edit{therefore,} faster
convergence. \edit{Our scaling laws for MoEs suggest that scaling number of
training tokens is more important the number of active parameters. This
unbalanced scaling is different from what is observed for dense models, due to
the higher number of total parameters for sparse models.} \edit{In addition,
}our analysis reveals that experts tend to specialize in different modalities,
with this specialization being particularly prominent in the early and last
layers. 




\input{figs/teaser}

\subsection{Summary of our findings}
Our findings can be summarized as follows:

\cpar{Native early and late fusion perform on par:} \edit{Early fusion models trained
from scratch}
perform on par with their late-fusion counterparts, with a
slight advantage to early-fusion models for low compute budgets
(\cref{fig:early_vs_early_init_scaledata}). Furthermore, our scaling laws study
indicates that the compute-optimal models for early and late fusion perform
similarly as the compute budget increases~(\cref{fig:teaser} Left).

\cpar{NMMs scale similarly to LLMs:} The scaling laws of native multimodal
models follow similar laws as text-only LLMs with slightly varying  scaling
exponents depending on the target data type and training mixture
(\cref{tab:early_vs_late_coeffs}).

\cpar{Late-fusion requires more parameters:}
Compute-optimal late-fusion models require a higher parameters-to-data ratio
when compared to early-fusion (\cref{fig:teaser} Right).

\cpar{Sparsity significantly benefits early-fusion NMMs:} Sparse NMMs exhibit
significant improvements compared to their dense counterparts at the same
inference cost~(\cref{fig:dense_vs_moe_scaledata}). Furthermore, they implicitly
learn modality-specific weights when trained with
sparsity~(\cref{fig:app_moes_specialization}). \edit{In addition,
compute-optimal models rely more on scaling the number of training tokens than
the number of active parameters as the compute-budget grows (\cref{fig:teaser} Right).}

\cpar{Modality-agnostic routing beats Modality-aware routing for Sparse NMMs:}
Training sparse mixture of experts with modality-agnostic routing consistently
outperforms models with modality-aware routing
(\cref{fig:hard_vs_moe_scaledata}).


\vspace{-5pt}




