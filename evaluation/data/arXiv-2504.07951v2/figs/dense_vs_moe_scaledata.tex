    \centering
    \captionsetup{type=figure}
    \begin{subfigure}[t]{0.49\linewidth}
        \input{graphs/dense_moe/dense_vs_moe_scaledata_getty}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \input{graphs/dense_moe/dense_vs_moe_scaledata_obelics}
    \end{subfigure}
    \vspace{-15pt}
    \begin{center}
        \ref{sharedlegend}
    \end{center}
    \caption{\textbf{MoE vs Dense: scaling training FLOPs.} We compare MoE and
    dense early-fusion models when scaling both the amount of training tokens
    and model sizes. MoEs beat dense models when matching the
    number of active parameters.}
    \label{fig:dense_vs_moe_scaledata}
