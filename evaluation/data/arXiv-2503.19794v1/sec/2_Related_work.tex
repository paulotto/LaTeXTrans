\section{Related Works}
\label{sec:related_works}

\noindent \textbf{Video large language models}.
Recent advances in instruction tuning with visual and text data~\cite{liu2023visualinstructiontuning, liu2023improvedllava, liu2024llavanext} have led to a surge of interest in developing Video LLMs. Many of these models share a common design, where visual features are extracted using a pre-trained visual encoder, projected into the text latent space of an LLM, and subsequently processed by the pre-trained LLM to generate responses. 
%
Video-ChatGPT~\cite{maaz2024videochatgptdetailedvideounderstanding} introduces instruction tuning into the video domain. Video-LLaVA~\cite{lin2023video} improves model performance with better text-aligned video features \cite{zhu2023languagebind}, while VideoChat2~\cite{li2023mvbench} resorts to increasing the quality and quantity of the video instruction tuning set. Recent vision-LLM models like LLaVA-NeXT~\cite{liu2024llavanext}, LLaVA-OneVision~\cite{li2024llava}, LLaVA-Video~\cite{zhang2024videoinstructiontuningsynthetic}, Qwen2-VL~\cite{Qwen2VL}, and mPlug-Owl3~\cite{ye2024mplugowl3longimagesequenceunderstanding} consider multi-stage training with both video and image, which substantially improves the model performance.
Recent works in VideoLLM focus on long video understanding. ~\cite{wang2024videollamblongcontextvideounderstanding, faure2024hermestemporalcoherentlongformunderstanding, weng2024longvlmefficientlongvideo, korbar2024textconditionedresamplerlongform, zhang2024longcontexttransferlanguage, shen2024longvu} propose to use Q-former~\cite{li2023blip2bootstrappinglanguageimagepretraining} or text-query-based cross-attention to compress vision tokens, while others~\cite{nguyen2024encodingcontrollingglobalsemantics, wang2024longllavascalingmultimodalllms} resort to state-space models~\cite{gu2024mambalineartimesequencemodeling}.
Researchers also extend the instructional tuning into different video sub-domains. For instance, CAT~\cite{ye2024catenhancingmultimodallarge} focuses on audio-visual understanding, while Scene-LLM~\cite{fu2024scenellmextendinglanguagemodel} and LLaVA-3D~\cite{zhu2024llava3dsimpleeffectivepathway} address 3D QA tasks.

Built on these developments, our work specifically focuses on adapting pre-trained Video LLMs to downstream tasks with side-channel signals, aiming to significantly extend the capabilities of these models. 

%LLaVA proposes visual instruction tuning which extended instruction tuning to computer vision and create the first vision-LLM. Subsequent research focus on create a better visual representation by increasing the input resolution \cite{liu2023improvedllava, liu2024llavanext} and developing higher-quality large-scale instruction tuning data \cite{li2024llava}. 

%While all these models need large scale pre-training with video instruction tuning set or task-special training set. Our works focus on adapting the existing Video LLM trained in the general video understanding to different video down-stream tasks efficiently.

% Yin: I have to re-write the following sub-section. 
\medskip
\noindent \textbf{Adaptation of vision foundation models}. Adapting vision foundation models to downstream tasks has received significant attention. Prior works have studied learning lightweight adapters~\cite{rebuffi2017learning,lian2022scaling,bhattacharjee2023vision,chen2022adaptformer,sung2022vl,chen2023vision}, prepending learnable input tokens (\eg prompts)~\cite{zhou2022learning,jia2022visual}, or in-context learning~\cite{wang2023images,xu2024towards,bar2022visual,zhang2023makes}. Recently, adapter- and prompt-based methods have been explored for Video LLMs. Adapt2Reward~\cite{yang2024adapt2reward} re-purposes a video-language model for robotic operation by using it as a language-conditioned reward function. Similarly, SeViLA~\cite{yu2024self} adapts an image-language model for video tasks by introducing Localizer and Answerer modules, derived from BLIP-2~\cite{li2023blip2bootstrappinglanguageimagepretraining}, to enable video event localization and question answering.

Our work is inspired by the success of adapter-based methods such as LoRA~\cite{hu2021loralowrankadaptationlarge}. These methods learn parameter-efficient modules without changing the base model's architecture and weights, and has been widely used to customize large, pre-trained diffusion models for image generation~\cite{kumari2022multiconcept,xiao2023fastcomposer,gu2023mixofshow,wang2023autostory,shah2023ziplora,zhong2024multi}, extending their capabilities in generating images with different styles or controlling the generated content. Moving beyond diffusion models, our goal is to offer a flexible framework that adapts Video LLMs to a wide range of tasks with patch-like adapters, allowing these models to effectively account for additional side-channel signals and adapt to downstream tasks.


%On the other hand, some techniques, such as LoRA~\cite{hu2021loralowrankadaptationlarge} can be widely used to customize foundation models in different settings. These LoRA adapters are first proposed in adapting LLMs and offer a patch to the base model by adding a small number of parameters and operations, without changing the base model's architecture and weights. Similar ideas have recently been embraced by the vision community. For example, patch-like adapters~\cite{kumari2022multiconcept,xiao2023fastcomposer,gu2023mixofshow,wang2023autostory,shah2023ziplora,zhong2024multi} have been designed to customize large, pre-trained diffusion models for image generation, significantly extending their capabilities in generating images with different styles or controlling the generated content. 

%Motivated by this line of research, instead of focusing on adapting the Video LLM to a specific setting, PAVE aims to offer a framework that adapts Video LLMs to a wide range of tasks with a patch-like adapter, allowing these models to effectively account for additional side-channel signals and adapt to downstream tasks.

\medskip
\noindent \textbf{Multimodal video representation learning}.
Learning a unified representation to connect video and other modalities, such as audio, text and point cloud, has received considerable attention. Prior methods~\cite{luo2022clip4clip} build on the idea of modality alignment using contrastive learning~\cite{radford2021learningtransferablevisualmodels}. More recent works~\cite{chen2023valor, chen2023vast,zhang2023meta,girdhar2023imagebind,zhu2023languagebind} extend the alignment across multiple modalities. Inspired by these works, our method leverages the shared representation between video and text learned by a pre-trained Video LLM, and further embeds side-channel signals into this latent space during adaptation. %Instead of a global video representation, our model aligns key video frames with side-channel signals in time, and matches patch-level vision tokens to those from side-channel signals, 


%Our key technical  aligns the video with the side-channel signals in time, and 


%A key design choice is when to fuse information from video and other modalities. Late fusion has been widely adopted by representation learning methods~\cite{luo2022clip4clip,chen2023valor, chen2023vast,zhang2023meta,girdhar2023imagebind,zhu2023languagebind}, where the video and other modality representations are separately encoded and later aligned in a shared latent space. This is, however, in contrast to many cross-modal video understanding models, which fuse video and other signals at the initial stage of processing~\cite{xu2019multilevel,zhang2019man}. Indeed, a few recent works find early fusion helpful to strengthen cross-modal reasoning~\cite{mo2024unveiling,barnum2020benefits}. 




%and further map supplementary information into this share semantic space during the adaptation. 



%CLAP \citep{laionclap2023} focus on audio and text representation learning, and PointCLIP \citep{zhang2022pointclip} aligns point clouds with text.

%VALOR \citep{chen2023valor} and VAST \citep{chen2023vast}, extending the alignment process with additional modalities can enhance the model's robustness while maintaining its performance. Meta-transformer \citep{zhang2023meta} supports 12 different modalities and employs unique tokenizers to unify the embedding space across these modalities. ImageBind \citep{girdhar2023imagebind} expands multi-modal alignment pretraining to encompass six modalities. LangugeBind~\cite{zhu2023languagebind} builds a multi-modal representation among 5 different modalities by treating all modalities as image.


%Multi-modal representation learning started with pretraining using both vision and language data. CLIP \citep{radford2021learningtransferablevisualmodels} initiates image-text representation learning by training on 400M text-image paired, effectively connecting these two domains and build a shared representation. Additionally, CLIP serves as a foundation for building up the representation in other modalities. For example, CLIP4Clip \citep{luo2022clip4clip} builds the shared representation between video and text, CLAP \citep{laionclap2023} focus on audio and text representation learning, and PointCLIP \citep{zhang2022pointclip} aligns point clouds with text.

%More recent effort aims to building the unified representation across multiple modalities.  Observed in VALOR \citep{chen2023valor} and VAST \citep{chen2023vast}, extending the alignment process with additional modalities can enhance the model's robustness while maintaining its performance. Meta-transformer \citep{zhang2023meta} supports 12 different modalities and employs unique tokenizers to unify the embedding space across these modalities. ImageBind \citep{girdhar2023imagebind} expands multi-modal alignment pretraining to encompass six modalities. LangugeBind~\cite{zhu2023languagebind} builds a multi-modal representation among 5 different modalities by treating all modalities as image.





%to adapt large-scale pre-trained diffusion models for image generation tasks. 

%Foundation models are defined by their ability to adapt to a wide range of downstream tasks. Specifically, in natural language processing (NLP) domain, Large language Models (LLM) \cite{chatgpt, gpt4, llama3_2} trained with large scale dataset has strong zero-shot ability on different Natural Language Processing (NLP) tasks. With instruction tuning \cite{brown2020language, ouyang2022training, wang2022benchmarking, wang2022self} human further adapt and align the pretrained LLM with human interests and improve the model performance on different down-stream task. 

%In computer vision domain, an example of foundation model adaptation is the concept customization of diffusion model. 
%Custom Diffusion~\cite{kumari2022multiconcept} fine-tunes the model on  multiple concept images for customization.
%To expedite customized generation, FastComposer~\cite{xiao2023fastcomposer} fine-tunes a diffusion model on a substantial amount of data, enabling it to take subject embeddings as input and create composed images featuring multiple concepts. Given the extensive use of LoRA for customization, several recent studies~\cite{gu2023mixofshow,wang2023autostory,shah2023ziplora,zhong2024multi} aim to achieve multi-concept customization by merging multiple LoRA weights associated with individual concepts.

%While instruction tuning in NLP and adding LoRA adapter for adapting the diffusion model only take input from same modality as the original model. PAVE aims adapt the Video LLM into a new setting with supplementary information.  

% Video understanding is a long-standing problem and it has many well-defined subdomains. For instance, video question answering\cite{le2020hierarchical} and Video Action Recognition\cite{feichtenhofer2016convolutional} aims to understand the human action appear in the video, temporal action localization\cite{zhang2022actionformer} detect the interested the actions and localize the event along the temporal axis, video grounding\cite{zeng2020dense, mu2024snagscalableaccuratevideo} find the temporal interval given the questions, captioning\cite{wang2018reconstruction} for generating the text description for the video,  video anticipation\cite{furnari2020rolling} to anticipate the actions the action in the following moments.

% To improve the model performance on different video tasks, finding a better video representation is a ever-lasting research topic.
% Two-Stream Network\cite{feichtenhofer2016convolutional} are well explored in the traditional video understanding task, Slow-fast\cite{feichtenhofer2019slowfastnetworksvideorecognition},TSP\cite{alwassel2021tsp}
% I3D\cite{carreira2018quovadisactionrecognition}. They propose different two stream network to complementary with each other. Specifically, the Slow-Fast Network combine the fast path which captures the motion information from temporal densely sampled video frames with the slow path which encodes the static vision semantic information from high spatial resolution frames.

% Our model inspired by the idea of the Slow-Fast Network. Instead of flatten all the information in slow path and fast path and combine them by concatenation. We use Cross-attention to absorb the information from the fast-path to the slow-path. By explicitly considering the spatial-temporal alignment, we are able to extend the length of the fast path to large number instead of restricted by a fixed scaling factor.


