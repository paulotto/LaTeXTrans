\begin{abstract}
%Video large language models (LLMs) have demonstrated remarkable capabilities in understanding the pixels of video frames. 
%Our study shows that they can surprisingly process other information in videos, such as audio and scene depth, yet fall behind the task-specific baselines.
%To this end, we introduce PAVE, a framework to patch and adapt video LLMs with supplementary temporal signals from videos. 
%Specifically, through a cross-attention mechanism, our framework fuses visual tokens with side-channel signals (\eg, audio, scene depth, dense frames), enabling effecting adaption to various downstream tasks without altering existing pre-trained weights.
%Extensive experiment results demonstrate that with under \textbf{1}\% additional parameters and FLOPs, PAVE consistently outperforms task-specific models across diverse tasks, \ie, surpassing the latest audio-visual LLM by \textbf{28} points, beating the state-of-the-art 3D LLM by \textbf{2-3}\%, and improving the base video LLM by \textbf{1-4}\% with high frame rate.
%Further, our analysis showcases that our framework can well generalize to various video LLMs.

Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos.
PAVE introduces lightweight adapters, referred to as ``patches,'' which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of $\sim$0.1\% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at \url{https://github.com/dragonlzm/PAVE}.


%We demonstrate that PAVE effectively enhances pre-trained Video LLMs across diverse tasks including audio-visual understanding, 3D reasoning, and multi-view video recognition, surpassing state-of-the-art task-specific models with the cost of adding $<$1\% additional FLOPs and parameters. Further, we show that when applied to high frame rate videos, PAVE further enhancing the performance of strong base models. Finally, our experiments show that our framework generalizes well across different Video-LLMs. 
%Our code is available at \url{https://github.com/dragonlzm/PAVE}.


% The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
% Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
% The abstract is to be in 10-point, single-spaced type.
% Leave two blank lines after the Abstract, then begin the main text.
% Look at previous \confName abstracts to get a feel for style and length.
\end{abstract}