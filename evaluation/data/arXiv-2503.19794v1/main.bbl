\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alamri et~al.(2019)Alamri, Cartillier, Das, Wang, Cherian, Essa, Batra, Marks, Hori, Anderson, et~al.]{alamri2019audiovisualsceneawaredialog}
Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim~K Marks, Chiori Hori, Peter Anderson, et~al.
\newblock Audio visual scene-aware dialog.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7558--7567, 2019.

\bibitem[Azuma et~al.(2022)Azuma, Miyanishi, Kurita, and Kawanabe]{azuma_2022_CVPR}
Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe.
\newblock Scan{QA}: {3D} question answering for spatial scene understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19129--19139, 2022.

\bibitem[Bar et~al.(2022)Bar, Gandelsman, Darrell, Globerson, and Efros]{bar2022visual}
Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros.
\newblock Visual prompting via image inpainting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25005--25017, 2022.

\bibitem[Bhattacharjee et~al.(2023)Bhattacharjee, S{\"u}sstrunk, and Salzmann]{bhattacharjee2023vision}
Deblina Bhattacharjee, Sabine S{\"u}sstrunk, and Mathieu Salzmann.
\newblock Vision transformer adapters for generalizable multitask learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 19015--19026, 2023.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Chen et~al.(2022)Chen, Ge, Tong, Wang, Song, Wang, and Luo]{chen2022adaptformer}
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
\newblock Adapt{F}ormer: Adapting vision transformers for scalable visual recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16664--16678, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Li, Wang, Zhao, Sun, Zhu, and Liu]{chen2023vast}
Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu.
\newblock Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 72842--72866, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Duan, Wang, He, Lu, Dai, and Qiao]{chen2023vision}
Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao.
\newblock Vision transformer adapter for dense predictions.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, 2023.

\bibitem[Dai et~al.(2017)Dai, Chang, Savva, Halber, Funkhouser, and Nie{\ss}ner]{dai2017scannet}
Angela Dai, Angel~X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie{\ss}ner.
\newblock {ScanNet}: Richly-annotated 3d reconstructions of indoor scenes.
\newblock In \emph{Proc. Computer Vision and Pattern Recognition (CVPR), IEEE}, 2017.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021imageworth16x16words}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Faure et~al.(2024)Faure, Yeh, Chen, Su, Hsu, and Lai]{faure2024hermestemporalcoherentlongformunderstanding}
Gueter~Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Winston~H. Hsu, and Shang-Hong Lai.
\newblock {HERMES}: temporal-coherent long-form understanding with episodes and semantics.
\newblock \emph{arXiv preprint arXiv:2408.17443}, 2024.

\bibitem[Feichtenhofer et~al.(2019)Feichtenhofer, Fan, Malik, and He]{feichtenhofer2019slowfastnetworksvideorecognition}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 6202--6211, 2019.

\bibitem[Fu et~al.(2024{\natexlab{a}})Fu, Dai, Luo, Li, Ren, Zhang, Wang, Zhou, Shen, Zhang, et~al.]{fu2024video}
Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et~al.
\newblock {Video-MME}: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.
\newblock \emph{arXiv preprint arXiv:2405.21075}, 2024{\natexlab{a}}.

\bibitem[Fu et~al.(2024{\natexlab{b}})Fu, Liu, Chen, Nie, and Xiong]{fu2024scenellmextendinglanguagemodel}
Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong.
\newblock {Scene-LLM}: Extending language model for 3d visual understanding and reasoning.
\newblock \emph{arXiv preprint arXiv:2403.11401}, 2024{\natexlab{b}}.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and Misra]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev Alwala, Armand Joulin, and Ishan Misra.
\newblock {Imagebind}: One embedding space to bind them all.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15180--15190, 2023.

\bibitem[Grauman et~al.(2024)Grauman, Westbury, Torresani, Kitani, Malik, Afouras, Ashutosh, Baiyya, Bansal, Boote, et~al.]{grauman2024ego}
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et~al.
\newblock {Ego-Exo4D}: Understanding skilled human activity from first-and third-person perspectives.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19383--19400, 2024.

\bibitem[Gu and Dao(2023)]{gu2024mambalineartimesequencemodeling}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2023)Gu, Wang, Wu, Shi, Chen, Fan, Xiao, Zhao, Chang, Wu, et~al.]{gu2023mixofshow}
Yuchao Gu, Xintao Wang, Jay~Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et~al.
\newblock Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 15890--15902, 2023.

\bibitem[Hong et~al.(2023)Hong, Zhen, Chen, Zheng, Du, Chen, and Gan]{hong20233d}
Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.
\newblock 3d-llm: Injecting the 3d world into large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 20482--20494, 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021loralowrankadaptationlarge}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Huang et~al.(2023)Huang, Yong, Ma, Linghu, Li, Wang, Li, Zhu, Jia, and Huang]{huang2023embodied}
Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang.
\newblock An embodied generalist agent in 3d world.
\newblock \emph{arXiv preprint arXiv:2311.12871}, 2023.

\bibitem[Jaegle et~al.(2022)Jaegle, Borgeaud, Alayrac, Doersch, Ionescu, Ding, Koppula, Zoran, Brock, Shelhamer, Henaff, Botvinick, Zisserman, Vinyals, and Carreira]{jaegle2022perceiveriogeneralarchitecture}
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier~J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.
\newblock Perceiver {IO}: A general architecture for structured inputs \& outputs.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Jia et~al.(2022)Jia, Tang, Chen, Cardie, Belongie, Hariharan, and Lim]{jia2022visual}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In \emph{European conference on computer vision}, pages 709--727. Springer, 2022.

\bibitem[Korbar et~al.(2024)Korbar, Xian, Tonioni, Zisserman, and Tombari]{korbar2024textconditionedresamplerlongform}
Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, and Federico Tombari.
\newblock Text-conditioned resampler for long form video understanding.
\newblock In \emph{European Conference on Computer Vision}, pages 271--288. Springer, 2024.

\bibitem[Kumari et~al.(2023)Kumari, Zhang, Zhang, Shechtman, and Zhu]{kumari2022multiconcept}
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.
\newblock Multi-concept customization of text-to-image diffusion.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 1931--1941, 2023.

\bibitem[Le et~al.(2019)Le, Sahoo, Chen, and Hoi]{Le_2019}
Hung Le, Doyen Sahoo, Nancy Chen, and Steven Hoi.
\newblock Multimodal transformer networks for end-to-end video-grounded dialogue systems.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2019.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Zhang, Guo, Zhang, Li, Zhang, Zhang, Li, Liu, and Li]{li2024llava}
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.
\newblock Llava-onevision: Easy visual task transfer.
\newblock \emph{arXiv preprint arXiv:2408.03326}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Zhang, Zhang, Zhang, Li, Li, Ma, and Li]{li2024llavainterleave}
Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li.
\newblock Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models.
\newblock \emph{arXiv preprint arXiv:2407.07895}, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Wei, Tian, Xu, Wen, and Hu]{Li2022Learning}
Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu.
\newblock Learning to answer questions in dynamic audio-visual scenarios.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Hou, and Hu]{li2023progressive}
Guangyao Li, Wenxuan Hou, and Di Hu.
\newblock Progressive spatio-temporal perception for audio-visual question answering.
\newblock In \emph{Proceedings of the 31st ACM International Conference on Multimedia}, pages 7808--7816, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi]{li2023blip2bootstrappinglanguageimagepretraining}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP}-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 19730--19742. PMLR, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Wang, He, Li, Wang, Liu, Wang, Xu, Chen, Luo, et~al.]{li2023mvbench}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et~al.
\newblock {MVBench}: A comprehensive multi-modal video understanding benchmark.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22195--22206, 2024{\natexlab{c}}.

\bibitem[Li et~al.(2024{\natexlab{d}})Li, Wang, He, Li, Wang, Liu, Wang, Xu, Chen, Luo, et~al.]{li2024mvbenchcomprehensivemultimodalvideo}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et~al.
\newblock Mvbench: A comprehensive multi-modal video understanding benchmark.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22195--22206, 2024{\natexlab{d}}.

\bibitem[Lian et~al.(2022)Lian, Zhou, Feng, and Wang]{lian2022scaling}
Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
\newblock Scaling \& shifting your features: A new baseline for efficient model tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 109--123, 2022.

\bibitem[Lin et~al.(2024)Lin, Ye, Zhu, Cui, Ning, Jin, and Yuan]{lin2023video}
Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan.
\newblock Video-{LL}a{VA}: Learning united visual representation by alignment before projection.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 5971--5984, Miami, Florida, USA, 2024. Association for Computational Linguistics.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{lin2015microsoftcococommonobjects}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26296--26306, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Li, Li, Li, Zhang, Shen, and Lee]{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock {LLaVA-NeXT}: Improved reasoning, ocr, and world knowledge, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Li, Wu, and Lee]{liu2023visualinstructiontuning}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{d}})Liu, Chen, He, Guo, Zhu, Wang, and Tang]{chen2023valor}
Jing Liu, Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, and Jinhui Tang.
\newblock Valor: Vision-audio-language omni-perception pretraining model and dataset.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024{\natexlab{d}}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupledweightdecayregularization}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2022)Luo, Ji, Zhong, Chen, Lei, Duan, and Li]{luo2022clip4clip}
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.
\newblock {CLIP4Clip}: An empirical study of clip for end to end video clip retrieval and captioning.
\newblock \emph{Neurocomputing}, 508:\penalty0 293--304, 2022.

\bibitem[Ma et~al.(2023)Ma, Yong, Zheng, Li, Liang, Zhu, and Huang]{ma2022sqa3d}
Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.
\newblock {SQA3D}: Situated question answering in 3d scenes.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Maaz et~al.(2024)Maaz, Rasheed, Khan, and Khan]{maaz2024videochatgptdetailedvideounderstanding}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock {Video-ChatGPT}: Towards detailed video understanding via large vision and language models.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)}, 2024.

\bibitem[Meta(2024)]{llama3_2}
Meta.
\newblock {LLaMA}3.2.
\newblock \url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}, 2024.

\bibitem[Nguyen et~al.(2024)Nguyen, Hu, Wu, Nguyen, Ng, and Luu]{nguyen2024encodingcontrollingglobalsemantics}
Thong~Thanh Nguyen, Zhiyuan Hu, Xiaobao Wu, Cong-Duy~T Nguyen, See-Kiong Ng, and Anh~Tuan Luu.
\newblock Encoding and controlling global semantics for long-form video question answering.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 7049--7066, Miami, Florida, USA, 2024. Association for Computational Linguistics.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Pham et~al.(2022)Pham, Le, Le, Phuong, and Tran]{pham2022videodialogconversationobjects}
Hoang-Anh Pham, Thao~Minh Le, Vuong Le, Tu~Minh Phuong, and Truyen Tran.
\newblock Video dialog as conversation about objects living in space-time.
\newblock In \emph{European Conference on Computer Vision}, pages 710--726. Springer, 2022.

\bibitem[Polyak et~al.(2024)Polyak, Zohar, Brown, Tjandra, Sinha, Lee, Vyas, Shi, Ma, Chuang, Yan, Choudhary, Wang, Sethi, Pang, Ma, Misra, Hou, Wang, Jagadeesh, Li, Zhang, Singh, Williamson, Le, Yu, Singh, Zhang, Vajda, Duval, Girdhar, Sumbaly, Rambhatla, Tsai, Azadi, Datta, Chen, Bell, Ramaswamy, Sheynin, Bhattacharya, Motwani, Xu, Li, Hou, Hsu, Yin, Dai, Taigman, Luo, Liu, Wu, Zhao, Kirstain, He, He, Pumarola, Thabet, Sanakoyeu, Mallya, Guo, Araya, Kerr, Wood, Liu, Peng, Vengertsev, Schonfeld, Blanchard, Juefei-Xu, Nord, Liang, Hoffman, Kohler, Fire, Sivakumar, Chen, Yu, Gao, Georgopoulos, Moritz, Sampson, Li, Parmeggiani, Fine, Fowler, Petrovic, and Du]{polyak2024moviegencastmedia}
Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh~Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai~Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas
  Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara~K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du.
\newblock {Movie Gen}: A cast of media foundation models.
\newblock \emph{arXiv preprint arXiv:2410.13720}, 2024.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learningtransferablevisualmodels}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022highresolutionimagesynthesislatent}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Shah et~al.(2024)Shah, Ruiz, Cole, Lu, Lazebnik, Li, and Jampani]{shah2023ziplora}
Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.
\newblock {ZipLoRA}: Any subject in any style by effectively merging {LoRAs}.
\newblock In \emph{European Conference on Computer Vision}, pages 422--438. Springer, 2024.

\bibitem[Shang et~al.(2024)Shang, Cai, Xu, Lee, and Yan]{shang2024llavaprumergeadaptivetokenreduction}
Yuzhang Shang, Mu Cai, Bingxin Xu, Yong~Jae Lee, and Yan Yan.
\newblock {LLaVA-PruMerge}: Adaptive token reduction for efficient large multimodal models.
\newblock \emph{arXiv preprint arXiv:2403.15388}, 2024.

\bibitem[Shen et~al.(2024)Shen, Xiong, Zhao, Wu, Chen, Zhu, Liu, Xiao, Varadarajan, Bordes, et~al.]{shen2024longvu}
Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et~al.
\newblock {LongVU}: Spatiotemporal adaptive compression for long video-language understanding.
\newblock \emph{arXiv preprint arXiv:2410.17434}, 2024.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{sung2022vl}
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.
\newblock {VL}-adapter: Parameter-efficient transfer learning for vision-and-language tasks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 5227--5237, 2022.

\bibitem[Vaswani(2017)]{vaswani2017attention}
A Vaswani.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, Fan, Dang, Du, Ren, Men, Liu, Zhou, Zhou, and Lin]{Qwen2VL}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin.
\newblock {Qwen2-VL}: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Zhao, Chen, Chen, Zheng, and Shen]{wang2023autostory}
Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen.
\newblock Autostory: Generating diverse storytelling images with minimal human efforts.
\newblock \emph{International Journal of Computer Vision}, pages 1--22, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Wang, Cao, Shen, and Huang]{wang2023images}
Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang.
\newblock Images speak in images: A generalist painter for in-context visual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6830--6839, 2023.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Song, Chen, Zhang, and Wang]{wang2024longllavascalingmultimodalllms}
Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang.
\newblock {LongLLaVA}: Scaling multi-modal llms to 1000 images efficiently via a hybrid architecture.
\newblock \emph{arXiv preprint arXiv:2409.02889}, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Xie, Liu, and Zheng]{wang2024videollamblongcontextvideounderstanding}
Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng.
\newblock {VideoLLaMB}: Long-context video understanding with recurrent memory bridges.
\newblock \emph{arXiv preprint arXiv:2409.01071}, 2024{\natexlab{d}}.

\bibitem[Weng et~al.(2024)Weng, Han, He, Chang, and Zhuang]{weng2024longvlmefficientlongvideo}
Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang.
\newblock Longvlm: Efficient long video understanding via large language models.
\newblock In \emph{European Conference on Computer Vision}, pages 453--470. Springer, 2024.

\bibitem[Xiao et~al.(2020)Xiao, Lee, Grauman, Malik, and Feichtenhofer]{xiao2020audiovisual}
Fanyi Xiao, Yong~Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Audiovisual slowfast networks for video recognition.
\newblock \emph{arXiv preprint arXiv:2001.08740}, 2020.

\bibitem[Xiao et~al.(2024)Xiao, Yin, Freeman, Durand, and Han]{xiao2023fastcomposer}
Guangxuan Xiao, Tianwei Yin, William~T Freeman, Fr{\'e}do Durand, and Song Han.
\newblock Fastcomposer: Tuning-free multi-subject image generation with localized attention.
\newblock \emph{International Journal of Computer Vision}, pages 1--20, 2024.

\bibitem[Xu et~al.(2024)Xu, Liu, Wang, Yao, and Fu]{xu2024towards}
Chengming Xu, Chen Liu, Yikai Wang, Yuan Yao, and Yanwei Fu.
\newblock Towards global optimal visual in-context learning prompt selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 74945--74965, 2024.

\bibitem[Yang et~al.(2022)Yang, Wang, Duan, Chen, Hou, Jin, and Zhu]{yang2022avqa}
Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu.
\newblock {AVQA}: A dataset for audio-visual question answering on videos.
\newblock In \emph{Proceedings of the 30th ACM international conference on multimedia}, pages 3480--3491, 2022.

\bibitem[Yang et~al.(2024)Yang, Chen, Qiu, Wu, Wang, Lin, Guan, and He]{yang2024adapt2reward}
Yanting Yang, Minghao Chen, Qibo Qiu, Jiahao Wu, Wenxiao Wang, Binbin Lin, Ziyu Guan, and Xiaofei He.
\newblock Adapt2reward: Adapting video-language models to generalizable robotic rewards via failure prompts.
\newblock In \emph{European Conference on Computer Vision}, pages 163--180. Springer, 2024.

\bibitem[Ye et~al.(2024{\natexlab{a}})Ye, Xu, Liu, Hu, Yan, Qian, Zhang, Huang, and Zhou]{ye2024mplugowl3longimagesequenceunderstanding}
Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
\newblock {mPLUG-Owl3}: Towards long image-sequence understanding in multi-modal large language models.
\newblock \emph{arXiv preprint arXiv:2408.04840}, 2024{\natexlab{a}}.

\bibitem[Ye et~al.(2024{\natexlab{b}})Ye, Yu, Shao, Xie, Torr, and Cao]{ye2024catenhancingmultimodallarge}
Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao.
\newblock Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios.
\newblock In \emph{European Conference on Computer Vision}, pages 146--164. Springer, 2024{\natexlab{b}}.

\bibitem[Yu et~al.(2024)Yu, Cho, Yadav, and Bansal]{yu2024self}
Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.
\newblock Self-chained image-language model for video localization and question answering.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yuan et~al.(2024)Yuan, Shang, Zhou, Dong, Zhou, Xue, Wu, Li, Gu, Lee, et~al.]{yuan2024llm}
Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong~Jae Lee, et~al.
\newblock {LLM} inference unveiled: Survey and roofline model insights.
\newblock \emph{arXiv preprint arXiv:2402.16363}, 2024.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{zhai2023sigmoidlosslanguageimage}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 11975--11986, 2023.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zhang, Li, Zeng, Yang, Zhang, Wang, Tan, Li, and Liu]{zhang2024longcontexttransferlanguage}
Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu.
\newblock Long context transfer from language to vision.
\newblock \emph{arXiv preprint arXiv:2406.16852}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Gong, Zhang, Li, Qiao, Ouyang, and Yue]{zhang2023meta}
Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue.
\newblock Meta-transformer: A unified framework for multimodal learning.
\newblock \emph{arXiv preprint arXiv:2307.10802}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhou, and Liu]{zhang2023makes}
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.
\newblock What makes good examples for visual in-context learning?
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 17773--17794, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wu, Li, Li, Ma, Liu, and Li]{zhang2024videoinstructiontuningsynthetic}
Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.
\newblock Video instruction tuning with synthetic data.
\newblock \emph{arXiv preprint arXiv:2410.02713}, 2024{\natexlab{b}}.

\bibitem[Zhong et~al.(2024)Zhong, Shen, Wang, Lu, Jiao, Ouyang, Yu, Han, and Chen]{zhong2024multi}
Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.
\newblock Multi-{LoRA} composition for image generation.
\newblock \emph{arXiv preprint arXiv:2402.16843}, 2024.

\bibitem[Zhou et~al.(2025)Zhou, Shu, Zhao, Wu, Liang, Xiao, Qin, Yang, Xiong, Zhang, Huang, and Liu]{zhou2025mlvubenchmarkingmultitasklong}
Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu.
\newblock {MLVU}: Benchmarking multi-task long video understanding.
\newblock \emph{arXiv preprint arXiv:2406.04264}, 2025.

\bibitem[Zhou et~al.(2022)Zhou, Yang, Loy, and Liu]{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0 (9):\penalty0 2337--2348, 2022.

\bibitem[Zhu et~al.(2024{\natexlab{a}})Zhu, Lin, Ning, Yan, Cui, HongFa, Pang, Jiang, Zhang, Li, Zhang, Li, Liu, and Yuan]{zhu2023languagebind}
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai~Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan.
\newblock Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.

\bibitem[Zhu et~al.(2024{\natexlab{b}})Zhu, Wang, Zhang, Pang, and Liu]{zhu2024llava3dsimpleeffectivepathway}
Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu.
\newblock {LLaVA-3D}: A simple yet effective pathway to empowering lmms with 3d-awareness.
\newblock \emph{arXiv preprint arXiv:2409.18125}, 2024{\natexlab{b}}.

\end{thebibliography}
