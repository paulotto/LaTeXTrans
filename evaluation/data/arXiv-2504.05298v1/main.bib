@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = ICCV,
pages = {234--778},
year = 2005
}

@article{meta2024moviegen,
  title={Movie Gen: A Cast of Media Foundation Models}, 
  author={The Movie Gen team},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{nvidia2025cosmos,
  title={Cosmos World Foundation Model Platform for Physical AI}, 
  author={NVIDIA},
  year={2025},
  journal={arXiv preprint arXiv:2501.03575},
}

@inproceedings{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{hong2023cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  booktitle={ICLR},
  year={2023}
}

@article{kong2025hunyuanvideo,
      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, 
      author={Weijie Kong and Qi Tian and Zijian Zhang and Rox Min and Zuozhuo Dai and Jin Zhou and Jiangfeng Xiong and Xin Li and Bo Wu and Jianwei Zhang and Kathrina Wu and Qin Lin and Junkun Yuan and Yanxin Long and Aladdin Wang and Andong Wang and Changlin Li and Duojun Huang and Fang Yang and Hao Tan and Hongmei Wang and Jacob Song and Jiawang Bai and Jianbing Wu and Jinbao Xue and Joey Wang and Kai Wang and Mengyang Liu and Pengyu Li and Shuai Li and Weiyan Wang and Wenqing Yu and Xinchi Deng and Yang Li and Yi Chen and Yutao Cui and Yuanbo Peng and Zhentao Yu and Zhiyu He and Zhiyong Xu and Zixiang Zhou and Zunnan Xu and Yangyu Tao and Qinglin Lu and Songtao Liu and Dax Zhou and Hongfa Wang and Yong Yang and Di Wang and Yuhong Liu and Jie Jiang and Caesar Zhong},
      journal={arXiv preprint arXiv 2412.03603},
      year={2025},
}

@inproceedings{zhou2024storydiffusion,
  title={StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation},
  author={Zhou, Yupeng and Zhou, Daquan and Cheng, Ming-Ming and Feng, Jiashi and Hou, Qibin},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{li2019storygan,
  title={Storygan: A sequential conditional gan for story visualization},
  author={Li, Yitong and Gan, Zhe and Shen, Yelong and Liu, Jingjing and Cheng, Yu and Wu, Yuexin and Carin, Lawrence and Carlson, David and Gao, Jianfeng},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{pan2024arldm,
  title={Synthesizing coherent story with auto-regressive latent diffusion models},
  author={Pan, Xichen and Qin, Pengda and Li, Yuhong and Xue, Hui and Chen, Wenhu},
  booktitle={WACV},
  year={2024}
}

@inproceedings{zhuang2024vlogger,
  title={Vlogger: Make your dream a vlog},
  author={Zhuang, Shaobin and Li, Kunchang and Chen, Xinyuan and Wang, Yaohui and Liu, Ziwei and Qiao, Yu and Wang, Yali},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{katharopoulos2020lineartransformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={ICML},
  year={2020},
}

@inproceedings{gu2024mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  booktitle={COLM},
  year={2024}
}

@inproceedings{dao2024mamba2,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={ICML},
  year={2024}
}

@article{sun2024ttt,
  title={Learning to (learn at test time): Rnns with expressive hidden states},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}

@inproceedings{yang2024parallelizing,
  title={Parallelizing linear transformers with the delta rule over sequence length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{schlag2021deltanet,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={ICML},
  year={2021},
}

@inproceedings{xie2024sana,
  title={Sana: Efficient high-resolution image synthesis with linear diffusion transformers},
  author={Xie, Enze and Chen, Junsong and Chen, Junyu and Cai, Han and Tang, Haotian and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Zhu, Ligeng and Lu, Yao and others},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{wang2021realesrgan,
  title={Real-esrgan: Training real-world blind super-resolution with pure synthetic data},
  author={Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  booktitle={ICCVW},
  year={2021}
}

@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}

@inproceedings{vincent2008dae,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={ICML},
  year={2008}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={UAI},
  year={2021},
}

@inproceedings{chen2024seine,
  title={Seine: Short-to-long video diffusion model for generative transition and prediction},
  author={Chen, Xinyuan and Wang, Yaohui and Zhang, Lingjun and Zhuang, Shaobin and Ma, Xin and Yu, Jiashuo and Wang, Yali and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{huang2024vbench,
  title={Vbench: Comprehensive benchmark suite for video generative models},
  author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{kondratyuk2024videopoet,
  title={Videopoet: A large language model for zero-shot video generation},
  author={Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others},
  booktitle={ICML},
  year={2024}
}

@article{ho2022imagenvideo,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{blattmann2023svd,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  year={2022}
}

@article{goodfellow2020gan,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  year={2020},
}

@article{ho2020ddpm,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={NeurIPS},
  year={2020}
}

@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv preprint arXiv:2104.10157},
  year={2021}
}

@inproceedings{tulyakov2018mocogan,
  title={Mocogan: Decomposing motion and content for video generation},
  author={Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{gupta2024walt,
  title={Photorealistic video generation with diffusion models},
  author={Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and Li, Fei-Fei and Essa, Irfan and Jiang, Lu and Lezama, Jos{\'e}},
  booktitle={ECCV},
  year={2024},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@inproceedings{cai2023efficientvit,
  title={Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction},
  author={Cai, Han and Li, Junyan and Hu, Muyan and Gan, Chuang and Han, Song},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhu2024vim,
    author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
    title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
    booktitle={ICML},
    year={2024}
}

@inproceedings{gupta2018flintstones,
  title={Imagine this! scripts to compositions to videos},
  author={Gupta, Tanmay and Schwenk, Dustin and Farhadi, Ali and Hoiem, Derek and Kembhavi, Aniruddha},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{liu2024storysalon,
  title={Intelligent grimm-open-ended visual storytelling via latent diffusion models},
  author={Liu, Chang and Wu, Haoning and Zhong, Yujie and Zhang, Xiaoyun and Wang, Yanfeng and Xie, Weidi},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{huang2016visual,
  title={Visual storytelling},
  author={Huang, Ting-Hao and Ferraro, Francis and Mostafazadeh, Nasrin and Misra, Ishan and Agrawal, Aishwarya and Devlin, Jacob and Girshick, Ross and He, Xiaodong and Kohli, Pushmeet and Batra, Dhruv and others},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{pan2024synthesizing,
  title={Synthesizing coherent story with auto-regressive latent diffusion models},
  author={Pan, Xichen and Qin, Pengda and Li, Yuhong and Xue, Hui and Chen, Wenhu},
  booktitle={WACV},
  year={2024}
}

@inproceedings{rahman2023makeastory,
  title={Make-a-story: Visual memory conditioned consistent story generation},
  author={Rahman, Tanzila and Lee, Hsin-Ying and Ren, Jian and Tulyakov, Sergey and Mahajan, Shweta and Sigal, Leonid},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{maharana2022storydalle,
  title={Storydall-e: Adapting pretrained text-to-image transformers for story continuation},
  author={Maharana, Adyasha and Hannan, Darryl and Bansal, Mohit},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2022}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{zhao2023fsdp,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{salimans2022progressive,
  title={Progressive distillation for fast sampling of diffusion models},
  author={Salimans, Tim and Ho, Jonathan},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{lin2024zerosnr,
  title={Common diffusion noise schedules and sample steps are flawed},
  author={Lin, Shanchuan and Liu, Bingchen and Li, Jiashi and Yang, Xiao},
  booktitle={WACV},
  year={2024}
}

@inproceedings{song2021ddim,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={ICLR},
  year={2021}
}

@article{ho2022cfg,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@misc{wang2024lingenhighresolutionminutelengthtexttovideo,
      title={LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity}, 
      author={Hongjie Wang and Chih-Yao Ma and Yen-Cheng Liu and Ji Hou and Tao Xu and Jialiang Wang and Felix Juefei-Xu and Yaqiao Luo and Peizhao Zhang and Tingbo Hou and Peter Vajda and Niraj K. Jha and Xiaoliang Dai},
      year={2024},
      eprint={2412.09856},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.09856}, 
}

@inproceedings{villegas2023phenaki,
  title={Phenaki: Variable length video generation from open domain textual description},
  author={Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{skorokhodov2022styleganv,
  title={Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2},
  author={Skorokhodov, Ivan and Tulyakov, Sergey and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{karras2020stylegan2,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chang2022maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={CVPR},
  year={2022}
}

@article{yin2023nuwa,
  title={Nuwa-xl: Diffusion over diffusion for extremely long video generation},
  author={Yin, Shengming and Wu, Chenfei and Yang, Huan and Wang, Jianfeng and Wang, Xiaodong and Ni, Minheng and Yang, Zhengyuan and Li, Linjie and Liu, Shuguang and Yang, Fan and others},
  journal={arXiv preprint arXiv:2303.12346},
  year={2023}
}

@inproceedings{spector2025thunderkittens,
  title={ThunderKittens: Simple, Fast, and Adorable AI Kernels},
  author={Spector, Benjamin F and Arora, Simran and Singhal, Aaryan and Fu, Daniel Y and R{\'e}, Christopher},
  booktitle={ICLR},
  year={2025}
}

@article{ren2025vamba,
  title={VAMBA: Understanding Hour-Long Videos with Hybrid Mamba-Transformers},
  author={Ren, Weiming and Ma, Wentao and Yang, Huan and Wei, Cong and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2503.11579},
  year={2025}
}

@article{schmidhuberlinearattn,
    author = {Schmidhuber, Jürgen},
    title = {Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks},
    journal = {Neural Computation},
    volume = {4},
    number = {1},
    pages = {131-139},
    year = {1992},
    month = {01},
    abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.1.131},
    url = {https://doi.org/10.1162/neco.1992.4.1.131},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/1/131/812242/neco.1992.4.1.131.pdf},
}

@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={ICASSP},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@inproceedings{denoisingautoencoder,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and Composing Robust Features with Denoising Autoencoders},
year = {2008},
booktitle = {ICML}
}

@article{chen2020improved,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={CVPR},
  year={2023}
}


@article{mo2024scalingdiffusionmambabidirectional,
      title={Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation}, 
      author={Shentong Mo and Yapeng Tian},
      year={2024},
      journal={arXiv preprint arXiv:2405.15881},
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  booktitle={NAACL},
  year={2024}
}

@article{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{chiang2024chatbotarenaopenplatform,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others},
  booktitle={ICML},
  year={2024}
}

@misc{shah2024flashattention3fastaccurateattention,
      title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}, 
      author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
      year={2024},
      eprint={2407.08608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08608}, 
}

@inproceedings{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others},
  booktitle={ICML},
  year={2024}
}

@article{schmidhuber1992learning,
  title={Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={4},
  number={1},
  pages={131--139},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{kirsch2021meta,
  title={Meta learning backpropagation and improving it},
  author={Kirsch, Louis and Schmidhuber, J{\"u}rgen},
  journal={NeurIPS},
  volume={34},
  pages={14122--14134},
  year={2021}
}

@article{irie2021going,
  title={Going beyond linear transformers with recurrent fast weight programmers},
  author={Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  journal={NeurIPS},
  year={2021}
}

@article{clark2022meta,
  title={Meta-Learning Fast Weight Language Models},
  author={Clark, Kevin and Guu, Kelvin and Chang, Ming-Wei and Pasupat, Panupong and Hinton, Geoffrey and Norouzi, Mohammad},
  journal={EMNLP},
  year={2022}
}

@article{sun2023learning,
  title={Learning to (learn at test time)},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Hsu, Chloe and Koyejo, Sanmi and Guestrin, Carlos and Wang, Xiaolong and Hashimoto, Tatsunori and Chen, Xinlei},
  journal={arXiv preprint arXiv:2310.13807},
  year={2023}
}

@article{behrouz2024titans,
  title={Titans: Learning to memorize at test time},
  author={Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2501.00663},
  year={2024}
}

@article{wang2025test,
  title={Test-time regression: a unifying framework for designing sequence models with associative memory},
  author={Wang, Ke Alexander and Shi, Jiaxin and Fox, Emily B},
  journal={arXiv preprint arXiv:2501.12352},
  year={2025}
}

@inproceedings{ge2022tats,
  title={Long video generation with time-agnostic vqgan and time-sensitive transformer},
  author={Ge, Songwei and Hayes, Thomas and Yang, Harry and Yin, Xi and Pang, Guan and Jacobs, David and Huang, Jia-Bin and Parikh, Devi},
  booktitle={ECCV},
  year={2022},
}

@article{he2022lvdm,
  title={Latent video diffusion models for high-fidelity long video generation},
  author={He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  journal={arXiv preprint arXiv:2211.13221},
  year={2022}
}

@article{henschel2024streamingt2v,
  title={Streamingt2v: Consistent, dynamic, and extendable long video generation from text},
  author={Henschel, Roberto and Khachatryan, Levon and Hayrapetyan, Daniil and Poghosyan, Hayk and Tadevosyan, Vahram and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  journal={arXiv preprint arXiv:2403.14773},
  year={2024}
}

@article{brooks2022generating,
  title={Generating long videos of dynamic scenes},
  author={Brooks, Tim and Hellsten, Janne and Aittala, Miika and Wang, Ting-Chun and Aila, Timo and Lehtinen, Jaakko and Liu, Ming-Yu and Efros, Alexei and Karras, Tero},
  journal={NeurIPS},
  year={2022}
}

@article{wang2024lavie,
  title={Lavie: High-quality video generation with cascaded latent diffusion models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={IJCV},
  year={2024},
}

@inproceedings{chen2023seine,
  title={Seine: Short-to-long video diffusion model for generative transition and prediction},
  author={Chen, Xinyuan and Wang, Yaohui and Zhang, Lingjun and Zhuang, Shaobin and Ma, Xin and Yu, Jiashuo and Wang, Yali and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{yang2025gateddeltanetworksimproving,
  title={Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author={Yang, Songlin and Kautz, Jan and Hatamizadeh, Ali},
  booktitle={ICLR},
  year={2025}
}

@misc{LMSys,
  title = {Chatbot Arena LLM Leaderboard: Community-driven Evaluation for Best LLM and AI chatbots},
  howpublished = {\url{https://lmarena.ai/}},
  note = {Accessed: 2025-03-20}
}