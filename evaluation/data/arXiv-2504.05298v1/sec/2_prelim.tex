\section{Test-Time Training Layers}
\label{sec:prelim}

Following standard practice~\cite{meta2024moviegen, yang2024cogvideox}, 
each video is pre-processed into a sequence of $T$ tokens, where $T$ is determined by its duration and resolution.
This section reviews Test-Time Training (TTT) layers for general sequence modeling, using some of the exposition in Section 2 of~\cite{sun2024ttt}.
We first discuss how to process general input sequences in a causal manner (chronological order).
Section~\ref{sec:method} discusses how to use RNN layers in a non-causal backbone by invoking them in opposite directions.

\subsection{TTT as Updating a Hidden State}
\label{subsec:hidden}
All RNN layers compress historical context in a hidden state of fixed size. This compression has two consequences.
On one hand, mapping an input token $x_t$ to output token $z_t$ is efficient, because both the update rule and output rule take constant time per token.
On the other hand, an RNN layer's ability to remember long context is limited by the amount of information its hidden state can store.
The goal of \cite{sun2024ttt} is to design RNN layers with expressive hidden states that can compress massive context.
As an inspiration, they observe that self-supervised learning can compress a massive training set into the weights of a machine learning model.

The key idea in \cite{sun2024ttt} is to use self-supervised learning to compress the historical context $x_1,\dots,x_t$ into a hidden state $W_t$, by making the context an unlabeled dataset and the hidden state the weights of a machine learning model $f$.
The update rule, illustrated in Figure~\ref{fig:ttt-layer}, is a step of gradient descent on some self-supervised loss $\ell$: 
\begin{equation}
\label{eq:update_naive}
W_t = W_{t-1} - \eta\,\nabla\ell(W_{t-1}; x_t),
\end{equation}
with learning rate $\eta$. Intuitively, the output token is just the prediction on $x_t$, made by $f$ with the updated weights $W_t$:
\begin{equation}
\label{eq:output_naive}
z_t = f(x_t; W_t).
\end{equation}

\noindent
One choice of $\ell$ is reconstructing $x_t$ itself. 
To make the learning problem nontrivial, one can first process $x_t$ into a corrupted input $\tilde{x}_t$ (see Subsection~\ref{subsec:task}), then optimize:
\begin{equation}    
\label{eq:recon}
\ell(W; x_t) = \| f(\tilde{x}_t; W) - x_t \|^2.
\end{equation}
Similar to denoising autoencoders~\citep{denoisingautoencoder}, $f$ needs to discover the correlations between dimensions of $x_t$ in order to reconstruct it from partial information $\tilde{x}_t$.

As with other RNN layers and self-attention, this algorithm that maps an input sequence $x_1,\dots,x_T$ to output sequence $z_1,\dots,z_T$ can be programmed into the forward pass of a sequence modeling layer.
Even at test time, the layer still trains a different sequence of weights $W_1, \dots, W_T$ for every input sequence. Therefore, it is called \emph{Test-Time Training (TTT) layer}.

Conceptually, calling backward on $\nabla\ell$ means taking gradients of gradients -- a well-explored technique in meta-learning.
TTT layers have the same interface as RNN layers and self-attention, therefore can be replaced in any larger network architecture. \cite{sun2024ttt} refers to training the larger network as the \emph{outer loop}, and training $W$ within each TTT layer as the \emph{inner loop}.

\subsection{Learning a Self-Supervised Task for TTT}
\label{subsec:task}
Arguably, the most important part of TTT is the self-supervised task specified by $\ell$. Instead of handcrafting a self-supervised task from human priors, \cite{sun2024ttt} takes a more end-to-end approach, learning it as part of the outer loop.
Starting from the naive reconstruction task in Equation~\ref{eq:recon}, they use a low-rank projection $\tilde{x}_t = \theta_Kx_t$, where $\theta_K$ is a matrix that is learnable in the outer loop.

Moreover, perhaps not all the information in $x_t$ is worth remembering, so the reconstruction label can also be a low-rank projection $\theta_Vx_t$ instead of $x_t$.
In summary, the self-supervised loss in \cite{sun2024ttt} is:
\begin{equation}
\label{eq:multi}
\ell(W; x_t) = \| f\left(\theta_K x_t; W\right) - \theta_V x_t \|^2.
\end{equation}
Lastly, since $\theta_Kx_t$ has fewer dimensions than $x_t$, \cite{sun2024ttt} can no longer use the output rule in Equation~\ref{eq:output_naive}.
So they make another projection $\theta_Qx_t$, and change the output rule to:
\begin{equation}
\label{eq:output}
z_t = f\left(\theta_Qx_t; W_t\right).
\end{equation}
Note that in the inner loop, only $W$ is optimized, therefore written as an argument of $\ell$; the $\theta$s are ``hyper-parameters" of this inner-loop loss function.
$\theta_K,\theta_V,\theta_Q$ are optimized in the outer loop, analogous to the Query, Key, and Value parameters of self-attention.

\subsection{TTT-MLP Instantiation}
Following \cite{sun2024ttt}, we instantiate the inner-loop model $f$ as a wrapper around $f_{\,\texttt{MLP}}$: a two-layer MLP similar to those in Transformers.
Specifically, the hidden dimension is $4 \times$ the input dimension, followed by a GELU activation~\cite{hendrycks2016gaussian}.
For better stability during TTT, $f$ always contains a Layer Norm and residual connection. That is,
$$f(x) = x + \texttt{LN}(f_{\,\texttt{MLP}}(x)).$$
A TTT layer with this $f$ is called TTT-MLP, which is the default instantiation throughout this paper.
In Section~\ref{sec:experiment} we also instantiate TTT-Linear (the $f$ above wrapping around a linear model) as a baseline.