\section{Related Work}
\label{sec:related}

\myparagraph{Modern RNN layers}, especially linear attention variants~\cite{schmidhuberlinearattn, katharopoulos2020lineartransformers}, such as Mamba~\cite{gu2024mamba, dao2024mamba2} and DeltaNet~\cite{schlag2021deltanet, yang2024parallelizing}, have demonstrated impressive performance in natural language tasks. 
Inspired by their success and ideas from Fast Weight Programmers~\cite{schmidhuber1992learning, kirsch2021meta, irie2021going, clark2022meta}, \cite{sun2024ttt} proposes scalable and practical ways to make the hidden states large and nonlinear, therefore more expressive.
Recent work~\cite{behrouz2024titans} develops even larger and more nonlinear hidden states, and updates them with more sophisticated optimization techniques.
The related work section in \cite{sun2024ttt} contains a detailed discussion of inspirations for TTT layers.
\cite{wang2025test} gives a good overview of recent developments in RNN layers.

\myparagraph{Long video modeling.}
Some early work~\cite{skorokhodov2022styleganv} generates long videos by training GAN~\cite{goodfellow2020gan,karras2020stylegan2} to predict the next frame based on the current frame and the motion vector.
Generation quality has improved significantly due to recent progress in auto-regression (AR) and diffusion-based approaches~\cite{gupta2024walt,meta2024moviegen,yang2024cogvideox,kong2025hunyuanvideo}.
TATS~\cite{ge2022tats} proposes the sliding window attention on the Transformer to generate videos longer than the training length.
Phenaki~\cite{villegas2023phenaki} works in a similar auto-regressive way, but each frame is generated by MaskGIT~\cite{chang2022maskgit}.
Pre-trained diffusion models can be extended to generate longer videos by using cascade~\cite{he2022lvdm,yin2023nuwa,wang2024lavie}, streaming~\cite{henschel2024streamingt2v}, and adding transitions~\cite{chen2023seine}.

% Recent advancements in diffusion models have enabled effective text-to-video (T2V) generation, as demonstrated by WALT~\cite{gupta2024walt}, MovieGen~\cite{meta2024moviegen}, Cosmos~\cite{nvidia2025cosmos}, CogVideo~\cite{hong2023cogvideo,yang2024cogvideox}, and HunyuanVideo~\cite{kong2025hunyuanvideo}. However, these attention-based diffusion models are computationally costly, typically limiting generation to short clips under 100 frames. In contrast, our proposed method uses TTT layers, a class of RNN layers, to enable coherent one-minute video generation.


\myparagraph{Story synthesis} methods such as \cite{li2019storygan,huang2016visual,pan2024synthesizing,maharana2022storydalle,rahman2023makeastory,liu2024storysalon} generate sequences of images or videos corresponding to individual sentences in a text story. 
For example, Craft~\cite{gupta2018flintstones} generates videos of complex scenes through retrieval, and StoryDiffusion~\cite{zhou2024storydiffusion} uses diffusion to improve the smoothness of transitions between frames. 
While related to text-to-video generation, story synthesis methods usually need additional components in their pipeline to maintain coherence across scenes, which are not processed end-to-end.
